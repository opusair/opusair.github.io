[
  {
    "id": "Twitterf72c2679-4da6-41bf-bc55-4b99d14cc5b8",
    "source": "Twitter",
    "url": "https://x.com/karpathy/status/1926812469810368669",
    "title": "Andrej Karpathy on X: \"@kalomaze Deep Learning horror genre 🫣 That fear of a kwarg that isn’t set right, not erroring, only silently making your results slightly worse.\"",
    "content": "Deep Learning horror genre 🫣 That fear of a kwarg that isn’t set right, not erroring, only silently making your results slightly worse.",
    "summary": "深度学习恐怖类型：担心某个参数没有设置正确，没有出现错误，而只是默默地让结果变得稍差。",
    "keywords": "深度学习,恐怖,参数,错误,结果",
    "area": "人工智能,深度学习,生成式AI",
    "published_time": "2025-05-26T09:27:00Z",
    "download_time": "2023-10-06 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitterf72c2679-4da6-41bf-bc55-4b99d14cc5b8.png"
    ]
  },
  {
    "id": "Twitter83aaa8ef-70a6-4643-8d04-80a47de0807a",
    "source": "Twitter",
    "url": "https://x.com/jxmnop/status/1927385194601886065",
    "title": "> be me, first year PhD student > get obsessed with polyphonic music transcription during pandemic > for some reason, advisor lets me work on music",
    "content": "> be me, first year PhD student > get obsessed with polyphonic music transcription during pandemic > for some reason, advisor lets me work on music > brilliant idea: explicitly encode chord structure into the model > spend months implementing complicated chord-aware audio model in PyTorch > aggressively monopolize all GPUs in the department to train model > finally beat transcription baseline, feeling unstoppable > convince self about to revolutionize the music industry > wake up to new Google paper on music transcription > it's about MT3, a transformer on thousands of hours of audio > zero explicit structure > just a transformer + lots of data > blows my (piano-specific) CNN out of the water > bitter_lesson.png > mogged again by Rich Sutton",
    "summary": "一位第一年博士生在大流行期间迷上了复调音乐转录，导师让他研究音乐。他想法绝妙，尝试在模型中显式编码和弦结构，并长时间用PyTorch实现复杂的和弦感知音频模型，占用了部门所有GPU进行训练，终于打败了转录基线。他自信将要在音乐产业引发变革，却被谷歌的新论文打脸，该论文介绍了一种基于成千上万小时音频训练的MT3 transformer，无需显式结构，仅需大量数据，直接把他专为钢琴设计的CNN比了下去。",
    "keywords": "博士生,复调音乐,和弦结构,PyTorch,MT3 transformer",
    "area": "人工智能,深度学习,音频理解",
    "published_time": "2025-05-27T23:23:00Z",
    "download_time": "2023-10-11 10:42:58",
    "visual_resource": [
      "screenshot/twitter_Twitter83aaa8ef-70a6-4643-8d04-80a47de0807a.png"
    ]
  },
  {
    "id": "Twittera19db12c-0a3c-48fe-a2a1-8245388c73bf",
    "source": "Twitter",
    "url": "https://x.com/scaling01/status/1926801814973804712",
    "title": "Remember the Great Ghiblification?",
    "content": "Remember the Great Ghiblification? Turns out, this is all part of a grander plan by OpenAI to make them appear cool and to win market share. Search, DeepResearch, Agents and Personlization with System Prompts, Tasks and the coming model unification are also part of that plan. Here is more information from court exhibits on OpenAI's strategy: In the previous post I covered the super-assistants OpenAI plans to build in 2025. To achieve that goal and their other goal of competing in search they plan to build out infrastructure to support 1B users. Their current infrastructure can \"only\" support multiple 100 Mio. users. Furthermore, they want to redefine the brand and move away from the OpenAI naming, OpenAI should only be a name in the background like Alphabet. \"We need a simple consumer mental model. Google owns information. Amazon owns commerce. ChatGPT needs to own one clear idea: [Agents/Action/something like that] A core feature of their new strategy is to focus on the younger generation. ChatGPT wants to be \"cool\". \"Right now, it's useful but not cool\". \"The path to being cool is being part of trends on social, full stop. We need to build a communit-lead growth motion\"",
    "summary": "记得大吉化事件吗？事实证明，这是OpenAI为显得酷和赢得市场份额所策划的庞大计划的一部分。搜索、深度研究、智能体和个性化功能与系统提示、任务和即将到来的模型统一也是该计划的一部分。上次我讨论了OpenAI计划于2025年构建的超级助理。为实现这些目标，他们计划扩展支持10亿用户的基础设施，而当前的只能支持数亿用户。此外，他们想重新定义品牌，远离OpenAI这一名字，OpenAI应仅在背景中如同Alphabet。ChatGPT需拥有一个明确的核心理念，并计击重新品牌战略，目标年轻一代。\"",
    "keywords": "OpenAI,大吉化,市场份额,ChatGPT,品牌战略",
    "area": "人工智能,多模态,大模型",
    "published_time": "2025-05-26T12:34:56Z",
    "download_time": "2025-05-26 21:21:00",
    "visual_resource": [
      "screenshot/twitter_Twittera19db12c-0a3c-48fe-a2a1-8245388c73bf.png"
    ]
  },
  {
    "id": "Twitterf824f474-0e6f-4101-9a1e-fe76302c2a4d",
    "source": "Twitter",
    "url": "https://x.com/scaling01/status/1926788548155293978",
    "title": "Lisan al Gaib on X: \"On OpenAI's product strategy - super assistants, their largest competitors and their moats, based on recently revealed court exhibits. In H1 2025 OpenAI will start evolving ChatGPT into super-assistant, as models like o2 and o3 (now o3 and o4) are finally smart enough to perform https://t.co/1oEl11nq3j\" / X",
    "content": "On OpenAI's product strategy - super assistants, their largest competitors and their moats, based on recently revealed court exhibits.\nIn H1 2025 OpenAI will start evolving ChatGPT into super-assistant, as models like o2 and o3 (now o3 and o4) are finally smart enough to perform agentic tasks. A super-assistant is an intelligent entitiy with T-shaped skills. It has broad skills for tedious daily tasks and deep expertise for tasks that most people find impossible.\nOpenAI recognizes that \"growth and revenue won't line up forever\", so they need to focus their efforts first on super-assistants to generate enough monetizable demand, to pursue more expensive models in H2. Interestingly, OpenAI sees Meta as their biggest competitor, since Google is at risk of cannibalizing their own core search business (as of Dec 2024).",
    "summary": "OpenAI计划在2025年上半年将ChatGPT发展为超级助手，因其新模型o2和o3具备执行代理任务的足够智能。超级助手是一种具备广度和深度技能的智能实体，适合日常繁琐任务及复杂问题。OpenAI为应对持续增长和收益不匹配，需专注于超级助手以产生足够的市场需求，并计划在下半年推出更昂贵的模型。Meta被视为最大竞争对手，而谷歌因其搜索业务或遭自我蚕食。",
    "keywords": "OpenAI,超级助手,ChatGPT,Meta,谷歌",
    "area": "人工智能,智能体,机器人",
    "published_time": "2025-05-26T07:52:00Z",
    "download_time": "2023-10-04 14:25:00",
    "visual_resource": [
      "screenshot/twitter_Twitterf824f474-0e6f-4101-9a1e-fe76302c2a4d.png"
    ]
  },
  {
    "id": "Twitterce9d4459-4576-4c59-a9f6-01fdb23c23e1",
    "source": "Twitter",
    "url": "https://x.com/_philschmid/status/1927019039269761064",
    "title": "Last Week was full of I/O announcement. Here is one you might have missed",
    "content": "Last Week was full of I/O announcement. Here is one you might have missed 🚨 Context URL tool is a new native tool that allows Gemini to extract content from provided URLs as additional context for prompts. - Provide URLs directly in prompts, up to 20 per prompt - Can be used in combination with google search tool to search based on website content - Supported with Gemini 2.0 Flash and 2.5 Flash and Pro - Free during its experimental phase, with billing to come later",
    "summary": "上周的公告中，你可能会错过的一个是上下文URL工具。这是一种新的原生工具，允许Gemini从提供的URL中提取内容作为上下文。该工具支持每个提示最多20个URL，并可与谷歌搜索工具结合使用，搜索网站内容。它支持Gemini 2.0 Flash和2.5 Flash及Pro版本，并且在实验阶段免费，以后将收费。",
    "keywords": "上下文URL工具, Gemini, 结合谷歌搜索, 多模态, 大模型",
    "area": "多模态, 大模型, 其他",
    "published_time": "2025-05-26T23:08:00Z",
    "download_time": "2023-10-05 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitterce9d4459-4576-4c59-a9f6-01fdb23c23e1.png"
    ]
  },
  {
    "id": "Twittera173c20c-2523-4aa1-b677-b1b30477de26",
    "source": "Twitter",
    "url": "https://x.com/llama_index/status/1926996451747356976",
    "title": "LlamaIndex now supports the new OpenAI Responses API features",
    "content": "LlamaIndex now supports the new OpenAI Responses API features: · Call any remote MCP server · Use code interpreters by using it as one of the built-in-tools · AND generate images with streaming.",
    "summary": "LlamaIndex现已支持新的OpenAI Responses API功能，包括远程MCP服务器调用、内置工具中的代码解释器使用以及流媒体图像生成。",
    "keywords": "LlamaIndex, OpenAI Responses API, MCP 服务器, 代码解释器, 图像生成",
    "area": "人工智能, 多模态, 生成式AI",
    "published_time": "2025-05-26T21:38:00Z",
    "download_time": "2025-05-14 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twittera173c20c-2523-4aa1-b677-b1b30477de26.png"
    ]
  },
  {
    "id": "Twitterea192279-7c72-4e4d-becf-9576937856b8",
    "source": "Twitter",
    "url": "https://x.com/ctnzr/status/1927391895879074047",
    "title": "Nemotron-CORTEXA just reached the top of the SWEBench leaderboard",
    "content": "Nemotron-CORTEXA just reached the top of the SWEBench leaderboard for using LLMs to solve software engineering problems, solving 68.2% of SWEBench GitHub issues! It does so by using a multi-step problem localization and repair process, generating multiple proposal candidates and then choosing a final solution with an LLM. The embedding model we built for this has been released, and code will be released soon. Paper will be at ICML 2025! More information here:",
    "summary": "Nemotron-CORTEXA成功地利用LLM解决软件工程问题，解决了68.2%的SWEBench GitHub问题。该系统使用多步骤的问题定位和修复过程，生成多个提议候选方案，然后选择LLM的最终解决方案。我们构建的嵌入模型已经发布，代码将很快发布。论文将在2025年ICML上发表。",
    "keywords": "Nemotron-CORTEXA, SWEBench, LLM, 嵌入模型, ICML",
    "area": "人工智能, 机器学习, 大模型",
    "published_time": "2025-05-27T23:50:00Z",
    "download_time": "2025-10-05 14:38:00",
    "visual_resource": [
      "screenshot/twitter_Twitterea192279-7c72-4e4d-becf-9576937856b8.png"
    ]
  },
  {
    "id": "Twitterfb552d5d-782d-49ee-8f71-03be7f4e9bf6",
    "source": "Twitter",
    "url": "https://x.com/danielhanchen/status/1926966742519091327",
    "title": "Hey guys! We noticed some of you sharing screenshots and links to our DeepSeek-V3-0526 article on @UnslothAI. The link was hidden and wasn’t meant to be shared publicly or taken as a fact but it seems a few of you were scrapping through the site and uncovered it early!",
    "content": "Hey guys! We noticed some of you sharing screenshots and links to our DeepSeek-V3-0526 article on @UnslothAI. The link was hidden and wasn’t meant to be shared publicly or taken as a fact but it seems a few of you were scrapping through the site and uncovered it early! 😅 The article was originally written as speculative prep for the rumored release of the model. As of now, there’s been no official confirmation about its existence or launch. So, it was never intended for broad distribution, so sorry for any confusion this may have caused. The text in the article was simply a placeholder, copied over from our earlier V3-0324 piece. So there's definitely nothing to take from it. And yep, lesson learned! We won’t be doing this again. The hype is real, and it turns out we need to be more careful about what we draft on the site, even behind the scenes. Thanks for your understanding!",
    "summary": "我们注意到有些人分享了我们的DeepSeek-V3-0526文章的截图和链接。这篇文章原本是为传闻中的模型发布而编写的，并没有官方确认其存在或发布。我们为由此引发的任何困惑表示歉意，文章的内容仅仅是一个占位符，来自于我们早期的V3-0324版本。因此，并没有什么实际信息可以从中获取。教训：我们以后会更加小心。感谢你的理解！",
    "keywords": "DeepSeek-V3-0526,UnslothAI,文章,占位符,理解",
    "area": "人工智能,机器学习,自然语言处理",
    "published_time": "2025-05-26T19:40:00Z",
    "download_time": "2023-09-29 20:17:35",
    "visual_resource": [
      "screenshot/twitter_Twitterfb552d5d-782d-49ee-8f71-03be7f4e9bf6.png"
    ]
  },
  {
    "id": "Twittere9da3d81-69b9-4312-81ab-a0d4eefbdf50",
    "source": "Twitter",
    "url": "https://x.com/Teknium1/status/1927089897833140647",
    "title": "Finally completed and merged the SWE_RL environment",
    "content": "Finally completed and merged the SWE_RL environment that was described by Meta's SWE RL paper into Atropos - A really difficult environment that can teach a model to be a much better coding agent! Check out the PR: github.com/NousResearch/a Check out Meta's SWE-RL paper: arxiv.org/abs/2502.18449",
    "summary": "该推文宣布完成并合并了根据Meta's SWE RL论文描述的SWE_RL环境到Atropos中，这是一个非常困难的环境，可以教一个模型成为更好的编码代理！查看PR和Meta's SWE-RL论文。",
    "keywords": "SWE_RL,Meta,Atropos,模型,编码代理",
    "area": "深度学习,智能体,视频理解",
    "published_time": "2025-05-27T03:50:00Z",
    "download_time": "2023-10-08 12:00:00",
    "visual_resource": [
      "screenshot/twitter_Twittere9da3d81-69b9-4312-81ab-a0d4eefbdf50.png"
    ]
  },
  {
    "id": "Twitter5845eb56-0834-48d0-9a4d-97d3f21922ed",
    "source": "Twitter",
    "url": "https://x.com/GoogleDeepMind/status/1927375853551235160",
    "title": "SignGemma: Translating Sign Language",
    "content": "We're thrilled to announce SignGemma, our most capable model for translating sign language into spoken text. 🧏 This open model is coming to the Gemma model family later this year, opening up new possibilities for inclusive tech. Share your feedback and interest in early testing →",
    "summary": "我们很高兴推出SignGemma，这是我们最强大的模型，用于将手语翻译成口语文本。这一开放模型将在今年晚些时候加入Gemma模型家族，为包容性技术开辟新的可能性。欢迎分享您的反馈和参与早期测试。",
    "keywords": "SignGemma,手语,翻译,开放模型,包容性技术",
    "area": "人工智能,自然语言处理,多模态",
    "published_time": "2025-05-27T22:46:00Z",
    "download_time": "2023-11-01 10:00:00",
    "visual_resource": [
      "screenshot/twitter_Twitter5845eb56-0834-48d0-9a4d-97d3f21922ed.png"
    ]
  },
  {
    "id": "Twitter3d96d459-24ef-4275-b510-5545c1e68210",
    "source": "Twitter",
    "url": "https://x.com/c_valenzuelab/status/1927149229966766373",
    "title": "Infinite Use Cases and the Path to Universality",
    "content": "This is pretty wild. We wanted to ensure our models have infinite use cases that are less prescriptive and linear than the simplistic \"text-to-X\" approach. Which means that are still plenty of uses cases we have not yet discovered. Gen-4 and References feel like a step toward the universality we have as a vision.",
    "summary": "Cristóbal Valenzuela在推文中强调了模型的无限用例及其潜力，阐述了超越简单“文本到X”方法的必要性。Gen-4和References被视为朝着普遍性愿景迈出的一步。",
    "keywords": "模型,用例,超越,文本,普遍性",
    "area": "多模态,生成式AI,其他",
    "published_time": "2025-05-27T07:45:00Z",
    "download_time": "2023-10-05 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitter3d96d459-24ef-4275-b510-5545c1e68210.png"
    ]
  },
  {
    "id": "Twitter12941d16-e73d-4214-9467-d4d251754481",
    "source": "Twitter",
    "url": "https://x.com/TheTuringPost/status/1927123359969468420",
    "title": "A new recipe for training multimodal models",
    "content": "A new recipe for training multimodal models 👉 Mixed together various data types: text next to images, video frames after captions, then webpages, etc. This way the model learns to connect what it reads with what it sees. ByteDance proposed and implemented this idea in their BAGEL, a new open-source multimodal model. Here's how it works:",
    "summary": "TuringPost提出了一种新的多模态模型训练配方，将文本、图像、视频帧和网页等多种数据类型混合在一起，模型通过这种方式学会连接阅读和视觉内容。ByteDance在开源项目BAGEL中实施了这一理念。",
    "keywords": "TuringPost,多模态模型,ByteDance,BAGEL,开源项目",
    "area": "多模态,机器学习,其他",
    "published_time": "6:03 AM · May 27, 2025",
    "download_time": "2023-11-01 10:16:32",
    "visual_resource": [
      "screenshot/twitter_Twitter12941d16-e73d-4214-9467-d4d251754481.png"
    ]
  },
  {
    "id": "Twitter7d9a856f-5910-4aab-9f92-9067eb82ccc3",
    "source": "Twitter",
    "url": "https://x.com/mervenoyann/status/1926987808360509636",
    "title": "open AI Vision LM & omni releases",
    "content": "what happened in open AI past week? so many vision LM & omni releases 🔥 here's our picks ❤️ multimodal 💬🖼️ > new moondream (VLM) is out: it's 4-bit quantized (with QAT) version of moondream-2b, runs on 2.5GB VRAM at 184 tps with only 0.6% drop in accuracy (OS) 🌚 > ByteDance released BAGEL-7B, an omni model that understands and generates both image + text. they also released Dolphin, a document parsing VLM (OS) > Google DeepMind dropped MedGemma in I/O, VLM that can interpret medical scans, and Gemma 3n, an omni model with competitive LLM performance > MMaDa is a new 8B diffusion language model that can generate image and text > Mistral released Devstral, a 24B coding assistant (OS) > Fairy R1-32B is a new reasoning model -- distilled version of DeepSeek-R1-Distill-Qwen-32B (OS) > NVIDIA released ACEReason-Nemotron-14B, new 14B math and code reasoning model > sarvam-m is a new Indic LM with hybrid thinking mode, based on Mistral Small (OS) image generation > MTVCrafter is a new human motion animation generator",
    "summary": "最近一周，开放AI领域涌现了大量的视觉语言模型和全能模型。 其中包括：新推出的Moondream（VLM）是Moondream-2b的4位量化版本，具有QAT特性；字节跳动发布了BAGEL-7B模型，该全能模型能理解并生成图像和文本；谷歌DeepMind推出了用于解释医学扫描的MedGemma和具有竞争力的Gemma 3n模型；MMaDa是一个新的8B扩散语言模型，能生成图像和文本；Mistral发布了24B编码助手Devstral；以及NVIDIA发布的新数学与代码推理模型ACEReason-Nemotron-14B及基于Mistral Small的混合思维模式Indic LM——sarvam-m。",
    "keywords": "视觉语言模型,全能模型,Moondream,VLM,BAGEL-7B",
    "area": "大模型,计算机视觉,多模态",
    "published_time": "2025-05-26T21:04:00Z",
    "download_time": "2023-10-01 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitter7d9a856f-5910-4aab-9f92-9067eb82ccc3.png"
    ]
  },
  {
    "id": "Twitterfb05453c-09a9-435b-86fd-75caeaa212f9",
    "source": "Twitter",
    "url": "https://x.com/lateinteraction/status/1927445094002487554",
    "title": "Missing nuance in the collective realization today: The non-trivial negative result is not that \"RL just amplifies skills that are already there with low probability\".",
    "content": "Missing nuance in the collective realization today: The non-trivial negative result is not that \"RL just amplifies skills that are already there with low probability\". Duh, that's obvious and not an issue actually. What got questioned today is that \"dumb pretraining teaches the model all these things and then RL surfaces them\". Had it been like that, we'd all be celebrating that. The problem is that it isn't this! The non-trivial negative result is that \"this form of RLVR seems (in certain cases) to ONLY work if your mid-training data mixtures deliberately encode these *specific* math and coding skills\". That is, it's not emergent. It's cultivated.",
    "summary": "今天的集体认识中缺乏一个关键细节：负面的结果不仅仅是“RL 只是放大了那些低概率已经存在的技能”。真正的问题在于，RL 似乎只在中期训练的数据中故意编码具体数学和编程技能时才有效。这意味着，它不是自发的，而是人为培养的。",
    "keywords": "RL,模型,技能,数据,训练",
    "area": "人工智能,机器学习,其他",
    "published_time": "2025-05-28T03:21:00Z",
    "download_time": "2023-11-24 08:26:20",
    "visual_resource": [
      "screenshot/twitter_Twitterfb05453c-09a9-435b-86fd-75caeaa212f9.png"
    ]
  },
  {
    "id": "Twitter3120e2b0-6519-4f0d-8fc3-ffc15e584559",
    "source": "Twitter",
    "url": "https://x.com/SakanaAILabs/status/1926798125060002243",
    "title": "Sudoku-based Reasoning Benchmark & New Leaderboard Launch",
    "content": "Following our Sudoku-based reasoning benchmark announcement, we've been evaluating the latest models to track improvements in their reasoning capabilities. Today, we’re launching the Sudoku-Bench Leaderboard: New technical report: You can now track new model progress on our live Leaderboard. Of the models we’ve benchmarked so far: OpenAI’s o3 Mini High leads overall. Interestingly, Gemini 2.5 Pro does better on the harder 6x6 puzzles! However, o3 is the only model that solves any of the 9x9 Sudokus, but only 2.9% and only the vanilla Sudoku’s. Crucially, NO model tested can yet conquer 9x9s requiring strong, creative reasoning. This benchmark remains a grand challenge! For a deeper dive into the benchmark, methodology, and our findings, check out our technical report. Want to test a model on Sudoku-Bench? It's simple! Visit the leaderboard. Choose a puzzle. We generate a prompt (puzzle + instructions) to paste into any model. Explore sample reasoning traces from our tests too!",
    "summary": "Sakana AI 实验室宣布了基于数独的推理基准，并发布了Sudoku-Bench排行榜。最新报告指出，OpenAI的o3 Mini High总体表现最佳，而Gemini 2.5 Pro在更复杂的6x6拼图中表现更好。然而，目前尚无模型能解决需要强推理能力的9x9数独，这项基准仍是重大挑战。",
    "keywords": "推理基准,数独,排行榜,模型,挑战",
    "area": "人工智能,机器学习,智能体",
    "published_time": "2025-05-26T08:30:00Z",
    "download_time": "2023-10-11 09:15:00",
    "visual_resource": [
      "screenshot/twitter_Twitter3120e2b0-6519-4f0d-8fc3-ffc15e584559.png"
    ]
  },
  {
    "id": "Twitter7b052acf-627d-49df-a155-c3effaff6d3c",
    "source": "Twitter",
    "url": "https://x.com/_lewtun/status/1927043160275923158",
    "title": "Happy to share 💭 Mixture of Thoughts 💭 A curated, general reasoning dataset that trims down over 1M samples from public datasets to ~350k through an extensive set of ablations 🧑‍🍳 Models trained on this mix match or exceed the performance of DeepSeek's distilled models -- not https://t.co/PgX1JedlWS",
    "content": "Happy to share Mixture of Thoughts A curated, general reasoning dataset that trims down over 1M samples from public datasets to ~350k through an extensive set of ablations. Models trained on this mix match or exceed the performance of DeepSeek's distilled models -- not just on math/code but also on scientific benchmarks like GPQA We also validate that the \"additive\" methodology from Phi-4-reasoning really works! You can optimise the data mixture independently per reasoning domain and then bring it all together for the final run Link to the dataset",
    "summary": "本推文介绍了一种名为“思想混合”的经过精心整理的通用推理数据集。通过一系列的推导，将超过100万的公共数据集样本精简至约35万个。训练于此混合数据集的模型在数学、代码及科学基准如GPQA等方面的性能优于或持平于DeepSeek的蒸馏模型。此外，验证了Phi-4-reasoning方法的可加性，这允许用户独立优化每个推理领域的数据混合，然后在最终运行时整合。",
    "keywords": "推理数据集,DeepSeek,GPQA,方法论,性能优越",
    "area": "人工智能,机器学习,深度学习",
    "published_time": "2025-05-27T00:44:00Z",
    "download_time": "2025-11-26 17:26:40",
    "visual_resource": [
      "screenshot/twitter_Twitter7b052acf-627d-49df-a155-c3effaff6d3c.png"
    ]
  },
  {
    "id": "Twitter29001433-1f48-407d-83f7-200ec967a026",
    "source": "Twitter",
    "url": "https://x.com/LangChainAI/status/1927413238733681027",
    "title": "🚀 Ready to deploy your own Open Agent Platform (OAP) instance? In our latest video, we show you how to self-host OAP in production—no managed instance required. OAP is an open-source, no-code platform for building, prototyping, and deploying intelligent agents.",
    "content": "🚀 Ready to deploy your own Open Agent Platform (OAP) instance? In our latest video, we show you how to self-host OAP in production—no managed instance required. OAP is an open-source, no-code platform for building, prototyping, and deploying intelligent agents. 阅读完整文章: youtube.com/watch?v=GQCGwn",
    "summary": "LangChain推出的Open Agent Platform (OAP)是一个无需代码即可搭建代理的平台，最新视频展示如何自托管OAP进行生产部署。OAP支持工具和监督代理的开箱即用设置，同时以直观的Web UI支持域知识的服务器插件接入。该平台面向开发者、产品经理和分析师，依托LangGraph迅速实现快速上手。",
    "keywords": "Open Agent Platform,OAP,LangChain,视频,部署",
    "area": "人工智能,多模态,自然语言处理",
    "published_time": "2025-05-28T01:15:00Z",
    "download_time": "2025-05-28 12:00:00",
    "visual_resource": [
      "screenshot/twitter_Twitter29001433-1f48-407d-83f7-200ec967a026.png"
    ]
  },
  {
    "id": "Twitterf11a80a5-eba7-4015-ba44-d281e5572666",
    "source": "Twitter",
    "url": "https://x.com/omarsar0/status/1927368367075197179",
    "title": "Agents Basics",
    "content": "Agents Basics It's super simple to create an agent by giving it a description, name, instructions, and tools.",
    "summary": "代理的基础内容：创建一个代理非常简单，只需提供描述、名称、说明和工具即可。",
    "keywords": "代理,基础,创建,工具,说明",
    "area": "人工智能,智能体,其他",
    "published_time": "2025-05-27T22:16:00Z",
    "download_time": "2025-10-05 14:21:17",
    "visual_resource": [
      "screenshot/twitter_Twitterf11a80a5-eba7-4015-ba44-d281e5572666.png"
    ]
  },
  {
    "id": "Twitterc1e4d32c-6e14-4f9e-85b8-f476770dee76",
    "source": "Twitter",
    "url": "https://x.com/omarsar0/status/1927366520985800849",
    "title": "Mistral AI announces Agents API",
    "content": "NEW: Mistral AI announces Agents API - code execution - web search - MCP tools - persistent memory - agentic orchestration capabilities Cool to see that Mistral AI has joined the growing number of agent frameworks. More below:",
    "summary": "Mistral AI 推出 Agents API，具备代码执行、网络搜索、MCP 工具、持久内存和代理编排功能，加入了越来越多的代理框架行列。",
    "keywords": "Mistral AI, Agents API, 代码执行, 网络搜索, 代理框架",
    "area": "多模态,生成式AI,其他",
    "published_time": "2025-05-27T22:09:00Z",
    "download_time": "2023-10-01 12:00:00",
    "visual_resource": [
      "screenshot/twitter_Twitterc1e4d32c-6e14-4f9e-85b8-f476770dee76.png"
    ]
  },
  {
    "id": "Twitter64a0eed0-7832-4402-ad8b-9e2d77c280cf",
    "source": "Twitter",
    "url": "https://x.com/omarsar0/status/1927372457578483828",
    "title": "Handoff The handoff feature enables agents to call other agents to complete tasks or hand over a conversation mid-action.",
    "content": "NEW: Mistral AI announces Agents API - code execution - web search - MCP tools - persistent memory - agentic orchestration capabilities Cool to see that Mistral AI has joined the growing number of agent frameworks. More below:",
    "summary": "Mistral AI发布新API，支持代码执行、网页搜索、MCP工具、持久内存、代理的编排能力，该功能拓展了代理框架的功能。",
    "keywords": "Mistral AI, Agents API, 持久内存, 代理框架, 代码执行",
    "area": "人工智能, 生成式AI, 智能体",
    "published_time": "2025-05-14T12:34:56Z",
    "download_time": "2025-10-08 14:28:45",
    "visual_resource": [
      "screenshot/twitter_Twitter64a0eed0-7832-4402-ad8b-9e2d77c280cf.png"
    ]
  },
  {
    "id": "6Ixh9_b8Bawf6XrQLh-vaA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/6Ixh9_b8Bawf6XrQLh-vaA",
    "title": "不懂建模也能做角色！VAST升级AI神器，一手实测来了：一键拆建/魔法笔刷/万物绑骨",
    "summary": "明星初创VAST旗下Tripo Studio迎来重大升级，推出智能部件分割、贴图魔法笔刷、智能低模生成及万物自动绑骨等核心功能，彻底革新传统3D建模流程。该工具直击行业痛点，大幅提升游戏开发、3D打印和动画制作效率，实现模型从“生成”到“应用”的全链路优化。Tripo Studio凭借AI驱动的“流程再造”，让非专业人士也能轻松创作，重新定义了3D从业者的工作台价值，标志着AI在3D领域从技术突破走向成果交付的质变。",
    "keywords": [
      "AI建模",
      "TripoStudio",
      "3D大模型",
      "智能部件分割",
      "贴图魔法笔刷",
      "智能低模",
      "万物绑骨",
      "流程再造"
    ],
    "area": [
      "人工智能",
      "生成式AI",
      "大模型"
    ],
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:00.445619",
    "visual_resource": [
      "screenshot/wechat_wx_f2953a8d.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtA7PAcYOazzjo8cK20odicaxGGDvhbqK2SQYmxQ1vKw8NmElGR06p6OHpu7j8OUenzvLlER8bMFvzg/0?wx_fmt=jpeg\", \"id\": \"6Ixh9_b8Bawf6XrQLh-vaA\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/6Ixh9_b8Bawf6XrQLh-vaA.txt\"}}"
  },
  {
    "id": "lCjfKhFfOdTtC6uEvhJG4w",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/lCjfKhFfOdTtC6uEvhJG4w",
    "title": "AI仅凭“自信”学会推理，浙大校友复刻DeepSeek长思维链涌现，强化学习无需外部奖励信号",
    "summary": "加州大学伯克利分校团队提出突破性AI训练范式INTUITOR，使大语言模型首次能仅凭自身“自信度”学会复杂推理，实现长思维链涌现，彻底摆脱对外部奖励信号或人工标注数据的依赖。该方法通过优化模型内在置信度信号，有效规避了传统强化学习高昂成本及“奖励黑客”等问题。实验证明，INTUITOR在数学推理、代码生成及指令遵循任务上显著提升模型性能，尤其展现出卓越的结构化推理能力及多任务泛化性。此研究不仅为大模型训练带来新突破，更为AI迈向更自主、类人化的学习范式开启了全新可能。",
    "keywords": [
      "大模型",
      "强化学习",
      "内在奖励",
      "置信度",
      "长思维链",
      "推理能力",
      "AI训练"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "大模型"
    ],
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:09.156781",
    "visual_resource": [
      "screenshot/wechat_wx_2d97fb51.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtA7PAcYOazzjo8cK20odicaxXsWrvUbdCNh3ZicXqu5KSVJtTw0FWDcVo6WxPeVbd1hIE4PnBnsA0vQ/0?wx_fmt=jpeg\", \"id\": \"lCjfKhFfOdTtC6uEvhJG4w\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/lCjfKhFfOdTtC6uEvhJG4w.txt\"}}"
  },
  {
    "id": "qC47ZdqdYcbGILMHPae51g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/qC47ZdqdYcbGILMHPae51g",
    "title": "英伟达再创历史纪录！Q1收入增长69%，数据中心贡献89%，游戏业务大涨42%",
    "summary": "英伟达最新财报显示，2026财年Q1营收同比激增69%，数据中心业务贡献89%成核心增长引擎。尽管美国出口限制导致H20芯片销售受阻，并产生45亿美元库存减值，但伴随GB300芯片投产、Blackwell架构引入游戏市场（RTX 50系列与任天堂Switch 2），以及汽车与机器人业务（Isaac GR00T）的强劲增长，英伟达仍在AI基础设施和技术创新领域保持领先，预示着未来业绩潜力巨大。",
    "keywords": [
      "英伟达",
      "财报",
      "数据中心",
      "H20芯片",
      "出口限制",
      "人工智能",
      "GB300",
      "机器人"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器人"
    ],
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:17.681354",
    "visual_resource": [
      "screenshot/wechat_wx_c5bd9cfc.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtA7PAcYOazzjo8cK20odicaxyZ4tPl0WyiaOBnPxQS0SJVWM4fQT2YzaIxp3weobItSTpFeyp2evvIA/0?wx_fmt=jpeg\", \"id\": \"qC47ZdqdYcbGILMHPae51g\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/qC47ZdqdYcbGILMHPae51g.txt\"}}"
  },
  {
    "id": "TRtITbsVftG8zGR1HecljQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/TRtITbsVftG8zGR1HecljQ",
    "title": "SOTA大模型遇上加密数据评测：Qwen3未破10%，o1也栽了丨上海AI Lab等联合研究",
    "summary": "上海AI Lab联合推出的CipherBank评测揭示，当前SOTA大语言模型在密码学解密任务中表现普遍不佳，连最新的Qwen3系列准确率也未破10%。该评测聚焦真实隐私场景数据和多类型加密算法，旨在考验模型的严密逻辑与细节精确度。结果显示，即使是Claude-3.5和o1等领先模型，准确率也未能突破50%，暴露出LLM在处理结构化与符号化推理方面的显著短板。研究指出，模型惧怕长文本、噪音干扰、数字转换，且存在提示依赖症。未来AI发展需摆脱过度语义依赖，增强模式学习与泛化能力，并优化推理执行稳定性，以克服在密码学领域的挑战。",
    "keywords": [
      "大语言模型",
      "密码学",
      "CipherBank",
      "解密",
      "评测",
      "推理能力",
      "SOTA"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:26.338471",
    "visual_resource": [
      "screenshot/wechat_wx_4080906e.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtByts5aLdmL34037Zsk2h6JCcq9MjJ0KtFsxugsZqdJf4HicxMAmDZWxg1DViamicEk5b8CEKRP1Y3RQ/0?wx_fmt=jpeg\", \"id\": \"TRtITbsVftG8zGR1HecljQ\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/TRtITbsVftG8zGR1HecljQ.txt\"}}"
  },
  {
    "id": "VE-3UCGJrHQ3feBga7svzA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/VE-3UCGJrHQ3feBga7svzA",
    "title": "基准测试揭秘大模型“字数危机”：26个模型长文本生成普遍拉胯，最大输出长度过度宣传",
    "summary": "最新研究《LIFEBENCH》深入揭示大语言模型（LLMs）在遵循长度指令，特别是长文本生成方面的普遍不足。通过对26个主流模型进行基准测试，结果显示大多数模型在明确要求生成特定长度文本时表现糟糕，尤其在长文本场景中得分普遍低于40分，远低于其声称的最大输出能力。研究发现，模型存在缺乏准确长度感知、对输入长度敏感及懒惰生成策略等核心瓶颈。文章强调，未来需通过增强预训练数据和引入预规划策略，全面提升大模型对长度指令的遵循能力与实际表现。",
    "keywords": [
      "大语言模型",
      "长度指令",
      "长文本生成",
      "LIFEBENCH",
      "基准测试",
      "模型能力瓶颈",
      "生成式AI",
      "模型评估"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "生成式AI"
    ],
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:37.340212",
    "visual_resource": [
      "screenshot/wechat_wx_80e9060d.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAHkwSrvicgK1yjPVBOuCG2fBicIFjm6vhjHSPrBaqGC6h8efuzTicxTOzFLYgcTa8MFia0vcUHcsU7BQ/0?wx_fmt=jpeg\", \"id\": \"VE-3UCGJrHQ3feBga7svzA\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/VE-3UCGJrHQ3feBga7svzA.txt\"}}"
  },
  {
    "id": "fq3F4OnOaG9PaF4_NKuRyg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/fq3F4OnOaG9PaF4_NKuRyg",
    "title": "成本暴降88%！通义实验室、北大发布ZeroSearch，无需搜索即可激活LLM检索能力",
    "summary": "通义实验室与北京大学联合发布ZeroSearch框架，旨在解决大型语言模型（LLM）强化学习训练中，因频繁调用真实搜索引擎导致的高昂API成本及文档质量不可控问题。ZeroSearch通过创新性地利用LLM模拟搜索引擎，结合结构化训练模板、模拟搜索微调和基于课程学习的文档生成策略，实现了训练成本降低88%，并在多项任务上超越依赖真实搜索的方法。该框架显著提升了LLM的检索能力和推理表现，展示了其在基础模型和指令微调模型上的强大泛化能力，以及通过仅3B参数规模的模型便能激活检索能力，14B模型甚至超越谷歌搜索引擎的潜力，为LLM的推理能力激发提供了经济高效且高性能的新范式。",
    "keywords": [
      "ZeroSearch",
      "大模型",
      "强化学习",
      "检索增强生成",
      "成本优化",
      "课程学习",
      "检索能力"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "智能体"
    ],
    "published_time": "2025-05-29T04:53:51.000Z",
    "download_time": "2025-05-29T23:47:14.546914",
    "visual_resource": [
      "screenshot/wechat_wx_59538ad4.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:53:51.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibPsWibzGOHYBMyw9VU4bSiaTlBxU3NAvbEnYiaSUz5BfagZme4w2dcMWxn2k3QbrpCDxtQmRKjmXTlw/0?wx_fmt=jpeg\", \"id\": \"fq3F4OnOaG9PaF4_NKuRyg\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/fq3F4OnOaG9PaF4_NKuRyg.txt\"}}"
  },
  {
    "id": "mDSMsZjDuSl5qcWckFSXrA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/mDSMsZjDuSl5qcWckFSXrA",
    "title": "还得是华为！Pangu Ultra MoE架构：不用GPU，你也可以这样训练准万亿MoE大模型",
    "summary": "华为盘古团队发布Pangu Ultra MoE模型架构与训练方法，揭示其如何在昇腾NPU上成功训练准万亿MoE大模型而无需GPU。通过创新性的DSSN稳定架构和TinyInit小初始化方法，该模型解决了超大规模MoE训练稳定性难题，实现10+T tokens数据长稳训练。同时，引入EP group loss优化负载均衡，提升专家特化能力。Pangu Ultra MoE还融合MLA和MTP等先进架构，配合Dropless训练及迭代强化学习等技术，显著提升模型效率和推理性能。这标志着华为在芯片协同大模型领域取得突破性进展，为超大规模AI模型训练提供了新范式。",
    "keywords": [
      "PanguUltraMoE",
      "华为",
      "昇腾NPU",
      "MoE大模型",
      "训练方法",
      "稳定性",
      "负载均衡",
      "投机推理"
    ],
    "area": [
      "人工智能",
      "大模型",
      "深度学习"
    ],
    "published_time": "2025-05-29T04:53:51.000Z",
    "download_time": "2025-05-29T23:47:27.988944",
    "visual_resource": [
      "screenshot/wechat_wx_4d9e09e5.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:53:51.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicx3biajeCNW6w51rG0cebQbwZbTspKfeXtibNVeerAiaNP24bpHyNwVo1DgNddDaPvdurj4t0MVpouQ/0?wx_fmt=jpeg\", \"id\": \"mDSMsZjDuSl5qcWckFSXrA\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/mDSMsZjDuSl5qcWckFSXrA.txt\"}}"
  },
  {
    "id": "wrq2ZkidYQ5PRGl525ol8A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/wrq2ZkidYQ5PRGl525ol8A",
    "title": "刚刚，AI科学家Zochi在ACL「博士毕业」，Beta测试今日上线",
    "summary": "Intology公司近日宣布其“AI科学家”Zochi的论文被自然语言处理顶级会议ACL主会录用，标志着Zochi成为首个独立通过A*级别科学会议同行评审的人工智能系统，初步实现“博士级”智能体。Zochi自主完成了针对大型语言模型的“越狱”方法“Tempest”的设计、实验及论文撰写，成功率高达97-100%，揭示了当前LLM安全机制的潜在漏洞。尽管其提交方式引发部分争议，但Zochi在模型微调、生物计算等多个领域展现出卓越的自主研究能力和超越人类中位数表现的水平，预示着AI在科学发现领域的巨大潜力。",
    "keywords": [
      "AI科学家",
      "Zochi",
      "ACL",
      "同行评审",
      "大型语言模型",
      "智能体",
      "越狱",
      "自主研究"
    ],
    "area": [
      "人工智能",
      "智能体",
      "大模型"
    ],
    "published_time": "2025-05-29T04:53:51.000Z",
    "download_time": "2025-05-29T23:47:37.462327",
    "visual_resource": [
      "screenshot/wechat_wx_fca33049.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:53:51.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicx3biajeCNW6w51rG0cebQb6Yog9YNdCHPOfcsrKkicOljClKWJVLAO2s0E4XFBu6nGmK5kpkfGLhg/0?wx_fmt=jpeg\", \"id\": \"wrq2ZkidYQ5PRGl525ol8A\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/wrq2ZkidYQ5PRGl525ol8A.txt\"}}"
  },
  {
    "id": "xK3mM0r8bH6XrREKGc0esA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/xK3mM0r8bH6XrREKGc0esA",
    "title": "RSS 2025｜从说明书学习复杂机器人操作任务：NUS邵林团队提出全新机器人装配技能学习框架Manual2Skill",
    "summary": "新加坡国立大学邵林团队在RSS 2025上发表全新框架Manual2Skill，旨在解决现有视觉语言模型（VLMs）在复杂机器人长时程操作任务中，因稀缺演示数据导致泛化受限的问题。该框架受人类学习启发，使机器人能通过解析人工设计的视觉说明书自主理解并执行家具装配等任务。Manual2Skill包含任务规划、分步位姿估计及动作生成三核心阶段，通过VLMs将抽象指令转化为机器人可执行动作。实验验证该方法在实际家具装配中表现出鲁棒性与有效性，显著优于传统方法，并实现零样本泛化，大幅降低了机器人复杂技能获取的成本与复杂度。",
    "keywords": [
      "机器人",
      "视觉语言模型",
      "家具装配",
      "技能学习",
      "说明书",
      "位姿估计",
      "动作生成"
    ],
    "area": [
      "人工智能",
      "机器人",
      "多模态"
    ],
    "published_time": "2025-05-29T04:53:51.000Z",
    "download_time": "2025-05-29T23:47:48.815889",
    "visual_resource": [
      "screenshot/wechat_wx_794b7716.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:53:51.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibPsWibzGOHYBMyw9VU4bSiaT7dBx2bxbsphvUtBSzicahBDr6RuhmkNyEZ091g1VWMLRBicZibZibibXvmQ/0?wx_fmt=jpeg\", \"id\": \"xK3mM0r8bH6XrREKGc0esA\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/xK3mM0r8bH6XrREKGc0esA.txt\"}}"
  },
  {
    "id": "3ivVjr6usfr3_IF68pdR_g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/3ivVjr6usfr3_IF68pdR_g",
    "title": "用Milvus构建RAG系统，N8N VS dify 如何选？",
    "summary": "本文详细阐述了如何利用Milvus向量数据库与N8N通用工作流工具构建高效的检索增强生成（RAG）系统。文章首先对比了N8N与专为生成式AI设计的Dify平台，强调N8N结合Milvus在集成灵活性与深度理解RAG系统方面的优势。随后，教程手把手指导用户配置Ollama模型、部署Milvus向量数据库，并利用N8N编排RAG工作流，包括文本向量化、数据存储和聊天检索等核心环节。RAG系统凭借Milvus的毫秒级向量检索能力，有效解决了大模型知识时效性、幻觉及专业领域适应性等核心痛点，为构建实时、准确的AI应用提供了实践方案。",
    "keywords": [
      "RAG",
      "Milvus",
      "N8N",
      "Dify",
      "向量数据库",
      "大模型",
      "检索增强生成",
      "工作流自动化"
    ],
    "area": [
      "人工智能",
      "大模型",
      "生成式AI"
    ],
    "published_time": "2025-05-29T10:06:14.000Z",
    "download_time": "2025-05-29T23:46:15.506401",
    "visual_resource": [
      "screenshot/wechat_wx_bb0c31af.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T10:06:14.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/MqgA8Ylgeh74JtlYs6oIOzUoInLuO2R1XTfRbQNYxYH0JuyWmgKSuWYwTyDEriaPrT5zw84qIRhQsOHEqyT5xsQ/0?wx_fmt=jpeg\", \"id\": \"3ivVjr6usfr3_IF68pdR_g\"}, \"extraction_info\": {\"account\": \"Zilliz\", \"file_path\": \"./database/content/wechat/3ivVjr6usfr3_IF68pdR_g.txt\"}}"
  },
  {
    "id": "agenticSeek",
    "source": "GitHub",
    "url": "https://github.com/Fosowl/agenticSeek",
    "title": "AgenticSeek: Private, Local Manus Alternative.",
    "summary": "AgenticSeek是一个100%本地化、注重隐私的AI助手，旨在成为Manus AI的替代品。它完全在用户设备上运行，无需云服务，确保数据安全。该助手基于本地推理模型，具备自主网络浏览、代码编写、智能代理选择、复杂任务规划与执行以及语音交互能力。它为用户提供了一个完全私有、无云依赖的个人AI助理解决方案。",
    "keywords": [
      "本地AI",
      "隐私保护",
      "智能体",
      "网络浏览",
      "代码生成",
      "任务规划",
      "语音交互",
      "大模型"
    ],
    "area": [
      "人工智能",
      "智能体",
      "大模型"
    ],
    "published_time": "2025-05-28T06:42:18+00:00",
    "download_time": "2024-07-28 08:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png"
    ],
    "extra_info": null
  },
  {
    "id": "Baileys",
    "source": "GitHub",
    "url": "https://github.com/WhiskeySockets/Baileys",
    "title": "Baileys",
    "summary": "Baileys是一个基于WebSocket的TypeScript开发库，专为与WhatsApp Web API进行交互而设计。该库提供了实现WhatsApp相关功能的底层工具，使开发者能够构建自定义应用。项目维护者强调用户应负责任地使用，并明确反对滥用、批量或自动化消息发送等违反WhatsApp服务条款的行为。",
    "keywords": [
      "WhatsApp Web API",
      "WebSocket",
      "TypeScript",
      "开发库",
      "即时通讯API"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T01:53:33+00:00",
    "download_time": "2024-07-28 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/WhiskeySockets/Baileys/refs/heads/master/Media/logo.png"
    ],
    "extra_info": null
  },
  {
    "id": "core",
    "source": "GitHub",
    "url": "https://github.com/vuejs/core",
    "title": "vuejs/core",
    "summary": "Vue.js 是一个渐进式 JavaScript 框架，用于构建用户界面。作为一个 MIT 许可的开源项目，其持续发展依赖于社区支持和赞助。README提供了获取文档、参与社区讨论、报告问题、保持联系以及贡献代码的指引。项目强调通过官方渠道获取帮助和报告bug，并鼓励开发者阅读贡献指南。Vue.js 拥有庞大的用户和贡献者群体，是前端开发领域广泛应用的技术栈之一。",
    "keywords": [
      "Vue.js",
      "前端框架",
      "JavaScript",
      "用户界面",
      "开源",
      "社区"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T01:04:53Z",
    "download_time": "2024-05-29 10:00:00",
    "visual_resource": [
      "https://sponsors.vuejs.org/images/appwrite.svg",
      "https://sponsors.vuejs.org/sponsors.svg?v3",
      "https://opencollective.com/vuejs/contributors.svg?width=890&limit=500"
    ],
    "extra_info": null
  },
  {
    "id": "prompt-eng-interactive-tutorial",
    "source": "GitHub",
    "url": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
    "title": "Welcome to Anthropic's Prompt Engineering Interactive Tutorial",
    "summary": "该GitHub仓库提供了Anthropic Claude模型交互式提示工程教程。教程旨在系统教授用户如何构建最优Claude提示，涵盖基础结构、常见问题解决、模型特性理解及实际用例提示构建。课程包含9个章节及练习，强调按顺序学习，并提供在线实验环境和答案。教程基于Claude 3 Haiku，但也提及Sonnet和Opus模型，并提供Google Sheets版本。这是一个实践性强的学习资源，适合希望提升Claude提示技能的开发者和用户。",
    "keywords": [
      "提示工程",
      "大语言模型",
      "Claude",
      "交互式教程",
      "自然语言处理",
      "AI模型应用",
      "Anthropic"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2024-04-08T03:17:07+00:00",
    "download_time": "2024-04-08 08:00:00",
    "visual_resource": [
      "screenshot/github_prompt-eng-interactive-tutorial.png"
    ],
    "extra_info": null
  },
  {
    "id": "livestore",
    "source": "GitHub",
    "url": "https://github.com/livestorejs/livestore",
    "title": "LiveStore",
    "summary": "LiveStore是一个功能完备的客户端中心数据层，旨在为应用提供强大的数据基础。它基于响应式嵌入式SQLite数据库和事件溯源技术，支持实时数据同步、离线优先工作流和自定义冲突解决。LiveStore提供多平台适配器，可替代传统状态管理库，通过即时、响应式查询和事件驱动的数据更新，简化跨客户端的数据管理和同步。",
    "keywords": [
      "数据层",
      "SQLite",
      "响应式查询",
      "离线优先",
      "数据同步",
      "事件溯源",
      "客户端数据管理"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-26T06:53:38Z",
    "download_time": "2024-05-27 08:00:00",
    "visual_resource": [
      "https://share.cleanshot.com/njfQBDqB+",
      "https://share.cleanshot.com/j1h8Z1P5+"
    ],
    "extra_info": null
  },
  {
    "id": "ai-agents-for-beginners",
    "source": "GitHub",
    "url": "https://github.com/microsoft/ai-agents-for-beginners",
    "title": "AI Agents for Beginners - A Course",
    "summary": "微软发布面向初学者的AI智能体课程，共11节课，涵盖构建AI智能体的基础知识。课程内容包括智能体框架探索、设计模式（工具使用、RAG、规划、多智能体、元认知等）以及生产实践。课程使用微软的Azure AI Agent Service、Semantic Kernel和AutoGen等框架和服务，提供Python代码示例，支持Azure AI Foundry和GitHub Models。",
    "keywords": [
      "AI智能体",
      "智能体框架",
      "设计模式",
      "Semantic Kernel",
      "AutoGen",
      "RAG",
      "多智能体"
    ],
    "area": [
      "人工智能",
      "智能体",
      "生成式AI"
    ],
    "published_time": "2025-05-26T09:37:24Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/images/repo-thumbnail.png"
    ],
    "extra_info": null
  },
  {
    "id": "computer-science",
    "source": "GitHub",
    "url": "https://github.com/ossu/computer-science",
    "title": "Open Source Society University",
    "summary": "Open Source Society University (OSSU) 提供一个完整的计算机科学免费自学教育路径，其课程体系参照本科学位要求设计，涵盖计算机科学的核心基础概念。课程精选自世界顶尖大学的在线资源，结构化地组织了从入门到核心再到高级的编程、数学、系统、理论、安全、应用等主题。该项目适合有自律性、希望系统性掌握计算机科学基础知识的个人，并提供全球学习者社区支持。预计投入每周20小时，约可在两年内完成全部核心及部分高级课程。",
    "keywords": [
      "计算机科学",
      "在线教育",
      "自学课程",
      "编程",
      "算法",
      "数据结构",
      "操作系统",
      "软件工程"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-21T12:07:52Z",
    "download_time": "2024-05-21 10:00:00",
    "visual_resource": [
      "screenshot/github_computer-science.png"
    ],
    "extra_info": null
  },
  {
    "id": "n8n",
    "source": "GitHub",
    "url": "https://github.com/n8n-io/n8n",
    "title": "n8n - Secure Workflow Automation for Technical Teams",
    "summary": "n8n是一个面向技术团队的安全工作流自动化平台，融合了代码的灵活性与无代码的便捷性。该平台拥有超过400个集成，内置AI能力，支持基于LangChain构建AI智能体工作流。n8n采用fair-code许可，允许用户完全控制数据和部署，支持自托管和云服务，并提供企业级功能。凭借活跃的社区和丰富的模板资源，n8n成为构建强大自动化流程的理想选择。",
    "keywords": [
      "工作流自动化",
      "集成",
      "无代码",
      "低代码",
      "自托管",
      "人工智能",
      "智能体",
      "LangChain"
    ],
    "area": [
      "人工智能",
      "智能体",
      "其他"
    ],
    "published_time": "2025-05-29T13:07:17+00:00",
    "download_time": "2024-05-30 10:00:00",
    "visual_resource": [
      "https://user-images.githubusercontent.com/10284570/173569848-c624317f-42b1-45a6-ab09-f0ea3c247648.png",
      "https://raw.githubusercontent.com/n8n-io/n8n/master/assets/n8n-screenshot-readme.png"
    ],
    "extra_info": null
  },
  {
    "id": "AppFlowy",
    "source": "GitHub",
    "url": "https://github.com/AppFlowy-IO/AppFlowy",
    "title": "AppFlowy",
    "summary": "AppFlowy是一个开源的AI工作空间，旨在成为Notion的强大替代品，核心优势在于强调数据隐私、提供可靠的原生体验以及支持社区驱动的扩展。该项目采用Flutter和Rust构建，具备跨平台能力，提供笔记、任务管理、数据库等核心功能，并支持用户进行自托管部署，从而实现对个人或企业数据的完全掌控和高度定制化。",
    "keywords": [
      "开源",
      "工作空间",
      "数据隐私",
      "Flutter",
      "Rust",
      "笔记",
      "任务管理",
      "自托管"
    ],
    "area": [
      "人工智能",
      "生成式AI",
      "其他"
    ],
    "published_time": "2025-05-29T02:17:27Z",
    "download_time": "2024-05-30 10:00:00",
    "visual_resource": [
      "https://appflowy.com/_next/static/media/tasks.796c753e.png",
      "https://appflowy.com/_next/static/media/Grid.9e30484b.png",
      "https://appflowy.com/_next/static/media/sites.a8d5b2b9.png"
    ],
    "extra_info": null
  },
  {
    "id": "langflow",
    "source": "GitHub",
    "url": "https://github.com/langflow-ai/langflow",
    "title": "Langflow",
    "summary": "Langflow是一个强大的工具，专注于构建和部署AI驱动的智能体及工作流。它提供直观的可视化构建界面和内置API服务器，能够将创建的智能体轻松转化为可集成到任何应用的技术接口。Langflow全面支持主流大语言模型、向量数据库和不断丰富的AI工具库，核心特性包括可视化构建、代码级定制、交互式Playground、多智能体编排、灵活的API部署选项以及与可观测性平台的集成。该项目支持自托管部署，并提供DataStax托管服务，具备企业级安全性和可扩展性。",
    "keywords": [
      "智能体",
      "工作流",
      "可视化构建",
      "API服务",
      "大语言模型",
      "向量数据库",
      "AI工具",
      "多智能体"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-05-28T21:21:05Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://github.com/langflow-ai/langflow/raw/main/docs/static/img/langflow-logo-color-black-solid.svg",
      "https://api.star-history.com/svg?repos=langflow-ai/langflow&type=Timeline",
      "https://contrib.rocks/image?repo=langflow-ai/langflow"
    ],
    "extra_info": null
  },
  {
    "id": "react-native",
    "source": "GitHub",
    "url": "https://github.com/facebook/react-native",
    "title": "React Native",
    "summary": "React Native是一个开源框架，允许开发者使用JavaScript和React构建原生iOS和Android应用。它将React的声明式UI范式带入移动开发，利用原生UI组件，提供组件化架构、快速开发周期（支持热重载）以及跨平台代码复用能力。React Native是构建高性能、美观移动应用的流行选择，由Facebook及社区共同维护。",
    "keywords": [
      "React Native",
      "React",
      "移动开发",
      "跨平台",
      "iOS",
      "Android",
      "UI框架",
      "原生应用"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T13:16:48Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "screenshot/github_react-native.png"
    ],
    "extra_info": null
  },
  {
    "id": "chatgpt-on-wechat",
    "source": "GitHub",
    "url": "https://github.com/zhayujie/chatgpt-on-wechat",
    "title": "chatgpt-on-wechat",
    "summary": "chatgpt-on-wechat（CoW）是一个基于大模型的智能对话机器人项目，支持微信公众号、企业微信、飞书、钉钉等多平台接入。项目集成了GPT、Claude、Gemini、文心一言等多种主流大模型，具备处理文本、语音、图片的多模态能力。通过丰富的插件系统，可扩展联网搜索、文档总结、角色扮演等功能，并支持基于LinkAI平台构建自有知识库，适用于智能客服、私域运营等企业级AI应用场景。",
    "keywords": [
      "大模型",
      "智能对话机器人",
      "微信",
      "企业微信",
      "飞书",
      "钉钉",
      "语音识别",
      "图像生成",
      "插件",
      "知识库"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2025-05-25T09:44:28Z",
    "download_time": "2024-05-25 10:00:00",
    "visual_resource": [
      "https://cdn.link-ai.tech/image/link-ai-intro.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "angular",
    "source": "GitHub",
    "url": "https://github.com/angular/angular",
    "title": "Angular - The modern web developer's platform",
    "summary": "Angular是一个现代化的Web开发平台，用于构建高性能的移动和桌面Web应用。它基于TypeScript/JavaScript等语言，提供跨平台、快速、可扩展的开发能力和强大的工具链，包括CLI、SSR、Schematics等，是构建大型复杂Web应用的流行前端框架。",
    "keywords": [
      "Angular",
      "前端框架",
      "Web开发",
      "TypeScript",
      "JavaScript",
      "CLI",
      "单页面应用",
      "跨平台"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T00:44:45Z",
    "download_time": "2025-05-29 01:00:00",
    "visual_resource": [
      "https://github.com/angular/angular/raw/main/adev/src/assets/images/press-kit/angular_icon_gradient.gif",
      "https://github.com/angular/angular/raw/main/contributing-docs/images/angular-ecosystem-logos.png"
    ],
    "extra_info": null
  },
  {
    "id": "LivePortrait",
    "source": "GitHub",
    "url": "https://github.com/KwaiVGI/LivePortrait",
    "title": "LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control",
    "summary": "LivePortrait是一个高效的肖像动画生成项目，基于PyTorch实现，支持图像或视频驱动，具备拼接和重定向控制能力。其核心技术特点包括区域控制、精确编辑、动物模式支持、视频编辑（v2v）以及驱动视频自动裁剪等。项目提供了Windows一键安装包和macOS支持，并可通过Gradio界面或HuggingFace Space便捷体验。该技术在肖像动画、视频编辑及实时人脸控制等领域具有广泛应用潜力。",
    "keywords": [
      "肖像动画",
      "人脸动画",
      "视频生成",
      "计算机视觉",
      "深度学习",
      "PyTorch",
      "Gradio"
    ],
    "area": [
      "人工智能",
      "计算机视觉",
      "生成式AI"
    ],
    "published_time": "2025-02-28T13:56:34Z",
    "download_time": "2024-07-26 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/showcase2.gif",
      "https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/inference.gif",
      "https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/inference-animals.gif"
    ],
    "extra_info": null
  },
  {
    "id": "xiaozhi-esp32",
    "source": "GitHub",
    "url": "https://github.com/78/xiaozhi-esp32",
    "title": "小智 AI 聊天机器人 （XiaoZhi AI Chatbot）",
    "summary": "小智AI聊天机器人是一个开源项目，旨在帮助用户入门AI硬件开发，将大语言模型应用于ESP32等硬件设备。项目支持Wi-Fi/4G连接、离线唤醒、流式语音对话、多语言识别（SenseVoice）、声纹识别、多种大模型（Qwen, DeepSeek, Doubao）集成及TTS功能。它兼容多种ESP32芯片平台和开源硬件，提供软硬件教程，是探索AI与嵌入式结合的良好实践平台。",
    "keywords": [
      "AI聊天机器人",
      "ESP32",
      "大语言模型",
      "语音识别",
      "硬件开发",
      "物联网",
      "SenseVoice",
      "Qwen"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器人"
    ],
    "published_time": "2025-05-29T12:12:21+00:00",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://github.com/78/xiaozhi-esp32/raw/main/docs/wiring2.jpg",
      "https://github.com/78/xiaozhi-esp32/raw/main/docs/v1/espbox3.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "ant-design",
    "source": "GitHub",
    "url": "https://github.com/ant-design/ant-design",
    "title": "Ant Design",
    "summary": "Ant Design是一个企业级UI设计语言和基于React的UI组件库，专注于为Web应用提供高质量的界面解决方案。该库采用TypeScript编写，提供丰富的开箱即用组件，并配套完整的UI设计资源和开发工具。Ant Design支持多语言国际化和强大的主题定制能力，是构建现代企业级前端应用的流行框架，广泛应用于各类中后台产品。",
    "keywords": [
      "UI设计",
      "React",
      "组件库",
      "TypeScript",
      "前端开发",
      "企业级应用",
      "国际化",
      "主题定制"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T06:18:47Z",
    "download_time": "2024-05-29 10:00:00",
    "visual_resource": [
      "https://gw.alipayobjects.com/zos/rmsportal/KDpgvguMpGfqaHPjicRK.svg",
      "https://user-images.githubusercontent.com/507615/209472919-6f7e8561-be8c-4b0b-9976-eb3c692aa20a.png"
    ],
    "extra_info": null
  },
  {
    "id": "fastapi",
    "source": "GitHub",
    "url": "https://github.com/fastapi/fastapi",
    "title": "FastAPI",
    "summary": "FastAPI是一个现代、高性能的Python Web框架，专为快速构建API而设计。它基于标准的Python类型提示，并利用Starlette和Pydantic的强大功能，提供了卓越的性能、高效的开发流程和减少的错误。框架完全兼容OpenAPI和JSON Schema等标准，并自动生成交互式API文档（Swagger UI和ReDoc）。FastAPI因其易用性和强大功能，已被Microsoft、Uber、Netflix、Cisco等公司广泛应用于构建ML服务、REST API和关键业务系统。",
    "keywords": [
      "Python",
      "Web框架",
      "API开发",
      "高性能",
      "类型提示",
      "OpenAPI",
      "Pydantic",
      "Starlette"
    ],
    "area": [
      "人工智能",
      "大模型",
      "其他"
    ],
    "published_time": "2025-05-22T09:45:56Z",
    "download_time": "2024-07-28 10:00:00",
    "visual_resource": [
      "https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png",
      "https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png",
      "https://fastapi.tiangolo.com/img/vscode-completion.png"
    ],
    "extra_info": null
  },
  {
    "id": "2505.22617",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22617",
    "title": "强化学习在推理语言模型中的熵机制",
    "summary": "本文旨在克服将强化学习应用于大型语言模型推理方面的一个主要障碍，即策略熵的坍塌。在大量未进行熵干预的RL运行中持续地观察到这种现象——策略熵在训练早期急剧下降，这种探索能力的减弱总是伴随着策略性能的饱和。实践中，我们建立了熵H与下游性能R之间的经验变换方程R=-a*e^H+b。这一经验规律强烈表明，策略性能以策略熵为代价换取，并受其耗尽的限制，而性能上限在H=0时完全可预测，即R=-a+b。我们的发现表明，为了进一步扩展RL计算能力，持续的探索需要熵管理。为此，我们从理论和实证两方面研究了熵的动态。我们的推导表明，策略熵的变化是由动作概率与logits变化（在使用类似策略梯度算法时，这与其优势函数成正比）之间的协方差驱动的。实证研究显示，协方差项的值与熵差异精确吻合，支持了理论结论。此外，协方差项在整个训练过程中大部分时间保持正值，进一步解释了为何策略熵会单调下降。通过理解熵动态背后的机制，我们提出通过限制高协方差token的更新来控制熵。具体地，我们提出了两种简单而有效的技术：Clip-Cov和KL-Cov，它们分别对高协方差的token进行剪裁和施加KL惩罚。实验表明，这些方法鼓励探索，从而帮助策略避免熵坍塌并获得更好的下游性能。",
    "keywords": [
      "强化学习",
      "大型语言模型",
      "策略熵",
      "熵坍塌",
      "推理"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "机器学习"
    ],
    "published_time": "2025-05-28T17:38:45.000Z",
    "download_time": "2025-05-29 07:03:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22617.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22617",
      "arxiv_url": "https://arxiv.org/abs/2505.22617"
    }
  },
  {
    "id": "2505.21600",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21600",
    "title": "R2R：利用大小模型 Token 路由高效导航分歧推理路径",
    "summary": "大型语言模型（LLMs）以巨大的推理开销为代价实现了令人印象深刻的推理能力，这带来了巨大的部署挑战。尽管精炼的小型语言模型（SLMs）显著提高了效率，但由于它们未能遵循LLMs的推理路径，其性能有所下降。幸运的是，我们发现LLMs和SLMs之间仅有极少部分的token会真正导致推理路径分歧。大多数生成的token要么是相同的，要么仅表现出中性的差异，例如缩写或表达方式的微小变动。利用这一洞察，我们引入了 罗马之路 (Roads to Rome, R2R)，这是一种神经token路由方法，它仅对这些关键的、导致路径分歧的token选择性地使用LLM，而将绝大多数token的生成留给SLM。我们还开发了一种自动数据生成管线，用于识别分歧token并生成token级的路由标签来训练轻量级路由器。我们将R2R应用于组合DeepSeek系列的R1-1.5B和R1-32B模型，并在有挑战性的数学、编码和问答基准上进行了评估。在平均激活参数量为5.6B的情况下，R2R的平均准确率是R1-7B的1.6倍，甚至超越了R1-14B模型。与R1-32B相比，在性能相当的情况下，它提供了2.8倍的实际耗时加速，推动了测试时缩放效率的帕累托前沿。我们的代码可在 https://github.com/thu-nics/R2R 获取。",
    "keywords": [
      "R2R",
      "Token Routing",
      "大小模型",
      "分歧推理",
      "推理效率"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "生成式AI"
    ],
    "published_time": "2025-05-27T16:57:20.000Z",
    "download_time": "2025-05-29 07:04:08",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21600.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21600",
      "arxiv_url": "https://arxiv.org/abs/2505.21600"
    }
  },
  {
    "id": "2505.22651",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22651",
    "title": "Sherlock：视觉语言模型的自主纠错推理",
    "summary": "推理视觉语言模型（VLMs）在复杂多模态任务上已展现出喜人的性能。然而，它们仍面临重大挑战：对推理错误高度敏感，需要大量标注数据或准确验证器，并且难以泛化到特定领域之外。为了解决这些局限性，我们探索将自主纠错作为一种增强推理VLMs的策略。我们首先对推理VLMs的自主纠错能力进行了深入分析，并指出了关键不足。基于我们的发现，我们提出了Sherlock，一个自主纠错和自主提升的训练框架。Sherlock引入了轨迹级自主纠错目标、基于视觉扰动的偏好数据构建方法，以及用于偏好调优的动态beta。一旦模型仅使用2万条随机采样的标注数据获得了自主纠错能力，它便可在没有外部监督的情况下持续自主提升。Sherlock基于Llama3.2-Vision-11B模型构建，在八个基准测试中取得了显著结果，直接生成平均准确率达到64.1，自主纠错后达到65.4。它超越了LLaVA-CoT (63.2)、Mulberry (63.9)和LlamaV-o1 (63.4)，同时使用的标注数据量不到它们的20%。",
    "keywords": [
      "自主纠错",
      "视觉语言模型",
      "推理",
      "自主提升",
      "少量数据训练"
    ],
    "area": [
      "多模态",
      "深度学习",
      "大模型"
    ],
    "published_time": "2025-05-28T17:58:03.000Z",
    "download_time": "2025-05-29 07:04:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22651.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22651",
      "arxiv_url": "https://arxiv.org/abs/2505.22651"
    }
  },
  {
    "id": "2505.22312",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22312",
    "title": "天工开源推理模型 1 号技术报告",
    "summary": "DeepSeek-R1 的成功凸显了强化学习 (RL) 在增强大型语言模型 (LLMs) 推理能力方面的重要作用。在本文中，我们提出了 Skywork-OR1，一种针对长链思维 (CoT) 模型的有效且可扩展的强化学习实现。基于 DeepSeek-R1-Distill 模型系列，我们的强化学习方法取得了显著的性能提升，将 Skywork-OR1-32B 模型在 AIME24、AIME25 和 LiveCodeBench 上的平均准确率从 57.8% 提高到 72.8% (+15.0%)，将 7B 模型从 43.6% 提高到 57.5% (+13.9%)。我们的 Skywork-OR1-32B 模型在 AIME24 和 AIME25 基准测试上超越了 DeepSeek-R1 和 Qwen3-32B，并在 LiveCodeBench 上取得了可比结果。Skywork-OR1-7B 和 Skywork-OR1-Math-7B 模型在同等规模模型中展现出具有竞争力的推理能力。我们对训练管线的核心组件进行了全面的消融研究，以验证其有效性。此外，我们深入研究了熵塌缩现象，确定了影响熵动态的关键因素，并表明缓解过早的熵塌缩对于提高测试性能至关重要。为了支持社区研究，我们完全开源了我们的模型权重、训练代码和训练数据集。",
    "keywords": [
      "Skywork-OR1",
      "强化学习",
      "大型语言模型",
      "链式思考",
      "推理能力"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器学习"
    ],
    "published_time": "2025-05-28T12:56:04.000Z",
    "download_time": "2025-05-29 07:04:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22312.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22312",
      "arxiv_url": "https://arxiv.org/abs/2505.22312"
    }
  },
  {
    "id": "2505.22453",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22453",
    "title": "基于 GRPO 的多模态大语言模型推理无监督后训练",
    "summary": "提升多模态大语言模型（MLLMs）在后训练阶段的性能通常依赖于监督微调（SFT）或强化学习（RL）。然而，这些监督方法需要昂贵且需手动标注的多模态数据，这是一种最终不可持续的资源。尽管近期已有工作探索无监督后训练，但其方法复杂且难以迭代。在本文中，我们首次探索使用 GRPO（一种稳定且可扩展的在线强化学习算法）来实现模型在没有任何外部监督下的持续自我提升。我们提出了 MM-UPT，一个简单但有效的 MLLMs 无监督后训练框架。MM-UPT 构建于 GRPO 之上，用基于多数投票采样多个响应的自奖励机制替代了传统的奖励信号。我们的实验表明，使用没有真实标签的标准数据集，MM-UPT 显著提升了 Qwen2.5-VL-7B 的推理能力（例如，在 MathVista 上从 66.3% 提升到 72.9%，在 We-Math 上从 62.9% 提升到 68.7%）。MM-UPT 还优于先前的无监督基线方法，甚至接近监督 GRPO 的结果。此外，我们展示了通过纳入完全由 MLLM 自身生成的合成问题也能提高性能，这突显了一种可扩展的自我提升的有前景的方法。总的来说，MM-UPT 为在没有外部监督的情况下持续、自主地增强 MLLMs 提供了一种新范式。我们的代码位于 https://github.com/waltonfuture/MM-UPT。",
    "keywords": [
      "多模态大语言模型",
      "无监督后训练",
      "GRPO",
      "自我提升",
      "推理"
    ],
    "area": [
      "多模态",
      "大模型",
      "机器学习"
    ],
    "published_time": "2025-05-28T15:11:16.000Z",
    "download_time": "2025-05-29 07:04:59",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22453.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22453",
      "arxiv_url": "https://arxiv.org/abs/2505.22453"
    }
  },
  {
    "id": "2505.20411",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20411",
    "title": "SWE-rebench：用于软件工程Agent任务收集和去污染评估的自动化流程",
    "summary": "基于LLM的Agent在日益增长的软件工程（SWE）任务中展现出令人瞩目的能力。然而，推进该领域面临两个关键挑战。首先，高质量的训练数据稀缺，特别是反映真实世界SWE场景的数据，Agent必须在此场景下与开发环境交互，执行代码并根据其行动结果调整行为。现有数据集要么仅限于一次性代码生成，要么由小规模、手动整理的交互式任务集合组成，缺乏规模和多样性。其次，新颖的交互式SWE任务的缺乏影响了对快速改进模型的评估，因为静态基准测试由于污染问题很快就会过时。为了解决这些局限性，我们引入了一种新颖、自动化且可扩展的流程，用于持续从不同的GitHub仓库中提取真实世界的交互式SWE任务。利用该流程，我们构建了SWE-rebench，这是一个包含超过21,000个基于Python的交互式SWE任务的公开数据集，适用于大规模SWE Agent的强化学习。此外，我们利用通过SWE-rebench方法收集的持续涌现的新鲜任务，构建了一个用于Agent化软件工程的无污染基准测试。我们将各种LLM在该基准测试上的结果与SWE-bench Verified上的结果进行比较，并发现某些语言模型的性能可能由于污染问题而被高估。",
    "keywords": [
      "SWE-rebench",
      "软件工程Agent",
      "自动化流程",
      "去污染评估",
      "交互式任务数据集"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-05-26T18:01:00.000Z",
    "download_time": "2025-05-29 07:05:14",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20411.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20411",
      "arxiv_url": "https://arxiv.org/abs/2505.20411"
    }
  },
  {
    "id": "2505.21136",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21136",
    "title": "SageAttention2++：SageAttention2 的更高效率实现",
    "summary": "注意力机制的效率至关重要，因为其时间复杂度随序列长度呈二次方增长。SageAttention2 通过利用量化技术来加速注意力机制中的矩阵乘法（Matmul）解决了这一问题。为了进一步加速 SageAttention2，我们提出利用在 FP16 中累积的 FP8 Matmul 更快的指令。该指令比 SageAttention2 中使用的 FP8 Matmul 快 2 倍。我们的实验表明，SageAttention2++ 在保持与 SageAttention2 相同注意力精度的同时，比 FlashAttention 实现了 3.9 倍的加速。这意味着 SageAttention2++ 可以有效加速包括语言、图像和视频生成在内的各种模型，且端到端指标损失可以忽略不计。代码将在 https://github.com/thu-ml/SageAttention 上提供。",
    "keywords": [
      "SageAttention2++",
      "注意力机制",
      "量化",
      "FP8 Matmul",
      "加速"
    ],
    "area": [
      "人工智能",
      "深度学习",
      "生成式AI"
    ],
    "published_time": "2025-05-27T12:50:36.000Z",
    "download_time": "2025-05-29 07:05:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21136.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21136",
      "arxiv_url": "https://arxiv.org/abs/2505.21136"
    }
  },
  {
    "id": "2505.22334",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22334",
    "title": "通过带冷启动的强化学习提升多模态推理能力",
    "summary": "近期大型语言模型（LLMs）的进展展示了令人印象深刻的思维链推理能力，其中强化学习（RL）在此进展中发挥了关键作用。虽然模型通过反思展现自我纠正的“顿悟时刻”模式通常被归因于RL的涌现特性，但我们首次证明这些模式在RL训练前已存在于多模态LLMs（MLLMs）中，且可能不一定与推理性能的提升相关。基于这些见解，我们提出了一项关于通过两阶段方法增强多模态推理的综合研究：(1) 作为冷启动的监督微调（SFT），采用结构化思维链推理模式，随后 (2) 通过GRPO进行强化学习，以进一步提升这些能力。我们的大量实验表明，这种组合方法在具有挑战性的多模态推理基准上，始终优于仅使用SFT和仅使用RL的方法。所得模型在3十亿和7十亿参数规模的开源MLLMs中均达到了最先进的性能，其中我们的7十亿模型相比基础模型显示出显著提升（例如，MathVista上从66.3%到73.4%，We-Math上从62.9%到70.4%），而我们的3十亿模型性能可与多个7十亿模型媲美。总而言之，这项工作为构建先进的多模态推理模型提供了实用指导。我们的代码可在 https://github.com/waltonfuture/RL-with-Cold-Start 查看。",
    "keywords": [
      "多模态推理",
      "强化学习",
      "冷启动",
      "MLLMs",
      "SFT"
    ],
    "area": [
      "多模态",
      "大模型",
      "深度学习"
    ],
    "published_time": "2025-05-28T13:21:38.000Z",
    "download_time": "2025-05-29 07:05:42",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22334.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22334",
      "arxiv_url": "https://arxiv.org/abs/2505.22334"
    }
  },
  {
    "id": "2505.22457",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22457",
    "title": "通过下一事件预测增强视频推理",
    "summary": "下一词元预测作为使大语言模型（LLMs）能够进行推理的基础学习任务。但是，当旨在赋予多模态大语言模型（MLLMs）处理视频输入的时间推理能力时，学习任务应该是什么？现有的任务，如视频问答，通常依赖于人类或更强的多模态大语言模型的标注，而视频字幕则倾向于将时间推理与空间信息纠缠在一起。为了弥补这一空白，我们提出了下一事件预测（NEP），这是一个利用未来视频片段作为丰富、自监督信号以促进时间推理的学习任务。我们将每个视频分割成过去和未来的帧：多模态大语言模型将过去的帧作为输入，并预测从未来帧提取的事件摘要，从而鼓励模型进行时间推理以完成任务。为了支持这项任务，我们整理了V1-33K数据集，该数据集包含33,000个自动提取的视频片段，涵盖了各种现实场景。我们进一步探索了一系列视频指令微调策略，以研究它们对时间推理的影响。为了评估进展，我们引入了FutureBench基准测试，用于评估预测未见过的未来事件的连贯性。实验验证了下一事件预测为增强多模态大语言模型的时间推理提供了一个可扩展且有效的训练范式。",
    "keywords": [
      "下一事件预测",
      "时间推理",
      "多模态大模型",
      "视频理解",
      "自监督学习"
    ],
    "area": [
      "视频理解",
      "多模态",
      "大模型"
    ],
    "published_time": "2025-05-28T15:13:34.000Z",
    "download_time": "2025-05-29 07:06:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22457.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22457",
      "arxiv_url": "https://arxiv.org/abs/2505.22457"
    }
  },
  {
    "id": "2505.21925",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21925",
    "title": "RenderFormer：基于 Transformer 的带全局光照三角网格神经渲染",
    "summary": "我们提出了 RenderFormer，一种神经渲染管线，它可以直接从包含完整全局光照效果的基于三角网格的场景表示渲染图像，且无需针对每个场景进行训练或微调。与采用以物理为中心的方法进行渲染不同，我们将渲染表述为一种序列到序列的转换：将代表带有材质属性的三角形的 token 序列转换为代表小块像素的输出 token 序列。RenderFormer 遵循一个两阶段管线：一个与视角无关的阶段，用于建模三角形到三角形的光传输；以及一个与视角相关的阶段，该阶段将代表光线束的 token 转换为相应的像素值，并由第一阶段的三角形序列引导。这两个阶段都基于 Transformer 架构，且以最小的先验约束进行学习。我们在具有不同形状和光传输复杂度的场景上演示并评估了 RenderFormer。",
    "keywords": [
      "神经渲染",
      "Transformer",
      "全局光照",
      "三角网格",
      "无需场景训练"
    ],
    "area": [
      "深度学习",
      "计算机视觉",
      "生成式AI"
    ],
    "published_time": "2025-05-28T03:20:46.000Z",
    "download_time": "2025-05-29 07:06:17",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21925.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21925",
      "arxiv_url": "https://arxiv.org/abs/2505.21925"
    }
  },
  {
    "id": "2505.19253",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19253",
    "title": "DeepResearchGym：一个免费、透明、可复现的深度研究评估沙箱",
    "summary": "深度研究系统代表了一种新兴的智能信息检索方法，能够针对复杂查询生成全面且有充分依据的报告。然而，大多数现有框架依赖于动态商业搜索API，这除了成本之外，还带来了可复现性和透明性方面的挑战。为了解决这些局限性，我们引入了 DeepResearchGym，这是一个开源沙箱，它结合了可复现的搜索API和严格的评估协议，用于对深度研究系统进行基准测试。该API使用先进的密集检索器和通过 DiskANN 实现的近似最近邻搜索，索引了大规模公共网络语料库，即 ClueWeb22 和 FineWeb。它比流行的商业API具有更低的延迟，同时确保跨运行文档排名的稳定性，并免费用于研究用途。为了评估深度研究系统的输出，我们通过“大模型即评委”（LLM-as-a-judge）评估，将自动指标扩展到 Researchy Questions 基准测试中，以衡量其与用户信息需求的匹配度、检索的忠实度以及报告质量。实验结果表明，与 DeepResearchGym集成的系统取得了与使用商业API的系统相当的性能，且性能排名在各评估指标上保持一致。人工评估研究进一步证实了我们的自动评估协议与人工偏好一致，验证了该框架支持对深度研究系统进行受控评估的能力。我们的代码和API文档可在 https://www.deepresearchgym.ai 获取。",
    "keywords": [
      "Deep Research",
      "Evaluation",
      "Reproducibility",
      "Information Retrieval",
      "LLM-as-a-judge"
    ],
    "area": [
      "智能体",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2025-05-25T18:16:13.000Z",
    "download_time": "2025-05-29 07:06:30",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19253.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19253",
      "arxiv_url": "https://arxiv.org/abs/2505.19253"
    }
  },
  {
    "id": "2505.18600",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18600",
    "title": "Chain-of-Zoom：通过尺度自回归与偏好对齐实现极端超分辨率",
    "summary": "现代单幅图像超分辨率 (SISR) 模型在其训练的尺度因子下能产生逼真的结果，但在远超该范围进行放大时性能会急剧下降。我们通过 Chain-of-Zoom (CoZ) 解决了这一可扩展性瓶颈。CoZ 是一个与具体模型无关的框架，它将 SISR 分解为一个由中间尺度状态组成的自回归链，并辅以多尺度感知的提示。CoZ 反复重用一个骨干 SR 模型，将条件概率分解为可处理的子问题，从而在无需额外训练的情况下实现极端分辨率。由于在高放大倍数下视觉线索会减少，我们在每个缩放步骤中都通过一个视觉-语言模型 (VLM) 生成多尺度感知的文本提示进行增强。提示提取器本身使用广义奖励策略优化 (GRPO) 和一个评论家 VLM 进行微调，以便将文本指导与人类偏好对齐。实验表明，一个用 CoZ 包装的标准 4 倍扩散 SR 模型可以实现超过 256 倍的放大，同时保持高感知质量和保真度。项目页面：https://bryanswkim.github.io/chain-of-zoom/ 。",
    "keywords": [
      "Extreme Super-Resolution",
      "尺度自回归",
      "偏好对齐",
      "Chain-of-Zoom",
      "视觉-语言模型"
    ],
    "area": [
      "计算机视觉",
      "多模态",
      "深度学习"
    ],
    "published_time": "2025-05-24T08:50:08.000Z",
    "download_time": "2025-05-29 07:06:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18600.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18600",
      "arxiv_url": "https://arxiv.org/abs/2505.18600"
    }
  },
  {
    "id": "2505.22232",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22232",
    "title": "跨语言质量评估：一种利用语言模型进行预训练数据过滤的多语言方法",
    "summary": "高质量的多语言训练数据对于有效预训练大型语言模型（LLM）至关重要。然而，合适的开源多语言数据集仍然有限。现有的最先进数据集主要依赖启发式过滤方法，这限制了它们的跨语言迁移能力和可扩展性。\n本文介绍 JQL，这是一种系统性方法，能够高效地大规模筛选多样化的高质量多语言数据，同时显著降低计算需求。JQL 将 LLM 的标注能力提炼到基于预训练多语言嵌入的轻量级标注器中。这些模型表现出强大的多语言和跨语言性能，甚至对于训练期间未见的语言和脚本也是如此。通过对 35 种语言的实证评估，由此产生的标注流程显著优于当前的启发式过滤方法，例如 Fineweb2。JQL 显著提高了下游模型训练质量，并提高了数据保留率。我们的研究为多语言数据管理提供了实用的见解和宝贵的资源，提升了多语言数据集开发的标准。",
    "keywords": [
      "多语言数据",
      "数据过滤",
      "LLMs",
      "预训练",
      "跨语言"
    ],
    "area": [
      "自然语言处理",
      "大模型",
      "机器学习"
    ],
    "published_time": "2025-05-28T11:06:54.000Z",
    "download_time": "2025-05-29 07:06:58",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22232.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22232",
      "arxiv_url": "https://arxiv.org/abs/2505.22232"
    }
  },
  {
    "id": "2505.19075",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19075",
    "title": "通用推理器：用于冻结大型语言模型的单一、可组合的即插即用推理器",
    "summary": "大型语言模型（LLMs）展示了非凡的通用能力，但增强推理等技能通常需要大量的计算资源，并可能影响其泛化能力。虽然参数高效微调（PEFT）方法提供了一种更资源节约的替代方案，但由于架构依赖性，它们通常需要针对每个LLM骨干模型重新训练。为了解决这些挑战，我们提出通用推理器（UniR）——一个单一、轻量级、可组合、即插即用的推理模块，可与任何冻结的LLM结合使用，赋予其专门的推理能力。具体来说，UniR将奖励分解为一个独立的推理模块，该模块使用预定义奖励独立训练，有效地将轨迹级信号转化为令牌级指导。训练完成后，UniR可以在推理时通过简单地将其输出logits叠加到LLM骨干模型的logits上，与任何冻结的LLM结合使用。这种叠加结构自然支持模块化组合：多个针对不同任务训练的UniR模块可以通过叠加其logits联合应用，通过组合实现复杂的推理。在数学推理和机器翻译任务上的实验结果表明，使用Llama3.2模型时，UniR显著优于现有的基线微调方法。此外，UniR展示了强大的弱监督到强监督泛化能力：在较小模型上训练的推理模块能有效地指导大得多的LLMs。这使得UniR成为一种经济高效、适应性强且稳健的解决方案，可在不损害其核心能力的情况下增强LLMs的推理能力。代码已在https://github.com/hangeol/UniR 开源。",
    "keywords": [
      "Universal Reasoner",
      "Frozen LLMs",
      "Reasoning",
      "Plug-and-Play",
      "Composable"
    ],
    "area": [
      "自然语言处理",
      "大模型",
      "机器学习"
    ],
    "published_time": "2025-05-25T10:19:10.000Z",
    "download_time": "2025-05-29 07:07:06",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19075.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19075",
      "arxiv_url": "https://arxiv.org/abs/2505.19075"
    }
  },
  {
    "id": "2505.21887",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21887",
    "title": "SVRPBench：一项面向随机车辆路径问题的现实基准测试",
    "summary": "在不确定性下实现鲁棒路径规划是现实物流的核心，然而大多数基准测试假定静态、理想化的设置。我们提出 SVRPBench，这是首个捕获城市规模车辆路径高保真随机动态的开放基准测试。它涵盖了 500 多个算例，服务客户数量高达 1000 个，模拟了现实的配送条件：时变拥堵、对数正态分布的延误、概率性事故以及基于实证数据的住宅和商业客户时间窗。我们的流程生成了多样化、约束丰富的场景，包括多仓库和多车辆设置。基准测试结果显示，POMO 和 AM 等最先进的强化学习求解器在分布迁移下性能下降超过 20%，而经典和元启发式方法则保持鲁棒性。为了促进可复现研究，我们发布了数据集和评估套件。SVRPBench 挑战社区设计能够超越合成假设、适应现实不确定性的求解器。",
    "keywords": [
      "SVRP",
      "Benchmark",
      "Uncertainty",
      "Logistics",
      "Reinforcement Learning"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "深度学习"
    ],
    "published_time": "2025-05-28T02:03:31.000Z",
    "download_time": "2025-05-29 07:07:22",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21887.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21887",
      "arxiv_url": "https://arxiv.org/abs/2505.21887"
    }
  },
  {
    "id": "2505.22129",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22129",
    "title": "基于 Stable Diffusion 的文本到 360 度全景图生成是如何实现的？",
    "summary": "近年来，文本到图像扩散模型（例如 Stable Diffusion）的繁荣激发了将其应用于 360 度全景图生成的研究。先前的工作展示了在预训练扩散模型上使用传统的低秩适应技术生成全景图的可行性。然而，透视图和全景图之间巨大的领域差异，引发了对促成这种经验成功的底层机制的疑问。我们假设并研究了在全景数据上微调时，可训练对应部分表现出不同的行为，并且这种适应隐藏了某种利用预训练扩散模型中先验知识的内在机制。我们的分析揭示如下：1）注意力模块中的查询（query）和键（key）矩阵负责可在全景和透视域之间共享的通用信息，因此与全景图生成的相关性较低；2）值（value）和输出权重矩阵专门用于将预训练知识适应到全景域，在全景图生成的微调过程中起着更关键的作用。我们通过引入一个名为 UniPano 的简单框架，对这些见解进行了实证验证，旨在为未来的研究提供一个优雅的基线。UniPano 不仅优于现有方法，而且与先前的双分支方法相比，显著减少了内存使用和训练时间，使其能够扩展到更高分辨率的端到端全景图生成。代码将会发布。",
    "keywords": [
      "文本到全景图生成",
      "Stable Diffusion",
      "扩散模型",
      "微调",
      "UniPano"
    ],
    "area": [
      "生成式AI",
      "计算机视觉",
      "深度学习"
    ],
    "published_time": "2025-05-28T08:54:04.000Z",
    "download_time": "2025-05-29 07:07:36",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22129.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22129",
      "arxiv_url": "https://arxiv.org/abs/2505.22129"
    }
  },
  {
    "id": "2505.22648",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22648",
    "title": "WebDancer：迈向自主信息搜索智能体",
    "summary": "解决复杂的实际问题需要深入的信息搜索和多步推理。智能体系统的最新进展，例如 Deep Research，强调了自主多步研究的潜力。在本文中，我们从以数据为中心和训练阶段的角度，提出了构建端到端智能信息搜索智能体的统一范式。我们的方法包含四个关键阶段：(1) 浏览数据构建，(2) 轨迹采样，(3) 用于有效冷启动的监督微调，以及 (4) 用于增强泛化能力的强化学习。我们在基于 ReAct 的 Web 智能体 WebDancer 中实例化了这一框架。在具有挑战性的信息搜索基准 GAIA 和 WebWalkerQA 上的实证评估表明了 WebDancer 的强大性能，取得了可观的结果，并凸显了我们训练范式的有效性。对智能体训练的进一步分析提供了宝贵的见解和可行的、系统的路径，用于开发更强大的智能体模型。代码和演示将在 https://github.com/Alibaba-NLP/WebAgent 上发布。",
    "keywords": [
      "信息搜索智能体",
      "训练范式",
      "强化学习",
      "Web Agent",
      "ReAct"
    ],
    "area": [
      "智能体",
      "强化学习",
      "自然语言处理"
    ],
    "published_time": "2025-05-28T17:57:07.000Z",
    "download_time": "2025-05-29 07:07:56",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22648.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22648",
      "arxiv_url": "https://arxiv.org/abs/2505.22648"
    }
  },
  {
    "id": "2505.19187",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19187",
    "title": "LIMOPro：用于高效有效测试时扩展的推理精炼",
    "summary": "大型语言模型（LLM）通过测试时扩展方法展现出卓越的推理能力，尤其是在使用从更强大的大型推理模型（LRM）中提炼出的思维链（CoT）数据进行微调时。然而，这些推理链通常包含冗余元素，反映了人类解决问题的过程，这些元素可分为渐进式推理（必需的解决方案发展路径）和功能性元素（验证过程、替代解决方案方法和错误修正）。虽然渐进式推理至关重要，但功能性元素在测试时推理过程中显著增加了计算需求。我们引入了 PIR（基于困惑度的重要性精炼），这是一个基于原则的框架，它根据每个推理步骤对答案预测置信度的影响来定量评估其重要性。PIR 系统地识别并选择性地剪枝仅具有较低重要性的功能性步骤，同时保留渐进式推理组成部分，从而创建优化的训练数据，在减少冗余的同时保持核心解决方案路径的完整性。在 PIR 优化数据上微调的模型表现出卓越的测试时扩展特性，生成更简洁的推理链，同时在具有挑战性的推理基准（AIME、AMC 和 GPQA Diamond）上实现了更高的准确率（+0.9% 至 +6.6%）和显著降低的令牌使用量（-3% 至 -41%）。我们的方法在不同模型尺寸、数据源和令牌预算下都展现出强大的泛化能力，为在高效测试时扩展、响应时间和计算效率是重要限制的场景中部署具有推理能力的 LLM 提供了一种实用的解决方案。",
    "keywords": [
      "大型语言模型",
      "推理精炼",
      "测试时扩展",
      "思维链",
      "PIR"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "人工智能"
    ],
    "published_time": "2025-05-25T15:17:57.000Z",
    "download_time": "2025-05-29 07:08:11",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19187.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19187",
      "arxiv_url": "https://arxiv.org/abs/2505.19187"
    }
  },
  {
    "id": "2505.17663",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17663",
    "title": "迈向动态心智理论：评估大型语言模型对人类状态时间演变的适应能力",
    "summary": "随着大型语言模型（LLMs）越来越多地参与到人机交互中，评估其心智理论（ToM）能力——特别是其跟踪动态心理状态的能力——变得至关重要。虽然现有基准评估基本的ToM能力，但它们主要关注心理状态的静态快照，忽略了现实社交互动中特有的时间演变。我们提出了DynToM，这是一个新颖的基准，专门设计用于评估LLMs在相互关联的场景中理解和跟踪心理状态时间进展的能力。通过一个系统的四步框架，我们生成了1,100个社交情境，包含5,500个场景和78,100个问题，每个情境都经过真实性和质量验证。我们对十个最先进LLMs的综合评估表明，它们的平均性能比人类低44.7%，并且在跟踪和推理心理状态变化时性能显著下降。这一性能差距突显了当前LLMs在建模人类心理状态动态性方面的根本性局限。",
    "keywords": [
      "大模型",
      "心智理论 (ToM)",
      "动态心理状态",
      "基准",
      "评估"
    ],
    "area": [
      "人工智能",
      "自然语言处理",
      "大模型"
    ],
    "published_time": "2025-05-23T09:27:40.000Z",
    "download_time": "2025-05-29 07:08:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17663.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17663",
      "arxiv_url": "https://arxiv.org/abs/2505.17663"
    }
  },
  {
    "id": "2505.22019",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22019",
    "title": "VRAG-RL：通过强化学习迭代推理增强基于视觉感知的RAG，以理解视觉丰富信息",
    "summary": "有效检索、推理和理解视觉丰富信息对于RAG方法来说仍然是一个挑战。传统的基于文本的方法无法处理视觉相关信息。另一方面，当前基于视觉的RAG方法常受限于固定流程，并且由于未能充分激活模型的基本能力，在有效推理方面常常遇到困难。由于强化学习已被证明有利于模型推理，我们引入了VRAG-RL，这是一个专为视觉丰富信息上的复杂推理量身定制的新型强化学习框架。在此框架下，VLMs与搜索引擎交互，借助视觉感知tokens自主采样单轮或多轮推理轨迹，并基于这些样本进行持续优化。我们的方法揭示了在RAG领域处理视觉信息的关键不足：（i）先前的多模态RAG方法倾向于仅将图像整合到上下文中，导致推理tokens分配不足，并忽略了视觉特有的感知；（ii）当模型与搜索引擎交互时，由于无法清晰表达需求，其查询常常无法检索到相关信息，从而导致性能欠佳。为了应对这些挑战，我们定义了一个专为视觉丰富输入量身定制的动作空间，动作包括裁剪和缩放，使模型能够从粗粒度到细粒度地收集信息。此外，为了弥合用户原始查询与检索器之间的差距，我们采用了一种简单而有效的奖励机制，该机制将查询重写、检索性能与基于模型的奖励相结合。我们的VRAG-RL利用专门设计的强化学习策略优化VLMs以执行RAG任务，使模型与实际应用场景对齐。代码可在 https://github.com/Alibaba-NLP/VRAG 获取。",
    "keywords": [
      "VRAG-RL",
      "RAG",
      "强化学习",
      "视觉语言模型",
      "迭代推理"
    ],
    "area": [
      "多模态",
      "强化学习",
      "计算机视觉"
    ],
    "published_time": "2025-05-28T06:30:51.000Z",
    "download_time": "2025-05-29 07:08:44",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22019.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22019",
      "arxiv_url": "https://arxiv.org/abs/2505.22019"
    }
  },
  {
    "id": "2505.22202",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22202",
    "title": "逐句预测",
    "summary": "自回归语言模型（LMs）每次生成一个词元（token），然而人类推理则是在句子、命题、概念等更高层次的抽象单位上进行的。这种对比引出了一个核心问题：LMs能否也像人类一样，学习在结构化的语义单元而非原始词元序列上进行推理？在本文中，我们基于预训练LMs学到的表示，探究它们是否能够被提升到这种抽象的推理空间。我们提出一个框架，该框架通过自回归地预测下一个句子的连续嵌入，将一个预训练的词元级LM调整到句子空间中运行。我们探索了两种受经典表示学习启发的嵌入范式：1）语义嵌入，通过自编码学习以保留表面含义；2）上下文嵌入，通过下一句预测训练以编码预期结构。我们在两种推理机制下评估了这两种范式：离散式（Discretized），它在重新编码前将每个预测的嵌入解码成文本；以及连续式（Continuous），它为了提高效率完全在嵌入空间中进行推理。在数学、逻辑、常识和规划这四个领域中，连续推理下的上下文嵌入展现出与思维链（Chain-of-Thought, CoT）相当的性能，同时将推理时的浮点运算次数（FLOPs）平均降低了一半。我们还展示了可扩展性和模块化适应性的初步迹象。最后，为了可视化潜在轨迹，我们引入了SentenceLens，这是一个将中间模型状态解码成可解释句子的诊断工具。总的来说，我们的结果表明，预训练LMs能够有效地在潜在嵌入空间中转化到抽象的、结构化的推理。",
    "keywords": [
      "逐句预测",
      "句子嵌入",
      "抽象推理",
      "连续推理",
      "Chain-of-Thought"
    ],
    "area": [
      "自然语言处理",
      "机器学习",
      "人工智能"
    ],
    "published_time": "2025-05-28T10:28:35.000Z",
    "download_time": "2025-05-29 07:08:59",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22202.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22202",
      "arxiv_url": "https://arxiv.org/abs/2505.22202"
    }
  },
  {
    "id": "2505.22613",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22613",
    "title": "RICO：通过视觉重构提高图像重标注的准确性和完整性",
    "summary": "图像重标注被广泛用于为各种多模态任务生成质量增强的训练数据集。现有重标注方法通常依赖强大的多模态大语言模型（MLLM）来增强文本描述，但由于幻觉和缺失细粒度细节导致的不完整性，常常存在不准确性。为了解决这些局限性，我们提出了RICO，一个通过视觉重构来优化标注的新颖框架。具体而言，我们利用文本到图像模型将标注重构为参考图像，并提示一个MLLM识别原始图像和重构图像之间的差异来完善标注。这一过程迭代执行，进一步逐步促进生成更忠实和全面的描述。为了减轻迭代过程带来的额外计算成本，我们引入了RICO-Flash，它使用DPO学习像RICO一样生成标注。大量实验表明，我们的方法在标注准确性和完整性方面取得了显著提升，在CapsBench和CompreCap上均比大多数基线模型高出约10%。代码已发布于 https://github.com/wangyuchi369/RICO。",
    "keywords": [
      "图像重标注",
      "视觉重构",
      "多模态大语言模型",
      "文本到图像模型",
      "迭代优化"
    ],
    "area": [
      "多模态",
      "计算机视觉",
      "生成式AI"
    ],
    "published_time": "2025-05-28T17:29:34.000Z",
    "download_time": "2025-05-29 07:09:12",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22613.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22613",
      "arxiv_url": "https://arxiv.org/abs/2505.22613"
    }
  },
  {
    "id": "2505.22525",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22525",
    "title": "基于生成图像的思考",
    "summary": "我们提出了“基于生成图像的思考”，这是一种新颖的范式，它通过使大型多模态模型（LMM）能够通过自发生成中间视觉思维步骤，以原生方式跨文本和视觉模态进行思考，从而从根本上改变了它们进行视觉推理的方式。当前大型多模态模型的视觉推理受限于处理固定的用户提供图像，或仅通过基于文本的思维链（CoT）进行推理。“基于生成图像的思考”开启了认知能力的新维度，模型可以在其中主动构建中间视觉思维，批判自身的视觉假设，并将其作为推理过程的组成部分进行完善。我们通过两种互补的机制展示了我们方法的有效性：（1）带有中间视觉子目标的视觉生成，其中模型将复杂的视觉任务分解为可管理的组成部分，这些部分被逐步生成和整合；（2）带有自我批判的视觉生成，其中模型生成初步的视觉假设，通过文本推理分析其不足，并根据自身的批判生成优化的输出。我们在视觉生成基准上的实验表明，与基准方法相比，我们的方法取得了显著改进，在处理复杂的多对象场景时，模型的相对改进高达50%（从38%提高到57%）。从探索新型蛋白质结构的生物化学家、迭代空间设计的建筑师，到重建犯罪现场的法医分析师以及构思战略性比赛的篮球运动员，我们的方法使人工智能模型能够进行那种类似于人类创造性、分析性和战略性思维的视觉想象和迭代完善。我们在 https://github.com/GAIR-NLP/thinking-with-generated-images 上发布了我们的开源套件。",
    "keywords": [
      "多模态",
      "大模型",
      "视觉推理",
      "生成图像",
      "自我批判"
    ],
    "area": [
      "多模态",
      "大模型",
      "生成式AI"
    ],
    "published_time": "2025-05-28T16:12:45.000Z",
    "download_time": "2025-05-29 07:09:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22525.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22525",
      "arxiv_url": "https://arxiv.org/abs/2505.22525"
    }
  },
  {
    "id": "2505.20779",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20779",
    "title": "CHIMERA：科学文献中的思想重组知识库",
    "summary": "人类创新的一个显著特征是重组过程——通过整合现有机制和概念的元素来创造原创思想。在这项工作中，我们自动挖掘科学文献并构建了CHIMERA：一个大规模的重组实例知识库（KB）。CHIMERA可用于大规模实证探索科学家如何重组概念并从不同领域汲取灵感，或用于训练监督机器学习模型，这些模型学习预测新的跨领域创新方向。为了构建这个知识库，我们提出了一个从科学论文摘要中提取重组的新颖信息提取任务，收集了一个包含数百个手动标注摘要的高质量语料库，并用它来训练一个基于LLM的提取模型。该模型应用于AI领域的大量论文语料库，生成了一个包含超过28K个重组实例的知识库。我们分析CHIMERA，以探索AI不同子领域中重组的属性。最后，我们利用该知识库训练了一个科学假设生成模型，该模型预测了现实世界研究人员认为具有启发性的新重组方向。我们的数据和代码可在 https://github.cs.huji.ac.il/tomhope-lab/CHIMERA 获取。",
    "keywords": [
      "Recombination",
      "Knowledge Base",
      "Information Extraction",
      "LLM",
      "AI"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2025-05-27T06:36:04.000Z",
    "download_time": "2025-05-29 07:09:44",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20779.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20779",
      "arxiv_url": "https://arxiv.org/abs/2505.20779"
    }
  },
  {
    "id": "2505.22523",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22523",
    "title": "PrismLayers：高质量多层透明图像生成模型的开放数据",
    "summary": "从文本提示生成高质量的多层透明图像可以开启新的创意控制层面，使用户能够像编辑大模型输出的文本一样轻松地编辑每个图层。然而，由于缺乏大型、高质量的多层透明数据语料库，多层生成模型的发展落后于传统的文本到图像模型。在本文中，我们通过以下方式解决了这一根本挑战：(i) 发布了首个开放、超高保真度的 PrismLayers (PrismLayersPro) 数据集，包含 200K (20K) 张具有精确 alpha 遮罩的多层透明图像，(ii) 引入了一种免训练的合成流程，该流程使用现有的扩散模型按需生成此类数据，以及 (iii) 提供了一个强大的开源多层生成模型 ART+，其美学效果与现代文本到图像生成模型相媲美。主要的技术贡献包括：LayerFLUX，它擅长生成具有精确 alpha 遮罩的高质量单层透明图像，以及 MultiLayerFLUX，它在人工标注的语义布局指导下，将多个 LayerFLUX 的输出合成为完整图像。为了确保更高质量，我们应用了严格的过滤阶段来去除伪影和语义不匹配，随后进行人工筛选。在合成的 PrismLayersPro 上对最先进的 ART 模型进行微调，得到了 ART+，在头对头用户研究比较中，ART+ 在 60% 的情况下优于原始 ART，甚至可与 FLUX.1-[dev] 模型生成的图像达到相同的视觉质量。我们预计，我们的工作将为多层透明图像生成任务奠定坚实的数据集基础，从而推动需要精确、可编辑且具有视觉吸引力的分层图像的研究和应用。",
    "keywords": [
      "多层透明图像",
      "数据集",
      "生成模型",
      "扩散模型",
      "LayerFLUX"
    ],
    "area": [
      "生成式AI",
      "计算机视觉",
      "深度学习"
    ],
    "published_time": "2025-05-28T16:09:33.000Z",
    "download_time": "2025-05-29 07:10:01",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22523.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22523",
      "arxiv_url": "https://arxiv.org/abs/2505.22523"
    }
  },
  {
    "id": "2505.22338",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22338",
    "title": "Text2Grad：基于自然语言反馈的强化学习",
    "summary": "传统的RLHF（人类反馈强化学习）通过粗粒度的标量奖励来优化语言模型，这种奖励掩盖了成功或失败背后的细粒度原因，导致学习缓慢且不透明。近期工作通过提示或反思的方式用文本批评增强了强化学习，提高了可解释性，但没有触及模型参数。我们引入了 Text2Grad，这是一种将自由形式文本反馈转化为跨度级梯度的强化学习范式。Text2Grad 接收人类（或程序化）批评，将每个反馈短语与相关的 token 跨度对齐，将这些对齐转换为可微分的奖励信号，并执行梯度更新，直接改进模型策略中存在问题的部分。这产生了精确的、基于反馈的调整，而非全局的微调。Text2Grad 通过三个组件实现：（1）一个高质量的反馈标注流程，将批评与 token 跨度配对；（2）一个细粒度奖励模型，在生成解释性批评的同时预测答案的跨度级奖励；（3）一个跨度级策略优化器，反向传播自然语言梯度。在摘要生成、代码生成和问答任务中，Text2Grad 持续超越标量奖励强化学习和仅提示的基线方法，提供了更高的任务指标和更丰富的可解释性。我们的结果表明，自然语言反馈在转化为梯度后，是进行细粒度策略优化的有力信号。",
    "keywords": [
      "Text2Grad",
      "自然语言反馈",
      "强化学习",
      "跨度级梯度",
      "细粒度优化"
    ],
    "area": [
      "自然语言处理",
      "机器学习",
      "生成式AI"
    ],
    "published_time": "2025-05-28T13:23:49.000Z",
    "download_time": "2025-05-29 07:10:15",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22338.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22338",
      "arxiv_url": "https://arxiv.org/abs/2505.22338"
    }
  },
  {
    "id": "2505.22203",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22203",
    "title": "基于规则和基于模型的验证器的弊端——以数学推理为例的案例研究",
    "summary": "可信的验证器对于带有可验证奖励的强化学习（RLVR）的成功至关重要，RLVR 是 DeepSeek-R1 等各种大型推理模型背后的核心方法。在数学推理等复杂领域，基于规则的验证器在以往工作中被广泛采用，用于训练强大的推理模型。然而，这些验证器的可靠性及其对强化学习训练过程的影响仍缺乏深入了解。在这项工作中，我们以数学推理为例，在静态评估和强化学习训练场景下对各种验证器进行了全面分析。首先，我们发现当前开源的基于规则的验证器在多个常用数学数据集中常常无法识别以不同格式表达的等价答案，导致不可忽略的假阴性率。这一局限性对强化学习训练性能产生不利影响，并且随着策略模型的增强而变得更加突出。随后，我们探讨基于模型的验证器作为解决这些局限性的潜在方案。虽然静态评估表明基于模型的验证器取得了显著更高的验证准确率，但进一步分析和强化学习训练结果表明它们极易被“攻击”（hacking），即将回复中的某些特定模式错误地分类为正确（即假阳性）。这种漏洞在策略模型优化过程中被利用，导致奖励被人为夸大。我们的研究结果强调了基于规则和基于模型的验证器各自固有的独特风险，旨在为开发更稳健的强化学习奖励系统提供宝贵见解。",
    "keywords": [
      "强化学习",
      "验证器",
      "数学推理",
      "可信度",
      "假阳性"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "大模型"
    ],
    "published_time": "2025-05-28T10:28:41.000Z",
    "download_time": "2025-05-29 07:10:33",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22203.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22203",
      "arxiv_url": "https://arxiv.org/abs/2505.22203"
    }
  },
  {
    "id": "2505.21876",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21876",
    "title": "EPiC：基于精确锚点视频引导的高效视频相机控制学习",
    "summary": "视频扩散模型（VDMs）中用于3D相机控制的最新方法通常通过从估计的点云中根据标注的相机轨迹进行渲染来创建锚点视频，以此作为结构化先验信息来引导扩散模型。然而，点云估计中固有的误差经常导致锚点视频不准确。此外，对大量相机轨迹标注的需求进一步增加了资源投入。为了解决这些限制，我们引入了 EPiC，一种高效精确的相机控制学习框架，它无需昂贵的相机轨迹标注即可自动构建高质量的锚点视频。具体而言，我们通过基于首帧可见性来遮罩源视频，从而为训练创建高精度的锚点视频。这种方法确保了高对齐度，消除了相机轨迹标注的需求，因此可以轻松应用于任意“野外”视频以生成图像到视频（I2V）训练对。此外，我们引入了 Anchor-ControlNet，一个轻量级条件模块，它将锚点视频的指导信息集成到预训练的VDMs中，仅使用不到骨干模型参数的1%。通过结合所提出的锚点视频数据和 ControlNet 模块，EPiC 实现了高效训练，显著减少了参数、训练步骤和数据需求，并且无需像通常那样为了减轻渲染不对齐问题而修改扩散模型骨干。尽管在基于遮罩的锚点视频上进行训练，我们的方法在推理时对使用点云制作的锚点视频表现出强大的泛化能力，从而实现了精确的3D感知相机控制。EPiC 在 RealEstate10K 和 MiraData 数据集上的 I2V 相机控制任务中取得了 SOTA 性能，定量和定性地展示了精确而稳健的相机控制能力。值得注意的是，EPiC 还对视频到视频场景表现出极强的零样本泛化能力。",
    "keywords": [
      "视频相机控制",
      "扩散模型",
      "锚点视频",
      "ControlNet",
      "图像到视频 (I2V)"
    ],
    "area": [
      "计算机视觉",
      "深度学习",
      "生成式AI"
    ],
    "published_time": "2025-05-28T01:45:26.000Z",
    "download_time": "2025-05-29 07:10:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21876.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21876",
      "arxiv_url": "https://arxiv.org/abs/2505.21876"
    }
  },
  {
    "id": "2505.18700",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18700",
    "title": "GRE 套件：基于微调视觉-语言模型和增强推理链的地理定位推断",
    "summary": "视觉语言模型 (VLM) 近期在视觉推理任务中展现出卓越性能。然而，地理定位任务具有独特挑战，需要从图像中提取多粒度视觉线索，并将其与外部世界知识结合以进行系统推理。当前地理定位方法常缺乏鲁棒推理机制和可解释性，限制了其有效性。为解决这些局限，我们提出地理推理增强 (GRE) 套件，这是一个新颖框架，通过结构化推理链增强 VLM，实现准确且可解释的位置推断。GRE 套件在数据集、模型和基准三个关键维度上进行系统开发。首先，我们引入高质量地理定位推理数据集 GRE30K，旨在促进细粒度视觉和上下文分析。其次，我们提出 GRE 模型，采用多阶段推理策略，逐步推断场景属性、局部细节和语义特征，以更高精度缩小潜在地理区域。最后，我们构建地理推理评估基准 (GREval-Bench)，该综合框架评估 VLM 在多样城市、自然和地标场景中的表现，衡量粗粒度（如国家、大陆）和细粒度（如城市、街道）定位性能。实验结果表明，GRE 在所有粒度的地理定位任务上均显著优于现有方法，凸显了推理增强 VLM 在复杂地理推断中的有效性。代码和数据将在 https://github.com/Thorin215/GRE 发布。",
    "keywords": [
      "Geo-localization",
      "Vision-Language Models",
      "Reasoning",
      "GRE30K",
      "GREval-Bench"
    ],
    "area": [
      "多模态",
      "计算机视觉",
      "深度学习"
    ],
    "published_time": "2025-05-24T13:48:57.000Z",
    "download_time": "2025-05-29 07:11:06",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18700.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18700",
      "arxiv_url": "https://arxiv.org/abs/2505.18700"
    }
  },
  {
    "id": "2505.17870",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17870",
    "title": "正如人类需要疫苗，模型亦然：模型免疫以对抗谬误",
    "summary": "生成式AI模型通常会学习并复制其训练语料库中存在的虚假信息。本立场文件认为，类比于生物免疫（通过受控接触弱化的病原体来建立免疫力），AI模型应该在一个小型的、隔离的、包含明确标记的谬误集合上进行微调，以此作为对抗错误信息的“疫苗”。这些精心策划的虚假示例在微调过程中周期性地注入，增强了模型识别和拒绝误导性主张的能力，同时保持了其在真实输入上的准确性。一个说明性案例研究表明，经过免疫处理的模型产生的错误信息显著少于基线模型。据我们所知，这是第一个将经过事实核查的谬误本身视为监督式疫苗的训练框架，而不是依赖于输入扰动或通用的人类反馈信号，旨在增强模型抵抗未来错误信息的能力。我们还概述了伦理保障和治理控制措施，以确保虚假数据的安全使用。模型免疫为使AI系统符合事实提供了一种主动范式。",
    "keywords": [
      "Model Immunization",
      "虚假信息",
      "生成式AI",
      "微调",
      "事实核查谬误"
    ],
    "area": [
      "生成式AI",
      "机器学习",
      "自然语言处理"
    ],
    "published_time": "2025-05-23T13:20:23.000Z",
    "download_time": "2025-05-29 07:11:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17870.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17870",
      "arxiv_url": "https://arxiv.org/abs/2505.17870"
    }
  },
  {
    "id": "2505.15813",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.15813",
    "title": "元学习一种人类高级视觉皮层的上下文学习Transformer模型",
    "summary": "理解高级视觉皮层内的功能表征是计算神经科学中的一个基本问题。尽管在大型数据集上预训练的人工神经网络在表征上与人类神经反应表现出显著的一致性，但学习视觉皮层的图像可计算模型依赖于个体层面的大型fMRI数据集。昂贵、耗时且通常不切实际的数据采集的必要性限制了编码器对新受试者和刺激的泛化能力。BraInCoRL利用上下文学习，从少样本示例中预测体素级神经反应，无需对新受试者和刺激进行额外的微调。我们利用一种Transformer架构，该架构可以灵活地基于可变数量的上下文图像刺激进行条件化，并在多个受试者上学习一种归纳偏置。在训练过程中，我们明确地针对上下文学习优化模型。通过联合基于图像特征和体素激活进行条件化，我们的模型学会直接生成性能更好的人类高级视觉皮层体素级模型。我们证明了BraInCoRL在低数据量条件下评估全新图像时，始终优于现有的体素级编码器设计，同时还表现出强大的测试时扩展行为。该模型还可以泛化到一个全新的视觉fMRI数据集，该数据集使用了不同的受试者和fMRI数据采集参数。此外，BraInCoRL通过关注语义相关的刺激，有助于更好地解释高级视觉皮层中的神经信号。最后，我们展示了我们的框架能够实现从自然语言查询到体素选择性的可解释映射。",
    "keywords": [
      "元学习",
      "上下文学习",
      "Transformer",
      "人类高级视觉皮层",
      "fMRI"
    ],
    "area": [
      "机器学习",
      "深度学习",
      "计算机视觉"
    ],
    "published_time": "2025-05-21T17:59:41.000Z",
    "download_time": "2025-05-29 07:11:43",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15813.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.15813",
      "arxiv_url": "https://arxiv.org/abs/2505.15813"
    }
  },
  {
    "id": "2505.21960",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21960",
    "title": "单程票：用于蒸馏文本到图像扩散模型的时间独立统一编码器",
    "summary": "文本到图像 (T2I) 扩散模型在生成建模方面取得了显著进展；然而，它们面临推理速度和图像质量之间的权衡，给高效部署带来了挑战。现有的蒸馏T2I模型虽然能以更少的采样步生成高保真图像，但往往在多样性和质量方面表现不佳，特别是在一步模型中。通过我们的分析，我们观察到UNet编码器存在冗余计算。我们的研究结果表明，对于T2I扩散模型而言，解码器更善于捕捉更丰富、更显式的语义信息，而编码器可以有效地在来自不同时间步的解码器之间共享。基于这些观察，我们引入了第一个时间独立统一编码器 TiUE，用于学生模型UNet架构，这是一种用于蒸馏T2I扩散模型的无循环图像生成方法。通过单次通过方案，TiUE在多个解码器时间步之间共享编码器特征，从而实现并行采样并显著降低推理时间复杂度。此外，我们引入了一个KL散度项来正则化噪声预测，从而增强生成图像的感知真实性和多样性。实验结果表明，TiUE优于现有的最先进方法，包括LCM、SD-Turbo和SwiftBrushv2，在保持计算效率的同时，产生了更多样化和更真实的结果。",
    "keywords": [
      "T2I扩散模型",
      "模型蒸馏",
      "高效推理",
      "时间独立统一编码器 (TiUE)",
      "单次采样"
    ],
    "area": [
      "生成式AI",
      "深度学习",
      "计算机视觉"
    ],
    "published_time": "2025-05-28T04:23:22.000Z",
    "download_time": "2025-05-29 07:11:54",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21960.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21960",
      "arxiv_url": "https://arxiv.org/abs/2505.21960"
    }
  },
  {
    "id": "2505.21191",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21191",
    "title": "揭示指令专用神经元与专家：大型语言模型指令遵循能力的分析框架",
    "summary": "大型语言模型（LLMs）的微调显著提升了其遵循指令的能力，然而驱动这些改进的底层计算机制仍不为人知。本研究通过分离和分析指令专用的稀疏组件，即密集模型中的神经元以及专家混合模型（MoE）架构中的神经元和专家，系统地考察了微调如何重新配置LLM的计算过程。特别地，我们引入了HexaInst，一个精心策划且平衡的、涵盖六个不同类别的指令数据集；并提出了SPARCOM，一个新颖的分析框架，包含三个主要贡献：（1）一种识别这些稀疏组件的方法，（2）评估其功能通用性和唯一性，以及（3）系统比较其变化。通过实验，我们证明了这些组件的功能通用性、唯一性及其在指令执行中的关键作用。通过阐明微调引起的适应性与稀疏计算基底之间的关系，这项工作为可信赖的大型语言模型社区深入了解LLMs如何内化指令遵循行为提供了深刻见解。",
    "keywords": [
      "大模型",
      "指令遵循",
      "微调",
      "稀疏组件",
      "分析框架"
    ],
    "area": [
      "大模型",
      "深度学习",
      "自然语言处理"
    ],
    "published_time": "2025-05-27T13:40:28.000Z",
    "download_time": "2025-05-29 07:12:08",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21191.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21191",
      "arxiv_url": "https://arxiv.org/abs/2505.21191"
    }
  },
  {
    "id": "2505.20715",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20715",
    "title": "MUSEG：通过时间戳感知多片段对齐增强视频时间理解",
    "summary": "视频时间理解对于多模态大型语言模型（MLLMs）推理视频中的事件至关重要。尽管通用视频理解取得了最新进展，但当前的多模态大模型在细粒度时间推理方面仍然存在困难。尽管最近探索了强化学习（RL）来解决此问题，但现有的强化学习方法效果仍然有限。在这项工作中，我们提出了 MUSEG，一种新颖的基于强化学习的方法，通过引入时间戳感知多片段对齐来增强时间理解。MUSEG 使多模态大模型能够将查询与多个相关的视频片段对齐，从而促进更全面的时间推理。为了促进有效学习，我们设计了一种定制的强化学习训练策略，采用分阶段奖励，逐步引导模型进行时间定位推理。在时间对齐和时间敏感的视频问答任务上的大量实验表明，MUSEG 显著优于现有方法，并在不同的时间理解场景中表现出良好的泛化能力。",
    "keywords": [
      "多片段对齐",
      "视频时间理解",
      "MLLMs",
      "强化学习",
      "时间推理"
    ],
    "area": [
      "多模态",
      "大模型",
      "视频理解"
    ],
    "published_time": "2025-05-27T04:50:07.000Z",
    "download_time": "2025-05-29 07:12:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20715.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20715",
      "arxiv_url": "https://arxiv.org/abs/2505.20715"
    }
  },
  {
    "id": "2505.17507",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17507",
    "title": "基于Hugging Face知识图谱的推荐、分类与追踪基准测试",
    "summary": "开源机器学习(ML)资源的快速增长，如模型和数据集，加速了信息检索(IR)研究。然而，Hugging Face等现有平台并未明确利用结构化表示，限制了诸如追踪模型演变、推荐相关数据集等高级查询和分析。为了弥补这一空白，我们构建了HuggingKG，这是首个为ML资源管理而从Hugging Face社区构建的大规模知识图谱。HuggingKG拥有260万个节点和620万条边，捕获了领域特定关系和丰富的文本属性。基于此，我们进一步提出了HuggingBench，一个包含三个新颖测试集的多任务基准，用于IR任务，包括资源推荐、分类和追踪。我们的实验揭示了HuggingKG和衍生任务的独特特性。这两个资源均已公开发布，有望推动开源资源共享与管理领域的研究。",
    "keywords": [
      "Hugging Face",
      "知识图谱",
      "基准测试",
      "推荐",
      "分类"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "自然语言处理"
    ],
    "published_time": "2025-05-23T06:00:20.000Z",
    "download_time": "2025-05-29 07:12:37",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17507.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17507",
      "arxiv_url": "https://arxiv.org/abs/2505.17507"
    }
  },
  {
    "id": "2505.12667",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.12667",
    "title": "Safe-Sora：基于图形水印的安全文本到视频生成",
    "summary": "生成式视频模型的爆炸性增长加剧了对人工智能生成内容可靠版权保护的需求。尽管隐形生成水印在图像合成领域广受欢迎，但在视频生成领域的研究却相对不足。为弥合这一空白，我们提出了 Safe-Sora，这是首个将图形水印直接嵌入视频生成过程的框架。基于水印性能与水印和载体内容（即视频）之间视觉相似度密切相关的观察，我们引入了一种层级式由粗到精的自适应匹配机制。具体而言，水印图像被分割成块，每个块被分配给最相似的视频帧，并进一步定位到最佳空间区域以实现无缝嵌入。为了实现水印块在视频帧间的时空融合，我们开发了一种结合 3D 小波变换增强的 Mamba 架构，并采用了新颖的时空局部扫描策略，在水印嵌入和提取过程中有效建模长程依赖关系。据我们所知，这是首次将状态空间模型应用于水印技术，为高效且鲁棒的水印保护开辟了新途径。广泛的实验表明，Safe-Sora 在视频质量、水印保真度及鲁棒性方面取得了最先进的性能，这主要归功于我们提出的方法。我们的代码将在论文发表后发布。",
    "keywords": [
      "Safe-Sora",
      "图形水印",
      "生成式视频",
      "Mamba 架构",
      "鲁棒性"
    ],
    "area": [
      "生成式AI",
      "计算机视觉",
      "多模态"
    ],
    "published_time": "2025-05-19T03:31:31.000Z",
    "download_time": "2025-05-29 07:12:56",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12667.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.12667",
      "arxiv_url": "https://arxiv.org/abs/2505.12667"
    }
  },
  {
    "id": "2505.22645",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22645",
    "title": "表征偏见：简体中文与繁体中文大语言模型基准测试",
    "summary": "尽管大语言模型（LLM）在简体中文和繁体中文环境下的能力已有所研究，但当使用这两种书面中文变体进行提示时，LLM是否表现出差异化的性能尚不明确。了解这一点至关重要，因为LLM响应质量的差异可能会忽略简体中文与繁体中文背后不同的文化背景，从而持续存在表征性危害，并可能加剧LLM驱动的决策（例如教育或招聘领域）所带来的下游危害。为了探究潜在的LLM性能差异，我们设计了两个反映真实场景的基准任务：区域术语选择（提示LLM命名在大陆和台湾地区称谓不同的物品）和区域名称选择（提示LLM从包含简体和繁体中文姓名列表中选择招聘对象）。在这两个任务中，我们评估了11种领先的商业LLM服务和开源模型的性能——这些模型主要训练于英文、简体中文或繁体中文。我们的分析表明，LLM响应中的偏见取决于任务和提示语言：在区域术语选择任务中，大多数LLM不成比例地偏向于简体中文响应；而在区域名称选择任务中，它们却出人意料地偏向于繁体中文姓名。我们发现，这些差异可能源于训练数据表示、书写字符偏好以及简体中文和繁体中文的分词（tokenization）方式的不同。这些发现强调了进一步分析LLM偏见的必要性；为此，我们提供了一个开源的基准数据集，以促进未来LLM在中国语言变体中的行为进行可复现的评估（https://github.com/brucelyu17/SC-TC-Bench）。",
    "keywords": [
      "大语言模型",
      "简体中文",
      "繁体中文",
      "偏见",
      "基准测试"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "人工智能"
    ],
    "published_time": "2025-05-28T17:56:49.000Z",
    "download_time": "2025-05-29 07:13:12",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22645.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22645",
      "arxiv_url": "https://arxiv.org/abs/2505.22645"
    }
  },
  {
    "id": "2505.21582",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21582",
    "title": "AITEE -- 电气工程智能体导师",
    "summary": "智能辅导系统结合大型语言模型为满足学生的多元化需求和促进自主高效学习提供了一种有前景的方法。尽管大型语言模型具备良好的电气工程基础知识，但在解决与电路相关的具体问题方面能力仍显不足。在本文中，我们介绍了 AITEE，一个基于智能体的电气工程辅导系统，旨在全程陪伴学生学习过程，提供个性化支持，并促进自主学习。AITEE 通过一种改进的电路重建过程支持手绘和数字电路，从而实现与学生的自然交互。我们新颖的基于图的相似性度量通过检索增强生成方法从讲义中识别相关上下文，而并行 Spice 仿真进一步提高了应用解题方法的准确性。该系统采用苏格拉底式对话来通过引导式提问培养学习者的自主性。实验评估表明，AITEE 在领域特定知识应用方面显著优于基线方法，即使是中等规模的 LLM 模型也表现出可接受的性能。我们的结果突出了智能体导师在电气工程教育中提供可扩展、个性化和有效学习环境的潜力。",
    "keywords": [
      "智能辅导系统",
      "大模型",
      "智能体",
      "电气工程",
      "检索增强生成 (RAG)"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-05-27T10:07:05.000Z",
    "download_time": "2025-05-29 07:13:28",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21582.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21582",
      "arxiv_url": "https://arxiv.org/abs/2505.21582"
    }
  },
  {
    "id": "2505.20444",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20444",
    "title": "HoPE：用于视觉-语言模型长度泛化的混合位置编码",
    "summary": "视觉-语言模型（VLM）在多模态任务中取得了显著进展。然而，在长上下文场景，特别是长视频中，其性能常常下降。尽管旋转位置编码（RoPE）已广泛用于大型语言模型（LLM）的长度泛化，但将传统的RoPE扩展以捕获视频中复杂的时空依赖性仍然是一个未解决的挑战。现有方法通常在RoPE内分配不同频率来编码3D位置信息。然而，这些分配策略主要依靠启发式方法，缺乏深入的理论分析。在本文中，我们首先研究了不同的分配策略如何影响VLM的长上下文能力。我们的分析表明，当前的多模态RoPE无法在扩展上下文中可靠地捕获语义相似性。为了解决这个问题，我们提出了HoPE，一种混合位置编码，旨在提高VLM的长上下文能力。HoPE引入了一种混合频率分配策略，用于在任意长上下文上进行可靠的语义建模，以及一种动态时间尺度机制，以促进在不同上下文长度下的鲁棒学习和灵活推理。在四个视频基准上进行的关于长视频理解和检索任务的广泛实验表明，HoPE持续优于现有方法，证实了其有效性。代码可在 https://github.com/hrlics/HoPE 获取。",
    "keywords": [
      "Vision-Language Models",
      "Length Generalization",
      "HoPE",
      "Long Video Understanding",
      "Position Embedding"
    ],
    "area": [
      "深度学习",
      "多模态",
      "视频理解"
    ],
    "published_time": "2025-05-26T18:37:40.000Z",
    "download_time": "2025-05-29 07:13:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20444.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20444",
      "arxiv_url": "https://arxiv.org/abs/2505.20444"
    }
  },
  {
    "id": "2505.20298",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20298",
    "title": "MangaVQA 和 MangaLMM: 一个用于多模态漫画理解的基准和专用模型",
    "summary": "漫画，即日本漫画，是一种丰富的多模态叙事形式，它以复杂的方式融合了图像和文本。教导大型多模态模型（LMMs）达到人类水平理解此类叙事，有助于漫画创作者反思和改进他们的故事。为此，我们引入了两个用于多模态漫画理解的基准：MangaOCR，用于页面内文本识别；以及 MangaVQA，一个旨在通过视觉问答评估上下文理解的新基准。MangaVQA 包含 526 对高质量、手动构建的问答对，可在不同的叙事和视觉情境下实现可靠评估。基于这些基准，我们开发了 MangaLMM，一个从开源 LMM Qwen2.5-VL 进行微调的、漫画专用的模型，用以联合处理这两项任务。通过广泛的实验，包括与 GPT-4o 和 Gemini 2.5 等专有模型的比较，我们评估了 LMMs 理解漫画的能力。我们的基准和模型为评估和推进 LMMs 在漫画这一丰富的叙事领域提供了全面的基础。",
    "keywords": [
      "多模态",
      "漫画理解",
      "LMMs",
      "基准",
      "视觉问答"
    ],
    "area": [
      "人工智能",
      "多模态",
      "大模型"
    ],
    "published_time": "2025-05-26T17:59:59.000Z",
    "download_time": "2025-05-29 07:14:00",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20298.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20298",
      "arxiv_url": "https://arxiv.org/abs/2505.20298"
    }
  },
  {
    "id": "2505.19051",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19051",
    "title": "通过影响力蒸馏实现大规模高效数据选择",
    "summary": "有效的数据选择对于现代大型语言模型 (LLMs) 的高效训练至关重要。本文介绍了影响力蒸馏（Influence Distillation），这是一种新颖且具有数学依据的数据选择框架，它利用二阶信息来最优地加权训练样本。通过蒸馏每个样本对目标分布的影响，我们的方法分配模型特定的权重，这些权重用于选择 LLM 微调的训练数据，从而引导模型在目标领域实现优异性能。我们推导了针对梯度下降（Gradient Descent）和 Adam 优化器的最优权重。为了确保可扩展性并降低计算成本，我们提出了一种基于地标的近似方法：精确计算一小部分“地标”样本的影响，然后将其高效地传播到所有其他样本，以确定它们的权重。我们通过将影响力蒸馏应用于 Tulu V2 数据集上的指令微调来验证其有效性，目标任务包括 GSM8k、SQuAD 和 MMLU，涵盖了 Llama 和 Qwen 系列的多种模型。实验表明，影响力蒸馏在实现高达 3.5 倍更快的选择速度的同时，能达到或超越现有最佳性能。",
    "keywords": [
      "大型语言模型",
      "数据选择",
      "影响力蒸馏",
      "模型微调",
      "高效训练"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "深度学习"
    ],
    "published_time": "2025-05-25T09:08:00.000Z",
    "download_time": "2025-05-29 07:14:14",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19051.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19051",
      "arxiv_url": "https://arxiv.org/abs/2505.19051"
    }
  },
  {
    "id": "2505.21060",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21060",
    "title": "Styl3R：任意场景和风格的即时3D风格化重建",
    "summary": "即时对3D场景进行风格化，同时保持多视角一致性并忠实地再现风格图像，仍然是一个重大挑战。当前最先进的3D风格化方法通常涉及计算密集型的测试时优化，以便将艺术特征迁移到预训练的3D表示中，并且通常需要密集的带姿态输入图像。相比之下，利用前向重建模型的最新进展，我们展示了一种新颖的方法，可以使用无姿态的稀疏视角场景图像和任意风格图像在不到一秒钟内实现直接3D风格化。为了解决重建和风格化之间固有的解耦问题，我们引入了一种分支架构，将结构建模和外观着色分离，有效防止风格迁移扭曲底层的3D场景结构。此外，我们采用了一种恒等损失（identity loss），以便通过新颖视角合成任务来促进我们风格化模型的预训练。这种策略还使得我们的模型在针对风格化进行微调的同时，能够保留其原始的重建能力。使用域内和域外数据集进行的全面评估表明，我们的方法能够生成高质量的风格化3D内容，实现了风格和场景外观的优越融合，同时在多视角一致性和效率方面也优于现有方法。",
    "keywords": [
      "3D Stylization",
      "即时",
      "Multi-view consistency",
      "3D Reconstruction",
      "前向模型"
    ],
    "area": [
      "计算机视觉",
      "深度学习",
      "生成式AI"
    ],
    "published_time": "2025-05-27T11:47:15.000Z",
    "download_time": "2025-05-29 07:14:32",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21060.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21060",
      "arxiv_url": "https://arxiv.org/abs/2505.21060"
    }
  },
  {
    "id": "2505.18149",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18149",
    "title": "先完成搜索：大型语言模型中的高效测试时缩放",
    "summary": "测试时缩放（TTS）涉及在推理过程中动态分配计算资源，提供了一种改进大型语言模型推理能力的有前景的方法。尽管现有的 TTS 方法效果不错，但它们通常依赖于长解码路径或需要生成大量样本，这增加了 token 用量和推理延迟。我们观察到一个令人惊讶的事实：对于推理任务，较短的轨迹（traces）比更长的轨迹更有可能正确。受此启发，我们引入了先完成搜索（FFS），这是一种无需训练的并行解码策略，它并行启动 n 个独立样本，并在任何一个完成时立即返回。我们在四种推理模型（DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B 和 Phi-4-Reasoning-Plus）和四个数据集（AIME24, AIME25-I, AIME25-II 和 GPQA Diamond）上，将 FFS 与简单解码、集束搜索、多数投票和预算强制等方法进行了评估。使用 DeepSeek-R1 模型时，FFS 在 AIME 数据集上达到了 82.23% 的准确率，比 DeepSeek-R1 的独立准确率提高了 15%，几乎与 OpenAI 的 o4-mini 性能相当。我们的理论分析解释了为什么在最短轨迹处停止更可能获得正确答案，并指出了早期停止可能不是最优的情况。FFS 的优雅和简洁表明，简单有效的 TTS 策略也能表现出色，揭示了推理时简单方法的巨大潜力。",
    "keywords": [
      "Test-Time Scaling",
      "First Finish Search",
      "Parallel Decoding",
      "大语言模型",
      "推理"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "人工智能"
    ],
    "published_time": "2025-05-23T17:57:43.000Z",
    "download_time": "2025-05-29 07:14:44",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18149.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18149",
      "arxiv_url": "https://arxiv.org/abs/2505.18149"
    }
  },
  {
    "id": "2505.18227",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18227",
    "title": "令牌缩减应超越生成模型中的效率考量 -- 从视觉、语言到多模态",
    "summary": "在Transformer架构中，tokens（记号）——源自原始数据的离散单元——通过将输入分割成定长块来形成。每个记号随后被映射到一个嵌入向量，从而在保留输入基本信息的同时，实现并行的注意力计算。由于Transformer自注意力机制的二次方计算复杂度，记号缩减主要被用作一种效率策略。这在单一的视觉和语言领域尤为如此，它有助于平衡计算成本、内存使用和推理延迟。尽管取得了这些进展，本文认为在大型生成模型时代，记号缩减应超越其传统的效率导向作用。相反，我们将其定位为生成建模中的一个基本原则，对模型架构和更广泛的应用产生关键影响。具体而言，我们主张在跨越视觉、语言和多模态系统中，记号缩减能够：(i) 促进更深度的多模态融合与对齐，(ii) 减轻“过度思考”和幻觉，(iii) 在长输入序列中保持连贯性，以及(iv) 增强训练稳定性等。我们将记号缩减重新定义为不仅仅是一种效率衡量。通过这样做，我们概述了有前景的未来方向，包括算法设计、强化学习引导的记号缩减、用于上下文学习的记号优化，以及更广泛的机器学习和科学领域。我们强调了它在推动新的模型架构和学习策略方面的潜力，这些架构和策略能够提高鲁棒性、增强可解释性，并更好地与生成建模的目标对齐。",
    "keywords": [
      "令牌缩减",
      "生成模型",
      "Transformer",
      "多模态",
      "基本原则"
    ],
    "area": [
      "深度学习",
      "多模态",
      "生成式AI"
    ],
    "published_time": "2025-05-23T11:30:30.000Z",
    "download_time": "2025-05-29 07:15:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18227.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18227",
      "arxiv_url": "https://arxiv.org/abs/2505.18227"
    }
  }
]