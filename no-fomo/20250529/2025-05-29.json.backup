[
  {
    "id": "Twitterf72c2679-4da6-41bf-bc55-4b99d14cc5b8",
    "source": "Twitter",
    "url": "https://x.com/karpathy/status/1926812469810368669",
    "title": "Andrej Karpathy on X: \"@kalomaze Deep Learning horror genre 🫣 That fear of a kwarg that isn’t set right, not erroring, only silently making your results slightly worse.\"",
    "content": "Deep Learning horror genre 🫣 That fear of a kwarg that isn’t set right, not erroring, only silently making your results slightly worse.",
    "summary": "深度学习恐怖类型：担心某个参数没有设置正确，没有出现错误，而只是默默地让结果变得稍差。",
    "keywords": "深度学习,恐怖,参数,错误,结果",
    "area": "人工智能,深度学习,生成式AI",
    "published_time": "2025-05-26T09:27:00Z",
    "download_time": "2023-10-06 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitterf72c2679-4da6-41bf-bc55-4b99d14cc5b8.png"
    ]
  },
  {
    "id": "Twitter83aaa8ef-70a6-4643-8d04-80a47de0807a",
    "source": "Twitter",
    "url": "https://x.com/jxmnop/status/1927385194601886065",
    "title": "> be me, first year PhD student > get obsessed with polyphonic music transcription during pandemic > for some reason, advisor lets me work on music",
    "content": "> be me, first year PhD student > get obsessed with polyphonic music transcription during pandemic > for some reason, advisor lets me work on music > brilliant idea: explicitly encode chord structure into the model > spend months implementing complicated chord-aware audio model in PyTorch > aggressively monopolize all GPUs in the department to train model > finally beat transcription baseline, feeling unstoppable > convince self about to revolutionize the music industry > wake up to new Google paper on music transcription > it's about MT3, a transformer on thousands of hours of audio > zero explicit structure > just a transformer + lots of data > blows my (piano-specific) CNN out of the water > bitter_lesson.png > mogged again by Rich Sutton",
    "summary": "一位第一年博士生在大流行期间迷上了复调音乐转录，导师让他研究音乐。他想法绝妙，尝试在模型中显式编码和弦结构，并长时间用PyTorch实现复杂的和弦感知音频模型，占用了部门所有GPU进行训练，终于打败了转录基线。他自信将要在音乐产业引发变革，却被谷歌的新论文打脸，该论文介绍了一种基于成千上万小时音频训练的MT3 transformer，无需显式结构，仅需大量数据，直接把他专为钢琴设计的CNN比了下去。",
    "keywords": "博士生,复调音乐,和弦结构,PyTorch,MT3 transformer",
    "area": "人工智能,深度学习,音频理解",
    "published_time": "2025-05-27T23:23:00Z",
    "download_time": "2023-10-11 10:42:58",
    "visual_resource": [
      "screenshot/twitter_Twitter83aaa8ef-70a6-4643-8d04-80a47de0807a.png"
    ]
  },
  {
    "id": "Twittera19db12c-0a3c-48fe-a2a1-8245388c73bf",
    "source": "Twitter",
    "url": "https://x.com/scaling01/status/1926801814973804712",
    "title": "Remember the Great Ghiblification?",
    "content": "Remember the Great Ghiblification? Turns out, this is all part of a grander plan by OpenAI to make them appear cool and to win market share. Search, DeepResearch, Agents and Personlization with System Prompts, Tasks and the coming model unification are also part of that plan. Here is more information from court exhibits on OpenAI's strategy: In the previous post I covered the super-assistants OpenAI plans to build in 2025. To achieve that goal and their other goal of competing in search they plan to build out infrastructure to support 1B users. Their current infrastructure can \"only\" support multiple 100 Mio. users. Furthermore, they want to redefine the brand and move away from the OpenAI naming, OpenAI should only be a name in the background like Alphabet. \"We need a simple consumer mental model. Google owns information. Amazon owns commerce. ChatGPT needs to own one clear idea: [Agents/Action/something like that] A core feature of their new strategy is to focus on the younger generation. ChatGPT wants to be \"cool\". \"Right now, it's useful but not cool\". \"The path to being cool is being part of trends on social, full stop. We need to build a communit-lead growth motion\"",
    "summary": "记得大吉化事件吗？事实证明，这是OpenAI为显得酷和赢得市场份额所策划的庞大计划的一部分。搜索、深度研究、智能体和个性化功能与系统提示、任务和即将到来的模型统一也是该计划的一部分。上次我讨论了OpenAI计划于2025年构建的超级助理。为实现这些目标，他们计划扩展支持10亿用户的基础设施，而当前的只能支持数亿用户。此外，他们想重新定义品牌，远离OpenAI这一名字，OpenAI应仅在背景中如同Alphabet。ChatGPT需拥有一个明确的核心理念，并计击重新品牌战略，目标年轻一代。\"",
    "keywords": "OpenAI,大吉化,市场份额,ChatGPT,品牌战略",
    "area": "人工智能,多模态,大模型",
    "published_time": "2025-05-26T12:34:56Z",
    "download_time": "2025-05-26 21:21:00",
    "visual_resource": [
      "screenshot/twitter_Twittera19db12c-0a3c-48fe-a2a1-8245388c73bf.png"
    ]
  },
  {
    "id": "Twitterf824f474-0e6f-4101-9a1e-fe76302c2a4d",
    "source": "Twitter",
    "url": "https://x.com/scaling01/status/1926788548155293978",
    "title": "Lisan al Gaib on X: \"On OpenAI's product strategy - super assistants, their largest competitors and their moats, based on recently revealed court exhibits. In H1 2025 OpenAI will start evolving ChatGPT into super-assistant, as models like o2 and o3 (now o3 and o4) are finally smart enough to perform https://t.co/1oEl11nq3j\" / X",
    "content": "On OpenAI's product strategy - super assistants, their largest competitors and their moats, based on recently revealed court exhibits.\nIn H1 2025 OpenAI will start evolving ChatGPT into super-assistant, as models like o2 and o3 (now o3 and o4) are finally smart enough to perform agentic tasks. A super-assistant is an intelligent entitiy with T-shaped skills. It has broad skills for tedious daily tasks and deep expertise for tasks that most people find impossible.\nOpenAI recognizes that \"growth and revenue won't line up forever\", so they need to focus their efforts first on super-assistants to generate enough monetizable demand, to pursue more expensive models in H2. Interestingly, OpenAI sees Meta as their biggest competitor, since Google is at risk of cannibalizing their own core search business (as of Dec 2024).",
    "summary": "OpenAI计划在2025年上半年将ChatGPT发展为超级助手，因其新模型o2和o3具备执行代理任务的足够智能。超级助手是一种具备广度和深度技能的智能实体，适合日常繁琐任务及复杂问题。OpenAI为应对持续增长和收益不匹配，需专注于超级助手以产生足够的市场需求，并计划在下半年推出更昂贵的模型。Meta被视为最大竞争对手，而谷歌因其搜索业务或遭自我蚕食。",
    "keywords": "OpenAI,超级助手,ChatGPT,Meta,谷歌",
    "area": "人工智能,智能体,机器人",
    "published_time": "2025-05-26T07:52:00Z",
    "download_time": "2023-10-04 14:25:00",
    "visual_resource": [
      "screenshot/twitter_Twitterf824f474-0e6f-4101-9a1e-fe76302c2a4d.png"
    ]
  },
  {
    "id": "Twitterce9d4459-4576-4c59-a9f6-01fdb23c23e1",
    "source": "Twitter",
    "url": "https://x.com/_philschmid/status/1927019039269761064",
    "title": "Last Week was full of I/O announcement. Here is one you might have missed",
    "content": "Last Week was full of I/O announcement. Here is one you might have missed 🚨 Context URL tool is a new native tool that allows Gemini to extract content from provided URLs as additional context for prompts. - Provide URLs directly in prompts, up to 20 per prompt - Can be used in combination with google search tool to search based on website content - Supported with Gemini 2.0 Flash and 2.5 Flash and Pro - Free during its experimental phase, with billing to come later",
    "summary": "上周的公告中，你可能会错过的一个是上下文URL工具。这是一种新的原生工具，允许Gemini从提供的URL中提取内容作为上下文。该工具支持每个提示最多20个URL，并可与谷歌搜索工具结合使用，搜索网站内容。它支持Gemini 2.0 Flash和2.5 Flash及Pro版本，并且在实验阶段免费，以后将收费。",
    "keywords": "上下文URL工具, Gemini, 结合谷歌搜索, 多模态, 大模型",
    "area": "多模态, 大模型, 其他",
    "published_time": "2025-05-26T23:08:00Z",
    "download_time": "2023-10-05 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitterce9d4459-4576-4c59-a9f6-01fdb23c23e1.png"
    ]
  },
  {
    "id": "Twittera173c20c-2523-4aa1-b677-b1b30477de26",
    "source": "Twitter",
    "url": "https://x.com/llama_index/status/1926996451747356976",
    "title": "LlamaIndex now supports the new OpenAI Responses API features",
    "content": "LlamaIndex now supports the new OpenAI Responses API features: · Call any remote MCP server · Use code interpreters by using it as one of the built-in-tools · AND generate images with streaming.",
    "summary": "LlamaIndex现已支持新的OpenAI Responses API功能，包括远程MCP服务器调用、内置工具中的代码解释器使用以及流媒体图像生成。",
    "keywords": "LlamaIndex, OpenAI Responses API, MCP 服务器, 代码解释器, 图像生成",
    "area": "人工智能, 多模态, 生成式AI",
    "published_time": "2025-05-26T21:38:00Z",
    "download_time": "2025-05-14 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twittera173c20c-2523-4aa1-b677-b1b30477de26.png"
    ]
  },
  {
    "id": "Twitterea192279-7c72-4e4d-becf-9576937856b8",
    "source": "Twitter",
    "url": "https://x.com/ctnzr/status/1927391895879074047",
    "title": "Nemotron-CORTEXA just reached the top of the SWEBench leaderboard",
    "content": "Nemotron-CORTEXA just reached the top of the SWEBench leaderboard for using LLMs to solve software engineering problems, solving 68.2% of SWEBench GitHub issues! It does so by using a multi-step problem localization and repair process, generating multiple proposal candidates and then choosing a final solution with an LLM. The embedding model we built for this has been released, and code will be released soon. Paper will be at ICML 2025! More information here:",
    "summary": "Nemotron-CORTEXA成功地利用LLM解决软件工程问题，解决了68.2%的SWEBench GitHub问题。该系统使用多步骤的问题定位和修复过程，生成多个提议候选方案，然后选择LLM的最终解决方案。我们构建的嵌入模型已经发布，代码将很快发布。论文将在2025年ICML上发表。",
    "keywords": "Nemotron-CORTEXA, SWEBench, LLM, 嵌入模型, ICML",
    "area": "人工智能, 机器学习, 大模型",
    "published_time": "2025-05-27T23:50:00Z",
    "download_time": "2025-10-05 14:38:00",
    "visual_resource": [
      "screenshot/twitter_Twitterea192279-7c72-4e4d-becf-9576937856b8.png"
    ]
  },
  {
    "id": "Twitterfb552d5d-782d-49ee-8f71-03be7f4e9bf6",
    "source": "Twitter",
    "url": "https://x.com/danielhanchen/status/1926966742519091327",
    "title": "Hey guys! We noticed some of you sharing screenshots and links to our DeepSeek-V3-0526 article on @UnslothAI. The link was hidden and wasn’t meant to be shared publicly or taken as a fact but it seems a few of you were scrapping through the site and uncovered it early!",
    "content": "Hey guys! We noticed some of you sharing screenshots and links to our DeepSeek-V3-0526 article on @UnslothAI. The link was hidden and wasn’t meant to be shared publicly or taken as a fact but it seems a few of you were scrapping through the site and uncovered it early! 😅 The article was originally written as speculative prep for the rumored release of the model. As of now, there’s been no official confirmation about its existence or launch. So, it was never intended for broad distribution, so sorry for any confusion this may have caused. The text in the article was simply a placeholder, copied over from our earlier V3-0324 piece. So there's definitely nothing to take from it. And yep, lesson learned! We won’t be doing this again. The hype is real, and it turns out we need to be more careful about what we draft on the site, even behind the scenes. Thanks for your understanding!",
    "summary": "我们注意到有些人分享了我们的DeepSeek-V3-0526文章的截图和链接。这篇文章原本是为传闻中的模型发布而编写的，并没有官方确认其存在或发布。我们为由此引发的任何困惑表示歉意，文章的内容仅仅是一个占位符，来自于我们早期的V3-0324版本。因此，并没有什么实际信息可以从中获取。教训：我们以后会更加小心。感谢你的理解！",
    "keywords": "DeepSeek-V3-0526,UnslothAI,文章,占位符,理解",
    "area": "人工智能,机器学习,自然语言处理",
    "published_time": "2025-05-26T19:40:00Z",
    "download_time": "2023-09-29 20:17:35",
    "visual_resource": [
      "screenshot/twitter_Twitterfb552d5d-782d-49ee-8f71-03be7f4e9bf6.png"
    ]
  },
  {
    "id": "Twittere9da3d81-69b9-4312-81ab-a0d4eefbdf50",
    "source": "Twitter",
    "url": "https://x.com/Teknium1/status/1927089897833140647",
    "title": "Finally completed and merged the SWE_RL environment",
    "content": "Finally completed and merged the SWE_RL environment that was described by Meta's SWE RL paper into Atropos - A really difficult environment that can teach a model to be a much better coding agent! Check out the PR: github.com/NousResearch/a Check out Meta's SWE-RL paper: arxiv.org/abs/2502.18449",
    "summary": "该推文宣布完成并合并了根据Meta's SWE RL论文描述的SWE_RL环境到Atropos中，这是一个非常困难的环境，可以教一个模型成为更好的编码代理！查看PR和Meta's SWE-RL论文。",
    "keywords": "SWE_RL,Meta,Atropos,模型,编码代理",
    "area": "深度学习,智能体,视频理解",
    "published_time": "2025-05-27T03:50:00Z",
    "download_time": "2023-10-08 12:00:00",
    "visual_resource": [
      "screenshot/twitter_Twittere9da3d81-69b9-4312-81ab-a0d4eefbdf50.png"
    ]
  },
  {
    "id": "Twitter5845eb56-0834-48d0-9a4d-97d3f21922ed",
    "source": "Twitter",
    "url": "https://x.com/GoogleDeepMind/status/1927375853551235160",
    "title": "SignGemma: Translating Sign Language",
    "content": "We're thrilled to announce SignGemma, our most capable model for translating sign language into spoken text. 🧏 This open model is coming to the Gemma model family later this year, opening up new possibilities for inclusive tech. Share your feedback and interest in early testing →",
    "summary": "我们很高兴推出SignGemma，这是我们最强大的模型，用于将手语翻译成口语文本。这一开放模型将在今年晚些时候加入Gemma模型家族，为包容性技术开辟新的可能性。欢迎分享您的反馈和参与早期测试。",
    "keywords": "SignGemma,手语,翻译,开放模型,包容性技术",
    "area": "人工智能,自然语言处理,多模态",
    "published_time": "2025-05-27T22:46:00Z",
    "download_time": "2023-11-01 10:00:00",
    "visual_resource": [
      "screenshot/twitter_Twitter5845eb56-0834-48d0-9a4d-97d3f21922ed.png"
    ]
  },
  {
    "id": "Twitter3d96d459-24ef-4275-b510-5545c1e68210",
    "source": "Twitter",
    "url": "https://x.com/c_valenzuelab/status/1927149229966766373",
    "title": "Infinite Use Cases and the Path to Universality",
    "content": "This is pretty wild. We wanted to ensure our models have infinite use cases that are less prescriptive and linear than the simplistic \"text-to-X\" approach. Which means that are still plenty of uses cases we have not yet discovered. Gen-4 and References feel like a step toward the universality we have as a vision.",
    "summary": "Cristóbal Valenzuela在推文中强调了模型的无限用例及其潜力，阐述了超越简单“文本到X”方法的必要性。Gen-4和References被视为朝着普遍性愿景迈出的一步。",
    "keywords": "模型,用例,超越,文本,普遍性",
    "area": "多模态,生成式AI,其他",
    "published_time": "2025-05-27T07:45:00Z",
    "download_time": "2023-10-05 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitter3d96d459-24ef-4275-b510-5545c1e68210.png"
    ]
  },
  {
    "id": "Twitter12941d16-e73d-4214-9467-d4d251754481",
    "source": "Twitter",
    "url": "https://x.com/TheTuringPost/status/1927123359969468420",
    "title": "A new recipe for training multimodal models",
    "content": "A new recipe for training multimodal models 👉 Mixed together various data types: text next to images, video frames after captions, then webpages, etc. This way the model learns to connect what it reads with what it sees. ByteDance proposed and implemented this idea in their BAGEL, a new open-source multimodal model. Here's how it works:",
    "summary": "TuringPost提出了一种新的多模态模型训练配方，将文本、图像、视频帧和网页等多种数据类型混合在一起，模型通过这种方式学会连接阅读和视觉内容。ByteDance在开源项目BAGEL中实施了这一理念。",
    "keywords": "TuringPost,多模态模型,ByteDance,BAGEL,开源项目",
    "area": "多模态,机器学习,其他",
    "published_time": "6:03 AM · May 27, 2025",
    "download_time": "2023-11-01 10:16:32",
    "visual_resource": [
      "screenshot/twitter_Twitter12941d16-e73d-4214-9467-d4d251754481.png"
    ]
  },
  {
    "id": "Twitter7d9a856f-5910-4aab-9f92-9067eb82ccc3",
    "source": "Twitter",
    "url": "https://x.com/mervenoyann/status/1926987808360509636",
    "title": "open AI Vision LM & omni releases",
    "content": "what happened in open AI past week? so many vision LM & omni releases 🔥 here's our picks ❤️ multimodal 💬🖼️ > new moondream (VLM) is out: it's 4-bit quantized (with QAT) version of moondream-2b, runs on 2.5GB VRAM at 184 tps with only 0.6% drop in accuracy (OS) 🌚 > ByteDance released BAGEL-7B, an omni model that understands and generates both image + text. they also released Dolphin, a document parsing VLM (OS) > Google DeepMind dropped MedGemma in I/O, VLM that can interpret medical scans, and Gemma 3n, an omni model with competitive LLM performance > MMaDa is a new 8B diffusion language model that can generate image and text > Mistral released Devstral, a 24B coding assistant (OS) > Fairy R1-32B is a new reasoning model -- distilled version of DeepSeek-R1-Distill-Qwen-32B (OS) > NVIDIA released ACEReason-Nemotron-14B, new 14B math and code reasoning model > sarvam-m is a new Indic LM with hybrid thinking mode, based on Mistral Small (OS) image generation > MTVCrafter is a new human motion animation generator",
    "summary": "最近一周，开放AI领域涌现了大量的视觉语言模型和全能模型。 其中包括：新推出的Moondream（VLM）是Moondream-2b的4位量化版本，具有QAT特性；字节跳动发布了BAGEL-7B模型，该全能模型能理解并生成图像和文本；谷歌DeepMind推出了用于解释医学扫描的MedGemma和具有竞争力的Gemma 3n模型；MMaDa是一个新的8B扩散语言模型，能生成图像和文本；Mistral发布了24B编码助手Devstral；以及NVIDIA发布的新数学与代码推理模型ACEReason-Nemotron-14B及基于Mistral Small的混合思维模式Indic LM——sarvam-m。",
    "keywords": "视觉语言模型,全能模型,Moondream,VLM,BAGEL-7B",
    "area": "大模型,计算机视觉,多模态",
    "published_time": "2025-05-26T21:04:00Z",
    "download_time": "2023-10-01 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitter7d9a856f-5910-4aab-9f92-9067eb82ccc3.png"
    ]
  },
  {
    "id": "Twitterfb05453c-09a9-435b-86fd-75caeaa212f9",
    "source": "Twitter",
    "url": "https://x.com/lateinteraction/status/1927445094002487554",
    "title": "Missing nuance in the collective realization today: The non-trivial negative result is not that \"RL just amplifies skills that are already there with low probability\".",
    "content": "Missing nuance in the collective realization today: The non-trivial negative result is not that \"RL just amplifies skills that are already there with low probability\". Duh, that's obvious and not an issue actually. What got questioned today is that \"dumb pretraining teaches the model all these things and then RL surfaces them\". Had it been like that, we'd all be celebrating that. The problem is that it isn't this! The non-trivial negative result is that \"this form of RLVR seems (in certain cases) to ONLY work if your mid-training data mixtures deliberately encode these *specific* math and coding skills\". That is, it's not emergent. It's cultivated.",
    "summary": "今天的集体认识中缺乏一个关键细节：负面的结果不仅仅是“RL 只是放大了那些低概率已经存在的技能”。真正的问题在于，RL 似乎只在中期训练的数据中故意编码具体数学和编程技能时才有效。这意味着，它不是自发的，而是人为培养的。",
    "keywords": "RL,模型,技能,数据,训练",
    "area": "人工智能,机器学习,其他",
    "published_time": "2025-05-28T03:21:00Z",
    "download_time": "2023-11-24 08:26:20",
    "visual_resource": [
      "screenshot/twitter_Twitterfb05453c-09a9-435b-86fd-75caeaa212f9.png"
    ]
  },
  {
    "id": "Twitter3120e2b0-6519-4f0d-8fc3-ffc15e584559",
    "source": "Twitter",
    "url": "https://x.com/SakanaAILabs/status/1926798125060002243",
    "title": "Sudoku-based Reasoning Benchmark & New Leaderboard Launch",
    "content": "Following our Sudoku-based reasoning benchmark announcement, we've been evaluating the latest models to track improvements in their reasoning capabilities. Today, we’re launching the Sudoku-Bench Leaderboard: New technical report: You can now track new model progress on our live Leaderboard. Of the models we’ve benchmarked so far: OpenAI’s o3 Mini High leads overall. Interestingly, Gemini 2.5 Pro does better on the harder 6x6 puzzles! However, o3 is the only model that solves any of the 9x9 Sudokus, but only 2.9% and only the vanilla Sudoku’s. Crucially, NO model tested can yet conquer 9x9s requiring strong, creative reasoning. This benchmark remains a grand challenge! For a deeper dive into the benchmark, methodology, and our findings, check out our technical report. Want to test a model on Sudoku-Bench? It's simple! Visit the leaderboard. Choose a puzzle. We generate a prompt (puzzle + instructions) to paste into any model. Explore sample reasoning traces from our tests too!",
    "summary": "Sakana AI 实验室宣布了基于数独的推理基准，并发布了Sudoku-Bench排行榜。最新报告指出，OpenAI的o3 Mini High总体表现最佳，而Gemini 2.5 Pro在更复杂的6x6拼图中表现更好。然而，目前尚无模型能解决需要强推理能力的9x9数独，这项基准仍是重大挑战。",
    "keywords": "推理基准,数独,排行榜,模型,挑战",
    "area": "人工智能,机器学习,智能体",
    "published_time": "2025-05-26T08:30:00Z",
    "download_time": "2023-10-11 09:15:00",
    "visual_resource": [
      "screenshot/twitter_Twitter3120e2b0-6519-4f0d-8fc3-ffc15e584559.png"
    ]
  },
  {
    "id": "Twitter7b052acf-627d-49df-a155-c3effaff6d3c",
    "source": "Twitter",
    "url": "https://x.com/_lewtun/status/1927043160275923158",
    "title": "Happy to share 💭 Mixture of Thoughts 💭 A curated, general reasoning dataset that trims down over 1M samples from public datasets to ~350k through an extensive set of ablations 🧑‍🍳 Models trained on this mix match or exceed the performance of DeepSeek's distilled models -- not https://t.co/PgX1JedlWS",
    "content": "Happy to share Mixture of Thoughts A curated, general reasoning dataset that trims down over 1M samples from public datasets to ~350k through an extensive set of ablations. Models trained on this mix match or exceed the performance of DeepSeek's distilled models -- not just on math/code but also on scientific benchmarks like GPQA We also validate that the \"additive\" methodology from Phi-4-reasoning really works! You can optimise the data mixture independently per reasoning domain and then bring it all together for the final run Link to the dataset",
    "summary": "本推文介绍了一种名为“思想混合”的经过精心整理的通用推理数据集。通过一系列的推导，将超过100万的公共数据集样本精简至约35万个。训练于此混合数据集的模型在数学、代码及科学基准如GPQA等方面的性能优于或持平于DeepSeek的蒸馏模型。此外，验证了Phi-4-reasoning方法的可加性，这允许用户独立优化每个推理领域的数据混合，然后在最终运行时整合。",
    "keywords": "推理数据集,DeepSeek,GPQA,方法论,性能优越",
    "area": "人工智能,机器学习,深度学习",
    "published_time": "2025-05-27T00:44:00Z",
    "download_time": "2025-11-26 17:26:40",
    "visual_resource": [
      "screenshot/twitter_Twitter7b052acf-627d-49df-a155-c3effaff6d3c.png"
    ]
  },
  {
    "id": "Twitter29001433-1f48-407d-83f7-200ec967a026",
    "source": "Twitter",
    "url": "https://x.com/LangChainAI/status/1927413238733681027",
    "title": "🚀 Ready to deploy your own Open Agent Platform (OAP) instance? In our latest video, we show you how to self-host OAP in production—no managed instance required. OAP is an open-source, no-code platform for building, prototyping, and deploying intelligent agents.",
    "content": "🚀 Ready to deploy your own Open Agent Platform (OAP) instance? In our latest video, we show you how to self-host OAP in production—no managed instance required. OAP is an open-source, no-code platform for building, prototyping, and deploying intelligent agents. 阅读完整文章: youtube.com/watch?v=GQCGwn",
    "summary": "LangChain推出的Open Agent Platform (OAP)是一个无需代码即可搭建代理的平台，最新视频展示如何自托管OAP进行生产部署。OAP支持工具和监督代理的开箱即用设置，同时以直观的Web UI支持域知识的服务器插件接入。该平台面向开发者、产品经理和分析师，依托LangGraph迅速实现快速上手。",
    "keywords": "Open Agent Platform,OAP,LangChain,视频,部署",
    "area": "人工智能,多模态,自然语言处理",
    "published_time": "2025-05-28T01:15:00Z",
    "download_time": "2025-05-28 12:00:00",
    "visual_resource": [
      "screenshot/twitter_Twitter29001433-1f48-407d-83f7-200ec967a026.png"
    ]
  },
  {
    "id": "Twitterf11a80a5-eba7-4015-ba44-d281e5572666",
    "source": "Twitter",
    "url": "https://x.com/omarsar0/status/1927368367075197179",
    "title": "Agents Basics",
    "content": "Agents Basics It's super simple to create an agent by giving it a description, name, instructions, and tools.",
    "summary": "代理的基础内容：创建一个代理非常简单，只需提供描述、名称、说明和工具即可。",
    "keywords": "代理,基础,创建,工具,说明",
    "area": "人工智能,智能体,其他",
    "published_time": "2025-05-27T22:16:00Z",
    "download_time": "2025-10-05 14:21:17",
    "visual_resource": [
      "screenshot/twitter_Twitterf11a80a5-eba7-4015-ba44-d281e5572666.png"
    ]
  },
  {
    "id": "Twitterc1e4d32c-6e14-4f9e-85b8-f476770dee76",
    "source": "Twitter",
    "url": "https://x.com/omarsar0/status/1927366520985800849",
    "title": "Mistral AI announces Agents API",
    "content": "NEW: Mistral AI announces Agents API - code execution - web search - MCP tools - persistent memory - agentic orchestration capabilities Cool to see that Mistral AI has joined the growing number of agent frameworks. More below:",
    "summary": "Mistral AI 推出 Agents API，具备代码执行、网络搜索、MCP 工具、持久内存和代理编排功能，加入了越来越多的代理框架行列。",
    "keywords": "Mistral AI, Agents API, 代码执行, 网络搜索, 代理框架",
    "area": "多模态,生成式AI,其他",
    "published_time": "2025-05-27T22:09:00Z",
    "download_time": "2023-10-01 12:00:00",
    "visual_resource": [
      "screenshot/twitter_Twitterc1e4d32c-6e14-4f9e-85b8-f476770dee76.png"
    ]
  },
  {
    "id": "Twitter64a0eed0-7832-4402-ad8b-9e2d77c280cf",
    "source": "Twitter",
    "url": "https://x.com/omarsar0/status/1927372457578483828",
    "title": "Handoff The handoff feature enables agents to call other agents to complete tasks or hand over a conversation mid-action.",
    "content": "NEW: Mistral AI announces Agents API - code execution - web search - MCP tools - persistent memory - agentic orchestration capabilities Cool to see that Mistral AI has joined the growing number of agent frameworks. More below:",
    "summary": "Mistral AI发布新API，支持代码执行、网页搜索、MCP工具、持久内存、代理的编排能力，该功能拓展了代理框架的功能。",
    "keywords": "Mistral AI, Agents API, 持久内存, 代理框架, 代码执行",
    "area": "人工智能, 生成式AI, 智能体",
    "published_time": "2025-05-14T12:34:56Z",
    "download_time": "2025-10-08 14:28:45",
    "visual_resource": [
      "screenshot/twitter_Twitter64a0eed0-7832-4402-ad8b-9e2d77c280cf.png"
    ]
  },
  {
    "id": "6Ixh9_b8Bawf6XrQLh-vaA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/6Ixh9_b8Bawf6XrQLh-vaA",
    "title": "不懂建模也能做角色！VAST升级AI神器，一手实测来了：一键拆建/魔法笔刷/万物绑骨",
    "summary": "明星初创VAST旗下Tripo Studio迎来重大升级，推出智能部件分割、贴图魔法笔刷、智能低模生成及万物自动绑骨等核心功能，彻底革新传统3D建模流程。该工具直击行业痛点，大幅提升游戏开发、3D打印和动画制作效率，实现模型从“生成”到“应用”的全链路优化。Tripo Studio凭借AI驱动的“流程再造”，让非专业人士也能轻松创作，重新定义了3D从业者的工作台价值，标志着AI在3D领域从技术突破走向成果交付的质变。",
    "keywords": [
      "AI建模",
      "TripoStudio",
      "3D大模型",
      "智能部件分割",
      "贴图魔法笔刷",
      "智能低模",
      "万物绑骨",
      "流程再造"
    ],
    "area": [
      "人工智能",
      "生成式AI",
      "大模型"
    ],
    "content": "标题：不懂建模也能做角色！VAST升级AI神器，一手实测来了：一键拆建/魔法笔刷/万物绑骨\n公众号：量子位\n--------------------------------------------------\n\nAI建模界的“作弊神器”真的来了！\n\n3D大模型明星初创VAST推出的Tripo Studio此次大升级——\n\n四大核心功能：智能部件分割、贴图魔法笔刷、智能低模生成、万物自动绑骨等。给人一种感受是，AI终于懂得建模了。不是那种只会给你乱糊一个模型出来的AI，而是每个功能都直击过去建模流程里的痛点。\n\n过去想做个角色模型只能外包，报价单上四位数的数字和“改到满意””的拉锯战，让3D创作成了可望不可即的梦。但现在大模型在3D建模应用这块，正以肉眼可见的速度落地。\n\n这背后离不开像VAST这样的玩家持续迭代和开源基础模型，比如TripoSF此前已刷新SOTA，到现在真正成果可交付的Tripo Studio升级，本质是实现了从“给模型”到“交成果”的质变。\n\n在CG领域的技术应用实践中，过往行业对市面其他3D生成算法工具的尝试（包括Tripo自身），普遍面临AI生成3D模型的局限性——若要将其纳入工业化管线，仍需大量手工调整以适配流程需求。\n\n而Tripo Studio作为一站式解决方案，其核心优势在于通过拓扑优化、智能贴图及部件级编辑等功能模块的协同运作，使输出模型在游戏开发、工业设计等多场景下，可直接替代传统DCC建模软件的繁杂工序，实现从生成到应用的全链路效率提升。\n\n多少用户在评论区高呼的一键拆分功能终于来了，真正是3D打印和游戏制作用户的福音。\n\n背后的算法就是之前曾分享过Tripo开源月全家桶的Holopart。\n\n视频、图片生成模型发展近三年，仍然在轨道编辑与图层操作方面有巨大的局限性，Tripo Studio率先实现了3D模态下的精准可控编辑。这一突破不仅显著提升了创作效率，更开拓了巨大的商业应用空间。\n\n对3D打印来说，Tripo的智能分割让模型自动按部件拆解，能直接在切片软件中为不同区域设置颜色，像PLA这样的四色打印也能轻松搞定；不再需要手动切面、重建拓扑，大大提高了多色打印的效率和精度。\n\n对游戏开发者而言，这功能也极其实用。\n\n拆分后的模型部件可以按需保留或替换，比如上传一个人形角色模型，想给它换个武器，以前不是得慢慢切面、拉mask，就是干脆从头建模。现在只要点一下“智能部件分割”，系统就能自动识别并拆分头、身体、四肢等区域，甚至连肩甲这类小结构都能准确定位。\n\n更爽的是，它还能手动微调——不满意某一块选区，拿画笔涂一下就能改，随时调整选区，整个流程从一下午缩短到20分钟。对需要频繁修改角色部件、快速切换装备或外观的项目来说，这功能简直是神助攻。\n\n在传统3D创作流程中，UV展开与贴图处理向来是制约效率的关键瓶颈。尤其是在游戏角色建模领域，贴图瑕疵处理往往需要耗费大量时间成本。以赛博朋克风格角色建模为例，材质不均匀、纹理错位等问题会直接影响最终视觉效果，而传统手动修复方式需要反复调整UV坐标，不仅效率低下，还对创作者的专业技能要求极高。\n\nMagic Brush直接把贴图这件事“傻瓜化”了：之前做游戏角色贴图时，最怕遇到“衣服破洞”“材质不均匀”的问题。\n\n有了“贴图魔法笔刷”，这一切都变得轻松。\n\n只需锁定问题区域，一键涂抹自动修复并补全风格化的纹理，想改哪里涂哪里。\n\n用户只需圈选目标区域，系统即可快速完成瑕疵修复，并保持纹理风格的高度一致性。\n\n最绝的是，它还能配合“智能部件分割”单独修改某个部位——比如给角色换脸，直接选中“头部”区块，对着贴图轻轻一涂，立马“改头换脸”！\n\n视频加载失败，请刷新页面再试\n\n做游戏模型的都懂，最怕高模导出后在引擎里卡成PPT。\n\n在实时渲染的数字世界里，高保真模型与性能瓶颈的矛盾始终存在。以次世代游戏开发为例，在开放世界项目中，单栋建筑模型面数常突破10万级，即便搭载超高性能显卡，场景加载延迟仍然很高。传统优化需手动拓扑减面，不仅耗时耗力，更易造成纹理拉伸、结构畸变等问题。\n\nTripo的“智能低模生成”尤其惊艳~\n\n一键优化后，通过智能识别模型关键特征，在保留法线贴图细节、UV布局完整性的前提下，模型面数从10万骤降到5千，但关键细节几乎完整保留。\n\n经过测试，Unity中的帧率翻倍，游戏运行更加流畅，完美平衡了视觉质量与运行效率。\n\n在做角色动画时，绑骨一直是最费时间的部分。\n\n这一环节也长期面临“数字鸿沟”——仅基础骨骼绑定环节，在Blender等传统DCC工具中就需耗费动画师8-10小时。复杂机械结构的反向动力学设置，或生物模型的肌肉系统模拟，更易导致动画穿帮、运动失真等问题。\n\nTripo Studio集成了自家的UniRig算法，可自动解析模型的生物力学特征。例如当输入鱼类模型时，系统能精准识别鳍部摆动轴、脊椎弯曲路径等关键运动维度，迅速完成骨骼权重分配与蒙皮优化。支持FBX、GLTF等主流格式导出。\n\n我第一次尝试绑定是Blender新手教学那套流程……做的一个机械兽模型，手动绑骨花了整整一天，结果一动起来关节卡顿得像机器人抽筋，结果用完直接放弃建模。\n\n这次把一只鱼的模型丢给Tripo Studio，它居然能自动识别形体结构，几十秒钟就绑定好了骨骼，还能导出标准动画格式。\n\n重点是：不仅人形能绑，虫鱼鸟兽也能顺利绑定，动画制作效率直接提升10倍以上！\n\n说到背后技术，Tripo Studio的研发团队——3D大模型明星初创公司 VAST，作为全球领先的3D生成技术提供方，VAST一直很看重在技术前沿的探索。\n\n前几个月刚刚一口气开源的算法全家桶，几乎都集成在Tripo Studio中：上文提到的HoloPart、UniRig等。\n\n之前一直提到TripoSF的完全体会很快以Tripo3.0的形式在Studio里满血呈现，那就期待一下顶级高模和完整工具台的强大协同性：\n\nTripo一直是在国内外社媒上非常火，从年前的Deepseek+Tripo，到GPT4o+Tripo，都产生了很多好玩又有实际价值的工作流。\n\n从2024年初发布算法模型持续收获大量好评，再到如今因为更早的行业判断和know-how而重新定义了3D创作管线，VAST和Tripo团队的每一步都走得扎实而前瞻。\n\n从单纯提供算法，到用完整的工作台承接3D创作者的用户意图，体验之后才能发现Tripo Studio提供的这些功能不是简单的效率提升，而是“流程再造”——传统3D建模需要美术、拓扑、绑定、贴图等多岗位协作，现在一个非专业人士就能通过Tripo Studio完成全流程，这种“去专业化””能力正是“成果型产品”的核心特征。\n\n尽管VAST的算法团队在不断刷新每个分支任务的SOTA，但它证明的不仅是技术进步，更是商业逻辑的重构——\n\n在AI时代，3D从业者需要的工作台价值不再取决于“用了多少高端工具”，而在于“交付了多少业务成果”。\n\n就像我终于删掉电脑里30G的传统插件时意识到的：不是我学会了3D建模，而是Tripo Studio让3D建模学会了为我服务。\n\n访问链接：www.tripo3d.ai\n\n*本文系量子位获授权刊载，观点仅为原作者所有。\n\n一键三连「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n🌟 点亮星标 🌟",
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:00.445619",
    "visual_resource": [
      "screenshot/wechat_wx_f2953a8d.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtA7PAcYOazzjo8cK20odicaxGGDvhbqK2SQYmxQ1vKw8NmElGR06p6OHpu7j8OUenzvLlER8bMFvzg/0?wx_fmt=jpeg\", \"id\": \"6Ixh9_b8Bawf6XrQLh-vaA\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/6Ixh9_b8Bawf6XrQLh-vaA.txt\"}}"
  },
  {
    "id": "lCjfKhFfOdTtC6uEvhJG4w",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/lCjfKhFfOdTtC6uEvhJG4w",
    "title": "AI仅凭“自信”学会推理，浙大校友复刻DeepSeek长思维链涌现，强化学习无需外部奖励信号",
    "summary": "加州大学伯克利分校团队提出突破性AI训练范式INTUITOR，使大语言模型首次能仅凭自身“自信度”学会复杂推理，实现长思维链涌现，彻底摆脱对外部奖励信号或人工标注数据的依赖。该方法通过优化模型内在置信度信号，有效规避了传统强化学习高昂成本及“奖励黑客”等问题。实验证明，INTUITOR在数学推理、代码生成及指令遵循任务上显著提升模型性能，尤其展现出卓越的结构化推理能力及多任务泛化性。此研究不仅为大模型训练带来新突破，更为AI迈向更自主、类人化的学习范式开启了全新可能。",
    "keywords": [
      "大模型",
      "强化学习",
      "内在奖励",
      "置信度",
      "长思维链",
      "推理能力",
      "AI训练"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "大模型"
    ],
    "content": "标题：AI仅凭“自信”学会推理，浙大校友复刻DeepSeek长思维链涌现，强化学习无需外部奖励信号\n公众号：量子位\n--------------------------------------------------\n\n复刻DeepSeek-R1的长思维链推理，大模型强化学习新范式RLIF成热门话题。\n\nUC Berkeley团队共同一作Xuandong Zhao把这项成果称为：\n\n大模型无需接触真实答案，仅通过优化自己的信心，就能学会复杂推理。\n\n具体来说，新方法完全不需要外部奖励信号或标注数据，只需使用模型自身的置信程度作为内在奖励信号。\n\n与使用外部奖励信号GRPO相比，新方法在数学任务上不需要标准答案也能提升基础模型性能，在代码任务上表现得更好。\n\n几乎同一时间，另外一篇论文《RENT: Reinforcement Learning via Entropy Minimization》也验证了相似的结论。\n\n作者表示两者的主要区别在于使用KL散度和最小化熵衡量自信程度。\n\nDropbox工程副总裁看后表示：Confidence is all you need。\n\n长期以来，训练大模型主要依赖两种方式：\n\n要么需要大量人工标注（如ChatGPT的RLHF），要么需要可验证的标准答案（如DeepSeek的RLVR）。\n\n前者成本高昂且可能引入偏见，后者则局限于数学、编程等有明确答案的领域。\n\n那么当AI能力逐渐接近甚至超越人类时，能否让模型仅凭自身产生的内在信号，摆脱对外部监督的依赖？\n\n针对这个问题，UC Berkeley团队提出新训练方法Intuitor，计算模型预测分布与均匀分布之间的KL散度作为“自信程度”。\n\n相当于人类做题时，如果对答案有把握思路也会更清晰，当自信不足的时候往往需要重新思考。\n\n通过优化这个内在信号，INTUITOR鼓励模型生成它自己”更有把握”的回答，也能促使模型生成更结构化的推理过程。\n\n在实验中，1.5B和3B的小模型也涌现出与DeepSeek-R1类似的长思维链推理行为。\n\n论文还指出，内在奖励信号还获得一个额外的好处：从机制上降低了“奖励黑客”的风险。\n\n传统外部奖励信号的强化学习容易被“钻空子”，如模型可能生成语法正确但逻辑错误的代码来匹配测试用例，或在数学题中直接背答案而非推理。\n\n在INTUITOR中，团队发现如果使用离线学习，在训练约100步的时候模型也学会了作弊：在回答中附加一个已经解决的简单问题来提高自信度分数。\n\n但使用在线学习就可以避免这个问题，评估标准随着模型能力一起进化，作弊策略变得无效。\n\n团队首先实证研究了INTUITOR框架对LLMs数学推理能力的提升。\n\n实验选取Qwen2.5-1.5B/3B作为基础模型，使用自我确定度作为唯一的奖励信号，并将其分别置于INTUITOR和两个基线方法（GRPO、GRPO-PV）在MATH数据集的预训练中。\n\n使用对话提示，每次处理128道题目并各生成7个候选解决方案，KL惩罚系数设置为0.005。\n\n在数学推理、代码生成、指令遵循的基准测试中进行性能评估，结果如图所示：\n\n实验表明，在通过INTUITOR进行微调后，Qwen2.5-1.5B从最初只会输出重复的无意义内容且对话任务得分均低于10%，转变为无效输出大幅减少、响应长度有效增加。\n\n在结构化推理能力上，团队还发现INTUITOR早期学习速度更快，如Qwen2.5-3B在GSM8K基准测试上INTUITOR（0.811）始终优于GRPO（0.758）。\n\n此外，INTUITOR在多任务泛化上也表现优秀，例如当Qwen2.5-3B在代码生成任务上，虽然相对滞后但持续增长，最终性能比GRPO高8%，相对提升65%。\n\n同时团队还观察到，在进行长链推理时，INTUITOR模型在生成完整代码前，都会添加自然语言推理 （如“为解决X问题，需先执行Y步骤”），据推测也许这就是INTUITOR能够在测试中始终表现出色的原因之一。\n\n它的演进过程大概可以描述为三个阶段：\n\n为了评估自我确定度作为奖励的鲁棒性，研究人员还将离线自我确定度（来自固定基础模型的奖励）与在线自我确定度（来自不断进化的策略模型的奖励）进行了比较。\n\n另外为进一步评估自我确定度作为奖励信号的质量，研究人员还分析了模型在MATH500响应中生成的自我确定度分数分布。\n\n值得注意的是，INTUITOR模型对正确答案的self-certainty显著更高，而GRPO虽提升了模型自评能力，但区分度明显低于INTUITOR。\n\n由于受计算资源限制，实验只在相对较小的无监督语料库上进行训练，未来可在更大规模的基础模型和更多样化的真实世界数据集上进一步研究INTUITOR的优势。\n\n本项研究来自UC Berkeley的Sergey Levine、宋晓东团队，作者一共有五位，分别是第一作者博士后研究员Xuandong Zhao、共同一作本科生Zhewei Kang、来自耶鲁大学的Aosong Feng，以及Sergey Levine和Dawn Song。\n\n2019年，Xuandong Zhao从浙江大学毕业后，就进入了加州大学圣塔芭芭拉分校攻读计算机科学博士学位，期间还曾在阿里巴巴、Microsoft和Google等公司实习。\n\n自2024年他进入UC Berkeley后，除本次的新成果外，至今一共还发表过十多篇论文，并先后被ICLR 2025、ICML 2025等接收。\n\n另外在今年2月，Xuandong Zhao和Zhewei Kang还合作发表了一篇论文，描述了基于自我确定性的LLMs推理能力提升新策略Best-of-N，可以看作是本篇论文的一次先验尝试。\n\n论文链接：https://arxiv.org/abs/2505.19590代码链接：https://github.com/sunblaze-ucb/Intuitor\n\n参考链接：[1]https://x.com/joshclemm/status/1927400772817285264[2]https://x.com/xuandongzhao/status/1927270931874910259 [3]https://x.com/xuandongzhao/status/192778163679341780[4]https://arxiv.org/abs/2502.18581\n\n— 完 —\n\n📪 量子位AI主题策划正在征集中！欢迎参与专题365行AI落地方案，一千零一个AI应用，或与我们分享你在寻找的AI产品，或发现的AI新动向。\n\n💬 也欢迎你加入量子位每日AI交流群，一起来畅聊AI吧～\n\n一键关注 👇 点亮星标\n\n一键三连「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！",
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:09.156781",
    "visual_resource": [
      "screenshot/wechat_wx_2d97fb51.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtA7PAcYOazzjo8cK20odicaxXsWrvUbdCNh3ZicXqu5KSVJtTw0FWDcVo6WxPeVbd1hIE4PnBnsA0vQ/0?wx_fmt=jpeg\", \"id\": \"lCjfKhFfOdTtC6uEvhJG4w\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/lCjfKhFfOdTtC6uEvhJG4w.txt\"}}"
  },
  {
    "id": "qC47ZdqdYcbGILMHPae51g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/qC47ZdqdYcbGILMHPae51g",
    "title": "英伟达再创历史纪录！Q1收入增长69%，数据中心贡献89%，游戏业务大涨42%",
    "summary": "英伟达最新财报显示，2026财年Q1营收同比激增69%，数据中心业务贡献89%成核心增长引擎。尽管美国出口限制导致H20芯片销售受阻，并产生45亿美元库存减值，但伴随GB300芯片投产、Blackwell架构引入游戏市场（RTX 50系列与任天堂Switch 2），以及汽车与机器人业务（Isaac GR00T）的强劲增长，英伟达仍在AI基础设施和技术创新领域保持领先，预示着未来业绩潜力巨大。",
    "keywords": [
      "英伟达",
      "财报",
      "数据中心",
      "H20芯片",
      "出口限制",
      "人工智能",
      "GB300",
      "机器人"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器人"
    ],
    "content": "标题：英伟达再创历史纪录！Q1收入增长69%，数据中心贡献89%，游戏业务大涨42%\n公众号：量子位\n--------------------------------------------------\n\n英伟达最新一季度财报出炉，不出所料，依旧亮眼。\n\n而这一切还是在H20芯片受到出口限制影响下实现的。\n\n财报公布后，英伟达股价盘后跳涨。\n\n据了解，英伟达最强性能GB300芯片及相关组件在今年5月开始生产，在第三季度正式推出。这意味着接下来英伟达的营收和毛利率表现更加值得期待。\n\n但在出口限制的影响下，H20芯片的损失也在扩大，预计下一财季将达到80亿美元。\n\n英伟达2026财年Q1的汇报期为2025年1月30日-2025年4月27日，本次财报表现超出预期。\n\n2026财年Q1营收环比增长12%、同比大涨69%。\n\nGAAP净利润同比增长26%、环比下降15%；非GAAP净利润为198.94亿美元，同比增长31%。\n\nGAAP稀释每股收益为0.76美元，同比增长27%；非GAAP稀释每股收益为0.81美元，若排除H20相关费用，为0.96美元。\n\nGAAP毛利率为60.5%，低于去年同期的78.4%，主要受H20产品库存减值和出口限制的影响。\n\n非GAAP毛利率为61.0%，若排除H20库存相关费用，毛利率可达71.3%。\n\n研发投入依旧上涨，同比增长46.7%，环比增长7.4%。\n\n在业务划分上，英伟达主要包括四部分。\n\n其中，数据中心业务仍是英伟达的核心增长驱动力。全球AI基础设施需求强劲，推动该业务持续增长。\n\n近期，英伟达推出Blackwell Ultra和NVIDIA Dynamo，加速AI推理模型扩展。并在全球多地合作共建AI基础设施集群、AI工厂。\n\n游戏业务创下历史新高。\n\nGeForce RTX 5070和RTX 5060的陆续推出，将 Blackwell架构引入主流市场，起售价299美元。\n\n任天堂Switch 2同样采用英伟达处理器，支持4K分辨率和AI驱动的DLSS技术。\n\n其中DLSS 4已支持125款游戏，包括《黑神话：悟空》等。\n\n另外，汽车与机器人业务虽然占比不多，但是同比增长达到72%。\n\n表现亮点包括：与通用汽车合作开发下一代车辆、工厂和机器人，集成了NVIDIA Omniverse和NVIDIA DRIVE AGX™。发布了Isaac GR00T N1，全球首个开源人形机器人基础模型，并推出升级版GR00T N1.5。\n\nQ1财报中，英伟达具体介绍了H20芯片的销售情况。\n\n本系列产品本来是英伟达专为中国市场设计的高性能AI计算芯片，主要用于AI训练和推理任务。但近期也受到了出口限制。\n\n数据方面，第一财季H20产品销售额为4.6亿美元，未能实现额外的25亿美元H20收入。\n\n同时计入了一笔45亿美元的库存减值和采购义务费用。\n\n第二财季及未来几个季度，出口限制将会继续产生影响，预计第二财季H20产品收入将减少80亿美元。\n\n前不久，黄仁勋在中国台湾之行中也提到，在2021年初，英伟达在中国AI市场拥有95%的市场份额，但如今这一比例已下降到只有50%。\n\n中国有很多本土AI芯片，如果没有英伟达的AI技术，中国客户将会使用本土技术。\n\n英伟达2026财年Q1财报：https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-first-quarter-fiscal-2026\n\n— 完 —\n\n📪 量子位AI主题策划正在征集中！欢迎参与专题365行AI落地方案，一千零一个AI应用，或与我们分享你在寻找的AI产品，或发现的AI新动向。\n\n💬 也欢迎你加入量子位每日AI交流群，一起来畅聊AI吧～\n\n一键关注 👇 点亮星标\n\n一键三连「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！",
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:17.681354",
    "visual_resource": [
      "screenshot/wechat_wx_c5bd9cfc.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtA7PAcYOazzjo8cK20odicaxyZ4tPl0WyiaOBnPxQS0SJVWM4fQT2YzaIxp3weobItSTpFeyp2evvIA/0?wx_fmt=jpeg\", \"id\": \"qC47ZdqdYcbGILMHPae51g\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/qC47ZdqdYcbGILMHPae51g.txt\"}}"
  },
  {
    "id": "TRtITbsVftG8zGR1HecljQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/TRtITbsVftG8zGR1HecljQ",
    "title": "SOTA大模型遇上加密数据评测：Qwen3未破10%，o1也栽了丨上海AI Lab等联合研究",
    "summary": "上海AI Lab联合推出的CipherBank评测揭示，当前SOTA大语言模型在密码学解密任务中表现普遍不佳，连最新的Qwen3系列准确率也未破10%。该评测聚焦真实隐私场景数据和多类型加密算法，旨在考验模型的严密逻辑与细节精确度。结果显示，即使是Claude-3.5和o1等领先模型，准确率也未能突破50%，暴露出LLM在处理结构化与符号化推理方面的显著短板。研究指出，模型惧怕长文本、噪音干扰、数字转换，且存在提示依赖症。未来AI发展需摆脱过度语义依赖，增强模式学习与泛化能力，并优化推理执行稳定性，以克服在密码学领域的挑战。",
    "keywords": [
      "大语言模型",
      "密码学",
      "CipherBank",
      "解密",
      "评测",
      "推理能力",
      "SOTA"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "content": "标题：SOTA大模型遇上加密数据评测：Qwen3未破10%，o1也栽了丨上海AI Lab等联合研究\n公众号：量子位\n--------------------------------------------------\n\n大语言模型遇上加密数据，即使是最新Qwen3也直冒冷汗！\n\n尽管当前的推理模型在各类基准测试中展现出卓越的性能，但在密码学这一对逻辑严密性和细节精确度要求近乎苛刻的专业领域，模型的推理能力仍有待深入探索。\n\n密码学不仅需要模型具备高阶数学运算能力和严密的逻辑推理链条，更要求其能够精准识别复杂加密模式中的潜在规律；成功解密需要模型具有极强的综合推理能力。\n\n上海AI Lab等联合推出的CipherBank评测，用海量真实隐私场景数据和多类型密码算法，硬核挑战SOTA大模型。\n\nCipherBank的评测结果显示，当前的大语言模型在密码学解密任务上整体表现不佳，最优模型准确率未能过半，绝大多数模型准确率不足20%，表明结构化和符号化推理仍是它们的显著短板。\n\n在CipherBank评测中，Claude-3.5-Sonnet和o1表现最佳，DeepSeek系列略优于通用模型，而 GPT-4o、Gemini等模型表现平庸，Qwen2.5, Llama3.1, Llama3.3等开源模型表现较差，即使是最新发布的Qwen3系列模型表现也不尽人意，30B和32B的模型准确率均未超过10%；整体显示当前大模型在解密推理任务上仍存在明显短板。\n\nCipherBank是一个全面、真实、精妙的密码学解密基准测试集。它不仅仅是随机文本的加密，而是精心构建了贴近现实世界隐私敏感场景的明文数据。\n\n数据：涵盖5大领域(如个人隐私、金融资产)、14个子领域(如身份信息、银行信息)、89个细粒度标签，共262个独特明文。这些数据反映了真实的加密需求。\n\n算法：包含3大类(替换密码、置换密码、自定义密码)、9种典型及创新加密算法，从经典的Rot13、Vigenère到定制的DualAvgCode、ParityShift、WordShift等。设计了5个难度层级，从基础到专家，全方位考验模型的解密能力。\n\n题库：总共生成了2,358道 经过严格验证的解密题目。每一题，都是对LLM推理能力的严峻拷问！\n\n用研究者的话说：CipherBank，就是要让LLMs在没有“场外提示”的情况下，纯靠本事闯过重重“密室”。\n\n研究团队邀请了当前AI界的18位“顶流”选手（包括GPT家族、DeepSeek系列、Gemini系列、Claude 3.5、o1系列等）进行了这场硬核PK。\n\n评估采用 3-shot 设置。模型拿到的是几个明文-密文示例，需要像一位真正的密码分析师一样，从这些例子中自主学习加密规则、推断密钥，最终才能解密全新的密文。这评估的是真正的推理能力，而不是简单的“记忆”或“穷举”。\n\n集体“不及格”？：令人震惊的是，绝大多数SOTA模型得分惨淡，部分甚至接近零分。即使是表现最好的Claude-3.5和o1，准确率也未能突破50%。这说明，即使是古典密码解密，对目前的LLMs来说依然是一个巨大的未被攻克的堡垒。\n\n推理模型「略有优势」：推理优化模型（DeepSeek-R1, o1）的平均表现确实优于通用聊天模型，这再次印证了推理优化在逻辑任务上的价值，但差距并没有拉开到大家想象的那么大。\n\n闭源模型「暂时领跑」：Claude-3.5以显著优势领跑，在替换密码、置换密码上展现了非凡能力，o1紧随其后。但DeepSeek-V3/R1等开源模型的进步也很亮眼，正在奋力追赶。\n\n性能差异「惊人」：同类模型在解密任务中的表现差异较大，例如o1与QwQ-32B-Preview的准确率相差几十倍。\n\n除此之外，研究团队还对全新发布的Qwen3 32B系列模型进行了测试，发现即使是最新发布的Qwen3模型，测试准确率依旧不足10%：\n\n为什么LLMs在解密上这么“挣扎”？研究团队进一步做了细致分析：\n\n怕长文本： 文本越长，模型越容易出错！与人类解密不同，人类一旦成功找到解密方法之后，便能以近100%的成功率破解，而LLMs的“脑容量”在解密时会受到长度限制。\n\n怕噪音干扰 ：明文中加点儿错别字或无关信息，模型性能“闪崩”！这暴露了模型在“猜测”而非“推理”——它们不是严格按规则解密，而是依赖文本的语义顺畅度，一旦语义被破坏，就歇菜了。\n\n怕数字转换 ：加密内容里混入数字？难度瞬间飙升！LLMs在处理涉及数字的转换规则时显得尤为吃力。\n\n“提示”依赖症 ：如果在Prompt里直接告诉模型是什么算法，推理模型表现会大幅提升，而通用模型提升有限。这说明推理模型在“有向”推理时更有效，但自主从示例中发现规则的能力还不足。\n\n研究团队对模型的错误输出进行了细致分类（遗漏/插入、姓名解密错误、语义推断、重组、推理失败等），将模型的错误分布总结为下图（左图为Chat model错误分布，右图为Reasoning model的错误分布），并发现了一些有意思的现象：\n\n推理模型「想太多」：有时在简单的算法（比如Reverse）上，推理模型反而会“过度分析”，绕了远路最终出错。\n\n对话模型「爱脑补」：更倾向于生成语义通顺但并未完全符合解密规则的文本，容易出现“遗漏/插入”或“重组”错误，像是在“自由发挥”。\n\n「姓名识别」的通病 ：处理姓名等专有名词的解密时，模型们普遍容易出错，这可能是预训练数据带来的某种“记忆”干扰。\n\n那么，未来的 AI 应该往哪个方向努力，才能征服密码解密这座“高山”呢？CipherBank的结果为人们指明了几个关键的突破口：\n\n摆脱「过度语义依赖」：让模型训练出纯粹的、抽象的符号和结构化推理能力，不再仅仅依赖表面文本的“猜意思”或进行“语义补全”，尤其在处理不具备强语义规律的加密数据时。\n\n增强「模式学习与泛化」：提升模型从少量示例中精准对比分析、高效提取隐含加密规则和密钥的能力，并能将这些规则稳健地泛化应用于各种情况，包括处理混合文本（如数字与字母）以及对抗轻微的噪音干扰。\n\n优化「推理执行的稳定性」：改进模型的思考流程，避免在看似简单的任务上“过度思考”或陷入不必要的递归修正，确保推理过程更加直接、高效和稳定，能够精确无误地执行推断出的解密步骤。\n\n未来，大语言模型有望在密码学领域取得更加显著的进展。\n\n项目主页：https://cipherbankeva.github.io/论文直达：https://arxiv.org/abs/2504.19093测试数据：https://huggingface.co/datasets/yu0226/CipherBank\n\n一键三连「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n🌟 点亮星标 🌟",
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:26.338471",
    "visual_resource": [
      "screenshot/wechat_wx_4080906e.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtByts5aLdmL34037Zsk2h6JCcq9MjJ0KtFsxugsZqdJf4HicxMAmDZWxg1DViamicEk5b8CEKRP1Y3RQ/0?wx_fmt=jpeg\", \"id\": \"TRtITbsVftG8zGR1HecljQ\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/TRtITbsVftG8zGR1HecljQ.txt\"}}"
  },
  {
    "id": "VE-3UCGJrHQ3feBga7svzA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/VE-3UCGJrHQ3feBga7svzA",
    "title": "基准测试揭秘大模型“字数危机”：26个模型长文本生成普遍拉胯，最大输出长度过度宣传",
    "summary": "最新研究《LIFEBENCH》深入揭示大语言模型（LLMs）在遵循长度指令，特别是长文本生成方面的普遍不足。通过对26个主流模型进行基准测试，结果显示大多数模型在明确要求生成特定长度文本时表现糟糕，尤其在长文本场景中得分普遍低于40分，远低于其声称的最大输出能力。研究发现，模型存在缺乏准确长度感知、对输入长度敏感及懒惰生成策略等核心瓶颈。文章强调，未来需通过增强预训练数据和引入预规划策略，全面提升大模型对长度指令的遵循能力与实际表现。",
    "keywords": [
      "大语言模型",
      "长度指令",
      "长文本生成",
      "LIFEBENCH",
      "基准测试",
      "模型能力瓶颈",
      "生成式AI",
      "模型评估"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "生成式AI"
    ],
    "content": "标题：基准测试揭秘大模型“字数危机”：26个模型长文本生成普遍拉胯，最大输出长度过度宣传\n公众号：量子位\n--------------------------------------------------\n\n你是否曾对大语言模型（LLMs）下达过明确的“长度指令”？\n\n比如，“写一篇10,000字的长文，详细分析某个议题。”看似简单的要求，实际却往往让这些模型“力不从心”：\n\n不是生成内容不足，就是重复啰嗦，甚至直接罢工拒绝生成。\n\n一篇最新研究论文《LIFEBENCH: Evaluating Length Instruction Following in Large Language Models》对这一问题进行了深入探讨，提出了一个全新的基准测试集 LIFEBENCH，系统评估大语言模型在长度指令遵循方面的表现。\n\n研究结果揭示：这些看似无所不能的模型在长度指令，特别是长文本生成任务中，表现不尽人意。当模型被明确要求生成特定长度的文本时，大多数模型表现糟糕。\n\n接下来，让我们一起来看看这篇论文是如何揭示这些“瓶颈”的！\n\nLIFEBENCH，全称“Length Instruction Following Evaluation Benchmark”，是一套专门评估大语言模型在长度指令下表现的测试集。它不仅覆盖了从短篇到长文的多种长度范围，还囊括了多种任务类型和语言，全面揭示了大模型在长度控制上的能力边界。\n\nLIFEBENCH的三大核心特性：\n\n数据集的多样性\n\n为了测试模型的全方位能力，LIFEBENCH设计了多维度的数据集：\n\n全面的长度范围与指令类型\n\nLIFEBENCH是首个系统性评估模型长度指令遵循能力的基准测试，它设计了三种常见的长度控制方法：\n\n同时，长度输出范围覆盖从短文本（<100字）、中等长度（100–2000字）到长文本（>2000字）的任务，评测的全面性远超以往研究。\n\n创新的评测指标\n\n为了更精准地分析模型的表现，LIFEBENCH提出了两项专门指标：长度偏差（Length Deviation, LD）：衡量生成文本长度与目标长度之间的差异，包括偏差方向和偏差幅度。\n\n长度评分（Length Score, LS）：综合评价模型对长度指令的遵循能力，量化偏差的整体影响。\n\n相较于简单的字数匹配，这两项指标提供了更细致的分析维度。\n\n通过上述设计，LIFEBENCH不仅覆盖了现有研究中涉及的所有长度指令评测范围，还首次系统性探索了模型在不同任务、语言和长度限制下的表现。\n\n研究团队对26个主流大语言模型进行了评测，结果揭示了它们在长度指令遵循上的重大不足，尤其是在长文本生成场景下。以下是一些关键发现：\n\n1. 总体表现：长度指令“等于”最难达标\n\n当模型被明确要求生成特定长度的文本时，大多数模型表现糟糕。\n\n在26个模型中，有23个模型的长度评分（LS）低于60分，只有少数模型（如o3-mini、Claude-Sonnet-Thinking和Gemini-2.5-Pro）勉强达到了75.4分、61.3分和60分。\n\n在“不超过”（At Most）和“至少”（At Least）指令下，由于限制更宽松，模型表现显著改善。其中，有19个模型在“不超过”指令下的长度评分超过90分，而“至少”指令下也有6个模型表现优异。\n\n2. 长文本生成：模型普遍“拉胯”\n\n大多数模型在短文本限制下表现稳定，如o3-mini和Gemini-2.5-Pro分别获得了80分和70分以上的长度评分。随着长度限制增加，模型的表现开始下降。虽然o3-mini依然保持了较强的稳定性（评分>70），但Gemini-2.5-Pro的评分从81分骤降至37分。\n\n在长文本生成任务中，所有模型的长度评分均显著下降，普遍低于40分，长文本生成成为模型的最大挑战。\n\n3. 输入特性：任务与语言的双重影响\n\n模型在不同任务中的表现差异显著。摘要任务的长度评分最低，有19个模型在这一任务中的表现显著下降，创意生成任务的评分则最高，14个模型表现优异。\n\n几乎所有模型在中文任务中的表现均劣于英文任务。此外，模型在处理中文指令时，出现了明显的“过度生成”现象，可能反映了模型对中文数据的处理能力不足。\n\n当面对极限长度指令时（比如“至少生成32,768字”），大部分大语言模型的表现堪称“言过其实”。它们的宣传似乎暗示自己是“长篇巨制大师”，但实际生成结果却经常让人失望。研究发现：\n\n1. 仅少数模型达标\n\n在26个模型中，只有Claude系列和Qwen系列的7个模型能在其10%最长输出中勉强符合长度要求。如果将目标放宽到25%最长输出，情况依然不乐观——只有Qwen2.5-72B-Instruct和Qwen3-235B-A22B达到了设定的长度要求。这些模型虽然声明的最大输出长度较其他模型“低调”许多，但恰恰因为如此，它们的表现更接近实际能力，算得上“务实派”。\n\n2. 大部分模型表现不符预期\n\n其他模型则颇具“宣传艺术”。除Gemini-2.0-Flash和部分Qwen系列模型因最大token限制受限外，其余模型的表现远低于它们声称的“最大输出能力”。换句话说，这些模型的不足并不是因为无法达到技术上限，而是生成能力本身存在局限性。\n\n有些模型在宣传时或许给人一种“我可以写出战争与和平”的错觉，但实际上，生成一篇“长篇朋友圈”都可能显得力不从心。\n\n基于上面的实验结果，论文深入分析了这个问题，总结出以下三大核心瓶颈：\n\n1. 缺乏准确的长度感知能力\n\n很多模型在“理解”目标长度上显得模糊不清：短输出任务时高估长度：目标是100字，模型可能“热情过度”写到150字。而长输出任务时反而低估长度：目标是5000字，模型却生成3000字，仿佛在说“这么长，够用了吧？”，除此之外模型还有假遵循现象：有些模型生成后自信满满地“认为自己已经完成了任务”，但实际结果却大相径庭：这种现象表明，模型更像是在“自我感觉良好”，而非真正理解并执行了指令。\n\n2. 对输入长度的敏感性\n\n输入文本的长度对模型的表现影响很大，当输入过长时，模型就有些“晕头转向”了，特别是在长输入场景（>5000字）中。\n\n这也解释了为什么摘要任务尤为糟糕：面对长篇输入时，模型不仅难以提取关键内容，还会生成过短或过长的内容，严重偏离指令要求。可以说，输入越长，模型越容易“迷失在海量信息中”。\n\n3. 懒惰生成策略\n\n当面临复杂的长文本任务时，许多模型选择了“偷懒”：\n\n提前终止：有些模型会在未完成任务的情况下突然“省略”后续部分，例如直接插入提示“（接下来还有6000字）”，仿佛在暗示“我知道还没写完，但后面的就不写了”。\n\n拒绝生成：在遇到超长的任务时，一些模型会直接选择放弃，例如明确表示“你的要求长度已经超过了我的能力极限，无法完成”。这种情况下，模型既没有尝试生成部分内容，也没有提供替代方案，而是干脆拒绝执行指令。\n\n研究发现，当目标长度超过8192字时，拒绝生成的比例显著上升，所有模型中平均超过10%因这种懒惰策略而失败。显然，越复杂的任务，模型越倾向于“放弃治疗”。\n\n除了上面的三个瓶颈，有一些模型也尝试解决这个问题：\n\n4. 动态校准的局限性：一场“低效的修补”\n\n为了纠正长度偏差，一些推理模型尝试了动态校准：\n\n他们会在推理过程总生成初稿后逐字统计输出长度，发现长度不符时选择重新生成，如此往复，直至接近目标长度。\n\n虽然这个方法在短文本任务中相对有效，但是耗时耗力，因为动态校准需要耗费大量计算资源和生成token，大幅增加时间成本。而且动态校准在长文本场景中就会失效：由于校准过程过于低效，模型无法在长文本任务中维持相似的策略，最终还是无法完成指定长度的内容。\n\n换句话说，动态校准看似“聪明”，但面对长文本时，最终还是成了一场“得不偿失”的努力。\n\n从三大“瓶颈”到动态校准的局限性，我们可以看到：大语言模型在长度指令遵循上的表现还有很多不足。要让这些模型真正“听话”，需要在感知能力、信息处理能力和生成策略上进行全面优化。\n\n通过更深入的分析，研究揭示了一些隐藏在模型长度指令遵循能力背后的有趣现象和改进可能。以下是关键发现：\n\n1. 长文本生成质量的“起伏之路”\n\n模型在不同长度限制下的表现如同一条“起伏的曲线”：\n\n短文本（512字）：“还行”：生成质量较高。\n\n中等长度（1024–2048字）：“巅峰表现”：大多数模型在这个区间表现最好，输出逻辑清晰，内容质量稳定。\n\n长文本（4096–8192字）：“质量滑坡”：许多模型在此阶段开始掉链子，生成内容重复甚至拒绝生成。例如，有些模型会在生成到一半时插入“（接下来还有6000字）”，直接“摆烂”。\n\n少数模型（如Claude-3.7-Sonnet）在超长文本上偶尔“逆风翻盘”，但这类情况较为罕见。大多数模型的长文本内容，质量随长度增加而显著下降，重复问题尤为突出。\n\n2. 格式化输出的“叠加挑战”\n\n在要求遵循长度指令的同时，还需要生成特定格式（如Markdown、HTML或LaTeX）时，模型的表现进一步恶化，复杂格式让模型“抓狂”：格式越复杂，模型越容易出错，甚至格式和内容双双崩溃。\n\n长文本中的额外压力：在8192字限制下，生成一篇带复杂格式的文档对模型来说几乎是“地狱难度”。生成的内容不仅格式错误，甚至可能中途放弃，输出一堆不完整的内容片段。\n\n3. EoS信号的“提前规划”\n\n在长文本生成任务中，EoS（End of Sequence，生成结束信号） token的异常行为揭示出一些有趣的现象：\n\n短文本时表现乖巧：在2000字以下的限制下，模型的EoS预测较为正常，生成内容完整且符合目标要求，EoS信号通常在内容接近目标长度时触发。\n\n长文本时“提前规划”倾向：当目标长度达到4096或8192字时，模型的行为变得耐人寻味——它似乎在生成开始前就“打好了自己的算盘”。EoS信号的触发概率一开始就显著升高，导致生成的内容远远少于目标长度，甚至仅生成寥寥数百字便戛然而止。这种现象表明，模型在生成之前可能已经“规划”好了要写多少，而不是在生成过程中逐步调整。\n\n这种提前终止的行为可能源于模型在长文本生成中的不确定性或自我限制，反映了其对任务长度的规划能力仍存在局限性。模型在面对超长文本指令时，可能会倾向于“保守估计”，提前结束生成以避免过度消耗计算资源或偏离任务要求。\n\n4. 预训练与后训练的“双管齐下”\n\n模型在长文本生成中的不足，既源于预训练的限制，也可以通过后训练优化：\n\n预训练的“偷懒基因”：由于预训练阶段长文本数据覆盖不足，模型可能学到了一些“偷懒策略”，比如提前终止或拒绝回答，以规避长文本中的复杂逻辑和连贯性问题。\n\n后训练的“预规划策略”：后训练提供了改进的机会。通过让模型在生成前先规划整体结构或章节大纲，生成内容更贴合长度要求，逻辑也更加清晰。例如，模型可以先生成“目录”，再逐步填充内容。这种方法显著提升了长文本的质量，且让模型对长度指令的遵循更为精准。\n\n从生成质量的“起伏之路”到复杂格式的双重挑战，再到EoS信号的“提前规划”，这些隐藏的现象揭示了模型长度指令遵循能力的深层次不足。不过，通过扩充预训练数据和引入预规划策略，未来的模型完全有希望实现“字够了，内容也对了”。\n\n论文提出了 LIFEBENCH，用于评估大型语言模型（LLMs）在多种任务、语言和长度限制下遵循长度指令的能力。\n\n分析表明，当前 LLMs 在长度指令执行上仍存在显著问题，尤其在长文本限制下，生成长度常低于声称的能力范围，甚至表现出“提前结束””的倾向。模型表现还受到任务类型、语言和输入长度等因素的显著影响。\n\n这些发现揭示了 LLMs 在长度指令遵循上的关键短板，表明未来需要更优的训练策略，以及更全面的评估体系，来提升其对长度指令的执行能力和实际表现。\n\ngithub仓库: https://github.com/LIFEBench/LIFEBench\n\nhuggingface链接: https://huggingface.co/datasets/LIFEBench/LIFEBench\n\n论文地址: https://arxiv.org/abs/2505.16234\n\n一键三连「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n🌟 点亮星标 🌟",
    "published_time": "2025-05-29T04:42:32.000Z",
    "download_time": "2025-05-29T23:49:37.340212",
    "visual_resource": [
      "screenshot/wechat_wx_80e9060d.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:42:32.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAHkwSrvicgK1yjPVBOuCG2fBicIFjm6vhjHSPrBaqGC6h8efuzTicxTOzFLYgcTa8MFia0vcUHcsU7BQ/0?wx_fmt=jpeg\", \"id\": \"VE-3UCGJrHQ3feBga7svzA\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/VE-3UCGJrHQ3feBga7svzA.txt\"}}"
  },
  {
    "id": "fq3F4OnOaG9PaF4_NKuRyg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/fq3F4OnOaG9PaF4_NKuRyg",
    "title": "成本暴降88%！通义实验室、北大发布ZeroSearch，无需搜索即可激活LLM检索能力",
    "summary": "通义实验室与北京大学联合发布ZeroSearch框架，旨在解决大型语言模型（LLM）强化学习训练中，因频繁调用真实搜索引擎导致的高昂API成本及文档质量不可控问题。ZeroSearch通过创新性地利用LLM模拟搜索引擎，结合结构化训练模板、模拟搜索微调和基于课程学习的文档生成策略，实现了训练成本降低88%，并在多项任务上超越依赖真实搜索的方法。该框架显著提升了LLM的检索能力和推理表现，展示了其在基础模型和指令微调模型上的强大泛化能力，以及通过仅3B参数规模的模型便能激活检索能力，14B模型甚至超越谷歌搜索引擎的潜力，为LLM的推理能力激发提供了经济高效且高性能的新范式。",
    "keywords": [
      "ZeroSearch",
      "大模型",
      "强化学习",
      "检索增强生成",
      "成本优化",
      "课程学习",
      "检索能力"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "智能体"
    ],
    "content": "标题：成本暴降88%！通义实验室、北大发布ZeroSearch，无需搜索即可激活LLM检索能力\n公众号：机器之心\n--------------------------------------------------\n\n本文作者来自通义实验室和北京大学，第一作者是北京大学智能学院博士生孙浩，主要研究方向是RAG和Agent，在 NeurIPS、ACL、EMNLP 等国际顶级会议上发表多篇论文，师从张岩教授。该工作在阿里巴巴通义实验室RAG团队实习期间完成。\n\n信息检索能力对提升大语言模型 (LLMs) 的推理表现至关重要，近期研究尝试引入强化学习 (RL) 框架激活 LLMs 主动搜集信息的能力，但现有方法在训练过程中面临两大核心挑战：\n\n文档质量不可控：真实搜索引擎返回内容不可控，训练过程易受噪声干扰。\n\n搜索 API 成本高昂：Rollout 阶段频繁调用搜索 API，训练成本极高。\n\n为了解决这些问题，我们提出了 ZeroSearch 框架 —— 无需真实搜索，直接用大语言模型模拟搜索引擎，并引入课程学习策略，在显著降低 88% 成本的同时，在多项任务上性能超过依赖真实搜索引擎的方法。\n\n论文标题：ZeroSearch: Incentivize the Search Capability of LLMs without Searching\n\n论文地址：https://arxiv.org/pdf/2505.04588\n\n代码地址：https://github.com/Alibaba-NLP/ZeroSearch\n\n项目主页：https://alibaba-nlp.github.io/ZeroSearch\n\nHuggingface 主页：https://huggingface.co/collections/sunhaonlp/zerosearch-v2-6827f4ee6b6265069d443d4e\n\n方法\n\n无需搜索的强化学习框架\n\n传统训练方法需要在 Rollout 阶段频繁与真实搜索引擎交互，产生大量 API 开销，而大语言模型在预训练阶段积累了丰富的世界知识，具备根据 query 返回相关信息的能力，因此 ZeroSearch 创新性地引入大语言模型作为模拟搜索引擎（Simulation LLM），无需真实搜索，即可为策略模型生成检索文档，大幅降低了训练成本：\n\n为了避免策略模型记住由 Simulation LLM 生成的文档，我们对文档进行了损失屏蔽（Loss Masking），仅对策略模型自己生成的 token 进行损失计算。\n\n结构化训练模板\n\nZeroSearch 无需初始监督微调（SFT），直接对预训练语言模型进行强化学习训练，通过采用结构化的训练模板，引导模型在每一轮交互中划分思维步骤：\n\n<think > 对已有信息分析，明确下一步行动 </think>\n\n<search > 提炼搜索 query </search>\n\n<answer > 总结推理过程，形成最终答案 </answer>\n\n这种结构化模板提升了模型推理路径的清晰度和可解释性，格式化的输出便于提取最终答案进行奖励计算。\n\n搜索模拟微调\n\n直接通过 Prompt 指导 LLM 生成的模拟检索内容，往往与真实搜索引擎返回的检索内容风格差异较大，且质量不稳定。为了解决这些问题，我们采用了模拟微调策略，具体包含以下三步：\n\n轨迹采集：从策略模型与真实搜索引擎的交互中采集 Query-Document 对\n\n质量评估：利用 Qwen-Max 作为评审，对文档进行有用性判别\n\n监督微调：构建高质量训练集，进行轻量级微调 (2 万条数据，7B 模型训练时间仅需 30 分钟)\n\n此外我们还在 Prompt 内引入原始问题的正确答案，从而扩充 Simulation LLM 的知识边界。\n\n基于课程学习的文档生成策略\n\n经过微调的 Simulation LLM 可通过调整在 Prompt 中添加 Useful/Noisy 指令，灵活控制生成文档的质量。基于这一能力，我们进一步引入了课程学习策略，通过逐步降低文档质量，循序渐进地提升训练难度，从而更有效地激发模型的推理能力。\n\n为实现训练难度的平滑过渡，我们设计了一个指数函数来控制 Noisy 文档的生成概率：\n\n训练初期：训练难度上升缓慢，模型能够稳步学习基本的输出格式以及任务逻辑。\n\n训练后期，训练难度快速上升，从而促使模型不断强化其推理能力与鲁棒性。\n\n该由易到难的训练过程能够持续激发策略模型的推理能力，有效提升强化学习训练的稳定性与最终表现。\n\n奖励函数设计\n\n在实验中，我们发现使用 Exact Match 作为奖励会诱导模型生成冗长内容以 “碰中” 答案，出现 Reward Hacking 问题，我们改用 F1 Score 作为奖励指标，更加关注输出的准确性与简洁性，有效抑制了冗余答案的产生。此外，我们发现模型在训练中即便不显式监督输出格式，也能生成结构规范的回答，因此没有引入格式奖励。\n\n实验结果\n\n主要性能表现\n\nZeroSearch 超越所有基线方法，该性能优势在域内以及域外数据集上均得以体现，展示了我们方法的鲁棒性。\n\nZeroSearch 的表现优于依赖真实搜索引擎的方法 Search-R1，凸显其在大规模强化学习中替代真实搜索引擎的潜力。\n\nZeroSearch 展现了强大的泛化能力，随着模型参数量增加，其性能进一步提升，体现了良好的扩展性。\n\n与真实搜索引擎对比\n\nZeroSearch 与真实搜索的奖励趋势相似，随着训练的推进，ZeroSearch 和 Search-R1 的奖励分数都稳步上升。\n\nZeroSearch 的奖励提升更加显著，虽然在训练初期 ZeroSearch 的奖励值低于 Search-R1，但它最终实现了超越，并且波动更小。\n\nZeroSearch 在基础模型和指令微调模型中都展现了良好的泛化能力，在这两类模型下，ZeroSearch 的奖励表现都持续提升。\n\n模拟搜索设定对比\n\n相对于 Base Model，不同类型的 Simulation LLM 均可有效激发策略模型的搜索能力。\n\n基于 Prompt 的方法效果较差，主要由于其生成的文档风格与真实搜索引擎差异较大，且质量不稳定，难以支撑稳定训练。\n\n经过微调的 Simulation LLM，即便仅有 3B 参数量，也能显著提升策略模型性能；随着模型规模扩大，性能进一步提升：SFT-7B 可达到与 Google 相当的效果，SFT-14B 甚至实现超越 Google 的性能。\n\n交互轮数研究\n\n训练初期：交互轮数迅速下降，奖励缓慢上升\n\n此阶段模型尚未掌握搜索调用机制，经常产生冗余交互，检索效果不佳。\n\n训练中期：交互轮数迅速回升，奖励同步显著提升\n\n模型逐渐学会如何高效调用搜索引擎，能够获取准确信息，回答质量显著提高。\n\n训练后期：交互轮数和奖励趋于稳定\n\n模型已适应数据集的跳数分布，交互策略逐步固化，在课程学习设定下，模型需提升推理能力以应对更低质量文档，从而维持较高奖励水平。\n\n课程学习策略研究\n\n实验结果表明，Curriculum（由易到难）训练策略显著优于 Random（随机难度）训练策略，验证了该训练范式在激发模型推理能力方面的有效性。\n\n与 Random 类似，真实搜索引擎在训练过程中难以控制文档难度，导致模型缺乏系统性的能力提升路径，从而限制了推理能力的持续进化。\n\n总结\n\n本文提出了 ZeroSearch，一种无需真实搜索引擎即可激活大语言模型搜索能力的强化学习框架，我们使用轻量级的监督微调将 LLM 转变为一个检索模块，在 RL 训练阶段，我们基于课程学习逐步降低检索模型生成文档的质量，通过不断提升检索难度，逐步激发策略模型推理能力。\n\n大量实验表明，ZeroSearch 使用 3B 参数规模的模型作为检索模块时即可激发语言模型检索能力，7B 模型的检索性能已接近真实搜索引擎，而 14B 模型甚至实现了超越。此外，ZeroSearch 对基础模型和指令微调模型均具有良好的泛化能力，并可兼容多种 RL 算法，具备极强的适应性与可扩展性。\n\n© THE END\n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com",
    "published_time": "2025-05-29T04:53:51.000Z",
    "download_time": "2025-05-29T23:47:14.546914",
    "visual_resource": [
      "screenshot/wechat_wx_59538ad4.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:53:51.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibPsWibzGOHYBMyw9VU4bSiaTlBxU3NAvbEnYiaSUz5BfagZme4w2dcMWxn2k3QbrpCDxtQmRKjmXTlw/0?wx_fmt=jpeg\", \"id\": \"fq3F4OnOaG9PaF4_NKuRyg\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/fq3F4OnOaG9PaF4_NKuRyg.txt\"}}"
  },
  {
    "id": "mDSMsZjDuSl5qcWckFSXrA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/mDSMsZjDuSl5qcWckFSXrA",
    "title": "还得是华为！Pangu Ultra MoE架构：不用GPU，你也可以这样训练准万亿MoE大模型",
    "summary": "华为盘古团队发布Pangu Ultra MoE模型架构与训练方法，揭示其如何在昇腾NPU上成功训练准万亿MoE大模型而无需GPU。通过创新性的DSSN稳定架构和TinyInit小初始化方法，该模型解决了超大规模MoE训练稳定性难题，实现10+T tokens数据长稳训练。同时，引入EP group loss优化负载均衡，提升专家特化能力。Pangu Ultra MoE还融合MLA和MTP等先进架构，配合Dropless训练及迭代强化学习等技术，显著提升模型效率和推理性能。这标志着华为在芯片协同大模型领域取得突破性进展，为超大规模AI模型训练提供了新范式。",
    "keywords": [
      "PanguUltraMoE",
      "华为",
      "昇腾NPU",
      "MoE大模型",
      "训练方法",
      "稳定性",
      "负载均衡",
      "投机推理"
    ],
    "area": [
      "人工智能",
      "大模型",
      "深度学习"
    ],
    "content": "标题：还得是华为！Pangu Ultra MoE架构：不用GPU，你也可以这样训练准万亿MoE大模型\n公众号：机器之心\n--------------------------------------------------\n\n机器之心编辑部\n\nPangu Ultra MoE 是一个全流程在昇腾 NPU 上训练的准万亿 MoE 模型，此前发布了英文技术报告[1]。最近华为盘古团队发布了 Pangu Ultra MoE 模型架构与训练方法的中文技术报告，进一步披露了这个模型的细节。\n\n训练超大规模和极高稀疏性的 MoE 模型极具挑战，训练过程中的稳定性往往难以保障。针对这一难题，盘古团队在模型架构和训练方法上进行了创新性设计，成功地在昇腾 NPU 上实现了准万亿 MoE 模型的全流程训练。\n\n盘古团队提出 Depth-Scaled Sandwich-Norm（DSSN）稳定架构和 TinyInit 小初始化的方法，在昇腾 NPU 上实现了 10+ T tokens 数据的长期稳定训练。此外，他们还提出了 EP group loss 负载优化方法，这一设计不仅保证了各个专家之间能保持较好的负载均衡，也提升了专家的领域特化能力。同时，Pangu Ultra MoE 使用了业界先进的 MLA 和 MTP 架构，在训练时使用了 Dropless 训练策略。\n\n技术报告标题：Pangu Ultra MoE 模型架构与训练方法\n\n技术报告地址：https://raw.gitcode.com/ascend-tribe/pangu-ultra-moe/raw/main/Pangu_Ultra_MoE_CN_Report.pdf\n\n破解准万亿 MoE 模型性能瓶颈\n\n打造芯片协同的先进架构\n\n近期，盘古团队在 MoE 模型训练领域再进一步，重磅推出参数规模高达 718B 的准万亿全新模型 ——Pangu Ultra MoE。该模型旨在实现超大规模 MoE 架构在模型效果与效率之间的最佳平衡。\n\n为了达到这个目标，研究团队在设计 Pangu Ultra MoE 架构的时候，充分考虑昇腾硬件特性，在昇腾 NPU 平台上，融合计算、通信和内存等多维度指标，构建了大规模系统模拟器，并系统性地探索约一万个不同的 MoE 结构组合，最终搜索出一套在训练与推理吞吐上均达最优的架构方案。\n\nPangu Ultra MoE 是一个超大规模、高稀疏比的架构，同时也包含 MLA 和 MTP 等先进架构和特有的 DSSN 稳定性架构和 EP group loss 负载优化。下面是 Pangu Ultra MoE 的主要的架构和训练特性：\n\n超大规模和超高稀疏比：采用 256 个路由专家，每个 token 激活 8 个专家，模型总参数量 718B，激活量 39B。\n\nMLA 注意力机制：引入 MLA（Multi-head Latent Attention），有效压缩 KV Cache 空间，缓解推理阶段的内存带宽瓶颈，优于传统 GQA 方案。\n\nMTP 多头扩展：采用单头 MTP 进行训练，后续复用 MTP 参数扩展至多头结构，实现多 Token 投机推理，加速整体推理过程。\n\nDropless 训练：采用 Dropless 训练可以避免 Drop&Pad 训推不一致问题，并且提升训练的数据效率。\n\nRL 训练：采用迭代难例挖掘与多能力项均衡的奖励函数，并参考 GRPO 算法，提升了模型的训练效率与最终推理性能。\n\n以下是 Pangu Ultra MoE 昇腾亲和设计考虑：\n\n隐藏维度贴合硬件：设置 7680 维隐藏层，精准匹配昇腾芯片的 16×16 MatMul 单元，充分发挥 Cube 核心的计算潜力。\n\n层数亲和流水线并行：设置 61 层 Transformer 结构，并预留额外 MTP 层空间，保障计算负载均衡的 PP/VPP 流水线调度，减少 pipeline 气泡，提升整体并行效率。\n\n专家规模符合幂次规律：路由专家数量设为2⁸=256，在 TP×EP 并行下提升 All-to-All 通信效率，有效加速分布式训练。\n\nPangu Ultra MoE 的预训练阶段在 6k 到 10k 张 NPU 上进行，全流程采用 dropless 训练模式。预训练阶段进行了长序列扩展，最终模型具备 128k 长序列能力。在后训练阶段，Pangu Ultra MoE 移除了负载均衡辅助损失，保留专家间已有的特化能力，从而进一步提升模型对目标数据的学习效率。如表1所示，最终模型在多个权威开源评测集上展现出一流的效果。\n\n表 1: Pangu Ultra MoE 与目前主流模型效果对比\n\n面向超大MoE模型稳定训练新范式：\n\nDSSN结构和TinyInit加持\n\n梯度突刺率下降 51%\n\n支撑 10+T tokens 数据长稳训练\n\n随着参数规模和数据体量的激增，大模型训练面临前所未有的稳定性挑战。频繁的梯度范数突刺已成为阻碍收敛效率与模型性能提升的主要瓶颈。如何在确保训练深度和宽度扩展的同时，维持梯度信号的稳定传递，成为构建高可靠性大模型架构的关键课题。在 Pangu Ultra 稠密模型 [2] 的训练中，Depth-Scaled Sandwich-Norm 和 TinyInit 方法在保障训练稳定性上起到了关键性的作用，所以 Pangu Ultra MoE 依旧采用这个方案来控制训练稳定性。经过实验证明，此设计在 Pangu Ultra MoE 的训练中同样能起到增强稳定性、加快收敛速度的作用。\n\nDepth-Scaled Sandwich-Norm（DSSN）：传统的 Pre-LN 结构存在因为子层输出规模波动而导致训练不稳定的现象，DSSN 是为了解决这一问题而提出的。通过在每个子层输出后加入额外的层归一化，并引入深度缩放的初始化方式，从而稳定网络各层的输出尺度，达到抑制梯度异常、降低范数波动的目的。\n\nTinyInit：Transformer 模型普遍采用较小的初始化尺度，TinyInit 提出一种标准差为的初始化方案，能够同时兼顾模型深度与宽度，其中d表示隐藏维度，L表示模型层数。同时，对词嵌入层采用标准差为 0.5 的初始化。实验表明，这样的初始化策略有助于提升模型性能和训练稳定性。\n\nDepth-Scaled Sandwich-Norm + TinyInit 的方案减少了 51% 的突刺量（见图 1），缓解了梯度范数频繁突刺的问题，能够有效降低大模型训练过程中的不稳定性，加快模型收敛，提升模型性能。同时 DSSN+TinyInit 被应用到 Pangu Ultra MoE 中实现了 10+T tokens 数据的长稳训练。\n\n图 1: 训练过程的梯度范数对比图（黑色实线为突刺分界线）。DSSN+TinyInit 使梯度突刺率从 1.54% 下降到 0.76%，相对下降 51%。\n\n基于 EP group 的负载均衡：\n\n让计算效率和路由表达能力可以兼得\n\n在训练混合专家模型（MoE）时，容易出现专家负载不均衡的情况。负载不均衡指的是不同专家被分配的 token 数量存在显著的差距。当采用专家并行策略（EP，expert parallelism）时，负载不均衡会影响计算效率，被分配过多 token 的专家会成为计算瓶颈，而其他专家则处于低利用率状态。同时负载过低的专家可能存在训练不充分的问题，影响最终的模型效果。因此如何使 token 更均衡地分布至不同专家，对提高混合专家模型的训练效率和效果非常重要。\n\n为了保证负载均衡，一般通过增加辅助的负载均衡 loss（auxiliary loss）来约束 tokens 在专家之间均衡分布。然而，如果负载均衡 loss 过度地约束 tokens 分配的均衡性，也会影响模型路由的表达能力。之前主流的负载均衡 loss 一般是约束单个序列或者单个 micro batch 内的 token 分配均衡性，而单个序列往往是来自同一领域的数据，过度的均衡可能影响专家特化（expert specialization）。\n\n盘古团队发现对于采用专家并行策略训练的模型，可以设计一种对模型路由约束更小，同时不影响计算均衡性的 EP-Group 负载均衡 loss。当采用了专家并行，专家会被分配到不同卡上进行并行计算。每块卡上的专家会接收来自 EP 组内所有卡上的 micro batch 路由给自己的 token。所以可以设计一个负载均衡 loss，来约束 EP 组内所有 micro batch 路由到组内专家之后的均衡性。这相当于把 EP 组内部的所有 micro batch 联合起来计算负载均衡的 loss, 这样训练时可以容忍单个 micro batch 的不均衡，只要多个 micro batch 的 token 路由到专家之后是均衡的即可。\n\n为了验证 EP-Group 均衡损失函数的效果，盘古团队使用一个 20B 参数量的 MoE 模型进行了 100B 数据量的对比实验。结果如表 2 所示，可以看到 EP-Group 均衡损失函数在大部分任务相比主流的 Micro-batch 上都有显著的优势，平均提升了 1.5 个点。\n\n表 2: Micro-batch 和 EP-Group 的 auxiliary loss 效果比较\n\n同时盘古团队对 Pangu Ultra MoE 的专家特化进行了分析，结果如图 2 所示，可以看到不同领域的数据对专家的选择存在显著的差异，这表明 EP-Group 均衡损失函数给模型提供了灵活的路由选择空间，促进了专家特化。\n\n图 2: Pangu Ultra MoE 的专家特化。其中 ar，de，fr，ru 分别代表阿拉伯语，德语，法语，以及俄语。\n\n多 Token 投机推理新路径：\n\nMTP 头延迟扩展策略\n\n投机接受长度预期提升 38%\n\n投机推理是一种提升大模型生成效率的有效方法，其核心思想是在主模型生成 token 之前，由一个轻量辅助模块预先预测多个候选 token，并通过快速校验机制决定是否接纳，从而实现推理过程的并行化与加速。在当前大模型推理中，Multi-token Prediction（MTP）技术已成为实现多 token 级别投机生成的重要手段。\n\n盘古团队在实践中发现，获取多 token 的投机推理能力并不需要从训练开始便配置多个 MTP 头，而是可以在训练后期对单头 MTP 进行扩展来达到类似的效果。为验证这一策略的有效性，团队使用 20B MoE 为主干模型，训练 185B 数据。具体对比设置为：以两个 token 的投机推理为目标，分别训练了从头开始配置单 / 两个 MTP 头的模型（即单头从头训练和双头从头训练），以及在单头 MTP 模型训练至收敛后，通过复制已有头的参数再增训出第二个 MTP 头的模型。对于扩增的模型，对比全参续训以及冻结主干和一头的续训的效果，即双头扩增全参训练和双头扩增冻结训练。下游使用 LAMBADA 续写作为评测任务。\n\n结果如图 3 所示。双头扩增模型的接受长度和延迟基本和双头从头训练一致，而双头的接受长度约 2.30，单头的接受长度约 1.67，双头相对单头提升约 38%。在模型效果方面，双头扩增模型全参训练和从零训练相当，而由于冻住了主干和一头，双头扩增冻结训练的精度在扩增的位置基本保持不变。这表明后期的 MTP 扩展可以达到多头的从头训练的投机推理效果，可以在模型训练早期保持较小的 MTP 配置并在后期再进行扩展，兼顾计算成本和推理能力。\n\n图 3: 20B MoE 的 MTP 在 LAMBADA 续写上的投机推理结果。在接受长度上，双头相对单头提升约 38%，而双头可以基本无损地通过后期扩增单头得到。\n\n迭代难例挖掘与多能力协同：\n\n后训练强化学习持续提升的关键\n\n模型后训练的过程中，团队参考了业界常规的 GRPO 算法提升模型的推理性能。然而，在超大参数规模情况下，直接应用 GRPO 会带来两方面的问题：1. 算法训练需要依赖多回复通过率在 (0,1) 内的数据，随着模型性能的提升，相同 prompt 的推理结果准确率越来越高，导致训练过程中被 “浪费” 的数据不断增加，降低推理效率；2. 模型训练需要兼顾多能力协同提升，包括数学、代码和通用能力等，不同能力项的奖励函数设计会导致模型能力增长上的不匹配，出现 “跷跷板” 问题。\n\n图 4: Pangu Ultra MoE 的强化学习训练系统\n\n为了解决上述两个实践难题，盘古团队设计了 Pangu Ultra MoE 的强化学习训练系统，如图 4 所示，提升了大 MoE 模型的训练稳定性与推理性能。系统设计的关键在于两个部分：（1）迭代难例挖掘：模型阶段性更新后，从初始的数据池中进行多回复推理，选取回复通过率在 (0,1) 的数据组成 RL 训练数据池，以保持推理效率最大化；（2）多能力项奖励系统：为了确保模型多能力项协同提升，数学和代码均采用了基于规则的奖励，通用奖励模型则使用 LLM-as-a-judge 的方法对生成的回复质量进行评分，并对最终的 reward 进行归一化处理，保证了模型在多个能力项的综合表现。\n\n[1] Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs\n\nhttps://arxiv.org/abs/2505.04519\n\n[2] Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs\n\nhttps://arxiv.org/abs/2504.07866\n\n© THE END\n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com",
    "published_time": "2025-05-29T04:53:51.000Z",
    "download_time": "2025-05-29T23:47:27.988944",
    "visual_resource": [
      "screenshot/wechat_wx_4d9e09e5.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:53:51.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicx3biajeCNW6w51rG0cebQbwZbTspKfeXtibNVeerAiaNP24bpHyNwVo1DgNddDaPvdurj4t0MVpouQ/0?wx_fmt=jpeg\", \"id\": \"mDSMsZjDuSl5qcWckFSXrA\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/mDSMsZjDuSl5qcWckFSXrA.txt\"}}"
  },
  {
    "id": "wrq2ZkidYQ5PRGl525ol8A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/wrq2ZkidYQ5PRGl525ol8A",
    "title": "刚刚，AI科学家Zochi在ACL「博士毕业」，Beta测试今日上线",
    "summary": "Intology公司近日宣布其“AI科学家”Zochi的论文被自然语言处理顶级会议ACL主会录用，标志着Zochi成为首个独立通过A*级别科学会议同行评审的人工智能系统，初步实现“博士级”智能体。Zochi自主完成了针对大型语言模型的“越狱”方法“Tempest”的设计、实验及论文撰写，成功率高达97-100%，揭示了当前LLM安全机制的潜在漏洞。尽管其提交方式引发部分争议，但Zochi在模型微调、生物计算等多个领域展现出卓越的自主研究能力和超越人类中位数表现的水平，预示着AI在科学发现领域的巨大潜力。",
    "keywords": [
      "AI科学家",
      "Zochi",
      "ACL",
      "同行评审",
      "大型语言模型",
      "智能体",
      "越狱",
      "自主研究"
    ],
    "area": [
      "人工智能",
      "智能体",
      "大模型"
    ],
    "content": "标题：刚刚，AI科学家Zochi在ACL「博士毕业」，Beta测试今日上线\n公众号：机器之心\n--------------------------------------------------\n\n编辑：+0\n\n又有一个 AI Scientist 的论文通过了顶会同行评审。\n\n今天，Intology 宣布他们的 AI 科学家 Zochi 的论文被顶会 ACL 主会录用，成为首个独立通过 A* 级别科学会议同行评审的人工智能系统 ，同时开放了 Zochi 的 Beta 测试。\n\nBeta 注册地址：https://docs.google.com/forms/d/e/1FAIpQLSeOMmImoaOchxihSkcBUNQIT65wq62aiHq8wfnyrK0ov4kTOg/viewform\n\n近几个月来，多个团队已证明了人工智能在研讨会级别的会议上能做出贡献，此前 Sakana 的 AI Scientist-v2 就以均分 6.25 通过了 ICLR 会议一个研讨会的同行评审，详见机器之心报道《AI 写的论文能过审？双盲评审 6.25 分，达到 ICLR 研讨会水平》。\n\n但论文被顶级科学会议的主会议录用，则意味着跨越了一个高得多的门槛。\n\n提交给 ICLR 2025 的研讨会论文录用率约为 60-70%，而像 ACL（以及 NeurIPS、ICML、ICLR、CVPR 等）这样的顶级会议的主会议录用率仅为 20% 左右。 ACL 是全球自然语言处理 (NLP) 领域排名第一的科学会议，在全球所有科学会议中排名前 40。\n\n此类顶级会议主会议的同行评审过程旨在进行高度筛选，对新颖性、技术深度和实验严谨性都有着极为严格的标准。大多数计算机科学领域的博士生需要花费数年时间才能在同等声望的会议上发表论文。\n\n这使得 Zochi 成为首个达到博士级别的智能体：人工智能系统首次独立完成了科学发现，并将其发表在与该领域顶尖研究人员相当的水平上。\n\nTempest：基于树搜索的大型语言模型自主多轮「越狱」\n\n话不多说，我们先来看看这篇论文吧。\n\n论文标题：Tempest: Automatic Multi-Turn Jailbreaking of Large Language Models with Tree Search\n\n论文地址：https://arxiv.org/pdf/2503.10619\n\n该研究的前期版本（名称为 Siege）曾被 ICLR 研讨会接收。后续，Zochi 对其设计进行了修改，并为提交 ACL 进行了更广泛的实验。\n\n这项研究的一个特点是其自主性程度：人类研究者仅设定了「开发新型『越狱』方法」的初始目标。Zochi 随后独立确定了多轮攻击这一具体研究方向，设计了 Tempest 方法，编写代码并进行了测试，执行了所有实验，并撰写了论文草稿。人类的参与主要限于图表创建和格式修订。\n\n该研究从分析「越狱」相关文献开始，设计了一种基于树搜索的方法。该方法利用并行探索同时扩展多个对抗性提示分支，并集成了跨分支学习和部分合规跟踪功能。系统自主实现了 Tempest，并在多个大型语言模型上进行了评估。\n\n评估结果显示，Tempest 在 GPT-3.5-turbo 上的成功率为 100%，在 GPT-4 上的成功率为 97%。与所比较的单轮和多轮基线方法相比，Tempest 在使用较少查询次数的情况下达到了更高的成功率。\n\n这项工作的结果提示，语言模型的安全措施可能通过多轮对话被系统性地绕过，其中逐步的策略性互动可能导致模型产生原本被限制的输出。这些发现反映了当前安全机制中可能存在的某些不足，并为研究更有效的多轮对抗攻击防御策略提供了数据和视角。\n\n批评风波\n\n2025 年 3 月 18 日，Intology 宣布推出了 Zochi， 并称其为世界上第一位 「做出最先进贡献」的 AI Scientist，它的研究成果已被 ICLR 2025 研讨会接收。\n\nIntology 官网：https://www.intology.ai/\n\n通过标准化的自动审稿人评估，Zochi 的论文平均得分为 7.67 分，而其他由人工智能系统生成的公开论文得分在 3 到 4 分之间。\n\n但 Intology 很快就陷入了批评风波。Sakana、Intology 和 Autoscience 都声称其使用 AI 生成的研究被 ICLR 接受，但只有 Sakana 在提交其 AI 生成的论文之前向 ICLR 领导通报了此事，并获得了同行评审者的同意。\n\n几位 AI 学术界人士在社交媒体上批评了 Intology 和 Autoscience 的行为，认为这是对科学同行评审过程的滥用。\n\n关于 Zochi\n\nZochi 是一个 AI research agent，能够自主完成从文献分析 到同行评审出版 的整个科学研究过程。该系统通过一个旨在模拟科学方法的多阶段流水线进行运作。\n\n技术报告：https://github.com/IntologyAI/Zochi/blob/main/Zochi_Technical_Report.pdf\n\n代码：https://github.com/IntologyAI/Zochi\n\nZochi 的工作成果\n\n通过正交知识空间实现高效模型自适应\n\n为解决模型微调（PEFT）中的「跨技能干扰」问题，Zochi 提出了 CS-ReFT。该方法创新地通过学习「正交子空间表征」来编辑模型行为，而非修改权重。这使得 Llama-2-7B 仅用 0.0098% 的参数就实现了 93.94% 的 AlpacaEval 胜率，超越了 GPT-3.5-Turbo，并获得了同行的高度评价。\n\n通过自主多轮红队测试发现 AI 漏洞\n\n在 AI 安全方面，Zochi 开发了 Siege 框架，利用树搜索算法进行高效的「多轮越狱」攻击。通过识别并利用 LLM 的「部分遵从」漏洞，Siege 对 GPT-3.5 和 GPT-4 实现了极高的攻击成功率（100%/97%），提示需要重新评估现有防御策略。其扩展工作已被 ACL 2025 接收。\n\n计算生物学进展（EGNN-Fusion）\n\nZochi 将 AI 技术应用于计算生物学，推出了 EGNN-Fusion，用于预测蛋白质 - 核酸结合位点。该方法在保持顶尖性能的同时，将参数数量锐减了 95%，证明了 Zochi 在解决复杂跨学科科学问题方面的强大实力和多功能性。\n\n评估结果\n\n与所有基线系统相比，Zochi 持续产出更高质量的研究论文。在使用基于 NeurIPS 会议指南的自动审稿人进行评估时，Zochi 的论文获得了 8、8 和 7 的高分，均远高于顶级机器学习会议平均录用论文 6 分的接收门槛。\n\n相比之下，其他 AI 系统的论文得分要低得多，平均约为 4 分。考虑到每个系统处理的问题复杂性存在巨大差异，这种评估差距尤其显著。基线系统专注于相对受限的问题 —— 例如二维扩散模型、玩具规模的语言模型或特定的认知偏差 —— 而 Zochi 则致力于解决开放式挑战，提出新颖且可验证的最先进方法。\n\n作为一项探索性练习，Zochi 在 MLE-Bench 的部分基于 Kaggle 的挑战上进行了评估，以考察其在传统机器学习工程任务上的表现。在没有任何任务特定优化的情况下，Zochi 达到了最先进水平，在 80% 的任务上超过了人类表现中位数，并在 50% 的任务中获得奖牌。这些成果超过了之前的基准测试，如 Agent Laboratory、AIDE 和 OpenHands，进一步突显了 Zochi 核心能力的稳健性和适应性。\n\n© THE END\n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com",
    "published_time": "2025-05-29T04:53:51.000Z",
    "download_time": "2025-05-29T23:47:37.462327",
    "visual_resource": [
      "screenshot/wechat_wx_fca33049.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:53:51.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicx3biajeCNW6w51rG0cebQb6Yog9YNdCHPOfcsrKkicOljClKWJVLAO2s0E4XFBu6nGmK5kpkfGLhg/0?wx_fmt=jpeg\", \"id\": \"wrq2ZkidYQ5PRGl525ol8A\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/wrq2ZkidYQ5PRGl525ol8A.txt\"}}"
  },
  {
    "id": "xK3mM0r8bH6XrREKGc0esA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/xK3mM0r8bH6XrREKGc0esA",
    "title": "RSS 2025｜从说明书学习复杂机器人操作任务：NUS邵林团队提出全新机器人装配技能学习框架Manual2Skill",
    "summary": "新加坡国立大学邵林团队在RSS 2025上发表全新框架Manual2Skill，旨在解决现有视觉语言模型（VLMs）在复杂机器人长时程操作任务中，因稀缺演示数据导致泛化受限的问题。该框架受人类学习启发，使机器人能通过解析人工设计的视觉说明书自主理解并执行家具装配等任务。Manual2Skill包含任务规划、分步位姿估计及动作生成三核心阶段，通过VLMs将抽象指令转化为机器人可执行动作。实验验证该方法在实际家具装配中表现出鲁棒性与有效性，显著优于传统方法，并实现零样本泛化，大幅降低了机器人复杂技能获取的成本与复杂度。",
    "keywords": [
      "机器人",
      "视觉语言模型",
      "家具装配",
      "技能学习",
      "说明书",
      "位姿估计",
      "动作生成"
    ],
    "area": [
      "人工智能",
      "机器人",
      "多模态"
    ],
    "content": "标题：RSS 2025｜从说明书学习复杂机器人操作任务：NUS邵林团队提出全新机器人装配技能学习框架Manual2Skill\n公众号：机器之心\n--------------------------------------------------\n\n本文共同第一作者为新加坡国立大学博士生铁宸睿和多伦多大学研究助理/本科生孙圣翔。合作者为朱锦轩、刘益伟、郭京翔、胡越、陈浩楠、陈俊廷、吴睿海。通讯作者为新加坡国立大学计算机学院助理教授邵林，研究方向为机器人和人工智能。\n\n视觉语言模型（Vision-Language Models, VLMs），为真实环境中的机器人操作任务提供了极具潜力的解决方案。\n\n尽管 VLMs 取得了显著进展，机器人仍难以胜任复杂的长时程任务（如家具装配），主要受限于人类演示数据和训练样本的稀缺性。\n\n为解决这一问题，研究团队提出 Manual2Skill，一种基于 VLMs 的创新框架，使机器人能通过高级视觉说明书自主理解并执行家具装配任务，模仿人类学习装配的过程。该方法弥合了抽象指令与物理执行之间的鸿沟，显著提升了机器人在真实操作场景中的实用性。\n\n目前，该论文已被机器人领域顶级会议 Robotics: Science and Systems XXI（RSS 2025）接收。\n\n论文链接：https://arxiv.org/abs/2502.10090\n\n项目主页：https://owensun2004.github.io/Furniture-Assembly-Web/\n\n家具装配是一项复杂的长时程任务，要求机器人：(A) 理解所有零件的拼接关系和顺序；(B) 估计每一步拼接时部件的位姿；(C) 生成物理可行的动作以完成部件组装。\n\n尽管许多计算机视觉方法通过几何或语义技术在部件位姿预测（B）方面取得显著成果，但它们大多忽视了同样关键的拼接顺序理解（A）和动作生成（C）环节 [1, 2]。\n\n现有的端到端机器人装配系统通常依赖模仿学习或强化学习。虽然在某些场景下有效，但这些方法需要大规模数据集和大量计算资源，难以推广至真实环境中的通用长时程操作任务 [3, 4]。\n\n近年来，视觉语言模型（VLMs）在高层规划、环境理解甚至直接机器人控制方面展现出潜力。部分研究尝试整合这些能力用于机器人装配，但多局限于简单几何物体且在真实装配场景中鲁棒性不足 [5]。\n\n关键问题在于，现有 VLM 方法（乃至多数当前方法）缺乏对结构化外部指导（如人工设计的说明书）的利用。这种缺失限制了它们在依赖抽象符号指令的复杂装配任务中的表现。\n\n相比之下，人类能够从抽象的说明书中提取信息并学习操作技能，这揭示了机器人能力的一个重要缺口：从抽象的、为人类设计的指导信息中学习物体操作技能。\n\n凭借强大的视觉与语言推理能力，VLMs 为弥合这一缺口提供了独特机遇。通过挖掘说明书中的结构化知识，VLMs 可使机器人更高效可靠地完成复杂多步骤装配任务。\n\n为解决复杂长时程装配的局限性，研究团队开发了 Manual2Skill —— 一种创新框架，利用 VLMs 将基于说明书的视觉指令转化为机器人装配技能。\n\nManual2Skill 包含三个核心阶段：\n\n图 1：Manual2Skill 框架\n\n该框架解决了现有机器人装配方法的两大核心限制：\n\nManual2Skill 的首阶段将人类可理解的说明书转化为机器人可执行的任务规划。通过视觉语言模型（GPT-4o）对说明书示意图和预装配场景图像进行联合推理，生成编码家具部件与子组件结构关系的层级化装配图。\n\n在此图中：\n\n为构建该图，Manual2Skill 通过整合多模态输入，特别是多张图像的视觉信息与文本指令组成的多轮提示序列，完成两个关键子阶段：\n\n该结构化图表征为下游位姿估计与运动规划奠定基础，确保复杂装配任务的精准顺序执行。\n\n在层级化装配图确定部件组合与装配顺序后，本阶段预测每个装配步骤中所有部件的 6D 位姿，实现部件间的精确物理对齐。\n\n与过往方法通常一次预测整个装配过程中所有零件的位姿不同，这里我们对每个装配步骤，预测这一步中涉及到的所有部件/子组件的位姿，这一设置既更贴合真实世界中的拼装过程，也能使模型避免单次输入部件数量过多引起的性能下降。\n\n同时我们还发现，尽管家具的形态有很大差别，但其基本部件的连接方式（比如板和棍的连接）较为固定，这种分步预测的方法能使模型更好地学习到这种基本连接方式，从而对测试集的物体实现更高的预测精度。\n\n为实现此目标，跨模态位姿估计模型对说明书图像与家具部件 3D 点云进行联合推理。模型架构包含四个核心组件：\n\n给定说明书图像 I_i 和涉及部件的点云集合，处理流程如下：\n\n为确保预测的鲁棒性与准确性，模型采用复合损失函数：\n\n该设计使模型能够处理可变数量的输入部件，适应视觉相似/对称部件，以及泛化到训练集上未见过的新物体。\n\n最终阶段将预测位姿转化为真实世界的机器人动作，实现装配计划的自主执行。我们在这一阶段使用基于启发式的抓取策略和稳健的运动规划算法，让机械臂抓取对应部件，并将其放置在预测位姿。\n\n我们使用 FoundationPose 与 SAM 估计场景中所有部件的初始位姿。根据部件几何特征应用启发式抓取策略：\n\n棒状部件：沿主轴在质心处抓取。\n\n扁平薄片部件：使用夹具/平台固定后沿边界稳定抓取。\n\n抓取后，机器人使用 RRT-Connect（基于采样的运动规划器）计算从当前位姿到目标位姿的无碰撞轨迹。所有其他物体被视为避障点云。通过锚定位姿在轨迹中段重新评估抓取部件位置，确保精确跟踪与控制。\n\n实验在仿真与真实环境中对多款宜家家具进行，验证 Manual2Skill 的鲁棒性与有效性。\n\n图 2：层级化装配图生成结果\n\n我们在 102 本真实宜家家具说明书上测试了我们提出的层级化装配图生成方法的表现，可以看出，对于简单和中等复杂程度的家具（部件数 ≤ 6），我们的方法能比较准确地生成装配图，同时在所有复杂程度的家具上，我们的方法表现均显著优于基线方法。尽管所有方法在复杂家具上表现受限，但随着 VLM 性能的提升，我们方法的表现会随之提升。\n\n图 3：层次化装配图可视化\n\n我们从 PartNet 数据集中选取了三类物体（椅子、台灯、桌子），每类物体各 100 个，并且在 Blender 中渲染出这些物体部件组合的示意图作为说明书图片。\n\n图 4：位姿估计实验结果\n\n实验结果表明，凭借多模态特征融合与 GNN 空间关系建模，本方法在全部四个评价指标上超越基线方法。\n\n图 5：位姿估计可视化\n\n在 50 件简单至中等难度家具的仿真测试中，Manual2Skill 达成 58% 成功率，显著超越现有启发式方法，验证了层级化装配图、位姿估计与运动规划结合的有效性。\n\n我们在四款真实宜家家具（Flisat 凳、Variera 架、Sundvik 椅、Knagglig 箱）上测试了我们整套框架，体现了我们的框架在真实机器人装配任务中的可行性和出色表现。\n\n图 6：真实世界家具装配过程可视化\n\n本方法可零样本推广至轮轴、玩具飞机甚至机械臂等手册引导式装配任务，成功率 100%，彰显 VLM-based 方案相比其他方法的泛化优势。\n\n图 7：零样本扩展可视化\n\n本文提出 Manual2Skill，一种开创性框架，通过 VLMs 使机器人能解析人工设计的视觉说明书并自主执行复杂家具装配任务。通过引入层级化图式指令解析与鲁棒位姿估计，Manual2Skill 有效弥合了抽象说明书与物理执行之间的鸿沟。\n\nManual2Skill 提出了一种新的机器人学习范式，机器人可以从为人类设计的说明书中学习复杂长程的操作技能，相比起收集大量人工示范数据做模仿学习，显著降低了复杂操作技能获取的成本和复杂度。同时，说明书通过抽象图表和符号表示传达操作知识，这种抽象化的表达方式捕获了操作过程的底层结构和核心逻辑，而非仅仅记录表面的动作序列。这种深层次的理解使得获得的技能能够在不同的物体配置、环境条件和机器人实体间实现有效泛化。\n\n[1] Yun-Chun Chen, Haoda Li, Dylan Turpin, Alec Jacobson, and Animesh Garg. 「Neural shape mating: Self-supervised object assembly with adversarial shape priors」. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12724–12733, 2022.\n\n[2] Benjamin Jones, Dalton Hildreth, Duowen Chen, Ilya Baran, Vladimir G Kim, and Adriana Schulz. 「Automate: A dataset and learning approach for automatic mating of cad assemblies」. ACM Transactions on Graphics (TOG), 40(6):1–18, 2021.\n\n[3] Mingxin Yu, Lin Shao, Zhehuan Chen, Tianhao Wu, Qingnan Fan, Kaichun Mo, and Hao Dong. 「Roboassembly: Learning generalizable furniture assembly policy in a novel multi-robot contact-rich simulation environment」. arXiv preprint arXiv:2112.10143, 2021.\n\n[4] Zuyuan Zhu and Huosheng Hu. 「Robot learning from demonstration in robotic assembly: A survey」. Robotics, 7(2):17, 2018.\n\n[5] Andrew Goldberg, Kavish Kondap, Tianshuang Qiu, Zehan Ma, Letian Fu, Justin Kerr, Huang Huang, Kaiyuan Chen, Kuan Fang, and Ken Goldberg. 「Blox-net: Generative design-for-robot-assembly using vlm supervision, physics simulation, and a robot with reset」. arXiv preprint arXiv:2409.17126, 2024.\n\n© THE END\n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com",
    "published_time": "2025-05-29T04:53:51.000Z",
    "download_time": "2025-05-29T23:47:48.815889",
    "visual_resource": [
      "screenshot/wechat_wx_794b7716.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T04:53:51.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibPsWibzGOHYBMyw9VU4bSiaT7dBx2bxbsphvUtBSzicahBDr6RuhmkNyEZ091g1VWMLRBicZibZibibXvmQ/0?wx_fmt=jpeg\", \"id\": \"xK3mM0r8bH6XrREKGc0esA\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/xK3mM0r8bH6XrREKGc0esA.txt\"}}"
  },
  {
    "id": "3ivVjr6usfr3_IF68pdR_g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/3ivVjr6usfr3_IF68pdR_g",
    "title": "用Milvus构建RAG系统，N8N VS dify 如何选？",
    "summary": "本文详细阐述了如何利用Milvus向量数据库与N8N通用工作流工具构建高效的检索增强生成（RAG）系统。文章首先对比了N8N与专为生成式AI设计的Dify平台，强调N8N结合Milvus在集成灵活性与深度理解RAG系统方面的优势。随后，教程手把手指导用户配置Ollama模型、部署Milvus向量数据库，并利用N8N编排RAG工作流，包括文本向量化、数据存储和聊天检索等核心环节。RAG系统凭借Milvus的毫秒级向量检索能力，有效解决了大模型知识时效性、幻觉及专业领域适应性等核心痛点，为构建实时、准确的AI应用提供了实践方案。",
    "keywords": [
      "RAG",
      "Milvus",
      "N8N",
      "Dify",
      "向量数据库",
      "大模型",
      "检索增强生成",
      "工作流自动化"
    ],
    "area": [
      "人工智能",
      "大模型",
      "生成式AI"
    ],
    "content": "标题：用Milvus构建RAG系统，N8N VS dify 如何选？\n公众号：Zilliz\n--------------------------------------------------\n\n如果将大模型视为一个知识丰富但记忆有限的专家，RAG系统则是为其配备了一个能够实时检索和提供准确信息的辅助工具。\n\n而关于如何低门槛搭建一个RAG系统系统，很多朋友可能会纠结究竟选择N8N 还是Dify。\n\n如何用Dify+Milvus搭建一个RAG系统，可以参考我们此前发布的教程：\n\nMilvus×Dify半小时轻松构建RAG系统\n\n本文，我们则将通过N8N和Milvus这两个实用工具来带大家手把手做一个RAG应用。\n\nN8N和Dify，分别代表了不同的工作流搭建思路，概括来说：N8N是通用工作流工具，可以连接几乎任何系统，处理各种自动化任务，不仅限于AI领域；而Dify是专注于AI应用的开发平台，专为生成式AI应用定制，内置了与各种LLM模型的连接能力。\n\n在本文中，我们选择N8N和Milvus的组合，是因为N8N可以连接数百种不同的服务和API，负责多系统集成和工作流编排，Milvus提供高效向量检索，二者结合既自由又高效，便于深入理解RAG系统。\n\n相比一体化平台，这种“从零搭建”更具学习价值，但实际应用中专业平台如Dify部署更快、功能更全。\n\n部署架构总览及说明\n\n本教程不含docker和docker-compose以及Ollama安装展示，请自行按照官方手册进行配置。\n\ndocker官网：https://www.docker.com/\n\nMilvus官网：https://milvus.io/docs/prerequisite-docker.md\n\nN8N官网：https://n8n.io/\n\nOllama官网：https://ollama.com\n\n1.模型配置\n\n本文采用Ollama运行嵌入模型并提供服务。\n\n1.1下载并检查embedding模型\n\n执行以下命令下载 nomic-embed-text:latest 模型：\n\n下载完成后，使用以下命令检查模型是否成功拉取：\n\n1.2配置LLM大模型\n\n说明：本文采用OpenRouter提供的在线免费模型进行演示。若使用其他收费在线模型，请注意其计费模式。\n\n说明：为保证一定能选到支持Tools的模型，请务必增加筛选条件。根据您的需求筛选并选择合适的模型。\n\n说明：在OpenRouter中为选定的模型创建一个API密钥（API-KEY），此密钥将用于后续的API调用。\n\nMilvus是由Zilliz开发的全球首款开源向量数据库产品，能够处理数百万乃至数十亿级的向量数据，在GitHub上获得了3.5W星标。基于开源Milvus，Zilliz还构建了商业化向量数据库产品Zilliz Cloud，这是一款全托管的向量数据库服务，通过采用云原生设计理念，在易用性、成本效益和安全性上实现了全面提升。\n\n必要条件：\n\n软件要求：Docker、Docker Compose\n\nCPU：8核\n\n内存：至少16GB\n\n硬盘：至少100GB\n\n执行以下命令下载Milvus独立部署所需的docker-compose.yml文件：\n\n使用下载的docker-compose.yml文件启动Milvus服务：\n\n检查Milvus容器是否成功启动：\n\n说明：N8N需要指明使用的向量数据库Collection，因此需要预先创建好备用。MILVUS_URL应填写Milvus服务所在的服务器IP地址及端口。 Attu是Milvus的图形化管理界面。使用以下命令启动Attu：\n\nAttu面板将运行在 服务器IP:8000 。\n\n说明：在Attu中创建用于存储向量的Collection\n\n参数说明：维度（Dimension）要和嵌入模型支持的大小相匹配，否则会导致嵌入失败。 根据您的嵌入模型输出向量的维度，正确配置Collection的维度参数。\n\n参数说明：N8N封装的Milvus向量数据库组件默认支持的索引参数是 metric_type: L2。如果创建的Collection配置的索引参数是 COSINE，可能会导致插入失败。（关于是否支持COSINE，请关注N8N官方更新）。 在创建Collection时，将索引的度量类型（Metric Type）选择为L2。\n\nN8N是一个强大的工作流自动化工具，我们将用它来编排整个RAG系统的流程。\n\n可以通过以下Docker命令安装N8N：\n\n特殊参数说明：\n\n设置环境变量 N8N_HOST 为 192.168.4.48，这可能是用来指定应用监听的主机地址。\n\n设置环境变量 N8N_LISTEN_ADDRESS 为 0.0.0.0，表示应用程序将监听所有网络接口。\n\n镜像地址已隐藏，请前往Docker Hub进行下载。\n\n安装完成后，您可以通过浏览器访问 IP地址:5678来打开N8N主页。\n\n说明首次访问N8N时，请根据提示完成账户信息的初始化设置。\n\n说明：本工作流共分为两个阶段。第一个阶段是文本向量化，第二个阶段是聊天对话检索向量。\n\n说明：在工作流编辑界面，点击“+”号，选择“Manual” (手动触发)作为工作流的起始节点。\n\n说明：上传文件的类型可支持多种格式，使用（.格式, 分割的方式区分和添加）。 在手动触发节点中，配置表单参数以允许用户上传文件。\n\n说明：配置完成后，可以测试文件上传功能，确保节点正常工作。\n\n说明：输入“vector”关键字搜索，并选择Milvus相关的组件。\n\n说明：在Milvus节点配置中，创建或选择已有的Milvus连接凭证，填入Milvus服务地址并选择创建好的Collection\n\n说明：我们选择通过Ollama运行的本地嵌入模型\n\n说明：如果使用Ollama服务，配置Ollama的连接凭证（如API地址）。\n\n说明：选择之前下载并配置好的嵌入模型（如 nomic-embed-text）。\n\n参数说明：\n\n类型选择：二进制 (Binary)\n\n模式选择：加载所有输入数据 (Load all input data)\n\n数据格式：自动选择 (Auto-select) 添加一个文档加载器节点，用于处理上传的文件数据。\n\n说明：选择适用于大部分场景的递归方式 (Recursive Character Text Splitter)。 在文档加载器之后添加文本分割器节点，将文档内容分割成小块以便进行向量化。\n\n说明：从触发节点上传一个任意PDF格式的文本即可自动完成向量存储过程。 运行整个第一阶段工作流，上传一个PDF文件进行测试。\n\n说明：测试完成后，检查Milvus数据库对应的Collection，确认文本向量是否已成功嵌入。\n\n说明：在N8N中创建一个新的工作流或在现有工作流中添加一个聊天触发器（例如 \"Webhook\" 节点或专门的聊天机器人触发器），用于接收用户输入的问题。\n\n说明：添加一个AI Agent或类似的逻辑处理节点，用于协调后续的LLM调用和工具使用。\n\n说明：提示词中英文都可以尝试，本文使用英文提示词测试如下。 为AI Agent配置系统提示词 (System Prompt)，指导其行为。例如：\n\n说明：使用OpenRouter供应商提供的在线LLM大模型。 添加一个LLM调用节点，用于与大语言模型进行交互。\n\n说明：添加刚才在OpenRouter创建好的Key并选择支持Tools工具的模型即可。 在LLM节点中配置OpenRouter的API密钥，并选择之前确定好的、支持Tools的模型。\n\n说明：本文采用简单记忆作为演示。 为聊天流程配置记忆功能（例如 \"Simple Memory\"），使得对话能够保持上下文。\n\n说明：采用N8N提供的向量数据库问答工具，LLM会调用此工具进行向量检索并将结果返回。（输入关键字vector进行工具搜索） 在AI Agent或LLM配置中添加一个工具 (Tool)，该工具用于从向量数据库中检索信息。\n\n说明：为该工具配置一个易于识别的名称\n\n说明：在此工具的配置中，指定之前设置好的Milvus向量存储（Collection）以及用于查询向量化的嵌入模型（与第一阶段一致）。\n\n说明：指定该工具在内部处理或生成响应时可能需要调用的LLM模型\n\n检查并确认第二阶段所有节点的配置均已正确完成\n\n说明：通过聊天触发组件发送测试问题，检验整个RAG流程是否能够正确检索信息并给出回答\n\nmilvus的设计理念是什么？\n\nRAG（检索增强生成）系统解决了大语言模型的三大核心痛点：\n\n知识时效性突破：通过实时检索外部知识库，确保回答包含最新信息，解决模型训练数据截止带来的信息滞后，适用于金融、新闻、医疗等需实时数据的领域。\n\n幻觉问题缓解：提供可验证的参考资料，显著降低模型生成不准确信息的风险，提高回答的准确性和可信度。\n\n专业领域适应：允许访问特定领域知识库，无需昂贵微调即可回答专业问题，增强模型在垂直领域的表现。\n\n而Milvus通过专注向量相似度搜索，支持毫秒级检索数十亿向量，可以快速定位相关知识片段，提升知识库的响应速度和准确性。其分布式架构支持水平扩展，能处理TB级数据和高并发请求，保证系统稳定。提供多种索引（如HNSW、IVF）和距离计算方法（余弦相似度、欧氏距离），便于根据场景优化检索效果。支持实时数据插入与更新，确保知识库时效性。\n\n如对本教程仍有不理解的地方，欢迎扫描文末二维码交流。\n\n作者介绍\n\nZilliz 黄金写手：尹珉",
    "published_time": "2025-05-29T10:06:14.000Z",
    "download_time": "2025-05-29T23:46:15.506401",
    "visual_resource": [
      "screenshot/wechat_wx_bb0c31af.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-29T10:06:14.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/MqgA8Ylgeh74JtlYs6oIOzUoInLuO2R1XTfRbQNYxYH0JuyWmgKSuWYwTyDEriaPrT5zw84qIRhQsOHEqyT5xsQ/0?wx_fmt=jpeg\", \"id\": \"3ivVjr6usfr3_IF68pdR_g\"}, \"extraction_info\": {\"account\": \"Zilliz\", \"file_path\": \"./database/content/wechat/3ivVjr6usfr3_IF68pdR_g.txt\"}}"
  },
  {
    "id": "agenticSeek",
    "source": "GitHub",
    "url": "https://github.com/Fosowl/agenticSeek",
    "title": "AgenticSeek: Private, Local Manus Alternative.",
    "content": "# AgenticSeek: Private, Local Manus Alternative.\n\n<p align=\"center\">\n<img align=\"center\" src=\"./media/agentic_seek_logo.png\" width=\"300\" height=\"300\" alt=\"Agentic Seek Logo\">\n<p>\n\n  English | [中文](./README_CHS.md) | [繁體中文](./README_CHT.md) | [Français](./README_FR.md) | [日本語](./README_JP.md)\n\n*A **100% local alternative to Manus AI**, this voice-enabled AI assistant autonomously browses the web, writes code, and plans tasks while keeping all data on your device. Tailored for local reasoning models, it runs entirely on your hardware, ensuring complete privacy and zero cloud dependency.*\n\n[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)\n\n### Why AgenticSeek ?\n\n* 🔒 Fully Local & Private - Everything runs on your machine — no cloud, no data sharing. Your files, conversations, and searches stay private.\n\n* 🌐 Smart Web Browsing - AgenticSeek can browse the internet by itself — search, read, extract info, fill web form — all hands-free.\n\n* 💻 Autonomous Coding Assistant - Need code? It can write, debug, and run programs in Python, C, Go, Java, and more — all without supervision.\n\n* 🧠 Smart Agent Selection - You ask, it figures out the best agent for the job automatically. Like having a team of experts ready to help.\n\n* 📋 Plans & Executes Complex Tasks - From trip planning to complex projects — it can split big tasks into steps and get things done using multiple AI agents.\n\n* 🎙️ Voice-Enabled - Clean, fast, futuristic voice and speech to text allowing you to talk to it like it's your personal AI from a sci-fi movie\n\n### **Demo**\n\n> *Can you search for the agenticSeek project, learn what skills are required, then open the CV_candidates.zip and then tell me which match best the project*\n\nhttps://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316\n\nDisclaimer: This demo, including all the files that appear (e.g: CV_candidates.zip), are entirely fictional. We are not a corporation, we seek open-source contributors not candidates.\n\n> 🛠⚠️️ **Active Work in Progress** – Please note that Code/Bash is not dockerized yet but will be soon (see docker_deployement branch) - Do not deploy over network or production.\n\n> 🙏 Please also understand that this project began as a side experiment, with no roadmap and no expectations, we didn't expect to end in Github trending. Financial backing is exactly $1/month (shoutout to my single sponsor). Contributions, feedback, and patience are deeply appreciated.\n\n## Installation\n\nMake sure you have chrome driver, docker and python3.10 installed.\n\nWe highly advice you use exactly python3.10 for the setup. Dependencies error might happen otherwise.\n\nFor issues related to chrome driver, see the **Chromedriver** section.\n\n### 1️⃣ **Clone the repository and setup**\n\n```sh\ngit clone https://github.com/Fosowl/agenticSeek.git\ncd agenticSeek\nmv .env.example .env\n```\n\n### 2️ **Create a virtual env**\n\n```sh\npython3 -m venv agentic_seek_env\nsource agentic_seek_env/bin/activate\n# On Windows: agentic_seek_env\\Scripts\\activate\n```\n\n### 3️⃣ **Install package**\n\nEnsure Python, Docker and docker compose, and Google chrome are installed.\n\nWe recommand Python 3.10.0.\n\n**Automatic Installation (Recommanded):**\n\nFor Linux/Macos:\n```sh\n./install.sh\n```\n\nFor windows:\n\n```sh\n./install.bat\n```\n\n**Manually:**\n\n**Note: For any OS, ensure the ChromeDriver you install matches your installed Chrome version. Run `google-chrome --version`. See known issues if you have chrome >135**\n\n- *Linux*: \n\nUpdate Package List: `sudo apt update`\n\nInstall Dependencies: `sudo apt install -y alsa-utils portaudio19-dev python3-pyaudio libgtk-3-dev libnotify-dev libgconf-2-4 libnss3 libxss1`\n\nInstall ChromeDriver matching your Chrome browser version:\n`sudo apt install -y chromium-chromedriver`\n\nInstall requirements: `pip3 install -r requirements.txt`\n\n- *Macos*:\n\nUpdate brew : `brew update`\n\nInstall chromedriver : `brew install --cask chromedriver`\n\nInstall portaudio: `brew install portaudio`\n\nUpgrade pip : `python3 -m pip install --upgrade pip`\n\nUpgrade wheel : : `pip3 install --upgrade setuptools wheel`\n\nInstall requirements: `pip3 install -r requirements.txt`\n\n- *Windows*:\n\nInstall pyreadline3 `pip install pyreadline3`\n\nInstall portaudio manually (e.g., via vcpkg or prebuilt binaries) and then run: `pip install pyaudio`\n\nDownload and install chromedriver manually from: https://sites.google.com/chromium.org/driver/getting-started\n\nPlace chromedriver in a directory included in your PATH.\n\nInstall requirements: `pip3 install -r requirements.txt`\n\n---\n\n## Setup for running LLM locally on your machine\n\n**Hardware Requirements:**\n\nTo run LLMs locally, you'll need sufficient hardware. At a minimum, a GPU capable of running Qwen/Deepseek 14B is required. See the FAQ for detailed model/performance recommendations.\n\n**Setup your local provider**  \n\nStart your local provider, for example with ollama:\n\n```sh\nollama serve\n```\n\nSee below for a list of local supported provider.\n\n**Update the config.ini**\n\nChange the config.ini file to set the provider_name to a supported provider and provider_model to a LLM supported by your provider. We recommand reasoning model such as *Qwen* or *Deepseek*.\n\nSee the **FAQ** at the end of the README for required hardware.\n\n```sh\n[MAIN]\nis_local = True # Whenever you are running locally or with remote provider.\nprovider_name = ollama # or lm-studio, openai, etc..\nprovider_model = deepseek-r1:14b # choose a model that fit your hardware\nprovider_server_address = 127.0.0.1:11434\nagent_name = Jarvis # name of your AI\nrecover_last_session = True # whenever to recover the previous session\nsave_session = True # whenever to remember the current session\nspeak = True # text to speech\nlisten = False # Speech to text, only for CLI\nwork_dir =  /Users/mlg/Documents/workspace # The workspace for AgenticSeek.\njarvis_personality = False # Whenever to use a more \"Jarvis\" like personality (experimental)\nlanguages = en zh # The list of languages, Text to speech will default to the first language on the list\n[BROWSER]\nheadless_browser = True # Whenever to use headless browser, recommanded only if you use web interface.\nstealth_mode = True # Use undetected selenium to reduce browser detection\n```\n\nWarning: Do *NOT* set provider_name to `openai` if using LM-studio for running LLMs. Set it to `lm-studio`.\n\nNote: Some provider (eg: lm-studio) require you to have `http://` in front of the IP. For example `http://127.0.0.1:1234`\n\n**List of local providers**\n\n| Provider  | Local? | Description                                               |\n|-----------|--------|-----------------------------------------------------------|\n| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |\n| lm-studio  | Yes    | Run LLM locally with LM studio (set `provider_name` to `lm-studio`)|\n| openai    | Yes     |  Use openai compatible API (eg: llama.cpp server)  |\n\nNext step: [Start services and run AgenticSeek](#Start-services-and-Run)  \n\n*See the **Known issues** section if you are having issues*\n\n*See the **Run with an API** section if your hardware can't run deepseek locally*\n\n*See the **Config** section for detailled config file explanation.*\n\n---\n\n## Setup to run with an API\n\nSet the desired provider in the `config.ini`. See below for a list of API providers.\n\n```sh\n[MAIN]\nis_local = False\nprovider_name = google\nprovider_model = gemini-2.0-flash\nprovider_server_address = 127.0.0.1:5000 # doesn't matter\n```\nWarning: Make sure there is not trailing space in the config.\n\nExport your API key: `export <<PROVIDER>>_API_KEY=\"xxx\"`\n\nExample: export `TOGETHER_API_KEY=\"xxxxx\"`\n\n**List of API providers**\n  \n| Provider  | Local? | Description                                               |\n|-----------|--------|-----------------------------------------------------------|\n| openai    | Depends  | Use ChatGPT API  |\n| deepseek  | No     | Deepseek API (non-private)                            |\n| huggingface| No    | Hugging-Face API (non-private)                            |\n| togetherAI | No    | Use together AI API (non-private)                         |\n| google | No    | Use google gemini API (non-private)                         |\n\n*We advice against using gpt-4o or other closedAI models*, performance are poor for web browsing and task planning.\n\nPlease also note that coding/bash might fail with gemini, it seem to ignore our prompt for format to respect, which are optimized for deepseek r1.\n\nNext step: [Start services and run AgenticSeek](#Start-services-and-Run)\n\n*See the **Known issues** section if you are having issues*\n\n*See the **Config** section for detailled config file explanation.*\n\n---\n\n## Start services and Run\n\nActivate your python env if needed.\n```sh\nsource agentic_seek_env/bin/activate\n```\n\nStart required services. This will start all services from the docker-compose.yml, including:\n    - searxng\n    - redis (required by searxng)\n    - frontend\n\n```sh\nsudo ./start_services.sh # MacOS\nstart ./start_services.cmd # Window\n```\n\n**Options 1:** Run with the CLI interface.\n\n```sh\npython3 cli.py\n```\n\nWe advice you set `headless_browser` to False in the config.ini for CLI mode.\n\n**Options 2:** Run with the Web interface.\n\nStart the backend.\n\n```sh\npython3 api.py\n```\n\nGo to `http://localhost:3000/` and you should see the web interface.\n\n---\n\n## Usage\n\nMake sure the services are up and running with `./start_services.sh` and run the AgenticSeek with `python3 cli.py` for CLI mode or `python3 api.py` then go to `localhost:3000` for web interface.\n\nYou can also use speech to text by setting `listen = True` in the config. Only for CLI mode.\n\nTo exit, simply say/type `goodbye`.\n\nHere are some example usage:\n\n> *Make a snake game in python!*\n\n> *Search the web for top cafes in Rennes, France, and save a list of three with their addresses in rennes_cafes.txt.*\n\n> *Write a Go program to calculate the factorial of a number, save it as factorial.go in your workspace*\n\n> *Search my summer_pictures folder for all JPG files, rename them with today’s date, and save a list of renamed files in photos_list.txt*\n\n> *Search online for popular sci-fi movies from 2024 and pick three to watch tonight. Save the list in movie_night.txt.*\n\n> *Search the web for the latest AI news articles from 2025, select three, and write a Python script to scrape their titles and summaries. Save the script as news_scraper.py and the summaries in ai_news.txt in /home/projects*\n\n> *Friday, search the web for a free stock price API, register with supersuper7434567@gmail.com then write a Python script to fetch using the API daily prices for Tesla, and save the results in stock_prices.csv*\n\n*Note that form filling capabilities are still experimental and might fail.*\n\n\n\nAfter you type your query, AgenticSeek will allocate the best agent for the task.\n\nBecause this is an early prototype, the agent routing system might not always allocate the right agent based on your query.\n\nTherefore, you should be very explicit in what you want and how the AI might proceed for example if you want it to conduct a web search, do not say:\n\n`Do you know some good countries for solo-travel?`\n\nInstead, ask:\n\n`Do a web search and find out which are the best country for solo-travel`\n\n---\n\n## **Setup to run the LLM on your own server**  \n\nIf you have a powerful computer or a server that you can use, but you want to use it from your laptop you have the options to run the LLM on a remote server using our custom llm server. \n\nOn your \"server\" that will run the AI model, get the ip address\n\n```sh\nip a | grep \"inet \" | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # local ip\ncurl https://ipinfo.io/ip # public ip\n```\n\nNote: For Windows or macOS, use ipconfig or ifconfig respectively to find the IP address.\n\nClone the repository and enter the `server/`folder.\n\n\n```sh\ngit clone --depth 1 https://github.com/Fosowl/agenticSeek.git\ncd agenticSeek/llm_server/\n```\n\nInstall server specific requirements:\n\n```sh\npip3 install -r requirements.txt\n```\n\nRun the server script.\n\n```sh\npython3 app.py --provider ollama --port 3333\n```\n\nYou have the choice between using `ollama` and `llamacpp` as a LLM service.\n\n\nNow on your personal computer:\n\nChange the `config.ini` file to set the `provider_name` to `server` and `provider_model` to `deepseek-r1:xxb`.\nSet the `provider_server_address` to the ip address of the machine that will run the model.\n\n```sh\n[MAIN]\nis_local = False\nprovider_name = server\nprovider_model = deepseek-r1:70b\nprovider_server_address = x.x.x.x:3333\n```\n\n\nNext step: [Start services and run AgenticSeek](#Start-services-and-Run)  \n\n---\n\n## Speech to Text\n\nPlease note that currently speech to text only work in english.\n\nThe speech-to-text functionality is disabled by default. To enable it, set the listen option to True in the config.ini file:\n\n```\nlisten = True\n```\n\nWhen enabled, the speech-to-text feature listens for a trigger keyword, which is the agent's name, before it begins processing your input. You can customize the agent's name by updating the `agent_name` value in the *config.ini* file:\n\n```\nagent_name = Friday\n```\n\nFor optimal recognition, we recommend using a common English name like \"John\" or \"Emma\" as the agent name\n\nOnce you see the transcript start to appear, say the agent's name aloud to wake it up (e.g., \"Friday\").\n\nSpeak your query clearly.\n\nEnd your request with a confirmation phrase to signal the system to proceed. Examples of confirmation phrases include:\n```\n\"do it\", \"go ahead\", \"execute\", \"run\", \"start\", \"thanks\", \"would ya\", \"please\", \"okay?\", \"proceed\", \"continue\", \"go on\", \"do that\", \"go it\", \"do you understand?\"\n```\n\n## Config\n\nExample config:\n```\n[MAIN]\nis_local = True\nprovider_name = ollama\nprovider_model = deepseek-r1:32b\nprovider_server_address = 127.0.0.1:11434\nagent_name = Friday\nrecover_last_session = False\nsave_session = False\nspeak = False\nlisten = False\nwork_dir =  /Users/mlg/Documents/ai_folder\njarvis_personality = False\nlanguages = en zh\n[BROWSER]\nheadless_browser = False\nstealth_mode = False\n```\n\n**Explanation**:\n\n- is_local -> Runs the agent locally (True) or on a remote server (False).\n\n- provider_name -> The provider to use (one of: `ollama`, `server`, `lm-studio`, `deepseek-api`)\n\n- provider_model -> The model used, e.g., deepseek-r1:32b.\n\n- provider_server_address -> Server address, e.g., 127.0.0.1:11434 for local. Set to anything for non-local API.\n\n- agent_name -> Name of the agent, e.g., Friday. Used as a trigger word for TTS.\n\n- recover_last_session -> Restarts from last session (True) or not (False).\n\n- save_session -> Saves session data (True) or not (False).\n\n- speak -> Enables voice output (True) or not (False).\n\n- listen -> listen to voice input (True) or not (False).\n\n- work_dir -> Folder the AI will have access to. eg: /Users/user/Documents/.\n\n- jarvis_personality -> Uses a JARVIS-like personality (True) or not (False). This simply change the prompt file.\n\n- languages -> The list of supported language, needed for the llm router to work properly, avoid putting too many or too similar languages.\n\n- headless_browser -> Runs browser without a visible window (True) or not (False).\n\n- stealth_mode -> Make bot detector time harder. Only downside is you have to manually install the anticaptcha extension.\n\n- languages -> List of supported languages. Required for agent routing system. The longer the languages list the more model will be downloaded.\n\n## Providers\n\nThe table below show the available providers:\n\n| Provider  | Local? | Description                                               |\n|-----------|--------|-----------------------------------------------------------|\n| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |\n| server    | Yes    | Host the model on another machine, run your local machine |\n| lm-studio  | Yes    | Run LLM locally with LM studio (`lm-studio`)             |\n| openai    | Depends  | Use ChatGPT API (non-private) or openai compatible API  |\n| deepseek-api  | No     | Deepseek API (non-private)                            |\n| huggingface| No    | Hugging-Face API (non-private)                            |\n| togetherAI | No    | Use together AI API (non-private)                         |\n| google | No    | Use google gemini API (non-private)                         |\n\nTo select a provider change the config.ini:\n\n```\nis_local = True\nprovider_name = ollama\nprovider_model = deepseek-r1:32b\nprovider_server_address = 127.0.0.1:5000\n```\n`is_local`: should be True for any locally running LLM, otherwise False.\n\n`provider_name`: Select the provider to use by it's name, see the provider list above.\n\n`provider_model`: Set the model to use by the agent.\n\n`provider_server_address`: can be set to anything if you are not using the server provider.\n\n# Known issues\n\n## Chromedriver Issues\n\n**Known error #1:** *chromedriver mismatch*\n\n`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113\nCurrent browser version is 134.0.6998.89 with binary path`\n\nThis happen if there is a mismatch between your browser and chromedriver version.\n\nYou need to navigate to download the latest version:\n\nhttps://developer.chrome.com/docs/chromedriver/downloads\n\nIf you're using Chrome version 115 or newer go to:\n\nhttps://googlechromelabs.github.io/chrome-for-testing/\n\nAnd download the chromedriver version matching your OS.\n\n![alt text](./media/chromedriver_readme.png)\n\nIf this section is incomplete please raise an issue.\n\n##  connection adapters Issues\n\n```\nException: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:11434/v1/chat/completions'\n```\n\nMake sure you have `http://` in front of the provider IP address :\n\n`provider_server_address = http://127.0.0.1:11434`\n\n## SearxNG base URL must be provided\n\n```\nraise ValueError(\"SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.\")\nValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.\n```\n\nMaybe you didn't move `.env.example` as `.env` ? You can also export SEARXNG_BASE_URL:\n\n`export  SEARXNG_BASE_URL=\"http://127.0.0.1:8080\"`\n\n## FAQ\n\n**Q: What hardware do I need?**  \n\n| Model Size  | GPU  | Comment                                               |\n|-----------|--------|-----------------------------------------------------------|\n| 7B        | 8GB Vram | ⚠️ Not recommended. Performance is poor, frequent hallucinations, and planner agents will likely fail. |\n| 14B        | 12 GB VRAM (e.g. RTX 3060) | ✅ Usable for simple tasks. May struggle with web browsing and planning tasks. |\n| 32B        | 24+ GB VRAM (e.g. RTX 4090) | 🚀 Success with most tasks, might still struggle with task planning |\n| 70B+        | 48+ GB Vram (eg. mac studio) | 💪 Excellent. Recommended for advanced use cases. |\n\n**Q: Why Deepseek R1 over other models?**  \n\nDeepseek R1 excels at reasoning and tool use for its size. We think it’s a solid fit for our needs other models work fine, but Deepseek is our primary pick.\n\n**Q: I get an error running `cli.py`. What do I do?**  \n\nEnsure local is running (`ollama serve`), your `config.ini` matches your provider, and dependencies are installed. If none work feel free to raise an issue.\n\n**Q: Can it really run 100% locally?**  \n\nYes with Ollama, lm-studio or server providers, all speech to text, LLM and text to speech model run locally. Non-local options (OpenAI or others API) are optional.\n\n**Q: Why should I use AgenticSeek when I have Manus?**\n\nThis started as Side-Project we did out of interest about AI agents. What’s special about it is that we want to use local model and avoid APIs.\nWe draw inspiration from Jarvis and Friday (Iron man movies) to make it \"cool\" but for functionnality we take more inspiration from Manus, because that's what people want in the first place: a local manus alternative.\nUnlike Manus, AgenticSeek prioritizes independence from external systems, giving you more control, privacy and avoid api cost.\n\n## Contribute\n\nWe’re looking for developers to improve AgenticSeek! Check out open issues or discussion.\n\n[Contribution guide](./docs/CONTRIBUTING.md)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)\n\n## Maintainers:\n\n > [Fosowl](https://github.com/Fosowl) | Paris Time \n\n > [antoineVIVIES](https://github.com/antoineVIVIES) | Taipei Time \n\n > [steveh8758](https://github.com/steveh8758) | Taipei Time \n",
    "summary": "AgenticSeek是一个100%本地化、注重隐私的AI助手，旨在成为Manus AI的替代品。它完全在用户设备上运行，无需云服务，确保数据安全。该助手基于本地推理模型，具备自主网络浏览、代码编写、智能代理选择、复杂任务规划与执行以及语音交互能力。它为用户提供了一个完全私有、无云依赖的个人AI助理解决方案。",
    "keywords": [
      "本地AI",
      "隐私保护",
      "智能体",
      "网络浏览",
      "代码生成",
      "任务规划",
      "语音交互",
      "大模型"
    ],
    "area": [
      "人工智能",
      "智能体",
      "大模型"
    ],
    "published_time": "2025-05-28T06:42:18+00:00",
    "download_time": "2024-07-28 08:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png"
    ],
    "extra_info": null
  },
  {
    "id": "Baileys",
    "source": "GitHub",
    "url": "https://github.com/WhiskeySockets/Baileys",
    "title": "Baileys",
    "content": "<h1><img alt=\"Baileys logo\" src=\"https://raw.githubusercontent.com/WhiskeySockets/Baileys/refs/heads/master/Media/logo.png\" height=\"75\"/></h1>\n\n![NPM Downloads](https://img.shields.io/npm/dw/%40whiskeysockets%2Fbaileys?label=npm&color=%23CB3837)\n![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/whiskeysockets/baileys)\n![Discord](https://img.shields.io/discord/725839806084546610?label=discord&color=%235865F2)\n\nBaileys is a WebSockets-based TypeScript library for interacting with the WhatsApp Web API.\n\n# Usage\nA new guide has been posted at https://baileys.wiki. The old guide can be accessed on [NPM](https://npmjs.com/package/baileys).\n\n# Sponsor\nIf you'd like to financially support this project, you can do so by supporting the current maintainer [here](https://purpshell.dev/sponsor).\n\n# Disclaimer\nThis project is not affiliated, associated, authorized, endorsed by, or in any way officially connected with WhatsApp or any of its subsidiaries or its affiliates.\nThe official WhatsApp website can be found at whatsapp.com. \"WhatsApp\" as well as related names, marks, emblems and images are registered trademarks of their respective owners.\n\nThe maintainers of Baileys do not in any way condone the use of this application in practices that violate the Terms of Service of WhatsApp. The maintainers of this application call upon the personal responsibility of its users to use this application in a fair way, as it is intended to be used.\nUse at your own discretion. Do not spam people with this. We discourage any stalkerware, bulk or automated messaging usage.\n\n# License\nCopyright (c) 2025 Rajeh Taher/WhiskeySockets\n\nLicensed under the MIT License:\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nThus, the maintainers of the project can't be held liable for any potential misuse of this project.",
    "summary": "Baileys是一个基于WebSocket的TypeScript开发库，专为与WhatsApp Web API进行交互而设计。该库提供了实现WhatsApp相关功能的底层工具，使开发者能够构建自定义应用。项目维护者强调用户应负责任地使用，并明确反对滥用、批量或自动化消息发送等违反WhatsApp服务条款的行为。",
    "keywords": [
      "WhatsApp Web API",
      "WebSocket",
      "TypeScript",
      "开发库",
      "即时通讯API"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T01:53:33+00:00",
    "download_time": "2024-07-28 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/WhiskeySockets/Baileys/refs/heads/master/Media/logo.png"
    ],
    "extra_info": null
  },
  {
    "id": "core",
    "source": "GitHub",
    "url": "https://github.com/vuejs/core",
    "title": "vuejs/core",
    "content": "# vuejs/core [![npm](https://img.shields.io/npm/v/vue.svg)](https://www.npmjs.com/package/vue) [![build status](https://github.com/vuejs/core/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/vuejs/core/actions/workflows/ci.yml) [![Download](https://img.shields.io/npm/dm/vue)](https://www.npmjs.com/package/vue)\n\n## Getting Started\n\nPlease follow the documentation at [vuejs.org](https://vuejs.org/)!\n\n## Sponsors\n\nVue.js is an MIT-licensed open source project with its ongoing development made possible entirely by the support of these awesome [backers](https://github.com/vuejs/core/blob/main/BACKERS.md). If you'd like to join them, please consider [ sponsoring Vue's development](https://vuejs.org/sponsor/).\n\n<p align=\"center\">\n  <h3 align=\"center\">Special Sponsor</h3>\n</p>\n\n<p align=\"center\">\n  <a target=\"_blank\" href=\"https://github.com/appwrite/appwrite\">\n  <img alt=\"special sponsor appwrite\" src=\"https://sponsors.vuejs.org/images/appwrite.svg\" width=\"300\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a target=\"_blank\" href=\"https://vuejs.org/sponsor/#current-sponsors\">\n    <img alt=\"sponsors\" src=\"https://sponsors.vuejs.org/sponsors.svg?v3\">\n  </a>\n</p>\n\n## Questions\n\nFor questions and support please use [the official forum](https://forum.vuejs.org) or [community chat](https://chat.vuejs.org/). The issue list of this repo is **exclusively** for bug reports and feature requests.\n\n## Issues\n\nPlease make sure to respect issue requirements and use [the new issue helper](https://new-issue.vuejs.org/) when opening an issue. Issues not conforming to the guidelines may be closed immediately.\n\n## Stay In Touch\n\n- [X](https://x.com/vuejs)\n- [Bluesky](https://bsky.app/profile/vuejs.org)\n- [Blog](https://blog.vuejs.org/)\n- [Job Board](https://vuejobs.com/?ref=vuejs)\n\n## Contribution\n\nPlease make sure to read the [Contributing Guide](https://github.com/vuejs/core/blob/main/.github/contributing.md) before making a pull request. If you have a Vue-related project/component/tool, add it with a pull request to [this curated list](https://github.com/vuejs/awesome-vue)!\n\nThank you to all the people who already contributed to Vue!\n\n<a href=\"https://github.com/vuejs/core/graphs/contributors\"><img src=\"https://opencollective.com/vuejs/contributors.svg?width=890&limit=500\" /></a>\n\n<sub>_Note: Showing the first 500 contributors only due to GitHub image size limitations_</sub>\n\n## License\n\n[MIT](https://opensource.org/licenses/MIT)\n\nCopyright (c) 2013-present, Yuxi (Evan) You",
    "summary": "Vue.js 是一个渐进式 JavaScript 框架，用于构建用户界面。作为一个 MIT 许可的开源项目，其持续发展依赖于社区支持和赞助。README提供了获取文档、参与社区讨论、报告问题、保持联系以及贡献代码的指引。项目强调通过官方渠道获取帮助和报告bug，并鼓励开发者阅读贡献指南。Vue.js 拥有庞大的用户和贡献者群体，是前端开发领域广泛应用的技术栈之一。",
    "keywords": [
      "Vue.js",
      "前端框架",
      "JavaScript",
      "用户界面",
      "开源",
      "社区"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T01:04:53Z",
    "download_time": "2024-05-29 10:00:00",
    "visual_resource": [
      "https://sponsors.vuejs.org/images/appwrite.svg",
      "https://sponsors.vuejs.org/sponsors.svg?v3",
      "https://opencollective.com/vuejs/contributors.svg?width=890&limit=500"
    ],
    "extra_info": null
  },
  {
    "id": "prompt-eng-interactive-tutorial",
    "source": "GitHub",
    "url": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
    "title": "Welcome to Anthropic's Prompt Engineering Interactive Tutorial",
    "content": "# Welcome to Anthropic's Prompt Engineering Interactive Tutorial\n\n## Course introduction and goals\n\nThis course is intended to provide you with a comprehensive step-by-step understanding of how to engineer optimal prompts within Claude.\n\n**After completing this course, you will be able to**:\n- Master the basic structure of a good prompt \n- Recognize common failure modes and learn the '80/20' techniques to address them\n- Understand Claude's strengths and weaknesses\n- Build strong prompts from scratch for common use cases\n\n## Course structure and content\n\nThis course is structured to allow you many chances to practice writing and troubleshooting prompts yourself. The course is broken up into **9 chapters with accompanying exercises**, as well as an appendix of even more advanced methods. It is intended for you to **work through the course in chapter order**. \n\n**Each lesson has an \"Example Playground\" area** at the bottom where you are free to experiment with the examples in the lesson and see for yourself how changing prompts can change Claude's responses. There is also an [answer key](https://docs.google.com/spreadsheets/d/1jIxjzUWG-6xBVIa2ay6yDpLyeuOh_hR_ZB75a47KX_E/edit?usp=sharing).\n\nNote: This tutorial uses our smallest, fastest, and cheapest model, Claude 3 Haiku. Anthropic has [two other models](https://docs.anthropic.com/claude/docs/models-overview), Claude 3 Sonnet and Claude 3 Opus, which are more intelligent than Haiku, with Opus being the most intelligent.\n\n*This tutorial also exists on [Google Sheets using Anthropic's Claude for Sheets extension](https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit?usp=sharing). We recommend using that version as it is more user friendly.*\n\nWhen you are ready to begin, go to `01_Basic Prompt Structure` to proceed.\n\n## Table of Contents\n\nEach chapter consists of a lesson and a set of exercises.\n\n### Beginner\n- **Chapter 1:** Basic Prompt Structure\n\n- **Chapter 2:** Being Clear and Direct  \n\n- **Chapter 3:** Assigning Roles\n\n### Intermediate \n- **Chapter 4:** Separating Data from Instructions\n\n- **Chapter 5:** Formatting Output & Speaking for Claude\n\n- **Chapter 6:** Precognition (Thinking Step by Step)\n\n- **Chapter 7:** Using Examples\n\n### Advanced\n- **Chapter 8:** Avoiding Hallucinations\n\n- **Chapter 9:** Building Complex Prompts (Industry Use Cases)\n  - Complex Prompts from Scratch - Chatbot\n  - Complex Prompts for Legal Services\n  - **Exercise:** Complex Prompts for Financial Services\n  - **Exercise:** Complex Prompts for Coding\n  - Congratulations & Next Steps\n\n- **Appendix:** Beyond Standard Prompting\n  - Chaining Prompts\n  - Tool Use\n  - Search & Retrieval",
    "summary": "该GitHub仓库提供了Anthropic Claude模型交互式提示工程教程。教程旨在系统教授用户如何构建最优Claude提示，涵盖基础结构、常见问题解决、模型特性理解及实际用例提示构建。课程包含9个章节及练习，强调按顺序学习，并提供在线实验环境和答案。教程基于Claude 3 Haiku，但也提及Sonnet和Opus模型，并提供Google Sheets版本。这是一个实践性强的学习资源，适合希望提升Claude提示技能的开发者和用户。",
    "keywords": [
      "提示工程",
      "大语言模型",
      "Claude",
      "交互式教程",
      "自然语言处理",
      "AI模型应用",
      "Anthropic"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2024-04-08T03:17:07+00:00",
    "download_time": "2024-04-08 08:00:00",
    "visual_resource": [
      "screenshot/github_prompt-eng-interactive-tutorial.png"
    ],
    "extra_info": null
  },
  {
    "id": "livestore",
    "source": "GitHub",
    "url": "https://github.com/livestorejs/livestore",
    "title": "LiveStore",
    "content": "![LiveStore Logo](https://share.cleanshot.com/njfQBDqB+)\n\n## What LiveStore does\n\n- 🏰 Provide a powerful data foundation for your app.\n- ⚡ Reactive query layer with full SQLite support.\n- 🔌 Adapters for most platforms (web, mobile, server/edge, desktop).\n- 📐 Flexible data modeling and schema management.\n- 📵 Support true offline-first workflows.\n- 💥 Custom merge conflict resolution.\n- 🔄 Sync with a [supported provider](https://docs.livestore.dev/reference/syncing/sync-provider/cloudflare/) or roll your own.\n\n## Getting Started\n\n- [React Web](https://docs.livestore.dev/getting-started/react-web/)\n- [Expo](https://docs.livestore.dev/getting-started/expo/)\n- [Node](https://docs.livestore.dev/getting-started/node/)\n- [Vue](https://docs.livestore.dev/getting-started/vue/)\n\n\n## How LiveStore works\n\nLiveStore is a fully-featured, client-centric data layer (replacing libraries like Redux, MobX, etc.) with a reactive embedded SQLite database powered by real-time sync (via event-sourcing).\n\n![How LiveStore works](https://share.cleanshot.com/j1h8Z1P5+)\n\n1. Instant, reactive queries to your local SQLite database (via built-in query builder or raw SQL).\n2. Data changes are commited to the store, applied instantly and synced across clients.\n3. Change events are persisted locally and synced across clients (and across tabs).\n4. Events are instantly applied to the local database via materializers.\n5. Query results are reactively and synchronously updated in the next render.\n6. The LiveStore sync backend propagates changes to all connected clients.\n\nIf you’d like to learn more about how LiveStore works under the hood, feel free to check out our in-depth guides in the [documentation](https://docs.livestore.dev/evaluation/how-livestore-works/) and dive into topics like:\n\n- [Concepts](https://docs.livestore.dev/reference/concepts/)\n- [Event Sourcing](https://docs.livestore.dev/evaluation/event-sourcing/)\n- [Design Decisions](https://docs.livestore.dev/evaluation/design-decisions/)\n- [Performance](https://docs.livestore.dev/evaluation/performance/)\n- [Date Modeling](https://docs.livestore.dev/data-modeling/)\n- [Technology comparison](https://docs.livestore.dev/evaluation/technology-comparison/)\n\n## License\n\nLivestore is licensed under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).",
    "summary": "LiveStore是一个功能完备的客户端中心数据层，旨在为应用提供强大的数据基础。它基于响应式嵌入式SQLite数据库和事件溯源技术，支持实时数据同步、离线优先工作流和自定义冲突解决。LiveStore提供多平台适配器，可替代传统状态管理库，通过即时、响应式查询和事件驱动的数据更新，简化跨客户端的数据管理和同步。",
    "keywords": [
      "数据层",
      "SQLite",
      "响应式查询",
      "离线优先",
      "数据同步",
      "事件溯源",
      "客户端数据管理"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-26T06:53:38Z",
    "download_time": "2024-05-27 08:00:00",
    "visual_resource": [
      "https://share.cleanshot.com/njfQBDqB+",
      "https://share.cleanshot.com/j1h8Z1P5+"
    ],
    "extra_info": null
  },
  {
    "id": "ai-agents-for-beginners",
    "source": "GitHub",
    "url": "https://github.com/microsoft/ai-agents-for-beginners",
    "title": "AI Agents for Beginners - A Course",
    "content": "# AI Agents for Beginners - A Course\n\n![Generative AI For Beginners](./images/repo-thumbnail.png)\n\n## 11 Lessons teaching everything you need to know to start building AI Agents\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg)](https://github.com/microsoft/ai-agents-for-beginners/blob/master/LICENSE?WT.mc_id=academic-105485-koreyst)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg)](https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst)\n\n### Language Support\n[![English](https://img.shields.io/badge/English-brightgreen.svg?style=flat-square)](README.md)\n[![Chinese Simplified](https://img.shields.io/badge/Chinese_Simplified-brightgreen.svg?style=flat-square)](./translations/zh/README.md)\n[![Chinese Traditional](https://img.shields.io/badge/Chinese_Traditional-brightgreen.svg?style=flat-square)](./translations/tw/README.md)     \n[![Chinese Hong Kong](https://img.shields.io/badge/Chinese_Hong_Kong-brightgreen.svg?style=flat-square)](./translations/hk/README.md) \n[![French](https://img.shields.io/badge/French-brightgreen.svg?style=flat-square)](./translations/fr/README.md)\n[![Japanese](https://img.shields.io/badge/Japanese-brightgreen.svg?style=flat-square)](./translations/ja/README.md) \n[![Korean](https://img.shields.io/badge/Korean-brightgreen.svg?style=flat-square)](./translations/ko/README.md)\n[![Portuguese Brazilian](https://img.shields.io/badge/Portuguese_Brazilian-brightgreen.svg?style=flat-square)](./translations/pt/README.md)\n[![Spanish](https://img.shields.io/badge/Spanish-brightgreen.svg?style=flat-square)](./translations/es/README.md)\n[![German](https://img.shields.io/badge/German-brightgreen.svg?style=flat-square)](./translations/de/README.md)  \n[![Persian](https://img.shields.io/badge/Persian-brightgreen.svg?style=flat-square)](./translations/fa/README.md) \n[![Polish](https://img.shields.io/badge/Polish-brightgreen.svg?style=flat-square)](./translations/pl/README.md) \n[![Hindi](https://img.shields.io/badge/Hindi-brightgreen.svg?style=flat-square)](./translations/hi/README.md)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n[![Azure AI Discord](https://dcbadge.limes.pink/api/server/kzRShWzttr)](https://discord.gg/kzRShWzttr)\n\n\n## 🌱 Getting Started\n\nThis course has 11 lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!\n\nThere is multi-language support for this course. Go to our [available languages here](#-multi-language-support). \n\nIf this is your first time building with Generative AI models, check out our [Generative AI For Beginners](https://aka.ms/genai-beginners) course, which includes 21 lessons on building with GenAI.\n\nDon't forget to [star (🌟) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst) and [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to run the code.\n\n### What You Need \n\nEach lesson in this course includes code examples, which can be found in the code_samples folder. You can [fork this repo](https://github.com/microsoft/ai-agents-for-beginners/fork) to create your own copy.  \n\nThe code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:\n\n- [Github Models](https://aka.ms/ai-agents-beginners/github-models) - Free / Limited\n- [Azure AI Foundry](https://aka.ms/ai-agents-beginners/ai-foundry) - Azure Account Required\n\nThis course also uses the following AI Agent frameworks and services from Microsoft:\n\n- [Azure AI Agent Service](https://aka.ms/ai-agents-beginners/ai-agent-service)\n- [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel)\n- [AutoGen](https://aka.ms/ai-agents/autogen)\n\nFor more information on running the code for this course, go to the [Course Setup](./00-course-setup/README.md).\n\n## 🙏 Want to help?\n\nDo you have suggestions or found spelling or code errors? [Raise an issue](https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst) or [Create a pull request](https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst)\n\nIf you get stuck or have any questions about building AI Agents, join our [Azure AI Foundry Community Discord](https://discord.gg/kzRShWzttr) \n\nIf you have product feedback or errors whilst building visit our [Azure AI Foundry Developer Forum](https://aka.ms/azureaifoundry/forum)\n\n## 📂 Each lesson includes\n\n- A written lesson located in the README and a short video\n- Python code samples supporting Azure AI Foundry and Github Models (Free)\n- Links to extra resources to continue your learning\n\n\n## 🗃️ Lessons\n\n| **Lesson**                               | **Text & Code**                                    | **Video**                                                  | **Extra Learning**                                                                     |\n|------------------------------------------|----------------------------------------------------|------------------------------------------------------------|----------------------------------------------------------------------------------------|\n| Intro to AI Agents and Agent Use Cases   | [Link](./01-intro-to-ai-agents/README.md)          | [Video](https://youtu.be/3zgm60bXmQk?si=z8QygFvYv-9WtO1)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Exploring AI Agentic Frameworks          | [Link](./02-explore-agentic-frameworks/README.md)  | [Video](https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Understanding AI Agentic Design Patterns | [Link](./03-agentic-design-patterns/README.md)     | [Video](https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Tool Use Design Pattern                  | [Link](./04-tool-use/README.md)                    | [Video](https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Agentic RAG                              | [Link](./05-agentic-rag/README.md)                 | [Video](https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Building Trustworthy AI Agents           | [Link](./06-building-trustworthy-agents/README.md) | [Video](https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK ) | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Planning Design Pattern                  | [Link](./07-planning-design/README.md)             | [Video](https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Multi-Agent Design Pattern               | [Link](./08-multi-agent/README.md)                 | [Video](https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| Metacognition Design Pattern             | [Link](./09-metacognition/README.md)               | [Video](https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| AI Agents in Production                  | [Link](./10-ai-agents-production/README.md)        | [Video](https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl)  | [Link](https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst) |\n| AI Agents with MCP                       | [Link](./11-mcp/README.md)                         |                                                            | [Link](https://aka.ms/mcp-for-beginners)                                               |\n\n### 🌐 Multi-Language Support\n\n#### Supported via GitHub Action (Automated & Always Up-to-Date)\n\n[French](./translations/fr/README.md) | [Spanish](./translations/es/README.md) | [German](./translations/de/README.md) | [Persian (Farsi)](./translations/fa/README.md) | [Chinese (Simplified)](./translations/zh/README.md) | [Chinese (Traditional, Hong Kong)](./translations/hk/README.md) | [Chinese (Traditional, Taiwan)](./translations/tw/README.md) | [Japanese](./translations/ja/README.md) | [Korean](./translations/ko/README.md) | [Hindi](./translations/hi/README.md) | [Portuguese (Portugal)](./translations/pt/README.md) | [Polish](./translations/pl/README.md)\n\n## 🎒 Other Courses\n\nOur team produces other courses! Check out:\n\n- [**NEW** Model Context Protocol (MCP) For Beginners](https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)\n- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n\n## 🌟 Community Thanks\n\nThanks to [Shivam Goyal](https://www.linkedin.com/in/shivam2003/) for contributing important code samples demonstrating Agentic RAG. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos is subject to those third-parties' policies.",
    "summary": "微软发布面向初学者的AI智能体课程，共11节课，涵盖构建AI智能体的基础知识。课程内容包括智能体框架探索、设计模式（工具使用、RAG、规划、多智能体、元认知等）以及生产实践。课程使用微软的Azure AI Agent Service、Semantic Kernel和AutoGen等框架和服务，提供Python代码示例，支持Azure AI Foundry和GitHub Models。",
    "keywords": [
      "AI智能体",
      "智能体框架",
      "设计模式",
      "Semantic Kernel",
      "AutoGen",
      "RAG",
      "多智能体"
    ],
    "area": [
      "人工智能",
      "智能体",
      "生成式AI"
    ],
    "published_time": "2025-05-26T09:37:24Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/images/repo-thumbnail.png"
    ],
    "extra_info": null
  },
  {
    "id": "computer-science",
    "source": "GitHub",
    "url": "https://github.com/ossu/computer-science",
    "title": "Open Source Society University",
    "content": "<div align=\"center\" style=\"text-align: center\">\n<img src=\"images/ossu-logo.webp\" alt=\"Open Source Society logo\"/>\n<h3>Open Source Society University</h3>\n<p>\n  Path to a free self-taught education in Computer Science!\n</p>\n<p>\n  <a href=\"https://github.com/sindresorhus/awesome\">\n    <img alt=\"Awesome\" src=\"https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg\"\n  ></a>\n  <a href=\"https://github.com/ossu/computer-science\">\n    <img alt=\"Open Source Society University - Computer Science\" src=\"https://img.shields.io/badge/OSSU-computer--science-blue.svg\"\n  ></a>\n</p>\n</div>\n\n# Contents\n\n- [Summary](#summary)\n- [Community](#community)\n- [Curriculum](#curriculum)\n- [Code of conduct](#code-of-conduct)\n- [Team](#team)\n\n# Summary\n\nThe OSSU curriculum is a **complete education in computer science** using online materials.\nIt's not merely for career training or professional development.\nIt's for those who want a proper, *well-rounded* grounding in concepts fundamental to all computing disciplines,\nand for those who have the discipline, will, and (most importantly!) good habits to obtain this education largely on their own,\nbut with support from a worldwide community of fellow learners.\n\nIt is designed according to the degree requirements of undergraduate computer science majors, minus general education (non-CS) requirements,\nas it is assumed most of the people following this curriculum are already educated outside the field of CS.\nThe courses themselves are among the very best in the world, often coming from Harvard, Princeton, MIT, etc.,\nbut specifically chosen to meet the following criteria.\n\n**Courses must**:\n- Be open for enrollment\n- Run regularly (ideally in self-paced format, otherwise running multiple times per year)\n- Be of generally high quality in teaching materials and pedagogical principles\n- Match the curricular standards of the [CS 2013](CURRICULAR_GUIDELINES.md): Curriculum Guidelines for Undergraduate Degree Programs in Computer Science\n\nWhen no course meets the above criteria, the coursework is supplemented with a book.\nWhen there are courses or books that don't fit into the curriculum but are otherwise of high quality,\nthey belong in [extras/courses](extras/courses.md) or [extras/readings](extras/readings.md).\n\n**Organization**. The curriculum is designed as follows:\n- *Intro CS*: for students to try out CS and see if it's right for them\n- *Core CS*: corresponds roughly to the first three years of a computer science curriculum, taking classes that all majors would be required to take\n- *Advanced CS*: corresponds roughly to the final year of a computer science curriculum, taking electives according to the student's interests\n- *Final Project*: a project for students to validate, consolidate, and display their knowledge, to be evaluated by their peers worldwide\n\n**Duration**. It is possible to finish within about 2 years if you plan carefully and devote roughly 20 hours/week to your studies. Learners can use [this spreadsheet](https://docs.google.com/spreadsheets/u/3/d/1Std_G_5dnajzm289vlsthIJPFnuxN5yOYNDOoiz9Juc/copy) to estimate their end date. Make a copy and input your start date and expected hours per week in the `Timeline` sheet. As you work through courses you can enter your actual course completion dates in the `Curriculum Data` sheet and get updated completion estimates.\n  \n> **Warning:** While the spreadsheet is a useful tool to estimate the time you need to complete this curriculum, it may not always be up-to-date with the curriculum. Use the [OSSU CS website](https://cs.ossu.dev) or [the repo](https://github.com/ossu/computer-science) to see what courses to do.\n\n**Cost**. All or nearly all course material is available for free. However, some courses may charge money for assignments/tests/projects to be graded.\nNote that both [Coursera](https://www.coursera.support/s/article/209819033-Apply-for-Financial-Aid-or-a-Scholarship?language=en_US) and [edX](https://courses.edx.org/financial-assistance/) offer financial aid.\n\nDecide how much or how little to spend based on your own time and budget;\njust remember that you can't purchase success!\n\n**Process**. Students can work through the curriculum alone or in groups, in order or out of order.\n- We recommend doing all courses in Core CS, only skipping a course when you are certain that you've already learned the material previously.\n- For simplicity, we recommend working through courses (especially Core CS) in order from top to bottom. Some students choose to study multiple courses at a time in order to vary the material they are working on in a day/week. A popular option is to take the math courses in parallel with the introductory courses. Course prerequisites are listed to help you determine if you are prepared for a given course.\n- Courses in Advanced CS are electives. Choose one subject (e.g. Advanced programming) you want to become an expert in and take all the courses under that heading. You can also create your own custom subject; the Discord community may provide feedback on your planned subject.\n\n**Content policy**. If you plan on showing off some of your coursework publicly, you must share only files that you are allowed to.\n*Respect the code of conduct* that you signed in the beginning of each course!\n\n**[How to contribute](CONTRIBUTING.md)**\n\n**[Getting help](HELP.md)** (Details about our FAQ and chatroom)\n\n# Community\n\n- We have a Discord server! [![Discord](https://img.shields.io/discord/744385009028431943.svg?label=&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2)](https://discord.gg/wuytwK5s9h) This should be your first stop to talk with other OSSU students. Why don't you introduce yourself right now? [Join the OSSU Discord](https://discord.gg/wuytwK5s9h)\n- You can also interact through GitHub issues. If there is a problem with a course, or a change needs to be made to the curriculum, this is the place to start the conversation. Read more [here](CONTRIBUTING.md).\n- Add **Open Source Society University** to your [Linkedin](https://www.linkedin.com/school/11272443/) profile!\n\n> **Warning:** There are a few third-party/deprecated/outdated material that you might find when searching for OSSU. We recommend you to ignore them, and only use the [OSSU CS website](https://cs.ossu.dev) or [OSSU CS Github Repo](https://github.com/ossu/computer-science). Some known outdated materials are:\n>  - An unmaintained and deprecated firebase app. Read more in the [FAQ](./FAQ.md#why-is-the-firebase-ossu-app-different-or-broken).\n>  - An unmaintained and deprecated trello board\n>  - Third-party notion templates\n\n# Curriculum\n\n- [Prerequisites](#prerequisites)\n- [Intro CS](#intro-cs)\n- [Core CS](#core-cs)\n  - [Core programming](#core-programming)\n  - [Core math](#core-math)\n  - [CS Tools](#cs-tools)\n  - [Core systems](#core-systems)\n  - [Core theory](#core-theory)\n  - [Core security](#core-security)\n  - [Core applications](#core-applications)\n  - [Core ethics](#core-ethics)\n- [Advanced CS](#advanced-cs)\n  - [Advanced programming](#advanced-programming)\n  - [Advanced systems](#advanced-systems)\n  - [Advanced theory](#advanced-theory)\n  - [Advanced information security](#advanced-information-security)\n  - [Advanced math](#advanced-math)\n- [Final project](#final-project)\n\n---\n\n## Prerequisites\n\n- [Core CS](#core-cs) assumes the student has already taken [high school math](https://ossu.dev/precollege-math), including algebra, geometry, and pre-calculus.\n- [Advanced CS](#advanced-cs) assumes the student has already taken the entirety of Core CS\nand is knowledgeable enough now to decide which electives to take.\n- Note that [Advanced systems](#advanced-systems) assumes the student has taken a basic physics course (e.g. AP Physics in high school).\n\n## Intro CS\n\nThis course will introduce you to the world of computer science and programming. This course gives you a flavor of the material to come. If you finish the course wanting more, Computer Science is likely for you!\n\n**Topics covered**:\n`computation`\n`imperative programming`\n`basic data structures and algorithms`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Introduction to Computer Science and Programming using Python](coursepages/intro-cs/README.md) | 14 weeks | 6-10 hours/week | [high school algebra](https://ossu.dev/precollege-math) | [chat](https://discord.gg/jvchSm9)\n\n## Core CS\n\nAll coursework under Core CS is **required**, unless otherwise indicated.\n\n### Core programming\n**Topics covered**:\n`functional programming`\n`design for testing`\n`program requirements`\n`common design patterns`\n`unit testing`\n`object-oriented design`\n`static typing`\n`dynamic typing`\n`ML-family languages (via Standard ML)`\n`Lisp-family languages (via Racket)`\n`Ruby`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Systematic Program Design](coursepages/spd/README.md) | 13 weeks | 8-10 hours/week | none | chat: [part 1](https://discord.gg/RfqAmGJ) / [part 2](https://discord.gg/kczJzpm)\n[Class-based Program Design](https://course.ccs.neu.edu/cs2510sp22/index.html) | 13 weeks | 5-10 hours/week | Systematic Program Design, High School Math | [chat](https://discord.com/channels/744385009028431943/891411727294562314)\n[Programming Languages, Part A](https://www.coursera.org/learn/programming-languages) | 5 weeks | 4-8 hours/week | Systematic Program Design ([Hear instructor](https://www.coursera.org/lecture/programming-languages/recommended-background-k1yuh)) | [chat](https://discord.gg/8BkJtXN)\n[Programming Languages, Part B](https://www.coursera.org/learn/programming-languages-part-b) | 3 weeks | 4-8 hours/week | Programming Languages, Part A | [chat](https://discord.gg/EeA7VR9)\n[Programming Languages, Part C](https://www.coursera.org/learn/programming-languages-part-c) | 3 weeks | 4-8 hours/week | Programming Languages, Part B | [chat](https://discord.gg/8EZUVbA)\n[Object-Oriented Design](https://course.ccs.neu.edu/cs3500f19/) | 13 weeks | 5-10 hours/week | Class Based Program Design | [chat](https://discord.com/channels/744385009028431943/891412022120579103)\n[Software Architecture](https://www.coursera.org/learn/software-architecture) | 4 weeks | 2-5 hours/week | Object Oriented Design | [chat](https://discord.com/channels/744385009028431943/891412169638432788)\n\n### Core math\nDiscrete math (Math for CS) is a prerequisite and closely related to the study of algorithms and data structures. Calculus both prepares students for discrete math and helps students develop mathematical maturity.\n\n**Topics covered**:\n`discrete mathematics`\n`mathematical proofs`\n`basic statistics`\n`O-notation`\n`discrete probability`\n`and more`\n\nCourses | Duration | Effort | Notes | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--: | :--:\n[Calculus 1A: Differentiation](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+18.01.1x+2T2019/about) ([alternative](https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010/index.htm)) | 13 weeks | 6-10 hours/week | The alternate covers this and the following 2 courses | [high school math](https://ossu.dev/precollege-math) | [chat](https://discord.gg/mPCt45F)\n[Calculus 1B: Integration](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+18.01.2x+3T2019/about) | 13 weeks | 5-10 hours/week | - | Calculus 1A | [chat](https://discord.gg/sddAsZg)\n[Calculus 1C: Coordinate Systems & Infinite Series](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+18.01.3x+1T2020/about) | 6 weeks | 5-10 hours/week | - | Calculus 1B | [chat](https://discord.gg/FNEcNNq)\n[Mathematics for Computer Science](https://openlearninglibrary.mit.edu/courses/course-v1:OCW+6.042J+2T2019/about) ([alternative](https://ocw.mit.edu/courses/6-042j-mathematics-for-computer-science-fall-2010/)) | 13 weeks | 5 hours/week | [2015/2019 solutions](https://github.com/spamegg1/Math-for-CS-solutions) [2010 solutions](https://github.com/frevib/mit-cs-math-6042-fall-2010-problems) [2005 solutions](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2005/assignments/). | Calculus 1C | [chat](https://discord.gg/EuTzNbF)\n\n\n### CS Tools\nUnderstanding theory is important, but you will also be expected to create programs. There are a number of tools that are widely used to make that process easier. Learn them now to ease your future work writing programs.\n\n**Topics covered**:\n`terminals and shell scripting`\n`vim`\n`command line environments`\n`version control`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[The Missing Semester of Your CS Education](https://missing.csail.mit.edu/) | 2 weeks | 12 hours/week | - | [chat](https://discord.gg/5FvKycS)\n\n### Core systems\n\n**Topics covered**:\n`procedural programming`\n`manual memory management`\n`boolean algebra`\n`gate logic`\n`memory`\n`computer architecture`\n`assembly`\n`machine language`\n`virtual machines`\n`high-level languages`\n`compilers`\n`operating systems`\n`network protocols`\n`and more`\n\nCourses | Duration | Effort | Additional Text / Assignments| Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--: | :--:\n[Build a Modern Computer from First Principles: From Nand to Tetris](https://www.coursera.org/learn/build-a-computer) ([alternative](https://www.nand2tetris.org/)) | 6 weeks | 7-13 hours/week | - | C-like programming language | [chat](https://discord.gg/vxB2DRV)\n[Build a Modern Computer from First Principles: Nand to Tetris Part II ](https://www.coursera.org/learn/nand2tetris2) | 6 weeks | 12-18 hours/week | - | one of [these programming languages](https://user-images.githubusercontent.com/2046800/35426340-f6ce6358-026a-11e8-8bbb-4e95ac36b1d7.png), From Nand to Tetris Part I | [chat](https://discord.gg/AsUXcPu)\n[Operating Systems: Three Easy Pieces](coursepages/ostep/README.md) | 10-12 weeks | 6-10 hours/week | - | Nand to Tetris Part II | [chat](https://discord.gg/wZNgpep)\n[Computer Networking: a Top-Down Approach](http://gaia.cs.umass.edu/kurose_ross/online_lectures.htm)| 8 weeks | 4–12 hours/week | [Wireshark Labs](http://gaia.cs.umass.edu/kurose_ross/wireshark.php) | algebra, probability, basic CS | [chat](https://discord.gg/MJ9YXyV)\n\n### Core theory\n\n**Topics covered**:\n`divide and conquer`\n`sorting and searching`\n`randomized algorithms`\n`graph search`\n`shortest paths`\n`data structures`\n`greedy algorithms`\n`minimum spanning trees`\n`dynamic programming`\n`NP-completeness`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Divide and Conquer, Sorting and Searching, and Randomized Algorithms](https://www.coursera.org/learn/algorithms-divide-conquer) | 4 weeks | 4-8 hours/week | any programming language, Mathematics for Computer Science | [chat](https://discord.gg/mKRS7tY)\n[Graph Search, Shortest Paths, and Data Structures](https://www.coursera.org/learn/algorithms-graphs-data-structures) | 4 weeks | 4-8 hours/week | Divide and Conquer, Sorting and Searching, and Randomized Algorithms | [chat](https://discord.gg/Qstqe4t)\n[Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming](https://www.coursera.org/learn/algorithms-greedy) | 4 weeks | 4-8 hours/week | Graph Search, Shortest Paths, and Data Structures | [chat](https://discord.gg/dWVvjuz)\n[Shortest Paths Revisited, NP-Complete Problems and What To Do About Them](https://www.coursera.org/learn/algorithms-npcomplete) | 4 weeks | 4-8 hours/week | Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming | [chat](https://discord.gg/dYuY78u)\n\n### Core security\n**Topics covered**\n`Confidentiality, Integrity, Availability`\n`Secure Design`\n`Defensive Programming`\n`Threats and Attacks`\n`Network Security`\n`Cryptography`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Cybersecurity Fundamentals](https://www.edx.org/course/cybersecurity-fundamentals) | 8 weeks | 10-12 hours/week | - | [chat](https://discord.gg/XdY3AwTFK4)\n[Principles of Secure Coding](https://www.coursera.org/learn/secure-coding-principles)| 4 weeks | 4 hours/week | - | [chat](https://discord.gg/5gMdeSK)\n[Identifying Security Vulnerabilities](https://www.coursera.org/learn/identifying-security-vulnerabilities) | 4 weeks | 4 hours/week | - | [chat](https://discord.gg/V78MjUS)\n\nChoose **one** of the following:\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Identifying Security Vulnerabilities in C/C++Programming](https://www.coursera.org/learn/identifying-security-vulnerabilities-c-programming) | 4 weeks | 5 hours/week | - | [chat](https://discord.gg/Vbxce7A)\n[Exploiting and Securing Vulnerabilities in Java Applications](https://www.coursera.org/learn/exploiting-securing-vulnerabilities-java-applications) | 4 weeks | 5 hours/week | - | [chat](https://discord.gg/QxC22rR)\n\n### Core applications\n\n**Topics covered**:\n`Agile methodology`\n`REST`\n`software specifications`\n`refactoring`\n`relational databases`\n`transaction processing`\n`data modeling`\n`neural networks`\n`supervised learning`\n`unsupervised learning`\n`OpenGL`\n`ray tracing`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Databases: Modeling and Theory](https://www.edx.org/course/modeling-and-theory)| 2 weeks | 10 hours/week | core programming | [chat](https://discord.gg/pMFqNf4)\n[Databases: Relational Databases and SQL](https://www.edx.org/course/databases-5-sql)| 2 weeks | 10 hours/week | core programming | [chat](https://discord.gg/P8SPPyF)\n[Databases: Semistructured Data](https://www.edx.org/course/semistructured-data)| 2 weeks | 10 hours/week | core programming | [chat](https://discord.gg/duCJ3GN)\n[Machine Learning](https://www.coursera.org/specializations/machine-learning-introduction)| 11 weeks | 9 hours/week | Basic coding | [chat](https://discord.gg/NcXHDjy)\n[Computer Graphics](https://www.edx.org/course/computer-graphics-2) ([alternative](https://cseweb.ucsd.edu/~viscomp/classes/cse167/wi22/schedule.html))| 6 weeks | 12 hours/week | C++ or Java, [Basic Linear Algebra](https://ossu.dev/precollege-math/coursepages/precalculus) | [chat](https://discord.gg/68WqMNV)\n[Software Engineering: Introduction](https://www.edx.org/learn/software-engineering/university-of-british-columbia-software-engineering-introduction) ([alternative](https://github.com/ubccpsc/310/blob/main/resources/README.md)) | 6 weeks | 8-10 hours/week | Core Programming, and a [sizable project](FAQ.md#why-require-experience-with-a-sizable-project-before-the-Software-Engineering-courses) | [chat](https://discord.gg/5Qtcwtz)\n\n### Core ethics\n\n**Topics covered**:\n`Social Context`\n`Analytical Tools`\n`Professional Ethics`\n`Intellectual Property`\n`Privacy and Civil Liberties`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Ethics, Technology and Engineering](https://www.coursera.org/learn/ethics-technology-engineering)| 9 weeks | 2 hours/week | none | [chat](https://discord.gg/6ttjPmzZbe)\n[Introduction to  Intellectual Property](https://www.coursera.org/learn/introduction-intellectual-property)| 4 weeks | 2 hours/week | none | [chat](https://discord.gg/YbuERswpAK)\n[Data Privacy Fundamentals](https://www.coursera.org/learn/northeastern-data-privacy)| 3 weeks | 3 hours/week | none | [chat](https://discord.gg/64J34ajNBd)\n\n## Advanced CS\n\nAfter completing **every required course** in Core CS, students should choose a subset of courses from Advanced CS based on interest.\nNot every course from a subcategory needs to be taken.\nBut students should take *every* course that is relevant to the field they intend to go into.\n\n### Advanced programming\n\n**Topics covered**:\n`debugging theory and practice`\n`goal-oriented programming`\n`parallel computing`\n`object-oriented analysis and design`\n`UML`\n`large-scale software architecture and design`\n`and more`\n\nCourses | Duration | Effort | Prerequisites\n:-- | :--: | :--: | :--:\n[Parallel Programming](https://www.coursera.org/learn/scala-parallel-programming)| 4 weeks | 6-8 hours/week | Scala programming\n[Compilers](https://www.edx.org/course/compilers) | 9 weeks | 6-8 hours/week | none\n[Introduction to Haskell](https://www.seas.upenn.edu/~cis194/fall16/)| 14 weeks | - | -\n[Learn Prolog Now!](https://www.let.rug.nl/bos/lpn//lpnpage.php?pageid=online) ([alternative](https://github.com/ossu/computer-science/files/6085884/lpn.pdf))*| 12 weeks | - | -\n[Software Debugging](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkxK63TiT88oEe-AIBhr96A)| 8 weeks | 6 hours/week | Python, object-oriented programming\n[Software Testing](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkWVHeC_8aSIbSxE_NXI76g) | 4 weeks | 6 hours/week | Python, programming experience\n\n(*) book by Blackburn, Bos, Striegnitz (compiled from [source](https://github.com/LearnPrologNow/lpn), redistributed under [CC license](https://creativecommons.org/licenses/by-sa/4.0/))\n\n### Advanced systems\n\n**Topics covered**:\n`digital signaling`\n`combinational logic`\n`CMOS technologies`\n`sequential logic`\n`finite state machines`\n`processor instruction sets`\n`caches`\n`pipelining`\n`virtualization`\n`parallel processing`\n`virtual memory`\n`synchronization primitives`\n`system call interface`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Notes\n:-- | :--: | :--: | :--: | :--:\n[Computation Structures 1: Digital Circuits](https://learning.edx.org/course/course-v1:MITx+6.004.1x_3+3T2016) [alternative 1](https://ocw.mit.edu/courses/6-004-computation-structures-spring-2017/) [alternative 2](https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/) | 10 weeks | 6 hours/week | [Nand2Tetris II](https://www.coursera.org/learn/nand2tetris2) | Alternate links contain all 3 courses.\n[Computation Structures 2: Computer Architecture](https://learning.edx.org/course/course-v1:MITx+6.004.2x+3T2015) | 10 weeks | 6 hours/week | Computation Structures 1 | - \n[Computation Structures 3: Computer Organization](https://learning.edx.org/course/course-v1:MITx+6.004.3x_2+1T2017) | 10 weeks | 6 hours/week | Computation Structures 2 | -\n\n### Advanced theory\n\n**Topics covered**:\n`formal languages`\n`Turing machines`\n`computability`\n`event-driven concurrency`\n`automata`\n`distributed shared memory`\n`consensus algorithms`\n`state machine replication`\n`computational geometry theory`\n`propositional logic`\n`relational logic`\n`Herbrand logic`\n`game trees`\n`and more`\n\nCourses | Duration | Effort | Prerequisites\n:-- | :--: | :--: | :--:\n[Theory of Computation](https://ocw.mit.edu/courses/18-404j-theory-of-computation-fall-2020/) ([alternative](https://www.youtube.com/playlist?list=PLEE7DF8F5E0203A56)) | 13 weeks | 10 hours/week | [Mathematics for Computer Science](https://openlearninglibrary.mit.edu/courses/course-v1:OCW+6.042J+2T2019/about), logic, algorithms\n[Computational Geometry](https://www.edx.org/course/computational-geometry) | 16 weeks | 8 hours/week | algorithms, C++\n[Game Theory](https://www.coursera.org/learn/game-theory-1) | 8 weeks | 3 hours/week | mathematical thinking, probability, calculus\n\n### Advanced Information Security\n\nCourses | Duration | Effort | Prerequisites\n:-- | :--: | :--: | :--:\n[Web Security Fundamentals](https://www.edx.org/course/web-security-fundamentals) | 5 weeks | 4-6 hours/week | understanding basic web technologies\n[Security Governance & Compliance](https://www.coursera.org/learn/security-governance-compliance) | 3 weeks | 3 hours/week | -\n[Digital Forensics Concepts](https://www.coursera.org/learn/digital-forensics-concepts) | 3 weeks | 2-3 hours/week | Core Security\n[Secure Software Development: Requirements, Design, and Reuse](https://www.edx.org/course/secure-software-development-requirements-design-and-reuse) | 7 weeks | 1-2 hours/week | Core Programming and Core Security\n[Secure Software Development: Implementation](https://www.edx.org/course/secure-software-development-implementation) | 7 weeks | 1-2 hours/week | Secure Software Development: Requirements, Design, and Reuse\n[Secure Software Development: Verification and More Specialized Topics](https://www.edx.org/course/secure-software-development-verification-and-more-specialized-topics) | 7 weeks | 1-2 hours/week | Secure Software Development: Implementation\n\n### Advanced math\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) | - | - | [high school math](https://ossu.dev/precollege-math) | [chat](https://discord.gg/m6wHbP6)\n[Linear Algebra](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/) | 14 weeks | 12 hours/week | corequisite: Essence of Linear Algebra | [chat](https://discord.gg/k7nSWJH)\n[Introduction to Numerical Methods](https://ocw.mit.edu/courses/mathematics/18-335j-introduction-to-numerical-methods-spring-2019/index.htm)| 14 weeks | 12 hours/week | [Linear Algebra](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/) | [chat](https://discord.gg/FNEcNNq)\n[Introduction to Formal Logic](https://forallx.openlogicproject.org/) | 10 weeks | 4-8 hours/week | [Set Theory](https://www.youtube.com/playlist?list=PL5KkMZvBpo5AH_5GpxMiryJT6Dkj32H6N) | [chat](https://discord.gg/MbM2Gg5)\n[Probability](https://projects.iq.harvard.edu/stat110/home) | 15 weeks | 5-10 hours/week | [Differentiation and Integration](https://www.edx.org/course/calculus-1b-integration) | [chat](https://discord.gg/UVjs9BU)\n\n## Final project\n\nPart of learning is doing.\nThe assignments and exams for each course are to prepare you to use your knowledge to solve real-world problems.\n\nAfter you've completed Core CS and the parts of Advanced CS relevant to you,\nyou should identify a problem that you can solve using the knowledge you've acquired.\nYou can create something entirely new, or you can improve some tool/program that you use and wish were better.\n\nStudents who would like more guidance in creating a project may choose to use a series of project oriented courses.\nHere is a sample of options\n(many more are available, at this point you should be capable of identifying a series that is interesting and relevant to you):\n\nCourses | Duration | Effort | Prerequisites\n:-- | :--: | :--: | :--:\n[Fullstack Open](https://fullstackopen.com/en/) | 12 weeks | 15 hours/week | programming\n[Modern Robotics (Specialization)](https://www.coursera.org/specializations/modernrobotics) | 26 weeks | 2-5 hours/week | freshman-level physics, linear algebra, calculus, [linear ordinary differential equations](https://www.khanacademy.org/math/differential-equations)\n[Data Mining (Specialization)](https://www.coursera.org/specializations/data-mining) | 30 weeks | 2-5 hours/week | machine learning\n[Big Data (Specialization)](https://www.coursera.org/specializations/big-data) | 30 weeks | 3-5 hours/week | none\n[Internet of Things (Specialization)](https://www.coursera.org/specializations/internet-of-things) | 30 weeks | 1-5 hours/week | strong programming\n[Cloud Computing (Specialization)](https://www.coursera.org/specializations/cloud-computing) | 30 weeks | 2-6 hours/week | C++ programming\n[Data Science (Specialization)](https://www.coursera.org/specializations/jhu-data-science) | 43 weeks | 1-6 hours/week | none\n[Functional Programming in Scala (Specialization)](https://www.coursera.org/specializations/scala) | 29 weeks | 4-5 hours/week | One year programming experience\n[Game Design and Development with Unity 2020 (Specialization)](https://www.coursera.org/specializations/game-design-and-development) | 6 months | 5 hours/week | programming, interactive design\n\n## Congratulations\n\nAfter completing the requirements of the curriculum above,\nyou will have completed the equivalent of a full bachelor's degree in Computer Science.\nCongratulations!\n\nWhat is next for you? The possibilities are boundless and overlapping:\n\n- Look for a job as a developer!\n- Check out the [readings](extras/readings.md) for classic books you can read that will sharpen your skills and expand your knowledge.\n- Join a local developer meetup (e.g. via [meetup.com](https://www.meetup.com/)).\n- Pay attention to emerging technologies in the world of software development:\n  + Explore the **actor model** through [Elixir](https://elixir-lang.org/), a new functional programming language for the web based on the battle-tested Erlang Virtual Machine!\n  + Explore **borrowing and lifetimes** through [Rust](https://www.rust-lang.org/), a systems language which achieves memory- and thread-safety without a garbage collector!\n  + Explore **dependent type systems** through [Idris](https://www.idris-lang.org/), a new Haskell-inspired language with unprecedented support for type-driven development.\n\n![keep learning](images/keep-learning.webp)\n\n# Code of conduct\n[OSSU's code of conduct](https://github.com/ossu/code-of-conduct).\n\n## How to show your progress\n\n[Fork](https://www.freecodecamp.org/news/how-to-fork-a-github-repository/) the [GitHub repo](https://github.com/ossu/computer-science) into your own GitHub account and put ✅ next to the stuff you've completed as you complete it. This can serve as your [kanban board](https://en.wikipedia.org/wiki/Kanban_board) and will be faster to implement than any other solution (giving you time to spend on the courses).\n\n# Team\n\n* **[Eric Douglas](https://github.com/ericdouglas)**: founder of OSSU\n* **[Josh Hanson](https://github.com/joshmhanson)**: lead technical maintainer\n* **[Waciuma Wanjohi](https://github.com/waciumawanjohi)**: lead academic maintainer\n* **[Contributors](https://github.com/ossu/computer-science/graphs/contributors)**",
    "summary": "Open Source Society University (OSSU) 提供一个完整的计算机科学免费自学教育路径，其课程体系参照本科学位要求设计，涵盖计算机科学的核心基础概念。课程精选自世界顶尖大学的在线资源，结构化地组织了从入门到核心再到高级的编程、数学、系统、理论、安全、应用等主题。该项目适合有自律性、希望系统性掌握计算机科学基础知识的个人，并提供全球学习者社区支持。预计投入每周20小时，约可在两年内完成全部核心及部分高级课程。",
    "keywords": [
      "计算机科学",
      "在线教育",
      "自学课程",
      "编程",
      "算法",
      "数据结构",
      "操作系统",
      "软件工程"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-21T12:07:52Z",
    "download_time": "2024-05-21 10:00:00",
    "visual_resource": [
      "screenshot/github_computer-science.png"
    ],
    "extra_info": null
  },
  {
    "id": "n8n",
    "source": "GitHub",
    "url": "https://github.com/n8n-io/n8n",
    "title": "n8n - Secure Workflow Automation for Technical Teams",
    "content": "![Banner image](https://user-images.githubusercontent.com/10284570/173569848-c624317f-42b1-45a6-ab09-f0ea3c247648.png)\n\n# n8n - Secure Workflow Automation for Technical Teams\n\nn8n is a workflow automation platform that gives technical teams the flexibility of code with the speed of no-code. With 400+ integrations, native AI capabilities, and a fair-code license, n8n lets you build powerful automations while maintaining full control over your data and deployments.\n\n![n8n.io - Screenshot](https://raw.githubusercontent.com/n8n-io/n8n/master/assets/n8n-screenshot-readme.png)\n\n## Key Capabilities\n\n- **Code When You Need It**: Write JavaScript/Python, add npm packages, or use the visual interface\n- **AI-Native Platform**: Build AI agent workflows based on LangChain with your own data and models\n- **Full Control**: Self-host with our fair-code license or use our [cloud offering](https://app.n8n.cloud/login)\n- **Enterprise-Ready**: Advanced permissions, SSO, and air-gapped deployments\n- **Active Community**: 400+ integrations and 900+ ready-to-use [templates](https://n8n.io/workflows)\n\n## Quick Start\n\nTry n8n instantly with [npx](https://docs.n8n.io/hosting/installation/npm/) (requires [Node.js](https://nodejs.org/en/)):\n\n```\nnpx n8n\n```\n\nOr deploy with [Docker](https://docs.n8n.io/hosting/installation/docker/):\n\n```\ndocker volume create n8n_data\ndocker run -it --rm --name n8n -p 5678:5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n\n```\n\nAccess the editor at http://localhost:5678\n\n## Resources\n\n- 📚 [Documentation](https://docs.n8n.io)\n- 🔧 [400+ Integrations](https://n8n.io/integrations)\n- 💡 [Example Workflows](https://n8n.io/workflows)\n- 🤖 [AI & LangChain Guide](https://docs.n8n.io/langchain/)\n- 👥 [Community Forum](https://community.n8n.io)\n- 📖 [Community Tutorials](https://community.n8n.io/c/tutorials/28)\n\n## Support\n\nNeed help? Our community forum is the place to get support and connect with other users:\n[community.n8n.io](https://community.n8n.io)\n\n## License\n\nn8n is [fair-code](https://faircode.io) distributed under the [Sustainable Use License](https://github.com/n8n-io/n8n/blob/master/LICENSE.md) and [n8n Enterprise License](https://github.com/n8n-io/n8n/blob/master/LICENSE_EE.md).\n\n- **Source Available**: Always visible source code\n- **Self-Hostable**: Deploy anywhere\n- **Extensible**: Add your own nodes and functionality\n\n[Enterprise licenses](mailto:license@n8n.io) available for additional features and support.\n\nAdditional information about the license model can be found in the [docs](https://docs.n8n.io/reference/license/).\n\n## Contributing\n\nFound a bug 🐛 or have a feature idea ✨? Check our [Contributing Guide](https://github.com/n8n-io/n8n/blob/master/CONTRIBUTING.md) to get started.\n\n## Join the Team\n\nWant to shape the future of automation? Check out our [job posts](https://n8n.io/careers) and join our team!\n\n## What does n8n mean?\n\n**Short answer:** It means \"nodemation\" and is pronounced as n-eight-n.\n\n**Long answer:** \"I get that question quite often (more often than I expected) so I decided it is probably best to answer it here. While looking for a good name for the project with a free domain I realized very quickly that all the good ones I could think of were already taken. So, in the end, I chose nodemation. 'node-' in the sense that it uses a Node-View and that it uses Node.js and '-mation' for 'automation' which is what the project is supposed to help with. However, I did not like how long the name was and I could not imagine writing something that long every time in the CLI. That is when I then ended up on 'n8n'.\" - **Jan Oberhauser, Founder and CEO, n8n.io**",
    "summary": "n8n是一个面向技术团队的安全工作流自动化平台，融合了代码的灵活性与无代码的便捷性。该平台拥有超过400个集成，内置AI能力，支持基于LangChain构建AI智能体工作流。n8n采用fair-code许可，允许用户完全控制数据和部署，支持自托管和云服务，并提供企业级功能。凭借活跃的社区和丰富的模板资源，n8n成为构建强大自动化流程的理想选择。",
    "keywords": [
      "工作流自动化",
      "集成",
      "无代码",
      "低代码",
      "自托管",
      "人工智能",
      "智能体",
      "LangChain"
    ],
    "area": [
      "人工智能",
      "智能体",
      "其他"
    ],
    "published_time": "2025-05-29T13:07:17+00:00",
    "download_time": "2024-05-30 10:00:00",
    "visual_resource": [
      "https://user-images.githubusercontent.com/10284570/173569848-c624317f-42b1-45a6-ab09-f0ea3c247648.png",
      "https://raw.githubusercontent.com/n8n-io/n8n/master/assets/n8n-screenshot-readme.png"
    ],
    "extra_info": null
  },
  {
    "id": "AppFlowy",
    "source": "GitHub",
    "url": "https://github.com/AppFlowy-IO/AppFlowy",
    "title": "AppFlowy",
    "content": "<h1 align=\"center\" style=\"border-bottom: none\">\n    <b>\n        <a href=\"https://www.appflowy.com\">AppFlowy</a><br>\n    </b>\n    ⭐️  The Open Source Alternative To Notion  ⭐️ <br>\n</h1>\n\n<p align=\"center\">\nAppFlowy is the AI workspace where you achieve more without losing control of your data\n</p>\n\n<p align=\"center\">\n<a href=\"https://discord.gg/9Q2xaN37tV\"><img src=\"https://img.shields.io/badge/AppFlowy.IO-discord-orange\"></a>\n<a href=\"https://github.com/AppFlowy-IO/appflowy\"><img src=\"https://img.shields.io/github/stars/AppFlowy-IO/appflowy.svg?style=flat&logo=github&colorB=deeppink&label=stars\"></a>\n<a href=\"https://github.com/AppFlowy-IO/appflowy\"><img src=\"https://img.shields.io/github/forks/AppFlowy-IO/appflowy.svg\"></a>\n<a href=\"https://opensource.org/licenses/AGPL-3.0\"><img src=\"https://img.shields.io/badge/license-AGPL-purple.svg\" alt=\"License: AGPL\"></a>\n\n</p>\n\n<p align=\"center\">\n    <a href=\"https://www.appflowy.com\"><b>Website</b></a> •\n    <a href=\"https://forum.appflowy.io/\"><b>Forum</b></a> •\n    <a href=\"https://discord.gg/9Q2xaN37tV\"><b>Discord</b></a> •\n    <a href=\"https://www.reddit.com/r/AppFlowy\"><b>Reddit</b></a> •\n    <a href=\"https://twitter.com/appflowy\"><b>Twitter</b></a>\n</p>\n\n<p align=\"center\"><img src=\"https://appflowy.com/_next/static/media/tasks.796c753e.png\" alt=\"AppFlowy Kanban Board for To-dos\"  /></p>\n<p align=\"center\"><img src=\"https://appflowy.com/_next/static/media/Grid.9e30484b.png\" alt=\"AppFlowy Databases for Tasks and Projects\"  /></p>\n<p align=\"center\"><img src=\"https://appflowy.com/_next/static/media/sites.a8d5b2b9.png\" alt=\"AppFlowy Sites for Beautiful documentation\"  /></p>\n<p align=\"center\"><img src=\"https://appflowy.com/_next/static/media/ai.e1460982.png\" alt=\"AppFlowy AI\" /></p>\n<p align=\"center\"><img src=\"https://appflowy.com/_next/static/media/template.9ea13c3b.png\" alt=\"AppFlowy Templates\"  /></p>\n\n<br></br>\n<p align=\"center\" >\n    <img src=\"https://github.com/user-attachments/assets/5841c491-b564-4a26-b9b6-191def430911\" alt=\"Work across devices\" width=\"1040px\" /></p>\n<p align=\"center\" >\n    <img src=\"https://github.com/user-attachments/assets/c2ba6bb8-746c-4743-9393-d008a669be95\" alt=\"Work across devices\" width=\"1040px\" /></p>\n<p align=\"center\" >\n    <img src=\"https://github.com/user-attachments/assets/e83dd1a3-4975-4d0e-91a1-9eb6e0d248cd\" alt=\"Work across devices\" width=\"1040px\" /></p>\n\n## User Installation\n\n- [Download AppFlowy Desktop (macOS, Windows, and Linux)](https://github.com/AppFlowy-IO/AppFlowy/releases)\n- Other\n  channels: [FlatHub](https://flathub.org/apps/io.appflowy.AppFlowy), [Snapcraft](https://snapcraft.io/appflowy), [Sourceforge](https://sourceforge.net/projects/appflowy/)\n- Available on\n    - [App Store](https://apps.apple.com/app/appflowy/id6457261352): iPhone\n    - [Play Store](https://play.google.com/store/apps/details?id=io.appflowy.appflowy): Android 10 or above; ARMv7 is\n      not supported\n- [Self-hosting AppFlowy](https://appflowy.com/docs/self-host-appflowy-overview)\n- [Source](https://docs.appflowy.io/docs/documentation/appflowy/from-source)\n\n## Built With\n\n- [Flutter](https://flutter.dev/)\n\n- [Rust](https://www.rust-lang.org/)\n\n## Stay Up-to-Date\n\n<p align=\"center\"><img src=\"https://github.com/AppFlowy-IO/appflowy/blob/main/doc/imgs/howtostar.gif\" alt=\"AppFlowy Github - how to star the repo\" width=\"100%\" /></p>\n\n## Getting Started with development\n\nPlease view the [documentation](https://docs.appflowy.io/docs/documentation/appflowy/from-source) for OS specific\ndevelopment instructions\n\n## Roadmap\n\n- [AppFlowy Roadmap ReadMe](https://docs.appflowy.io/docs/appflowy/roadmap)\n- [AppFlowy Public Roadmap](https://github.com/orgs/AppFlowy-IO/projects/5/views/12)\n\nIf you'd like to propose a feature, submit a feature\nrequest [here](https://github.com/AppFlowy-IO/AppFlowy/issues/new?assignees=&labels=&template=feature_request.yaml&title=%5BFR%5D+) <br/>\nIf you'd like to report a bug, submit a bug\nreport [here](https://github.com/AppFlowy-IO/AppFlowy/issues/new?assignees=&labels=&template=bug_report.yaml&title=%5BBug%5D+)\n\n## **Releases**\n\nPlease see the [changelog](https://appflowy.com/what-is-new) for more details about a given release.\n\n## Contributing\n\nContributions make the open-source community a fantastic place to learn, inspire, and create. Any contributions you make\nare **greatly appreciated**. Please look\nat [Contributing to AppFlowy](https://docs.appflowy.io/docs/documentation/software-contributions/contributing-to-appflowy)\nfor details.\n\nIf your Pull Request is accepted as it fixes a bug, adds functionality, or makes AppFlowy's codebase significantly\neasier to use or understand, **Congratulations!** If your administrative and managerial work behind the scenes sustains\nthe community, **Congratulations!** You are now an official contributor to AppFlowy.\n\n## Translations 🌎🗺\n\n[![translation badge](https://inlang.com/badge?url=github.com/AppFlowy-IO/AppFlowy)](https://inlang.com/editor/github.com/AppFlowy-IO/AppFlowy?ref=badge)\n\nTo add translations, you can manually edit the JSON translation files in `/frontend/resources/translations`, use\nthe [inlang online editor](https://inlang.com/editor/github.com/AppFlowy-IO/AppFlowy), or\nrun `npx inlang machine translate` to add missing translations.\n\n## Join the community to build AppFlowy together\n\n<a href=\"https://github.com/AppFlowy-IO/AppFlowy/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=AppFlowy-IO/AppFlowy\" />\n</a>\n\n## Why Are We Building This?\n\nNotion has been our favourite project and knowledge management tool in recent years because of its aesthetic appeal and\nfunctionality. Our team uses it daily, and we are on its paid plan. However, as we all know, Notion has its limitations.\nThese include weak data security and poor compatibility with mobile devices. Likewise, alternative collaborative\nworkplace management tools also have their constraints.\n\nThe limitations we encountered using these tools and our past work experience with collaborative productivity tools have\nled to our firm belief that there is a glass ceiling on what's possible for these tools in the future. This emanates\nfrom the fact that these tools will probably struggle to scale horizontally at some point and be forced to prioritize a\nproportion of customers whose needs differ from the rest. While decision-makers want a workplace OS, it is impossible to\ncome up with a one-size fits all solution in such a fragmented market.\n\nWhen a customer's evolving core needs are not satisfied, they either switch to another or build one from the ground up,\nin-house. Consequently, they either go under another ceiling or buy an expensive ticket to learn a hard lesson. This is\na requirement for many resources and expertise, building a reliable and easy-to-use collaborative tool, not to mention\nthe speed and native experience. The same may apply to individual users as well.\n\nAll these restrictions necessitate our mission - to make it possible for anyone to create apps that suit their needs\nwell.\n\n- To individuals, we would like to offer Notion's functionality, data security, and cross-platform native experience.\n- To enterprises and hackers, AppFlowy is dedicated to offering building blocks and collaboration infra services to\n  enable you to make apps on your own. Moreover, you have 100% control of your data. You can design and modify AppFlowy\n  your way, with a single codebase written in Flutter and Rust supporting multiple platforms armed with long-term\n  maintainability.\n\nWe decided to achieve this mission by upholding the three most fundamental values:\n\n- Data privacy first\n- Reliable native experience\n- Community-driven extensibility\n\nWe do not claim to outperform Notion in terms of functionality and design, at least for now. Besides, our priority\ndoesn't lie in more functionality at the moment. Instead, we would like to cultivate a community to democratize the\nknowledge and wheels of making complex workplace management tools while enabling people and businesses to create\nbeautiful things on their own by equipping them with a versatile toolbox of building blocks.\n\n## License\n\nDistributed under the AGPLv3 License. See [`LICENSE.md`](https://github.com/AppFlowy-IO/AppFlowy/blob/main/LICENSE) for\nmore information.\n\n## Acknowledgments\n\nSpecial thanks to these amazing projects which help power AppFlowy:\n\n- [cargo-make](https://github.com/sagiegurari/cargo-make)\n- [contrib.rocks](https://contrib.rocks)\n- [flutter_chat_ui](https://pub.dev/packages/flutter_chat_ui)\n",
    "summary": "AppFlowy是一个开源的AI工作空间，旨在成为Notion的强大替代品，核心优势在于强调数据隐私、提供可靠的原生体验以及支持社区驱动的扩展。该项目采用Flutter和Rust构建，具备跨平台能力，提供笔记、任务管理、数据库等核心功能，并支持用户进行自托管部署，从而实现对个人或企业数据的完全掌控和高度定制化。",
    "keywords": [
      "开源",
      "工作空间",
      "数据隐私",
      "Flutter",
      "Rust",
      "笔记",
      "任务管理",
      "自托管"
    ],
    "area": [
      "人工智能",
      "生成式AI",
      "其他"
    ],
    "published_time": "2025-05-29T02:17:27Z",
    "download_time": "2024-05-30 10:00:00",
    "visual_resource": [
      "https://appflowy.com/_next/static/media/tasks.796c753e.png",
      "https://appflowy.com/_next/static/media/Grid.9e30484b.png",
      "https://appflowy.com/_next/static/media/sites.a8d5b2b9.png"
    ],
    "extra_info": null
  },
  {
    "id": "langflow",
    "source": "GitHub",
    "url": "https://github.com/langflow-ai/langflow",
    "title": "Langflow",
    "content": "<!-- markdownlint-disable MD030 -->\n\n![Langflow logo](./docs/static/img/langflow-logo-color-black-solid.svg)\n\n\n[![Release Notes](https://img.shields.io/github/release/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/releases)\n[![PyPI - License](https://img.shields.io/badge/license-MIT-orange)](https://opensource.org/licenses/MIT)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/langflow?style=flat-square)](https://pypistats.org/packages/langflow)\n[![GitHub star chart](https://img.shields.io/github/stars/langflow-ai/langflow?style=flat-square)](https://star-history.com/#langflow-ai/langflow)\n[![Open Issues](https://img.shields.io/github/issues-raw/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/issues)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langflow-ai.svg?style=social&label=Follow%20%40Langflow)](https://twitter.com/langflow_ai)\n[![YouTube Channel](https://img.shields.io/youtube/channel/subscribers/UCn2bInQrjdDYKEEmbpwblLQ?label=Subscribe)](https://www.youtube.com/@Langflow)\n[![Discord Server](https://img.shields.io/discord/1116803230643527710?logo=discord&style=social&label=Join)](https://discord.gg/EqksyE2EX9)\n\n\n[Langflow](https://langflow.org) is a powerful tool for building and deploying AI-powered agents and workflows. It provides developers with both a visual authoring experience and a built-in API server that turns every agent into an API endpoint that can be integrated into applications built on any framework or stack. Langflow comes with batteries included and supports all major LLMs, vector databases and a growing library of AI tools.\n\n## ✨ Highlight features\n\n1. **Visual Builder** to get started quickly and iterate. \n1. **Access to Code** so developers can tweak any component using Python.\n1. **Playground** to immediately test and iterate on their flows with step-by-step control.\n1. **Multi-agent** orchestration and conversation management and retrieval.\n1. **Deploy as an API** or export as JSON for Python apps.\n1. **Observability** with LangSmith, LangFuse and other integrations.\n1. **Enterprise-ready** security and scalability.\n\n## ⚡️ Quickstart\n\nLangflow works with Python 3.10 to 3.13.\n\nInstall with uv **(recommended)** \n\n```shell\nuv pip install langflow\n```\n\nInstall with pip\n\n```shell\npip install langflow\n```\n\n## 📦 Deployment\n\n### Self-managed\n\nLangflow is completely open source and you can deploy it to all major deployment clouds. Follow this [guide](https://docs.langflow.org/deployment-docker) to learn how to use Docker to deploy Langflow.\n\n### Fully-managed by DataStax\n\nDataStax Langflow is a full-managed environment with zero setup. Developers can [sign up for a free account](https://astra.datastax.com/signup?type=langflow) to get started.\n\n## ⭐ Stay up-to-date\n\nStar Langflow on GitHub to be instantly notified of new releases.\n\n![Star Langflow](https://github.com/user-attachments/assets/03168b17-a11d-4b2a-b0f7-c1cce69e5a2c)\n\n## 👋 Contribute\n\nWe welcome contributions from developers of all levels. If you'd like to contribute, please check our [contributing guidelines](./CONTRIBUTING.md) and help make Langflow more accessible.\n\n---\n\n[![Star History Chart](https://api.star-history.com/svg?repos=langflow-ai/langflow&type=Timeline)](https://star-history.com/#langflow-ai/langflow&Date)\n\n## ❤️ Contributors\n\n[![langflow contributors](https://contrib.rocks/image?repo=langflow-ai/langflow)](https://github.com/langflow-ai/langflow/graphs/contributors)\n",
    "summary": "Langflow是一个强大的工具，专注于构建和部署AI驱动的智能体及工作流。它提供直观的可视化构建界面和内置API服务器，能够将创建的智能体轻松转化为可集成到任何应用的技术接口。Langflow全面支持主流大语言模型、向量数据库和不断丰富的AI工具库，核心特性包括可视化构建、代码级定制、交互式Playground、多智能体编排、灵活的API部署选项以及与可观测性平台的集成。该项目支持自托管部署，并提供DataStax托管服务，具备企业级安全性和可扩展性。",
    "keywords": [
      "智能体",
      "工作流",
      "可视化构建",
      "API服务",
      "大语言模型",
      "向量数据库",
      "AI工具",
      "多智能体"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-05-28T21:21:05Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://github.com/langflow-ai/langflow/raw/main/docs/static/img/langflow-logo-color-black-solid.svg",
      "https://api.star-history.com/svg?repos=langflow-ai/langflow&type=Timeline",
      "https://contrib.rocks/image?repo=langflow-ai/langflow"
    ],
    "extra_info": null
  },
  {
    "id": "react-native",
    "source": "GitHub",
    "url": "https://github.com/facebook/react-native",
    "title": "React Native",
    "content": "<h1 align=\"center\">\n  <a href=\"https://reactnative.dev/\">\n    React Native\n  </a>\n</h1>\n\n<p align=\"center\">\n  <strong>Learn once, write anywhere:</strong><br>\n  Build mobile apps with React.\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/facebook/react-native/blob/HEAD/LICENSE\">\n    <img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"React Native is released under the MIT license.\" />\n  </a>\n  <a href=\"https://www.npmjs.org/package/react-native\">\n    <img src=\"https://img.shields.io/npm/v/react-native?color=brightgreen&label=npm%20package\" alt=\"Current npm package version.\" />\n  </a>\n  <a href=\"https://reactnative.dev/docs/contributing\">\n    <img src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg\" alt=\"PRs welcome!\" />\n  </a>\n  <a href=\"https://twitter.com/intent/follow?screen_name=reactnative\">\n    <img src=\"https://img.shields.io/twitter/follow/reactnative.svg?label=Follow%20@reactnative\" alt=\"Follow @reactnative\" />\n  </a>\n</p>\n\n<h3 align=\"center\">\n  <a href=\"https://reactnative.dev/docs/getting-started\">Getting Started</a>\n  <span> · </span>\n  <a href=\"https://reactnative.dev/docs/tutorial\">Learn the Basics</a>\n  <span> · </span>\n  <a href=\"https://reactnative.dev/showcase\">Showcase</a>\n  <span> · </span>\n  <a href=\"https://reactnative.dev/docs/contributing\">Contribute</a>\n  <span> · </span>\n  <a href=\"https://reactnative.dev/help\">Community</a>\n  <span> · </span>\n  <a href=\"https://github.com/facebook/react-native/blob/HEAD/.github/SUPPORT.md\">Support</a>\n</h3>\n\nReact Native brings [**React**'s][r] declarative UI framework to iOS and Android. With React Native, you use native UI controls and have full access to the native platform.\n\n- **Declarative.** React makes it painless to create interactive UIs. Declarative views make your code more predictable and easier to debug.\n- **Component-Based.** Build encapsulated components that manage their state, then compose them to make complex UIs.\n- **Developer Velocity.** See local changes in seconds. Changes to JavaScript code can be live reloaded without rebuilding the native app.\n- **Portability.** Reuse code across iOS, Android, and [other platforms][p].\n\nReact Native is developed and supported by many companies and individual core contributors. Find out more in our [ecosystem overview][e].\n\n[r]: https://react.dev/\n[p]: https://reactnative.dev/docs/out-of-tree-platforms\n[e]: https://github.com/facebook/react-native/blob/HEAD/ECOSYSTEM.md\n\n## Contents\n\n- [Requirements](#-requirements)\n- [Building your first React Native app](#-building-your-first-react-native-app)\n- [Documentation](#-documentation)\n- [Upgrading](#-upgrading)\n- [How to Contribute](#-how-to-contribute)\n- [Code of Conduct](#code-of-conduct)\n- [License](#-license)\n\n\n## 📋 Requirements\n\nReact Native apps may target iOS 15.1 and Android 7.0 (API 24) or newer. You may use Windows, macOS, or Linux as your development operating system, though building and running iOS apps is limited to macOS. Tools like [Expo](https://expo.dev) can be used to work around this.\n\n## 🎉 Building your first React Native app\n\nFollow the [Getting Started guide](https://reactnative.dev/docs/getting-started). The recommended way to install React Native depends on your project. Here you can find short guides for the most common scenarios:\n\n- [Trying out React Native][hello-world]\n- [Creating a New Application][new-app]\n- [Adding React Native to an Existing Application][existing]\n\n[hello-world]: https://snack.expo.dev/@samples/hello-world\n[new-app]: https://reactnative.dev/docs/getting-started\n[existing]: https://reactnative.dev/docs/integration-with-existing-apps\n\n## 📖 Documentation\n\nThe full documentation for React Native can be found on our [website][docs].\n\nThe React Native documentation discusses components, APIs, and topics that are specific to React Native. For further documentation on the React API that is shared between React Native and React DOM, refer to the [React documentation][r-docs].\n\nThe source for the React Native documentation and website is hosted on a separate repository, [**@facebook/react-native-website**][repo-website].\n\n[docs]: https://reactnative.dev/docs/getting-started\n[r-docs]: https://react.dev/learn\n[repo-website]: https://github.com/facebook/react-native-website\n\n## 🚀 Upgrading\n\nUpgrading to new versions of React Native may give you access to more APIs, views, developer tools, and other goodies. See the [Upgrading Guide][u] for instructions.\n\nReact Native releases are discussed [in this discussion repo](https://github.com/reactwg/react-native-releases/discussions).\n\n[u]: https://reactnative.dev/docs/upgrading\n[repo-releases]: https://github.com/react-native-community/react-native-releases\n\n## 👏 How to Contribute\n\nThe main purpose of this repository is to continue evolving React Native core. We want to make contributing to this project as easy and transparent as possible, and we are grateful to the community for contributing bug fixes and improvements. Read below to learn how you can take part in improving React Native.\n\n### [Code of Conduct][code]\n\nFacebook has adopted a Code of Conduct that we expect project participants to adhere to.\nPlease read the [full text][code] so that you can understand what actions will and will not be tolerated.\n\n[code]: https://code.fb.com/codeofconduct/\n\n### [Contributing Guide][contribute]\n\nRead our [**Contributing Guide**][contribute] to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to React Native.\n\n[contribute]: https://reactnative.dev/docs/contributing\n\n### [Open Source Roadmap][roadmap]\n\nYou can learn more about our vision for React Native in the [**Roadmap**][roadmap].\n\n[roadmap]: https://github.com/facebook/react-native/wiki/Roadmap\n\n### Good First Issues\n\nWe have a list of [good first issues][gfi] that contain bugs which have a relatively limited scope. This is a great place to get started, gain experience, and get familiar with our contribution process.\n\n[gfi]: https://github.com/facebook/react-native/labels/good%20first%20issue\n\n### Discussions\n\nLarger discussions and proposals are discussed in [**@react-native-community/discussions-and-proposals**][repo-meta].\n\n[repo-meta]: https://github.com/react-native-community/discussions-and-proposals\n\n## 📄 License\n\nReact Native is MIT licensed, as found in the [LICENSE][l] file.\n\n[l]: https://github.com/facebook/react-native/blob/main/LICENSE",
    "summary": "React Native是一个开源框架，允许开发者使用JavaScript和React构建原生iOS和Android应用。它将React的声明式UI范式带入移动开发，利用原生UI组件，提供组件化架构、快速开发周期（支持热重载）以及跨平台代码复用能力。React Native是构建高性能、美观移动应用的流行选择，由Facebook及社区共同维护。",
    "keywords": [
      "React Native",
      "React",
      "移动开发",
      "跨平台",
      "iOS",
      "Android",
      "UI框架",
      "原生应用"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T13:16:48Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "screenshot/github_react-native.png"
    ],
    "extra_info": null
  },
  {
    "id": "chatgpt-on-wechat",
    "source": "GitHub",
    "url": "https://github.com/zhayujie/chatgpt-on-wechat",
    "title": "chatgpt-on-wechat",
    "content": "<p align=\"center\"><img src= \"https://github.com/user-attachments/assets/31fb4eab-3be4-477d-aa76-82cf62bfd12c\" alt=\"Chatgpt-on-Wechat\" width=\"600\" /></p>\n\n<p align=\"center\">\n   <a href=\"https://github.com/zhayujie/chatgpt-on-wechat/releases/latest\"><img src=\"https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat\" alt=\"Latest release\"></a>\n  <a href=\"https://github.com/zhayujie/chatgpt-on-wechat/blob/master/LICENSE\"><img src=\"https://img.shields.io/github/license/zhayujie/chatgpt-on-wechat\" alt=\"License: MIT\"></a>\n  <a href=\"https://github.com/zhayujie/chatgpt-on-wechat\"><img src=\"https://img.shields.io/github/stars/zhayujie/chatgpt-on-wechat?style=flat-square\" alt=\"Stars\"></a> <br/>\n</p>\n\nchatgpt-on-wechat（简称CoW）项目是基于大模型的智能对话机器人，支持微信公众号、企业微信应用、飞书、钉钉接入，可选择GPT3.5/GPT4.0/Claude/Gemini/LinkAI/ChatGLM/KIMI/文心一言/讯飞星火/通义千问/LinkAI/ModelScope，能处理文本、语音和图片，通过插件访问操作系统和互联网等外部资源，支持基于自有知识库定制企业AI应用。\n\n# 简介\n\n最新版本支持的功能如下：\n\n-  ✅   **多端部署：** 有多种部署方式可选择且功能完备，目前已支持微信公众号、企业微信应用、飞书、钉钉等部署方式\n-  ✅   **基础对话：** 私聊及群聊的消息智能回复，支持多轮会话上下文记忆，支持 GPT-4o系列, GPT-4.1系列, GPT-4o, Claude, Gemini, 文心一言, 讯飞星火, 通义千问，ChatGLM-4，Kimi, MiniMax, GiteeAI, ModelScope\n-  ✅   **语音能力：** 可识别语音消息，通过文字或语音回复，支持 azure, baidu, google, openai(whisper/tts) 等多种语音模型\n-  ✅   **图像能力：** 支持图片生成、图片识别、图生图（如照片修复），可选择 Dall-E-3, stable diffusion, replicate, midjourney, CogView-3, vision模型\n-  ✅   **丰富插件：** 支持个性化插件扩展，已实现多角色切换、文字冒险、敏感词过滤、聊天记录总结、文档总结和对话、联网搜索等插件\n-  ✅   **知识库：** 通过上传知识库文件自定义专属机器人，可作为数字分身、智能客服、私域助手使用，基于 [LinkAI](https://link-ai.tech) 实现\n\n## 声明\n\n1. 本项目遵循 [MIT开源协议](/LICENSE)，仅用于技术研究和学习，使用本项目时需遵守所在地法律法规、相关政策以及企业章程，禁止用于任何违法或侵犯他人权益的行为\n2. 境内使用该项目时，请使用国内厂商的大模型服务，并进行必要的内容安全审核及过滤\n3. 本项目主要接入协同办公平台，推荐使用公众号、企微自建应用、钉钉、飞书等接入通道，其他通道为历史产物已不维护\n4. 任何个人、团队和企业，无论以何种方式使用本项目、对何对象提供服务，所产生的一切后果，本项目均不承担任何责任\n\n## 演示\n\nDEMO视频：https://cdn.link-ai.tech/doc/cow_demo.mp4\n\n## 社区\n\n添加小助手微信加入开源项目交流群：\n\n<img width=\"160\" src=\"https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/open-community.png\">\n\n<br>\n\n# 企业服务\n\n<a href=\"https://link-ai.tech\" target=\"_blank\"><img width=\"800\" src=\"https://cdn.link-ai.tech/image/link-ai-intro.jpg\"></a>\n\n> [LinkAI](https://link-ai.tech/) 是面向企业和开发者的一站式AI应用平台，聚合多模态大模型、知识库、Agent 插件、工作流等能力，支持一键接入主流平台并进行管理，支持SaaS、私有化部署多种模式。\n>\n> LinkAI 目前 已在私域运营、智能客服、企业效率助手等场景积累了丰富的 AI 解决方案， 在电商、文教、健康、新消费、科技制造等各行业沉淀了大模型落地应用的最佳实践，致力于帮助更多企业和开发者拥抱 AI 生产力。\n\n**企业服务和产品咨询** 可联系产品顾问：\n\n<img width=\"160\" src=\"https://cdn.link-ai.tech/consultant-s.jpg\">\n\n<br>\n\n# 🏷 更新日志\n\n>**2025.05.23：** [1.7.6版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.6) 优化web网页channel、新增[AgentMesh多智能体插件](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/agent/README.md)、百度语音合成优化、企微应用`access_token`获取优化、支持`claude-4-sonnet`和`claude-4-opus`模型\n\n>**2025.04.11：** [1.7.5版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.5) 新增支持 [wechatferry](https://github.com/zhayujie/chatgpt-on-wechat/pull/2562) 协议、新增 deepseek 模型、新增支持腾讯云语音能力、新增支持 ModelScope 和 Gitee-AI API接口\n\n>**2024.12.13：** [1.7.4版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.4) 新增 Gemini 2.0 模型、新增web channel、解决内存泄漏问题、解决 `#reloadp` 命令重载不生效问题\n\n>**2024.10.31：** [1.7.3版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.3) 程序稳定性提升、数据库功能、Claude模型优化、linkai插件优化、离线通知\n\n>**2024.09.26：** [1.7.2版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.2)  和 [1.7.1版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.1) 文心，讯飞等模型优化、o1 模型、快速安装和管理脚本\n\n>**2024.08.02：** [1.7.0版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.0) 新增 讯飞4.0 模型、知识库引用来源展示、相关插件优化\n\n>**2024.07.19：** [1.6.9版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.9) 新增 gpt-4o-mini 模型、阿里语音识别、企微应用渠道路由优化\n\n>**2024.07.05：** [1.6.8版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.8) 和 [1.6.7版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.7)，Claude3.5, Gemini 1.5 Pro, MiniMax模型、工作流图片输入、模型列表完善\n\n>**2024.06.04：** [1.6.6版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.6) 和 [1.6.5版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.5)，gpt-4o模型、钉钉流式卡片、讯飞语音识别/合成\n\n>**2024.04.26：** [1.6.0版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.0)，新增 Kimi 接入、gpt-4-turbo版本升级、文件总结和语音识别问题修复\n\n>**2024.03.26：** [1.5.8版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.8) 和 [1.5.7版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.7)，新增 GLM-4、Claude-3 模型，edge-tts 语音支持\n\n>**2024.01.26：** [1.5.6版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.6) 和 [1.5.5版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.5)，钉钉接入，tool插件升级，4-turbo模型更新\n\n>**2023.11.11：** [1.5.3版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.3) 和 [1.5.4版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.4)，新增通义千问模型、Google Gemini\n\n>**2023.11.10：** [1.5.2版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.2)，新增飞书通道、图像识别对话、黑名单配置\n\n>**2023.11.10：** [1.5.0版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.0)，新增 `gpt-4-turbo`, `dall-e-3`, `tts` 模型接入，完善图像理解&生成、语音识别&生成的多模态能力\n\n>**2023.10.16：** 支持通过意图识别使用LinkAI联网搜索、数学计算、网页访问等插件，参考[插件文档](https://docs.link-ai.tech/platform/plugins)\n\n>**2023.09.26：** 插件增加 文件/文章链接 一键总结和对话的功能，使用参考：[插件说明](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai#3%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93%E5%AF%B9%E8%AF%9D%E5%8A%9F%E8%83%BD)\n\n>**2023.08.08：** 接入百度文心一言模型，通过 [插件](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai) 支持 Midjourney 绘图\n\n>**2023.06.12：** 接入 [LinkAI](https://link-ai.tech/console) 平台，可在线创建领域知识库，打造专属客服机器人。使用参考 [接入文档](https://link-ai.tech/platform/link-app/wechat)。\n\n更早更新日志查看: [归档日志](/docs/version/old-version.md)\n\n<br>\n\n# 🚀 快速开始\n\n- 快速开始详细文档：[项目搭建文档](https://docs.link-ai.tech/cow/quick-start)\n\n- 快速安装脚本，详细使用指导：[一键安装启动脚本](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC)\n```bash\nbash <(curl -sS https://cdn.link-ai.tech/code/cow/install.sh)\n```\n- 项目管理脚本，详细使用指导：[项目管理脚本](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%84%9A%E6%9C%AC)\n## 一、准备\n\n### 1. 账号注册\n\n项目默认使用OpenAI接口，需前往 [OpenAI注册页面](https://beta.openai.com/signup) 创建账号，创建完账号则前往 [API管理页面](https://beta.openai.com/account/api-keys) 创建一个 API Key 并保存下来，后面需要在项目中配置这个key。接口需要海外网络访问及绑定信用卡支付。\n\n> 默认对话模型是 openai 的 gpt-3.5-turbo，计费方式是约每 1000tokens (约750个英文单词 或 500汉字，包含请求和回复) 消耗 $0.002，图片生成是Dell E模型，每张消耗 $0.016。\n\n项目同时也支持使用 LinkAI 接口，无需代理，可使用 Kimi、文心、讯飞、GPT-3.5、GPT-4o 等模型，支持 定制化知识库、联网搜索、MJ绘图、文档总结、工作流等能力。修改配置即可一键使用，参考 [接入文档](https://link-ai.tech/platform/link-app/wechat)。\n\n### 2.运行环境\n\n支持 Linux、MacOS、Windows 系统（可在Linux服务器上长期运行)，同时需安装 `Python`。\n> 建议Python版本在 3.7.1~3.9.X 之间，推荐3.8版本，3.10及以上版本在 MacOS 可用，其他系统上不确定能否正常运行。\n\n> 注意：Docker 或 Railway 部署无需安装python环境和下载源码，可直接快进到下一节。\n\n**(1) 克隆项目代码：**\n\n```bash\ngit clone https://github.com/zhayujie/chatgpt-on-wechat\ncd chatgpt-on-wechat/\n```\n\n注: 如遇到网络问题可选择国内镜像 https://gitee.com/zhayujie/chatgpt-on-wechat\n\n**(2) 安装核心依赖 (必选)：**\n> 能够使用`itchat`创建机器人，并具有文字交流功能所需的最小依赖集合。\n```bash\npip3 install -r requirements.txt\n```\n\n**(3) 拓展依赖 (可选，建议安装)：**\n\n```bash\npip3 install -r requirements-optional.txt\n```\n> 如果某项依赖安装失败可注释掉对应的行再继续\n\n## 二、配置\n\n配置文件的模板在根目录的`config-template.json`中，需复制该模板创建最终生效的 `config.json` 文件：\n\n```bash\n  cp config-template.json config.json\n```\n\n然后在`config.json`中填入配置，以下是对默认配置的说明，可根据需要进行自定义修改（注意实际使用时请去掉注释，保证JSON格式的完整）：\n\n```bash\n# config.json文件内容示例\n{\n  \"model\": \"gpt-4o-mini\",                                     # 模型名称, 支持 gpt-4o-mini, gpt-4.1, gpt-4o, wenxin, xunfei, glm-4, claude-3-7-sonnet-latest, moonshot等\n  \"open_ai_api_key\": \"YOUR API KEY\",                          # 如果使用openAI模型则填入上面创建的 OpenAI API KEY\n  \"open_ai_api_base\": \"https://api.openai.com/v1\",            # OpenAI接口代理地址\n  \"proxy\": \"\",                                                # 代理客户端的ip和端口，国内环境开启代理的需要填写该项，如 \"127.0.0.1:7890\"\n  \"single_chat_prefix\": [\"bot\", \"@bot\"],                      # 私聊时文本需要包含该前缀才能触发机器人回复\n  \"single_chat_reply_prefix\": \"[bot] \",                       # 私聊时自动回复的前缀，用于区分真人\n  \"group_chat_prefix\": [\"@bot\"],                              # 群聊时包含该前缀则会触发机器人回复\n  \"group_name_white_list\": [\"ChatGPT测试群\", \"ChatGPT测试群2\"], # 开启自动回复的群名称列表\n  \"group_chat_in_one_session\": [\"ChatGPT测试群\"],              # 支持会话上下文共享的群名称  \n  \"image_create_prefix\": [\"画\", \"看\", \"找\"],                   # 开启图片回复的前缀\n  \"conversation_max_tokens\": 1000,                            # 支持上下文记忆的最多字符数\n  \"speech_recognition\": false,                                # 是否开启语音识别\n  \"group_speech_recognition\": false,                          # 是否开启群组语音识别\n  \"voice_reply_voice\": false,                                 # 是否使用语音回复语音\n  \"character_desc\": \"你是基于大语言模型的AI智能助手，旨在回答并解决人们的任何问题，并且可以使用多种语言与人交流。\",  # 人格描述\n  # 订阅消息，公众号和企业微信channel中请填写，当被订阅时会自动回复，可使用特殊占位符。目前支持的占位符有{trigger_prefix}，在程序中它会自动替换成bot的触发词。\n  \"subscribe_msg\": \"感谢您的关注！\\n这里是ChatGPT，可以自由对话。\\n支持语音对话。\\n支持图片输出，画字开头的消息将按要求创作图片。\\n支持角色扮演和文字冒险等丰富插件。\\n输入{trigger_prefix}#help 查看详细指令。\",\n  \"use_linkai\": false,                                        # 是否使用LinkAI接口，默认关闭，开启后可国内访问，使用知识库和MJ\n  \"linkai_api_key\": \"\",                                       # LinkAI Api Key\n  \"linkai_app_code\": \"\"                                       # LinkAI 应用或工作流code\n}\n```\n**配置说明：**\n\n**1.个人聊天**\n\n+ 个人聊天中，需要以 \"bot\"或\"@bot\" 为开头的内容触发机器人，对应配置项 `single_chat_prefix` (如果不需要以前缀触发可以填写  `\"single_chat_prefix\": [\"\"]`)\n+ 机器人回复的内容会以 \"[bot] \" 作为前缀， 以区分真人，对应的配置项为 `single_chat_reply_prefix` (如果不需要前缀可以填写 `\"single_chat_reply_prefix\": \"\"`)\n\n**2.群组聊天**\n\n+ 群组聊天中，群名称需配置在 `group_name_white_list ` 中才能开启群聊自动回复。如果想对所有群聊生效，可以直接填写 `\"group_name_white_list\": [\"ALL_GROUP\"]`\n+ 默认只要被人 @ 就会触发机器人自动回复；另外群聊天中只要检测到以 \"@bot\" 开头的内容，同样会自动回复（方便自己触发），这对应配置项 `group_chat_prefix`\n+ 可选配置: `group_name_keyword_white_list`配置项支持模糊匹配群名称，`group_chat_keyword`配置项则支持模糊匹配群消息内容，用法与上述两个配置项相同。（Contributed by [evolay](https://github.com/evolay))\n+ `group_chat_in_one_session`：使群聊共享一个会话上下文，配置 `[\"ALL_GROUP\"]` 则作用于所有群聊\n\n**3.语音识别**\n\n+ 添加 `\"speech_recognition\": true` 将开启语音识别，默认使用openai的whisper模型识别为文字，同时以文字回复，该参数仅支持私聊 (注意由于语音消息无法匹配前缀，一旦开启将对所有语音自动回复，支持语音触发画图)；\n+ 添加 `\"group_speech_recognition\": true` 将开启群组语音识别，默认使用openai的whisper模型识别为文字，同时以文字回复，参数仅支持群聊 (会匹配group_chat_prefix和group_chat_keyword, 支持语音触发画图)；\n+ 添加 `\"voice_reply_voice\": true` 将开启语音回复语音（同时作用于私聊和群聊）\n\n**4.其他配置**\n\n+ `model`: 模型名称，目前支持 `gpt-4o-mini`, `gpt-4.1`, `gpt-4o`, `gpt-3.5-turbo`, `wenxin` , `claude` , `gemini`, `glm-4`,  `xunfei`, `moonshot`等，全部模型名称参考[common/const.py](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/common/const.py)文件\n+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat API接口参数，详情参考[OpenAI官方文档。](https://platform.openai.com/docs/api-reference/chat)\n+ `proxy`：由于目前 `openai` 接口国内无法访问，需配置代理客户端的地址，详情参考  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)\n+ 对于图像生成，在满足个人或群组触发条件外，还需要额外的关键词前缀来触发，对应配置 `image_create_prefix `\n+ 关于OpenAI对话及图片接口的参数配置（内容自由度、回复字数限制、图片大小等），可以参考 [对话接口](https://beta.openai.com/docs/api-reference/completions) 和 [图像接口](https://beta.openai.com/docs/api-reference/completions)  文档，在[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)中检查哪些参数在本项目中是可配置的。\n+ `conversation_max_tokens`：表示能够记忆的上下文最大字数（一问一答为一组对话，如果累积的对话字数超出限制，就会优先移除最早的一组对话）\n+ `rate_limit_chatgpt`，`rate_limit_dalle`：每分钟最高问答速率、画图速率，超速后排队按序处理。\n+ `clear_memory_commands`: 对话内指令，主动清空前文记忆，字符串数组可自定义指令别名。\n+ `hot_reload`: 程序退出后，暂存等于状态，默认关闭。\n+ `character_desc` 配置中保存着你对机器人说的一段话，他会记住这段话并作为他的设定，你可以为他定制任何人格      (关于会话上下文的更多内容参考该 [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))\n+ `subscribe_msg`：订阅消息，公众号和企业微信channel中请填写，当被订阅时会自动回复， 可使用特殊占位符。目前支持的占位符有{trigger_prefix}，在程序中它会自动替换成bot的触发词。\n\n**5.LinkAI配置 (可选)**\n\n+ `use_linkai`: 是否使用LinkAI接口，开启后可国内访问，使用知识库和 `Midjourney` 绘画, 参考 [文档](https://link-ai.tech/platform/link-app/wechat)\n+ `linkai_api_key`: LinkAI Api Key，可在 [控制台](https://link-ai.tech/console/interface) 创建\n+ `linkai_app_code`: LinkAI 应用或工作流的code，选填\n\n**本说明文档可能会未及时更新，当前所有可选的配置项均在该[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)中列出。**\n\n## 三、运行\n\n### 1.本地运行\n\n如果是开发机 **本地运行**，直接在项目根目录下执行：\n\n```bash\npython3 app.py                                    # windows环境下该命令通常为 python app.py\n```\n\n终端输出二维码后，进行扫码登录，当输出 \"Start auto replying\" 时表示自动回复程序已经成功运行了（注意：用于登录的账号需要在支付处已完成实名认证）。扫码登录后你的账号就成为机器人了，可以在手机端通过配置的关键词触发自动回复 (任意好友发送消息给你，或是自己发消息给好友)，参考[#142](https://github.com/zhayujie/chatgpt-on-wechat/issues/142)。\n\n### 2.服务器部署\n\n使用nohup命令在后台运行程序：\n\n```bash\nnohup python3 app.py & tail -f nohup.out          # 在后台运行程序并通过日志输出二维码\n```\n扫码登录后程序即可运行于服务器后台，此时可通过 `ctrl+c` 关闭日志，不会影响后台程序的运行。使用 `ps -ef | grep app.py | grep -v grep` 命令可查看运行于后台的进程，如果想要重新启动程序可以先 `kill` 掉对应的进程。日志关闭后如果想要再次打开只需输入 `tail -f nohup.out`。此外，`scripts` 目录下有一键运行、关闭程序的脚本供使用。\n\n> **多账号支持：** 将项目复制多份，分别启动程序，用不同账号扫码登录即可实现同时运行。\n\n> **特殊指令：** 用户向机器人发送 **#reset** 即可清空该用户的上下文记忆。\n\n\n### 3.Docker部署\n\n> 使用docker部署无需下载源码和安装依赖，只需要获取 docker-compose.yml 配置文件并启动容器即可。\n\n> 前提是需要安装好 `docker` 及 `docker-compose`，安装成功的表现是执行 `docker -v` 和 `docker-compose version` (或 docker compose version) 可以查看到版本号，可前往 [docker官网](https://docs.docker.com/engine/install/) 进行下载。\n\n**(1) 下载 docker-compose.yml 文件**\n\n```bash\nwget https://open-1317903499.cos.ap-guangzhou.myqcloud.com/docker-compose.yml\n```\n\n下载完成后打开 `docker-compose.yml` 修改所需配置，如 `OPEN_AI_API_KEY` 和 `GROUP_NAME_WHITE_LIST` 等。\n\n**(2) 启动容器**\n\n在 `docker-compose.yml` 所在目录下执行以下命令启动容器：\n\n```bash\nsudo docker compose up -d\n```\n\n运行 `sudo docker ps` 能查看到 NAMES 为 chatgpt-on-wechat 的容器即表示运行成功。\n\n注意：\n\n - 如果 `docker-compose` 是 1.X 版本 则需要执行 `sudo  docker-compose up -d` 来启动容器\n - 该命令会自动去 [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) 拉取 latest 版本的镜像，latest 镜像会在每次项目 release 新的版本时生成\n\n最后运行以下命令可查看容器运行日志，扫描日志中的二维码即可完成登录：\n\n```bash\nsudo docker logs -f chatgpt-on-wechat\n```\n\n**(3) 插件使用**\n\n如果需要在docker容器中修改插件配置，可通过挂载的方式完成，将 [插件配置文件](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)\n重命名为 `config.json`，放置于 `docker-compose.yml` 相同目录下，并在 `docker-compose.yml` 中的 `chatgpt-on-wechat` 部分下添加 `volumes` 映射:\n\n```\nvolumes:\n  - ./config.json:/app/plugins/config.json\n```\n**注**：采用docker方式部署的详细教程可以参考：[docker部署CoW项目](https://www.wangpc.cc/ai/docker-deploy-cow/)\n### 4. Railway部署\n\n> Railway 每月提供5刀和最多500小时的免费额度。 (07.11更新: 目前大部分账号已无法免费部署)\n\n1. 进入 [Railway](https://railway.app/template/qApznZ?referralCode=RC3znh)\n2. 点击 `Deploy Now` 按钮。\n3. 设置环境变量来重载程序运行的参数，例如`open_ai_api_key`, `character_desc`。\n\n**一键部署:**\n  \n  [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/qApznZ?referralCode=RC3znh)\n\n<br>\n\n# 🔎 常见问题\n\nFAQs： <https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs>\n\n或直接在线咨询 [项目小助手](https://link-ai.tech/app/Kv2fXJcH)  (语料持续完善中，回复仅供参考)\n\n# 🛠️ 开发\n\n欢迎接入更多应用，参考 [Terminal代码](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) 实现接收和发送消息逻辑即可接入。 同时欢迎增加新的插件，参考 [插件说明文档](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)。\n\n# ✉ 联系\n\n欢迎提交PR、Issues，以及Star支持一下。程序运行遇到问题可以查看 [常见问题列表](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ，其次前往 [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) 中搜索。个人开发者可加入开源交流群参与更多讨论，企业用户可联系[产品顾问](https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg)咨询。\n\n# 🌟 贡献者\n\n![cow contributors](https://contrib.rocks/image?repo=zhayujie/chatgpt-on-wechat&max=1000)",
    "summary": "chatgpt-on-wechat（CoW）是一个基于大模型的智能对话机器人项目，支持微信公众号、企业微信、飞书、钉钉等多平台接入。项目集成了GPT、Claude、Gemini、文心一言等多种主流大模型，具备处理文本、语音、图片的多模态能力。通过丰富的插件系统，可扩展联网搜索、文档总结、角色扮演等功能，并支持基于LinkAI平台构建自有知识库，适用于智能客服、私域运营等企业级AI应用场景。",
    "keywords": [
      "大模型",
      "智能对话机器人",
      "微信",
      "企业微信",
      "飞书",
      "钉钉",
      "语音识别",
      "图像生成",
      "插件",
      "知识库"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2025-05-25T09:44:28Z",
    "download_time": "2024-05-25 10:00:00",
    "visual_resource": [
      "https://cdn.link-ai.tech/image/link-ai-intro.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "angular",
    "source": "GitHub",
    "url": "https://github.com/angular/angular",
    "title": "Angular - The modern web developer's platform",
    "content": "<h1 align=\"center\">Angular - The modern web developer's platform</h1>\n\n<p align=\"center\">\n  <img src=\"adev/src/assets/images/press-kit/angular_icon_gradient.gif\" alt=\"angular-logo\" width=\"120px\" height=\"120px\"/>\n  <br>\n  <em>Angular is a development platform for building mobile and desktop web applications\n    <br> using TypeScript/JavaScript and other languages.</em>\n  <br>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://angular.dev/\"><strong>angular.dev</strong></a>\n  <br>\n</p>\n\n<p align=\"center\">\n  <a href=\"CONTRIBUTING.md\">Contributing Guidelines</a>\n  ·\n  <a href=\"https://github.com/angular/angular/issues\">Submit an Issue</a>\n  ·\n  <a href=\"https://blog.angular.dev/\">Blog</a>\n  <br>\n  <br>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/@angular/core\">\n    <img src=\"https://img.shields.io/npm/v/@angular/core.svg?logo=npm&logoColor=fff&label=NPM+package&color=limegreen\" alt=\"Angular on npm\" />\n  </a>\n</p>\n\n<hr>\n\n## Documentation\n\nGet started with Angular, learn the fundamentals and explore advanced topics on our documentation website.\n\n- [Getting Started][quickstart]\n- [Architecture][architecture]\n- [Components and Templates][componentstemplates]\n- [Forms][forms]\n- [API][api]\n\n### Advanced\n\n- [Angular Elements][angularelements]\n- [Server Side Rendering][ssr]\n- [Schematics][schematics]\n- [Lazy Loading][lazyloading]\n- [Animations][animations]\n\n### Local Development\n\nTo contribute to the Angular Docs, check out the [Angular.dev README](adev/README.md)\n\n## Development Setup\n\n### Prerequisites\n\n- Install [Node.js] which includes [Node Package Manager][npm]\n\n### Setting Up a Project\n\nInstall the Angular CLI globally:\n\n```\nnpm install -g @angular/cli\n```\n\nCreate workspace:\n\n```\nng new [PROJECT NAME]\n```\n\nRun the application:\n\n```\ncd [PROJECT NAME]\nng serve\n```\n\nAngular is cross-platform, fast, scalable, has incredible tooling, and is loved by millions.\n\n## Quickstart\n\n[Get started in 5 minutes][quickstart].\n\n## Ecosystem\n\n<p>\n  <img src=\"/contributing-docs/images/angular-ecosystem-logos.png\" alt=\"angular ecosystem logos\" width=\"500px\" height=\"auto\">\n</p>\n\n- [Angular Command Line (CLI)][cli]\n- [Angular Material][angularmaterial]\n\n## Changelog\n\n[Learn about the latest improvements][changelog].\n\n## Upgrading\n\nCheck out our [upgrade guide](https://angular.dev/update-guide/) to find out the best way to upgrade your project.\n\n## Contributing\n\n### Contributing Guidelines\n\nRead through our [contributing guidelines][contributing] to learn about our submission process, coding rules, and more.\n\n### Want to Help?\n\nWant to report a bug, contribute some code, or improve the documentation? Excellent! Read up on our guidelines for [contributing][contributing] and then check out one of our issues labeled as <kbd>[help wanted](https://github.com/angular/angular/labels/help%20wanted)</kbd> or <kbd>[good first issue](https://github.com/angular/angular/labels/good%20first%20issue)</kbd>.\n\n### Code of Conduct\n\nHelp us keep Angular open and inclusive. Please read and follow our [Code of Conduct][codeofconduct].\n\n## Community\n\nJoin the conversation and help the community.\n\n- [X (formerly Twitter)][X (formerly Twitter)]\n- [Bluesky][bluesky]\n- [Discord][discord]\n- [YouTube][youtube]\n- [StackOverflow][stackoverflow]\n- Find a Local [Meetup][meetup]\n\n[![Love Angular badge](https://img.shields.io/badge/angular-love-blue?logo=angular&angular=love)](https://www.github.com/angular/angular)\n\n**Love Angular? Give our repo a star :star: :arrow_up:.**\n\n[contributing]: CONTRIBUTING.md\n[quickstart]: https://angular.dev/tutorials/learn-angular\n[changelog]: CHANGELOG.md\n[ng]: https://angular.dev\n[documentation]: https://angular.dev/overview\n[angularmaterial]: https://material.angular.dev/\n[cli]: https://angular.dev/tools/cli\n[architecture]: https://angular.dev/essentials\n[componentstemplates]: https://angular.dev/tutorials/learn-angular/1-components-in-angular\n[forms]: https://angular.dev/tutorials/learn-angular/15-forms\n[api]: https://angular.dev/api\n[angularelements]: https://angular.dev/guide/elements\n[ssr]: https://angular.dev/guide/ssr\n[schematics]: https://angular.dev/tools/cli/schematics\n[lazyloading]: https://angular.dev/guide/ngmodules/lazy-loading\n[node.js]: https://nodejs.org/\n[npm]: https://www.npmjs.com/get-npm\n[codeofconduct]: CODE_OF_CONDUCT.md\n[X (formerly Twitter)]: https://www.twitter.com/angular\n[bluesky]: https://bsky.app/profile/angular.dev\n[discord]: https://discord.gg/angular\n[stackoverflow]: https://stackoverflow.com/questions/tagged/angular\n[youtube]: https://youtube.com/angular\n[meetup]: https://www.meetup.com/find/?keywords=angular\n[animations]: https://angular.dev/guide/animations]",
    "summary": "Angular是一个现代化的Web开发平台，用于构建高性能的移动和桌面Web应用。它基于TypeScript/JavaScript等语言，提供跨平台、快速、可扩展的开发能力和强大的工具链，包括CLI、SSR、Schematics等，是构建大型复杂Web应用的流行前端框架。",
    "keywords": [
      "Angular",
      "前端框架",
      "Web开发",
      "TypeScript",
      "JavaScript",
      "CLI",
      "单页面应用",
      "跨平台"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T00:44:45Z",
    "download_time": "2025-05-29 01:00:00",
    "visual_resource": [
      "https://github.com/angular/angular/raw/main/adev/src/assets/images/press-kit/angular_icon_gradient.gif",
      "https://github.com/angular/angular/raw/main/contributing-docs/images/angular-ecosystem-logos.png"
    ],
    "extra_info": null
  },
  {
    "id": "LivePortrait",
    "source": "GitHub",
    "url": "https://github.com/KwaiVGI/LivePortrait",
    "title": "LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control",
    "content": "LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control Jianzhu Guo 1*† Dingyun Zhang 1,2* Xiaoqiang Liu 1 Zhizhou Zhong 1,3 Yuan Zhang 1 Pengfei Wan 1 Di Zhang 1 1 Kuaishou Technology 2 University of Science and Technology of China 3 Fudan University * Equal contributions † Corresponding author arXiv LivePortrait Project LivePortrait Hugging Face Spaces English | 简体中文 🔥 For more results, visit our homepage 🔥 🔥 Updates 2025/01/01: 🐶 We updated a new version of the Animals model with more data, see here. 2024/10/18: ❗ We have updated the versions of the transformers and gradio libraries to avoid security vulnerabilities. Details here. 2024/08/29: 📦 We update the Windows one-click installer and support auto-updates, see changelog. 2024/08/19: 🖼️ We support image driven mode and regional control. For details, see here. 2024/08/06: 🎨 We support precise portrait editing in the Gradio interface, inspired by ComfyUI-AdvancedLivePortrait. See here. 2024/08/05: 📦 Windows users can now download the one-click installer for Humans mode and Animals mode now! For details, see here. 2024/08/02: 😸 We released a version of the Animals model, along with several other updates and improvements. Check out the details here! 2024/07/25: 📦 Windows users can now download the package from HuggingFace. Simply unzip and double-click run_windows.bat to enjoy! 2024/07/24: 🎨 We support pose editing for source portraits in the Gradio interface. We’ve also lowered the default detection threshold to increase recall. Have fun! 2024/07/19: ✨ We support 🎞️ portrait video editing (aka v2v)! More to see here. 2024/07/17: 🍎 We support macOS with Apple Silicon, modified from jeethu's PR #143. 2024/07/10: 💪 We support audio and video concatenating, driving video auto-cropping, and template making to protect privacy. More to see here. 2024/07/09: 🤗 We released the HuggingFace Space, thanks to the HF team and Gradio! 2024/07/04: 😊 We released the initial version of the inference code and models. Continuous updates, stay tuned! 2024/07/04: 🔥 We released the homepage and technical report on arXiv. Introduction 📖 This repo, named LivePortrait, contains the official PyTorch implementation of our paper LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control. We are actively updating and improving this repository. If you find any bugs or have suggestions, welcome to raise issues or submit pull requests (PR) 💖. Getting Started 🏁 1. Clone the code and prepare the environment 🛠️ Note Make sure your system has git, conda, and FFmpeg installed. For details on FFmpeg installation, see how to install FFmpeg. git clone https://github.com/KwaiVGI/LivePortrait cd LivePortrait # create env using conda conda create -n LivePortrait python=3.10 conda activate LivePortrait For Linux or Windows Users X-Pose requires your torch version to be compatible with the CUDA version. Firstly, check your current CUDA version by: nvcc -V # example versions: 11.1, 11.8, 12.1, etc. Then, install the corresponding torch version. Here are examples for different CUDA versions. Visit the PyTorch Official Website for installation commands if your CUDA version is not listed: # for CUDA 11.1 pip install torch==1.10.1+cu111 torchvision==0.11.2 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html # for CUDA 11.8 pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118 # for CUDA 12.1 pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121 # ... Note: On Windows systems, some higher versions of CUDA (such as 12.4, 12.6, etc.) may lead to unknown issues. You may consider downgrading CUDA to version 11.8 for stability. See the downgrade guide by @dimitribarbot. Finally, install the remaining dependencies: pip install -r requirements.txt For macOS with Apple Silicon Users The X-Pose dependency does not support macOS, so you can skip its installation. While Humans mode works as usual, Animals mode is not supported. Use the provided requirements file for macOS with Apple Silicon: # for macOS with Apple Silicon users pip install -r requirements_macOS.txt 2. Download pretrained weights 📥 The easiest way to download the pretrained weights is from HuggingFace: # !pip install -U \"huggingface_hub[cli]\" huggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude \"*.git*\" \"README.md\" \"docs\" If you cannot access to Huggingface, you can use hf-mirror to download: # !pip install -U \"huggingface_hub[cli]\" export HF_ENDPOINT=https://hf-mirror.com huggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude \"*.git*\" \"README.md\" \"docs\" Alternatively, you can download all pretrained weights from Google Drive or Baidu Yun. Unzip and place them in ./pretrained_weights. Ensuring the directory structure is as or contains this. 3. Inference 🚀 Fast hands-on (humans) 👤 # For Linux and Windows users python inference.py # For macOS users with Apple Silicon (Intel is not tested). NOTE: this maybe 20x slower than RTX 4090 PYTORCH_ENABLE_MPS_FALLBACK=1 python inference.py If the script runs successfully, you will get an output mp4 file named animations/s6--d0_concat.mp4. This file includes the following results: driving video, input image or video, and generated result. Or, you can change the input by specifying the -s and -d arguments: # source input is an image python inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d0.mp4 # source input is a video ✨ python inference.py -s assets/examples/source/s13.mp4 -d assets/examples/driving/d0.mp4 # more options to see python inference.py -h Fast hands-on (animals) 🐱🐶 Animals mode is ONLY tested on Linux and Windows with NVIDIA GPU. You need to build an OP named MultiScaleDeformableAttention first, which is used by X-Pose, a general keypoint detection framework. cd src/utils/dependencies/XPose/models/UniPose/ops python setup.py build install cd - # equal to cd ../../../../../../../ Then python inference_animals.py -s assets/examples/source/s39.jpg -d assets/examples/driving/wink.pkl --driving_multiplier 1.75 --no_flag_stitching If the script runs successfully, you will get an output mp4 file named animations/s39--wink_concat.mp4. Driving video auto-cropping 📢📢📢 IMPORTANT To use your own driving video, we recommend: ⬇️ - Crop it to a 1:1 aspect ratio (e.g., 512x512 or 256x256 pixels), or enable auto-cropping by --flag_crop_driving_video. - Focus on the head area, similar to the example videos. - Minimize shoulder movement. - Make sure the first frame of driving video is a frontal face with neutral expression. Below is an auto-cropping case by --flag_crop_driving_video: python inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d13.mp4 --flag_crop_driving_video If you find the results of auto-cropping is not well, you can modify the --scale_crop_driving_video, --vy_ratio_crop_driving_video options to adjust the scale and offset, or do it manually. Motion template making You can also use the auto-generated motion template files ending with .pkl to speed up inference, and protect privacy, such as: python inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d5.pkl # portrait animation python inference.py -s assets/examples/source/s13.mp4 -d assets/examples/driving/d5.pkl # portrait video editing 3. Gradio interface 🤗 We also provide a Gradio interface for a better experience, just run by: # For Linux and Windows users (and macOS with Intel??) python app.py # humans mode # For macOS with Apple Silicon users, Intel not supported, this maybe 20x slower than RTX 4090 PYTORCH_ENABLE_MPS_FALLBACK=1 python app.py # humans mode We also provide a Gradio interface of animals mode, which is only tested on Linux with NVIDIA GPU: python app_animals.py # animals mode 🐱🐶 You can specify the --server_port, --share, --server_name arguments to satisfy your needs! 🚀 We also provide an acceleration option --flag_do_torch_compile. The first-time inference triggers an optimization process (about one minute), making subsequent inferences 20-30% faster. Performance gains may vary with different CUDA versions. # enable torch.compile for faster inference python app.py --flag_do_torch_compile Note: This method is not supported on Windows and macOS. Or, try it out effortlessly on HuggingFace 🤗 5. Inference speed evaluation 🚀🚀🚀 We have also provided a script to evaluate the inference speed of each module: # For NVIDIA GPU python speed.py The results are here. Community Resources 🤗 Discover the invaluable resources contributed by our community to enhance your LivePortrait experience. Community-developed Projects Repo | Description | Author / Links ------|------|--------| ditto-talkinghead | Real-time audio-driven talking head. | ArXiv, Homepage FasterLivePortrait | Faster real-time version using TensorRT. | @warmshao AdvancedLivePortrait-WebUI | Dedicated gradio based WebUI started from ComfyUI-AdvancedLivePortrait. | @jhj0517 FacePoke | A real-time head transformation app, controlled by your mouse! | @jbilcke-hf FaceFusion | FaceFusion 3.0 integregates LivePortrait as expression_restorer and face_editor processors. | @henryruhs sd-webui-live-portrait | WebUI extension of LivePortrait, adding atab to the original Stable Diffusion WebUI to benefit from LivePortrait features. | @dimitribarbot ComfyUI-LivePortraitKJ | A ComfyUI node to use LivePortrait, with MediaPipe as as an alternative to Insightface. | @kijai ComfyUI-AdvancedLivePortrait | A faster ComfyUI node with real-time preview that has inspired many other community-developed tools and projects. | @PowerHouseMan comfyui-liveportrait | A ComfyUI node to use LivePortrait, supporting multi-faces, expression interpolation etc, with a tutorial. Playgrounds, 🤗 HuggingFace Spaces and Others FacePoke Space Expression Editor Space Expression Editor Replicate Face Control Realtime Demo on FAL Replicate Playground Nuke can use LivePortrait through CompyUI node, details here LivePortrait lives on Poe Video Tutorials Workflow of LivePortrait Video to Video by @curiousrefuge Google Colab tutorial by @Planet Ai Paper reading by @TwoMinutePapers ComfyUI Advanced LivePortrait by TutoView LivePortarit exploration and A deep dive into LivePortrait by TheoreticallyMedia LivePortrait hands-on tutorial by @AI Search ComfyUI tutorial by @Sebastian Kamph A tutorial on BiliBili And so MANY amazing contributions from our community, too many to list them all 💖 Acknowledgements 💐 We would like to thank the contributors of FOMM, Open Facevid2vid, SPADE, InsightFace and X-Pose repositories, for their open research and contributions. Ethics Considerations 🛡️ Portrait animation technologies come with social risks, particularly the potential for misuse in creating deepfakes. To mitigate these risks, it’s crucial to follow ethical guidelines and adopt responsible usage practices. At present, the synthesized results contain visual artifacts that may help in detecting deepfakes. Please note that we do not assume any legal responsibility for the use of the results generated by this project. Citation 💖 If you find LivePortrait useful for your research, welcome to 🌟 this repo and cite our work using the following BibTeX: @article{guo2024liveportrait, title = {LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control}, author = {Guo, Jianzhu and Zhang, Dingyun and Liu, Xiaoqiang and Zhong, Zhizhou and Zhang, Yuan and Wan, Pengfei and Zhang, Di}, journal = {arXiv preprint arXiv:2407.03168}, year = {2024} } Long live in arXiv. Contact 📧 Jianzhu Guo (郭建珠); guojianzhu1994@gmail.com",
    "summary": "LivePortrait是一个高效的肖像动画生成项目，基于PyTorch实现，支持图像或视频驱动，具备拼接和重定向控制能力。其核心技术特点包括区域控制、精确编辑、动物模式支持、视频编辑（v2v）以及驱动视频自动裁剪等。项目提供了Windows一键安装包和macOS支持，并可通过Gradio界面或HuggingFace Space便捷体验。该技术在肖像动画、视频编辑及实时人脸控制等领域具有广泛应用潜力。",
    "keywords": [
      "肖像动画",
      "人脸动画",
      "视频生成",
      "计算机视觉",
      "深度学习",
      "PyTorch",
      "Gradio"
    ],
    "area": [
      "人工智能",
      "计算机视觉",
      "生成式AI"
    ],
    "published_time": "2025-02-28T13:56:34Z",
    "download_time": "2024-07-26 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/showcase2.gif",
      "https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/inference.gif",
      "https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/inference-animals.gif"
    ],
    "extra_info": null
  },
  {
    "id": "xiaozhi-esp32",
    "source": "GitHub",
    "url": "https://github.com/78/xiaozhi-esp32",
    "title": "小智 AI 聊天机器人 （XiaoZhi AI Chatbot）",
    "content": "# 小智 AI 聊天机器人 （XiaoZhi AI Chatbot）\n\n（中文 | [English](README_en.md) | [日本語](README_ja.md)）\n\n## 视频介绍\n\n👉 [ESP32+SenseVoice+Qwen72B 打造你的 AI 聊天伴侣！【bilibili】](https://www.bilibili.com/video/BV11msTenEH3/)\n\n👉 [给小智装上 DeepSeek 的聪明大脑【bilibili】](https://www.bilibili.com/video/BV1GQP6eNEFG/)\n\n👉 [手工打造你的 AI 女友，新手入门教程【bilibili】](https://www.bilibili.com/video/BV1XnmFYLEJN/)\n\n## 项目目的\n\n本项目是由虾哥开源的一个开源项目，以 MIT 许可证发布，允许任何人免费使用，并可以用于商业用途。\n\n我们希望通过这个项目，能够帮助更多人入门 AI 硬件开发，了解如何将当下飞速发展的大语言模型应用到实际的硬件设备中。无论你是对 AI 感兴趣的学生，还是想要探索新技术的开发者，都可以通过这个项目获得宝贵的学习经验。\n\n欢迎所有人参与到项目的开发和改进中来。如果你有任何想法或建议，请随时提出 Issue 或加入群聊。\n\n学习交流 QQ 群：376893254\n\n## 已实现功能\n\n- Wi-Fi / ML307 Cat.1 4G\n- BOOT 键唤醒和打断，支持点击和长按两种触发方式\n- 离线语音唤醒 [ESP-SR](https://github.com/espressif/esp-sr)\n- 流式语音对话（WebSocket 或 UDP 协议）\n- 支持国语、粤语、英语、日语、韩语 5 种语言识别 [SenseVoice](https://github.com/FunAudioLLM/SenseVoice)\n- 声纹识别，识别是谁在喊 AI 的名字 [3D Speaker](https://github.com/modelscope/3D-Speaker)\n- 大模型 TTS（火山引擎 或 CosyVoice）\n- 大模型 LLM（Qwen, DeepSeek, Doubao）\n- 可配置的提示词和音色（自定义角色）\n- 短期记忆，每轮对话后自我总结\n- OLED / LCD 显示屏，显示信号强弱或对话内容\n- 支持 LCD 显示图片表情\n- 支持多语言（中文、英文）\n\n## ✅ 已支持的芯片平台\n\n- ✅ ESP32-S3\n- ✅ ESP32-C3\n- ✅ ESP32-P4\n\n## 硬件部分\n\n### 面包板手工制作实践\n\n详见飞书文档教程：\n\n👉 [《小智 AI 聊天机器人百科全书》](https://ccnphfhqs21z.feishu.cn/wiki/F5krwD16viZoF0kKkvDcrZNYnhb?from=from_copylink)\n\n面包板效果图如下：\n\n![面包板效果图](docs/wiring2.jpg)\n\n### 已支持的开源硬件\n\n- <a href=\"https://oshwhub.com/li-chuang-kai-fa-ban/li-chuang-shi-zhan-pai-esp32-s3-kai-fa-ban\" target=\"_blank\" title=\"立创·实战派 ESP32-S3 开发板\">立创·实战派 ESP32-S3 开发板</a>\n- <a href=\"https://github.com/espressif/esp-box\" target=\"_blank\" title=\"乐鑫 ESP32-S3-BOX3\">乐鑫 ESP32-S3-BOX3</a>\n- <a href=\"https://docs.m5stack.com/zh_CN/core/CoreS3\" target=\"_blank\" title=\"M5Stack CoreS3\">M5Stack CoreS3</a>\n- <a href=\"https://docs.m5stack.com/en/atom/Atomic%20Echo%20Base\" target=\"_blank\" title=\"AtomS3R + Echo Base\">AtomS3R + Echo Base</a>\n- <a href=\"https://docs.m5stack.com/en/core/ATOM%20Matrix\" target=\"_blank\" title=\"AtomMatrix + Echo Base\">AtomMatrix + Echo Base</a>\n- <a href=\"https://gf.bilibili.com/item/detail/1108782064\" target=\"_blank\" title=\"神奇按钮 2.4\">神奇按钮 2.4</a>\n- <a href=\"https://www.waveshare.net/shop/ESP32-S3-Touch-AMOLED-1.8.htm\" target=\"_blank\" title=\"微雪电子 ESP32-S3-Touch-AMOLED-1.8\">微雪电子 ESP32-S3-Touch-AMOLED-1.8</a>\n- <a href=\"https://github.com/Xinyuan-LilyGO/T-Circle-S3\" target=\"_blank\" title=\"LILYGO T-Circle-S3\">LILYGO T-Circle-S3</a>\n- <a href=\"https://oshwhub.com/tenclass01/xmini_c3\" target=\"_blank\" title=\"虾哥 Mini C3\">虾哥 Mini C3</a>\n- <a href=\"https://oshwhub.com/movecall/moji-xiaozhi-ai-derivative-editi\" target=\"_blank\" title=\"Movecall Moji ESP32S3\">Moji 小智 AI 衍生版</a>\n- <a href=\"https://oshwhub.com/movecall/cuican-ai-pendant-lights-up-y\" target=\"_blank\" title=\"Movecall CuiCan ESP32S3\">璀璨·AI 吊坠</a>\n- <a href=\"https://github.com/WMnologo/xingzhi-ai\" target=\"_blank\" title=\"无名科技Nologo-星智-1.54\">无名科技 Nologo-星智-1.54TFT</a>\n- <a href=\"https://www.seeedstudio.com/SenseCAP-Watcher-W1-A-p-5979.html\" target=\"_blank\" title=\"SenseCAP Watcher\">SenseCAP Watcher</a>\n<div style=\"display: flex; justify-content: space-between;\">\n  <a href=\"docs/v1/lichuang-s3.jpg\" target=\"_blank\" title=\"立创·实战派 ESP32-S3 开发板\">\n    <img src=\"docs/v1/lichuang-s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/espbox3.jpg\" target=\"_blank\" title=\"乐鑫 ESP32-S3-BOX3\">\n    <img src=\"docs/v1/espbox3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/m5cores3.jpg\" target=\"_blank\" title=\"M5Stack CoreS3\">\n    <img src=\"docs/v1/m5cores3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/atoms3r.jpg\" target=\"_blank\" title=\"AtomS3R + Echo Base\">\n    <img src=\"docs/v1/atoms3r.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/magiclick.jpg\" target=\"_blank\" title=\"神奇按钮 2.4\">\n    <img src=\"docs/v1/magiclick.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/waveshare.jpg\" target=\"_blank\" title=\"微雪电子 ESP32-S3-Touch-AMOLED-1.8\">\n    <img src=\"docs/v1/waveshare.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/lilygo-t-circle-s3.jpg\" target=\"_blank\" title=\"LILYGO T-Circle-S3\">\n    <img src=\"docs/lilygo-t-circle-s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/xmini-c3.jpg\" target=\"_blank\" title=\"虾哥 Mini C3\">\n    <img src=\"docs/xmini-c3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/movecall-moji-esp32s3.jpg\" target=\"_blank\" title=\"Movecall Moji 小智AI衍生版\">\n    <img src=\"docs/v1/movecall-moji-esp32s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/movecall-cuican-esp32s3.jpg\" target=\"_blank\" title=\"CuiCan\">\n    <img src=\"docs/v1/movecall-cuican-esp32s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/wmnologo_xingzhi_1.54.jpg\" target=\"_blank\" title=\"无名科技Nologo-星智-1.54\">\n    <img src=\"docs/v1/wmnologo_xingzhi_1.54.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/sensecap_watcher.jpg\" target=\"_blank\" title=\"SenseCAP Watcher\">\n    <img src=\"docs/v1/sensecap_watcher.jpg\" width=\"240\" />\n  </a>\n</div>\n\n## 固件部分\n\n### 免开发环境烧录\n\n新手第一次操作建议先不要搭建开发环境，直接使用免开发环境烧录的固件。\n\n固件默认接入 [xiaozhi.me](https://xiaozhi.me) 官方服务器，目前个人用户注册账号可以免费使用 Qwen 实时模型。\n\n👉 [Flash 烧录固件（无 IDF 开发环境）](https://ccnphfhqs21z.feishu.cn/wiki/Zpz4wXBtdimBrLk25WdcXzxcnNS)\n\n### 开发环境\n\n- Cursor 或 VSCode\n- 安装 ESP-IDF 插件，选择 SDK 版本 5.3 或以上\n- Linux 比 Windows 更好，编译速度快，也免去驱动问题的困扰\n- 使用 Google C++ 代码风格，提交代码时请确保符合规范\n\n### 开发者文档\n\n- [开发板定制指南](main/boards/README.md) - 学习如何为小智创建自定义开发板适配\n- [物联网控制模块](main/iot/README.md) - 了解如何通过 AI 语音控制物联网设备\n\n## 智能体配置\n\n如果你已经拥有一个小智 AI 聊天机器人设备，可以登录 [xiaozhi.me](https://xiaozhi.me) 控制台进行配置。\n\n👉 [后台操作视频教程（旧版界面）](https://www.bilibili.com/video/BV1jUCUY2EKM/)\n\n## 技术原理与私有化部署\n\n👉 [一份详细的 WebSocket 通信协议文档](docs/websocket.md)\n👉 [MCP 协议 通信协议](docs/mcp_protocol.md)\n\n在个人电脑上部署服务器，可以参考另一位作者同样以 MIT 许可证开源的项目 [xiaozhi-esp32-server](https://github.com/xinnan-tech/xiaozhi-esp32-server)\n\n## Star History\n\n<a href=\"https://star-history.com/#78/xiaozhi-esp32&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=78/xiaozhi-esp32&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=78/xiaozhi-esp32&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=78/xiaozhi-esp32&type=Date\" />\n </picture>\n</a>",
    "summary": "小智AI聊天机器人是一个开源项目，旨在帮助用户入门AI硬件开发，将大语言模型应用于ESP32等硬件设备。项目支持Wi-Fi/4G连接、离线唤醒、流式语音对话、多语言识别（SenseVoice）、声纹识别、多种大模型（Qwen, DeepSeek, Doubao）集成及TTS功能。它兼容多种ESP32芯片平台和开源硬件，提供软硬件教程，是探索AI与嵌入式结合的良好实践平台。",
    "keywords": [
      "AI聊天机器人",
      "ESP32",
      "大语言模型",
      "语音识别",
      "硬件开发",
      "物联网",
      "SenseVoice",
      "Qwen"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器人"
    ],
    "published_time": "2025-05-29T12:12:21+00:00",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://github.com/78/xiaozhi-esp32/raw/main/docs/wiring2.jpg",
      "https://github.com/78/xiaozhi-esp32/raw/main/docs/v1/espbox3.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "ant-design",
    "source": "GitHub",
    "url": "https://github.com/ant-design/ant-design",
    "title": "Ant Design",
    "content": "<div align=\"center\"><a name=\"readme-top\"></a>\n\n<img height=\"180\" src=\"https://gw.alipayobjects.com/zos/rmsportal/KDpgvguMpGfqaHPjicRK.svg\">\n\n<h1>Ant Design</h1>\n\nAn enterprise-class UI design language and React UI library.\n\n[![CI status][github-action-image]][github-action-url] [![codecov][codecov-image]][codecov-url] [![NPM version][npm-image]][npm-url] [![NPM downloads][download-image]][download-url]\n\n[![][bundlephobia-image]][bundlephobia-url] [![][jsdelivr-image]][jsdelivr-url] [![FOSSA Status][fossa-image]][fossa-url] [![DeepWiki][deepwiki-image]][deepwiki-url]\n\n[![Follow Twitter][twitter-image]][twitter-url] [![Renovate status][renovate-image]][renovate-dashboard-url] [![][issues-helper-image]][issues-helper-url] [![dumi][dumi-image]][dumi-url] [![Issues need help][help-wanted-image]][help-wanted-url]\n\n[Changelog](./CHANGELOG.en-US.md) · [Report Bug][github-issues-url] · [Request Feature][github-issues-url] · English · [中文](./README-zh_CN.md)\n\n## ❤️ Sponsors and Backers [![](https://opencollective.com/ant-design/tiers/sponsors/badge.svg?label=Sponsors&color=brightgreen)](https://opencollective.com/ant-design#support) [![](https://opencollective.com/ant-design/tiers/backers/badge.svg?label=Backers&color=brightgreen)](https://opencollective.com/ant-design#support)\n\n[![](https://opencollective.com/ant-design/tiers/sponsors.svg?avatarHeight=72)](https://opencollective.com/ant-design/contribute/sponsors-218/checkout) [![](https://opencollective.com/ant-design/tiers/backers.svg?avatarHeight=72)](https://opencollective.com/ant-design/contribute/backers-217/checkout)\n\n[npm-image]: https://img.shields.io/npm/v/antd.svg?style=flat-square\n[npm-url]: https://npmjs.org/package/antd\n[github-action-image]: https://github.com/ant-design/ant-design/actions/workflows/test.yml/badge.svg\n[github-action-url]: https://github.com/ant-design/ant-design/actions/workflows/test.yml\n[codecov-image]: https://img.shields.io/codecov/c/github/ant-design/ant-design/master.svg?style=flat-square\n[codecov-url]: https://codecov.io/gh/codecov/codecov-action\n[download-image]: https://img.shields.io/npm/dm/antd.svg?style=flat-square\n[download-url]: https://npmjs.org/package/antd\n[fossa-image]: https://app.fossa.io/api/projects/git%2Bgithub.com%2Fant-design%2Fant-design.svg?type=shield\n[fossa-url]: https://app.fossa.io/projects/git%2Bgithub.com%2Fant-design%2Fant-design?ref=badge_shield\n[help-wanted-image]: https://flat.badgen.net/github/label-issues/ant-design/ant-design/help%20wanted/open\n[help-wanted-url]: https://github.com/ant-design/ant-design/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22\n[twitter-image]: https://img.shields.io/twitter/follow/AntDesignUI.svg?label=Ant%20Design\n[twitter-url]: https://twitter.com/AntDesignUI\n[jsdelivr-image]: https://data.jsdelivr.com/v1/package/npm/antd/badge\n[jsdelivr-url]: https://www.jsdelivr.com/package/npm/antd\n[bundlephobia-image]: https://badgen.net/bundlephobia/minzip/antd?style=flat-square\n[bundlephobia-url]: https://bundlephobia.com/package/antd\n[issues-helper-image]: https://img.shields.io/badge/using-actions--cool-blue?style=flat-square\n[issues-helper-url]: https://github.com/actions-cool\n[renovate-image]: https://img.shields.io/badge/renovate-enabled-brightgreen.svg?style=flat-square\n[renovate-dashboard-url]: https://github.com/ant-design/ant-design/issues/32498\n[dumi-image]: https://img.shields.io/badge/docs%20by-dumi-blue?style=flat-square\n[dumi-url]: https://github.com/umijs/dumi\n[github-issues-url]: https://new-issue.ant.design\n[deepwiki-url]: https://deepwiki.com/ant-design/ant-design\n[deepwiki-image]: https://img.shields.io/badge/Chat%20with-DeepWiki%20🤖-20B2AA?style=flat-square\n\n</div>\n\n[![](https://user-images.githubusercontent.com/507615/209472919-6f7e8561-be8c-4b0b-9976-eb3c692aa20a.png)](https://ant.design)\n\n## ✨ Features\n\n- 🌈 Enterprise-class UI designed for web applications.\n- 📦 A set of high-quality React components out of the box.\n- 🛡 Written in TypeScript with predictable static types.\n- ⚙️ Whole package of design resources and development tools.\n- 🌍 Internationalization support for dozens of languages.\n- 🎨 Powerful theme customization based on CSS-in-JS.\n\n## 🖥 Environment Support\n\n- Modern browsers\n- Server-side Rendering\n- [Electron](https://www.electronjs.org/)\n\n| [<img src=\"https://raw.githubusercontent.com/alrra/browser-logos/master/src/edge/edge_48x48.png\" alt=\"Edge\" width=\"24px\" height=\"24px\" />](https://godban.github.io/browsers-support-badges/)<br>Edge | [<img src=\"https://raw.githubusercontent.com/alrra/browser-logos/master/src/firefox/firefox_48x48.png\" alt=\"Firefox\" width=\"24px\" height=\"24px\" />](https://godban.github.io/browsers-support-badges/)<br>Firefox | [<img src=\"https://raw.githubusercontent.com/alrra/browser-logos/master/src/chrome/chrome_48x48.png\" alt=\"Chrome\" width=\"24px\" height=\"24px\" />](https://godban.github.io/browsers-support-badges/)<br>Chrome | [<img src=\"https://raw.githubusercontent.com/alrra/browser-logos/master/src/safari/safari_48x48.png\" alt=\"Safari\" width=\"24px\" height=\"24px\" />](https://godban.github.io/browsers-support-badges/)<br>Safari | [<img src=\"https://raw.githubusercontent.com/alrra/browser-logos/master/src/electron/electron_48x48.png\" alt=\"Electron\" width=\"24px\" height=\"24px\" />](https://godban.github.io/browsers-support-badges/)<br>Electron |\n| --- | --- | --- | --- | --- |\n| Edge | last 2 versions | last 2 versions | last 2 versions | last 2 versions |\n\n## 📦 Install\n\n```bash\nnpm install antd\n```\n\n```bash\nyarn add antd\n```\n\n```bash\npnpm add antd\n```\n\n```bash\nbun add antd\n```\n\n## 🔨 Usage\n\n```tsx\nimport { Button, DatePicker } from 'antd';\n\nexport default () => (\n  <>\n    <Button type=\"primary\">PRESS ME</Button>\n    <DatePicker placeholder=\"select date\" />\n  </>\n);\n```\n\n## 🔗 Links\n\n- [Home page](https://ant.design/)\n- [Components Overview](https://ant.design/components/overview)\n- [Change Log](CHANGELOG.en-US.md)\n- [rc-components](https://react-component.github.io/)\n- [🆕 Ant Design X](https://x.ant.design/index-cn)\n- [Ant Design Pro](https://pro.ant.design/)\n- [Pro Components](https://procomponents.ant.design)\n- [Ant Design Mobile](https://mobile.ant.design)\n- [Ant Design Mini](https://mini.ant.design)\n- [Ant Design Charts](https://charts.ant.design)\n- [Ant Design Web3](https://web3.ant.design)\n- [Landing Pages](https://landing.ant.design)\n- [Ant Motion](https://motion.ant.design)\n- [Scaffold Market](https://scaffold.ant.design)\n- [Developer Instruction](https://github.com/ant-design/ant-design/wiki/Development)\n- [Versioning Release Note](https://github.com/ant-design/ant-design/wiki/%E8%BD%AE%E5%80%BC%E8%A7%84%E5%88%99%E5%92%8C%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83%E6%B5%81%E7%A8%8B)\n- [FAQ](https://ant.design/docs/react/faq)\n- [Online Playground](https://u.ant.design/reproduce) for bug reports\n- [Customize Theme](https://ant.design/docs/react/customize-theme)\n- [How to Apply for Being A Collaborator](https://github.com/ant-design/ant-design/wiki/Collaborators#how-to-apply-for-being-a-collaborator)\n\n## ⌨️ Development\n\nUse [opensumi.run](https://opensumi.run), a free online pure front-end dev environment.\n\n[![opensumi.run](https://custom-icon-badges.demolab.com/badge/opensumi-run-blue.svg?logo=opensumi)](https://opensumi.run/ide/ant-design/ant-design)\n\nOr clone locally:\n\n```bash\n$ git clone git@github.com:ant-design/ant-design.git\n$ cd ant-design\n$ npm install\n$ npm start\n```\n\nOpen your browser and visit http://127.0.0.1:8001, see more at [Development](https://github.com/ant-design/ant-design/wiki/Development).\n\n## 🤝 Contributing [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://makeapullrequest.com)\n\n<table>\n<tr>\n  <td>\n    <a href=\"https://next.ossinsight.io/widgets/official/compose-recent-top-contributors?repo_id=34526884\" target=\"_blank\" style=\"display: block\" align=\"center\">\n      <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://next.ossinsight.io/widgets/official/compose-recent-top-contributors/thumbnail.png?repo_id=34526884&image_size=auto&color_scheme=dark\" width=\"280\">\n        <img alt=\"Top Contributors of ant-design/ant-design - Last 28 days\" src=\"https://next.ossinsight.io/widgets/official/compose-recent-top-contributors/thumbnail.png?repo_id=34526884&image_size=auto&color_scheme=light\" width=\"280\">\n      </picture>\n    </a>\n  </td>\n  <td rowspan=\"2\">\n    <a href=\"https://next.ossinsight.io/widgets/official/compose-last-28-days-stats?repo_id=34526884\" target=\"_blank\" style=\"display: block\" align=\"center\">\n      <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://next.ossinsight.io/widgets/official/compose-last-28-days-stats/thumbnail.png?repo_id=34526884&image_size=auto&color_scheme=dark\" width=\"655\" height=\"auto\">\n        <img alt=\"Performance Stats of ant-design/ant-design - Last 28 days\" src=\"https://next.ossinsight.io/widgets/official/compose-last-28-days-stats/thumbnail.png?repo_id=34526884&image_size=auto&color_scheme=light\" width=\"655\" height=\"auto\">\n      </picture>\n    </a>\n  </td>\n</tr>\n<tr>\n  <td>\n    <a href=\"https://next.ossinsight.io/widgets/official/compose-org-active-contributors?period=past_28_days&activity=new&owner_id=12101536&repo_ids=34526884\" target=\"_blank\" style=\"display: block\" align=\"center\">\n      <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?period=past_28_days&activity=new&owner_id=12101536&repo_ids=34526884&image_size=2x3&color_scheme=dark\" width=\"273\" height=\"auto\">\n        <img alt=\"New participants of ant-design - past 28 days\" src=\"https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?period=past_28_days&activity=new&owner_id=12101536&repo_ids=34526884&image_size=2x3&color_scheme=light\" width=\"273\" height=\"auto\">\n      </picture>\n    </a>\n  </td>\n</tr>\n</table>\n\n<a href=\"https://openomy.app/github/ant-design/ant-design\" target=\"_blank\" style=\"display: block; width: 100%;\" align=\"center\">\n  <img src=\"https://openomy.app/svg?repo=ant-design/ant-design&chart=bubble&latestMonth=3\" target=\"_blank\" alt=\"Contribution Leaderboard\" style=\"display: block; width: 100%;\" />\n</a>\n\nLet's build a better antd together.\n\nWe warmly invite contributions from everyone. Before you get started, please take a moment to review our [Contribution Guide](https://ant.design/docs/react/contributing). Feel free to share your ideas through [Pull Requests](https://github.com/ant-design/ant-design/pulls) or [GitHub Issues](https://github.com/ant-design/ant-design/issues). If you're interested in enhancing our codebase, explore the [Development Instructions](https://github.com/ant-design/ant-design/wiki/Development) and enjoy your coding journey! :)\n\nFor collaborators, adhere to our [Pull Request Principle](https://github.com/ant-design/ant-design/wiki/PR-principle) and utilize our [Pull Request Template](https://github.com/ant-design/ant-design/wiki/PR-principle#pull-request-template) when creating a Pull Request.\n\n## Issue funding\n\nWe use [Issuehunt](https://issuehunt.io/repos/3452688) to up-vote and promote specific features that you would like to see and implement. Check our backlog and help us:\n\n[![Let's fund issues in this repository](https://raw.githubusercontent.com/BoostIO/issuehunt-materials/master/v1/issuehunt-button-v1.svg)](https://issuehunt.io/repos/34526884)",
    "summary": "Ant Design是一个企业级UI设计语言和基于React的UI组件库，专注于为Web应用提供高质量的界面解决方案。该库采用TypeScript编写，提供丰富的开箱即用组件，并配套完整的UI设计资源和开发工具。Ant Design支持多语言国际化和强大的主题定制能力，是构建现代企业级前端应用的流行框架，广泛应用于各类中后台产品。",
    "keywords": [
      "UI设计",
      "React",
      "组件库",
      "TypeScript",
      "前端开发",
      "企业级应用",
      "国际化",
      "主题定制"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-29T06:18:47Z",
    "download_time": "2024-05-29 10:00:00",
    "visual_resource": [
      "https://gw.alipayobjects.com/zos/rmsportal/KDpgvguMpGfqaHPjicRK.svg",
      "https://user-images.githubusercontent.com/507615/209472919-6f7e8561-be8c-4b0b-9976-eb3c692aa20a.png"
    ],
    "extra_info": null
  },
  {
    "id": "fastapi",
    "source": "GitHub",
    "url": "https://github.com/fastapi/fastapi",
    "title": "FastAPI",
    "content": "FastAPI framework, high performance, easy to learn, fast to code, ready for production\n\n---\n\nDocumentation: https://fastapi.tiangolo.com\n\nSource Code: https://github.com/fastapi/fastapi\n\n---\n\nFastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.\n\nThe key features are:\n\n*   **Fast**: Very high performance, on par with **NodeJS** and **Go** (thanks to Starlette and Pydantic). [One of the fastest Python frameworks available](#performance).\n*   **Fast to code**: Increase the speed to develop features by about 200% to 300%. *\n*   **Fewer bugs**: Reduce about 40% of human (developer) induced errors. *\n*   **Intuitive**: Great editor support. Completion everywhere. Less time debugging.\n*   **Easy**: Designed to be easy to use and learn. Less time reading docs.\n*   **Short**: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.\n*   **Robust**: Get production-ready code. With automatic interactive documentation.\n*   **Standards-based**: Based on (and fully compatible with) the open standards for APIs: OpenAPI (previously known as Swagger) and JSON Schema.\n\n* estimation based on tests on an internal development team, building production applications.\n\n## Sponsors\n\n\nBlockBee Cryptocurrency Payment Gateway\nBuild, run and scale your apps on a modern, reliable, and secure PaaS.\nDeploy FastAPI on AWS with a few clicks\nScalar: Beautiful Open-Source API References from Swagger/OpenAPI files\nAuth, user management and more for your B2B product\nZuplo: Deploy, Secure, Document, and Monetize your FastAPI\nliblab - Generate SDKs from FastAPI\nDeploy & scale any full-stack web app on Render. Focus on building apps, not infra.\nCut Code Review Time & Bugs in Half with CodeRabbit\nThe Gold Standard in Retail Account Linking\nPay as you go for market data\nSDKs for your API | Speakeasy\nSvix - Webhooks as a service\nStainless | Generate best-in-class SDKs\nFine-Grained Authorization for FastAPI\nInterviewPal - AI Interview Coach for Engineers and Devs\n\n\nOther sponsors\n\n## Opinions\n\n\"[...] I'm using **FastAPI** a ton these days. [...] I'm actually planning to use it for all of my team's **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products.\"\n\nKabir Khan - **Microsoft** (ref)\n\n---\n\n\"_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_\"\n\nPiero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - **Uber** (ref)\n\n---\n\n\"_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_\"\n\nKevin Glisson, Marc Vilanova, Forest Monsen - **Netflix** (ref)\n\n---\n\n\"_I’m over the moon excited about **FastAPI**. It’s so fun!_\"\n\nBrian Okken - **Python Bytes** podcast host (ref)\n\n---\n\n\"_Honestly, what you've built looks super solid and polished. In many ways, it's what I wanted **Hug** to be - it's really inspiring to see someone build that._\"\n\nTimothy Crosley - **Hug** creator (ref)\n\n---\n\n\"_If you're looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It's fast, easy to use and easy to learn [...]_\"\n\n\"_We've switched over to **FastAPI** for our **APIs** [...] I think you'll like it [...]_\"\n\nInes Montani - Matthew Honnibal - **Explosion AI** founders - **spaCy** creators (ref) - (ref)\n\n---\n\n\"_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._\"\n\nDeon Pillsbury - **Cisco** (ref)\n\n---\n\n## **Typer**, the FastAPI of CLIs\n\n\nIf you are building a CLI app to be used in the terminal instead of a web API, check out **Typer**.\n\n**Typer** is FastAPI's little sibling. And it's intended to be the **FastAPI of CLIs**. ⌨️ 🚀\n\n## Requirements\n\nFastAPI stands on the shoulders of giants:\n\n*   Starlette for the web parts.\n*   Pydantic for the data parts.\n\n## Installation\n\nCreate and activate a virtual environment and then install FastAPI:\n\n\n```console\n$ pip install \"fastapi[standard]\"\n\n---> 100%\n```\n\n\n**Note**: Make sure you put `\"fastapi[standard]\"` in quotes to ensure it works in all terminals.\n\n## Example\n\n### Create it\n\nCreate a file `main.py` with:\n\n```Python\nfrom typing import Union\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}\n```\n\n\nOr use `async def`...\n\nIf your code uses `async` / `await`, use `async def`:\n\n```Python hl_lines=\"9  14\"\nfrom typing import Union\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}\n```\n\n**Note**:\n\nIf you don't know, check the _\"In a hurry?\"_ section about `async` and `await` in the docs.\n\n\n### Run it\n\nRun the server with:\n\n\n```console\n$ fastapi dev main.py\n\n ╭────────── FastAPI CLI - Development mode ───────────╮\n │                                                     │\n │  Serving at: http://127.0.0.1:8000                  │\n │                                                     │\n │  API docs: http://127.0.0.1:8000/docs               │\n │                                                     │\n │  Running in development mode, for production use:   │\n │                                                     │\n │  fastapi run                                        │\n │                                                     │\n ╰─────────────────────────────────────────────────────╯\n\nINFO:     Will watch for changes in these directories: ['/home/user/code/awesomeapp']\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [2248755] using WatchFiles\nINFO:     Started server process [2248757]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n```\n\n\nAbout the command `fastapi dev main.py`...\n\nThe command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using Uvicorn.\n\nBy default, `fastapi dev` will start with auto-reload enabled for local development.\n\nYou can read more about it in the FastAPI CLI docs.\n\n\n### Check it\n\nOpen your browser at http://127.0.0.1:8000/items/5?q=somequery.\n\nYou will see the JSON response as:\n\n```JSON\n{\"item_id\": 5, \"q\": \"somequery\"}\n```\n\nYou already created an API that:\n\n*   Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.\n*   Both _paths_ take `GET` operations (also known as HTTP _methods_).\n*   The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.\n*   The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.\n\n### Interactive API docs\n\nNow go to http://127.0.0.1:8000/docs.\n\nYou will see the automatic interactive API documentation (provided by Swagger UI):\n\n\n### Alternative API docs\n\nAnd now, go to http://127.0.0.1:8000/redoc.\n\nYou will see the alternative automatic documentation (provided by ReDoc):\n\n\n## Example upgrade\n\nNow modify the file `main.py` to receive a body from a `PUT` request.\n\nDeclare the body using standard Python types, thanks to Pydantic.\n\n```Python hl_lines=\"4  9-12  25-27\"\nfrom typing import Union\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: str\n    price: float\n    is_offer: Union[bool, None] = None\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}\n\n\n@app.put(\"/items/{item_id}\")\ndef update_item(item_id: int, item: Item):\n    return {\"item_name\": item.name, \"item_id\": item_id}\n```\n\nThe `fastapi dev` server should reload automatically.\n\n### Interactive API docs upgrade\n\nNow go to http://127.0.0.1:8000/docs.\n\n*   The interactive API documentation will be automatically updated, including the new body:\n\n\n*   Click on the button \"Try it out\", it allows you to fill the parameters and directly interact with the API:\n\n\n*   Then click on the \"Execute\" button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:\n\n\n### Alternative API docs upgrade\n\nAnd now, go to http://127.0.0.1:8000/redoc.\n\n*   The alternative documentation will also reflect the new query parameter and body:\n\n\n### Recap\n\nIn summary, you declare **once** the types of parameters, body, etc. as function parameters.\n\nYou do that with standard modern Python types.\n\nYou don't have to learn a new syntax, the methods or classes of a specific library, etc.\n\nJust standard **Python**.\n\nFor example, for an `int`:\n\n```Python\nitem_id: int\n```\n\nor for a more complex `Item` model:\n\n```Python\nitem: Item\n```\n\n...and with that single declaration you get:\n\n*   Editor support, including:\n    *   Completion.\n    *   Type checks.\n*   Validation of data:\n    *   Automatic and clear errors when the data is invalid.\n    *   Validation even for deeply nested JSON objects.\n*   Conversion of input data: coming from the network to Python data and types. Reading from:\n    *   JSON.\n    *   Path parameters.\n    *   Query parameters.\n    *   Cookies.\n    *   Headers.\n    *   Forms.\n    *   Files.\n*   Conversion of output data: converting from Python data and types to network data (as JSON):\n    *   Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).\n    *   `datetime` objects.\n    *   `UUID` objects.\n    *   Database models.\n    *   ...and many more.\n*   Automatic interactive API documentation, including 2 alternative user interfaces:\n    *   Swagger UI.\n    *   ReDoc.\n\n---\n\nComing back to the previous code example, **FastAPI** will:\n\n*   Validate that there is an `item_id` in the path for `GET` and `PUT` requests.\n*   Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.\n    *   If it is not, the client will see a useful, clear error.\n*   Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.\n    *   As the `q` parameter is declared with `= None`, it is optional.\n    *   Without the `None` it would be required (as is the body in the case with `PUT`).\n*   For `PUT` requests to `/items/{item_id}`, read the body as JSON:\n    *   Check that it has a required attribute `name` that should be a `str`.\n    *   Check that it has a required attribute `price` that has to be a `float`.\n    *   Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.\n    *   All this would also work for deeply nested JSON objects.\n*   Convert from and to JSON automatically.\n*   Document everything with OpenAPI, that can be used by:\n    *   Interactive documentation systems.\n    *   Automatic client code generation systems, for many languages.\n*   Provide 2 interactive documentation web interfaces directly.\n\n---\n\nWe just scratched the surface, but you already get the idea of how it all works.\n\nTry changing the line with:\n\n```Python\n    return {\"item_name\": item.name, \"item_id\": item_id}\n```\n\n...from:\n\n```Python\n        ... \"item_name\": item.name ...\n```\n\n...to:\n\n```Python\n        ... \"item_price\": item.price ...\n```\n\n...and see how your editor will auto-complete the attributes and know their types:\n\n\nFor a more complete example including more features, see the Tutorial - User Guide.\n\n**Spoiler alert**: the tutorial - user guide includes:\n\n*   Declaration of **parameters** from other different places as: **headers**, **cookies**, **form fields** and **files**.\n*   How to set **validation constraints** as `maximum_length` or `regex`.\n*   A very powerful and easy to use **Dependency Injection** system.\n*   Security and authentication, including support for **OAuth2** with **JWT tokens** and **HTTP Basic** auth.\n*   More advanced (but equally easy) techniques for declaring **deeply nested JSON models** (thanks to Pydantic).\n*   **GraphQL** integration with Strawberry and other libraries.\n*   Many extra features (thanks to Starlette) as:\n    *   **WebSockets**\n    *   extremely easy tests based on HTTPX and `pytest`\n    *   **CORS**\n    *   **Cookie Sessions**\n    *   ...and more.\n\n## Performance\n\nIndependent TechEmpower benchmarks show **FastAPI** applications running under Uvicorn as one of the fastest Python frameworks available, only below Starlette and Uvicorn themselves (used internally by FastAPI). (*)\n\nTo understand more about it, see the section Benchmarks.\n\n## Dependencies\n\nFastAPI depends on Pydantic and Starlette.\n\n### `standard` Dependencies\n\nWhen you install FastAPI with `pip install \"fastapi[standard]\"` it comes with the `standard` group of optional dependencies:\n\nUsed by Pydantic:\n\n*   `email-validator` - for email validation.\n\nUsed by Starlette:\n\n*   `httpx` - Required if you want to use the `TestClient`.\n*   `jinja2` - Required if you want to use the default template configuration.\n*   `python-multipart` - Required if you want to support form \"parsing\", with `request.form()`.\n\nUsed by FastAPI / Starlette:\n\n*   `uvicorn` - for the server that loads and serves your application. This includes `uvicorn[standard]`, which includes some dependencies (e.g. `uvloop`) needed for high performance serving.\n*   `fastapi-cli` - to provide the `fastapi` command.\n\n### Without `standard` Dependencies\n\nIf you don't want to include the `standard` optional dependencies, you can install with `pip install fastapi` instead of `pip install \"fastapi[standard]\"`.\n\n### Additional Optional Dependencies\n\nThere are some additional dependencies you might want to install.\n\nAdditional optional Pydantic dependencies:\n\n*   `pydantic-settings` - for settings management.\n*   `pydantic-extra-types` - for extra types to be used with Pydantic.\n\nAdditional optional FastAPI dependencies:\n\n*   `orjson` - Required if you want to use `ORJSONResponse`.\n*   `ujson` - Required if you want to use `UJSONResponse`.\n\n## License\n\nThis project is licensed under the terms of the MIT license.",
    "summary": "FastAPI是一个现代、高性能的Python Web框架，专为快速构建API而设计。它基于标准的Python类型提示，并利用Starlette和Pydantic的强大功能，提供了卓越的性能、高效的开发流程和减少的错误。框架完全兼容OpenAPI和JSON Schema等标准，并自动生成交互式API文档（Swagger UI和ReDoc）。FastAPI因其易用性和强大功能，已被Microsoft、Uber、Netflix、Cisco等公司广泛应用于构建ML服务、REST API和关键业务系统。",
    "keywords": [
      "Python",
      "Web框架",
      "API开发",
      "高性能",
      "类型提示",
      "OpenAPI",
      "Pydantic",
      "Starlette"
    ],
    "area": [
      "人工智能",
      "大模型",
      "其他"
    ],
    "published_time": "2025-05-22T09:45:56Z",
    "download_time": "2024-07-28 10:00:00",
    "visual_resource": [
      "https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png",
      "https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png",
      "https://fastapi.tiangolo.com/img/vscode-completion.png"
    ],
    "extra_info": null
  },
  {
    "id": "2505.22617",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22617",
    "title": "强化学习在推理语言模型中的熵机制",
    "summary": "本文旨在克服将强化学习应用于大型语言模型推理方面的一个主要障碍，即策略熵的坍塌。在大量未进行熵干预的RL运行中持续地观察到这种现象——策略熵在训练早期急剧下降，这种探索能力的减弱总是伴随着策略性能的饱和。实践中，我们建立了熵H与下游性能R之间的经验变换方程R=-a*e^H+b。这一经验规律强烈表明，策略性能以策略熵为代价换取，并受其耗尽的限制，而性能上限在H=0时完全可预测，即R=-a+b。我们的发现表明，为了进一步扩展RL计算能力，持续的探索需要熵管理。为此，我们从理论和实证两方面研究了熵的动态。我们的推导表明，策略熵的变化是由动作概率与logits变化（在使用类似策略梯度算法时，这与其优势函数成正比）之间的协方差驱动的。实证研究显示，协方差项的值与熵差异精确吻合，支持了理论结论。此外，协方差项在整个训练过程中大部分时间保持正值，进一步解释了为何策略熵会单调下降。通过理解熵动态背后的机制，我们提出通过限制高协方差token的更新来控制熵。具体地，我们提出了两种简单而有效的技术：Clip-Cov和KL-Cov，它们分别对高协方差的token进行剪裁和施加KL惩罚。实验表明，这些方法鼓励探索，从而帮助策略避免熵坍塌并获得更好的下游性能。",
    "keywords": [
      "强化学习",
      "大型语言模型",
      "策略熵",
      "熵坍塌",
      "推理"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "机器学习"
    ],
    "content": "本文旨在克服将强化学习应用于大型语言模型推理方面的一个主要障碍，即策略熵的坍塌。在大量未进行熵干预的RL运行中持续地观察到这种现象——策略熵在训练早期急剧下降，这种探索能力的减弱总是伴随着策略性能的饱和。实践中，我们建立了熵H与下游性能R之间的经验变换方程R=-a*e^H+b。这一经验规律强烈表明，策略性能以策略熵为代价换取，并受其耗尽的限制，而性能上限在H=0时完全可预测，即R=-a+b。我们的发现表明，为了进一步扩展RL计算能力，持续的探索需要熵管理。为此，我们从理论和实证两方面研究了熵的动态。我们的推导表明，策略熵的变化是由动作概率与logits变化（在使用类似策略梯度算法时，这与其优势函数成正比）之间的协方差驱动的。实证研究显示，协方差项的值与熵差异精确吻合，支持了理论结论。此外，协方差项在整个训练过程中大部分时间保持正值，进一步解释了为何策略熵会单调下降。通过理解熵动态背后的机制，我们提出通过限制高协方差token的更新来控制熵。具体地，我们提出了两种简单而有效的技术：Clip-Cov和KL-Cov，它们分别对高协方差的token进行剪裁和施加KL惩罚。实验表明，这些方法鼓励探索，从而帮助策略避免熵坍塌并获得更好的下游性能。",
    "published_time": "2025-05-28T17:38:45.000Z",
    "download_time": "2025-05-29 07:03:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22617.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22617",
      "arxiv_url": "https://arxiv.org/abs/2505.22617"
    }
  },
  {
    "id": "2505.21600",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21600",
    "title": "R2R：利用大小模型 Token 路由高效导航分歧推理路径",
    "summary": "大型语言模型（LLMs）以巨大的推理开销为代价实现了令人印象深刻的推理能力，这带来了巨大的部署挑战。尽管精炼的小型语言模型（SLMs）显著提高了效率，但由于它们未能遵循LLMs的推理路径，其性能有所下降。幸运的是，我们发现LLMs和SLMs之间仅有极少部分的token会真正导致推理路径分歧。大多数生成的token要么是相同的，要么仅表现出中性的差异，例如缩写或表达方式的微小变动。利用这一洞察，我们引入了 罗马之路 (Roads to Rome, R2R)，这是一种神经token路由方法，它仅对这些关键的、导致路径分歧的token选择性地使用LLM，而将绝大多数token的生成留给SLM。我们还开发了一种自动数据生成管线，用于识别分歧token并生成token级的路由标签来训练轻量级路由器。我们将R2R应用于组合DeepSeek系列的R1-1.5B和R1-32B模型，并在有挑战性的数学、编码和问答基准上进行了评估。在平均激活参数量为5.6B的情况下，R2R的平均准确率是R1-7B的1.6倍，甚至超越了R1-14B模型。与R1-32B相比，在性能相当的情况下，它提供了2.8倍的实际耗时加速，推动了测试时缩放效率的帕累托前沿。我们的代码可在 https://github.com/thu-nics/R2R 获取。",
    "keywords": [
      "R2R",
      "Token Routing",
      "大小模型",
      "分歧推理",
      "推理效率"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "生成式AI"
    ],
    "content": "大型语言模型（LLMs）以巨大的推理开销为代价实现了令人印象深刻的推理能力，这带来了巨大的部署挑战。尽管精炼的小型语言模型（SLMs）显著提高了效率，但由于它们未能遵循LLMs的推理路径，其性能有所下降。幸运的是，我们发现LLMs和SLMs之间仅有极少部分的token会真正导致推理路径分歧。大多数生成的token要么是相同的，要么仅表现出中性的差异，例如缩写或表达方式的微小变动。利用这一洞察，我们引入了 罗马之路 (Roads to Rome, R2R)，这是一种神经token路由方法，它仅对这些关键的、导致路径分歧的token选择性地使用LLM，而将绝大多数token的生成留给SLM。我们还开发了一种自动数据生成管线，用于识别分歧token并生成token级的路由标签来训练轻量级路由器。我们将R2R应用于组合DeepSeek系列的R1-1.5B和R1-32B模型，并在有挑战性的数学、编码和问答基准上进行了评估。在平均激活参数量为5.6B的情况下，R2R的平均准确率是R1-7B的1.6倍，甚至超越了R1-14B模型。与R1-32B相比，在性能相当的情况下，它提供了2.8倍的实际耗时加速，推动了测试时缩放效率的帕累托前沿。我们的代码可在 https://github.com/thu-nics/R2R 获取。",
    "published_time": "2025-05-27T16:57:20.000Z",
    "download_time": "2025-05-29 07:04:08",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21600.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21600",
      "arxiv_url": "https://arxiv.org/abs/2505.21600"
    }
  },
  {
    "id": "2505.22651",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22651",
    "title": "Sherlock：视觉语言模型的自主纠错推理",
    "summary": "推理视觉语言模型（VLMs）在复杂多模态任务上已展现出喜人的性能。然而，它们仍面临重大挑战：对推理错误高度敏感，需要大量标注数据或准确验证器，并且难以泛化到特定领域之外。为了解决这些局限性，我们探索将自主纠错作为一种增强推理VLMs的策略。我们首先对推理VLMs的自主纠错能力进行了深入分析，并指出了关键不足。基于我们的发现，我们提出了Sherlock，一个自主纠错和自主提升的训练框架。Sherlock引入了轨迹级自主纠错目标、基于视觉扰动的偏好数据构建方法，以及用于偏好调优的动态beta。一旦模型仅使用2万条随机采样的标注数据获得了自主纠错能力，它便可在没有外部监督的情况下持续自主提升。Sherlock基于Llama3.2-Vision-11B模型构建，在八个基准测试中取得了显著结果，直接生成平均准确率达到64.1，自主纠错后达到65.4。它超越了LLaVA-CoT (63.2)、Mulberry (63.9)和LlamaV-o1 (63.4)，同时使用的标注数据量不到它们的20%。",
    "keywords": [
      "自主纠错",
      "视觉语言模型",
      "推理",
      "自主提升",
      "少量数据训练"
    ],
    "area": [
      "多模态",
      "深度学习",
      "大模型"
    ],
    "content": "推理视觉语言模型（VLMs）在复杂多模态任务上已展现出喜人的性能。然而，它们仍面临重大挑战：对推理错误高度敏感，需要大量标注数据或准确验证器，并且难以泛化到特定领域之外。为了解决这些局限性，我们探索将自主纠错作为一种增强推理VLMs的策略。我们首先对推理VLMs的自主纠错能力进行了深入分析，并指出了关键不足。基于我们的发现，我们提出了Sherlock，一个自主纠错和自主提升的训练框架。Sherlock引入了轨迹级自主纠错目标、基于视觉扰动的偏好数据构建方法，以及用于偏好调优的动态beta。一旦模型仅使用2万条随机采样的标注数据获得了自主纠错能力，它便可在没有外部监督的情况下持续自主提升。Sherlock基于Llama3.2-Vision-11B模型构建，在八个基准测试中取得了显著结果，直接生成平均准确率达到64.1，自主纠错后达到65.4。它超越了LLaVA-CoT (63.2)、Mulberry (63.9)和LlamaV-o1 (63.4)，同时使用的标注数据量不到它们的20%。",
    "published_time": "2025-05-28T17:58:03.000Z",
    "download_time": "2025-05-29 07:04:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22651.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22651",
      "arxiv_url": "https://arxiv.org/abs/2505.22651"
    }
  },
  {
    "id": "2505.22312",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22312",
    "title": "天工开源推理模型 1 号技术报告",
    "summary": "DeepSeek-R1 的成功凸显了强化学习 (RL) 在增强大型语言模型 (LLMs) 推理能力方面的重要作用。在本文中，我们提出了 Skywork-OR1，一种针对长链思维 (CoT) 模型的有效且可扩展的强化学习实现。基于 DeepSeek-R1-Distill 模型系列，我们的强化学习方法取得了显著的性能提升，将 Skywork-OR1-32B 模型在 AIME24、AIME25 和 LiveCodeBench 上的平均准确率从 57.8% 提高到 72.8% (+15.0%)，将 7B 模型从 43.6% 提高到 57.5% (+13.9%)。我们的 Skywork-OR1-32B 模型在 AIME24 和 AIME25 基准测试上超越了 DeepSeek-R1 和 Qwen3-32B，并在 LiveCodeBench 上取得了可比结果。Skywork-OR1-7B 和 Skywork-OR1-Math-7B 模型在同等规模模型中展现出具有竞争力的推理能力。我们对训练管线的核心组件进行了全面的消融研究，以验证其有效性。此外，我们深入研究了熵塌缩现象，确定了影响熵动态的关键因素，并表明缓解过早的熵塌缩对于提高测试性能至关重要。为了支持社区研究，我们完全开源了我们的模型权重、训练代码和训练数据集。",
    "keywords": [
      "Skywork-OR1",
      "强化学习",
      "大型语言模型",
      "链式思考",
      "推理能力"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器学习"
    ],
    "content": "DeepSeek-R1 的成功凸显了强化学习 (RL) 在增强大型语言模型 (LLMs) 推理能力方面的重要作用。在本文中，我们提出了 Skywork-OR1，一种针对长链思维 (CoT) 模型的有效且可扩展的强化学习实现。基于 DeepSeek-R1-Distill 模型系列，我们的强化学习方法取得了显著的性能提升，将 Skywork-OR1-32B 模型在 AIME24、AIME25 和 LiveCodeBench 上的平均准确率从 57.8% 提高到 72.8% (+15.0%)，将 7B 模型从 43.6% 提高到 57.5% (+13.9%)。我们的 Skywork-OR1-32B 模型在 AIME24 和 AIME25 基准测试上超越了 DeepSeek-R1 和 Qwen3-32B，并在 LiveCodeBench 上取得了可比结果。Skywork-OR1-7B 和 Skywork-OR1-Math-7B 模型在同等规模模型中展现出具有竞争力的推理能力。我们对训练管线的核心组件进行了全面的消融研究，以验证其有效性。此外，我们深入研究了熵塌缩现象，确定了影响熵动态的关键因素，并表明缓解过早的熵塌缩对于提高测试性能至关重要。为了支持社区研究，我们完全开源了我们的模型权重、训练代码和训练数据集。",
    "published_time": "2025-05-28T12:56:04.000Z",
    "download_time": "2025-05-29 07:04:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22312.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22312",
      "arxiv_url": "https://arxiv.org/abs/2505.22312"
    }
  },
  {
    "id": "2505.22453",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22453",
    "title": "基于 GRPO 的多模态大语言模型推理无监督后训练",
    "summary": "提升多模态大语言模型（MLLMs）在后训练阶段的性能通常依赖于监督微调（SFT）或强化学习（RL）。然而，这些监督方法需要昂贵且需手动标注的多模态数据，这是一种最终不可持续的资源。尽管近期已有工作探索无监督后训练，但其方法复杂且难以迭代。在本文中，我们首次探索使用 GRPO（一种稳定且可扩展的在线强化学习算法）来实现模型在没有任何外部监督下的持续自我提升。我们提出了 MM-UPT，一个简单但有效的 MLLMs 无监督后训练框架。MM-UPT 构建于 GRPO 之上，用基于多数投票采样多个响应的自奖励机制替代了传统的奖励信号。我们的实验表明，使用没有真实标签的标准数据集，MM-UPT 显著提升了 Qwen2.5-VL-7B 的推理能力（例如，在 MathVista 上从 66.3% 提升到 72.9%，在 We-Math 上从 62.9% 提升到 68.7%）。MM-UPT 还优于先前的无监督基线方法，甚至接近监督 GRPO 的结果。此外，我们展示了通过纳入完全由 MLLM 自身生成的合成问题也能提高性能，这突显了一种可扩展的自我提升的有前景的方法。总的来说，MM-UPT 为在没有外部监督的情况下持续、自主地增强 MLLMs 提供了一种新范式。我们的代码位于 https://github.com/waltonfuture/MM-UPT。",
    "keywords": [
      "多模态大语言模型",
      "无监督后训练",
      "GRPO",
      "自我提升",
      "推理"
    ],
    "area": [
      "多模态",
      "大模型",
      "机器学习"
    ],
    "content": "提升多模态大语言模型（MLLMs）在后训练阶段的性能通常依赖于监督微调（SFT）或强化学习（RL）。然而，这些监督方法需要昂贵且需手动标注的多模态数据，这是一种最终不可持续的资源。尽管近期已有工作探索无监督后训练，但其方法复杂且难以迭代。在本文中，我们首次探索使用 GRPO（一种稳定且可扩展的在线强化学习算法）来实现模型在没有任何外部监督下的持续自我提升。我们提出了 MM-UPT，一个简单但有效的 MLLMs 无监督后训练框架。MM-UPT 构建于 GRPO 之上，用基于多数投票采样多个响应的自奖励机制替代了传统的奖励信号。我们的实验表明，使用没有真实标签的标准数据集，MM-UPT 显著提升了 Qwen2.5-VL-7B 的推理能力（例如，在 MathVista 上从 66.3% 提升到 72.9%，在 We-Math 上从 62.9% 提升到 68.7%）。MM-UPT 还优于先前的无监督基线方法，甚至接近监督 GRPO 的结果。此外，我们展示了通过纳入完全由 MLLM 自身生成的合成问题也能提高性能，这突显了一种可扩展的自我提升的有前景的方法。总的来说，MM-UPT 为在没有外部监督的情况下持续、自主地增强 MLLMs 提供了一种新范式。我们的代码位于 https://github.com/waltonfuture/MM-UPT。",
    "published_time": "2025-05-28T15:11:16.000Z",
    "download_time": "2025-05-29 07:04:59",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22453.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22453",
      "arxiv_url": "https://arxiv.org/abs/2505.22453"
    }
  },
  {
    "id": "2505.20411",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20411",
    "title": "SWE-rebench：用于软件工程Agent任务收集和去污染评估的自动化流程",
    "summary": "基于LLM的Agent在日益增长的软件工程（SWE）任务中展现出令人瞩目的能力。然而，推进该领域面临两个关键挑战。首先，高质量的训练数据稀缺，特别是反映真实世界SWE场景的数据，Agent必须在此场景下与开发环境交互，执行代码并根据其行动结果调整行为。现有数据集要么仅限于一次性代码生成，要么由小规模、手动整理的交互式任务集合组成，缺乏规模和多样性。其次，新颖的交互式SWE任务的缺乏影响了对快速改进模型的评估，因为静态基准测试由于污染问题很快就会过时。为了解决这些局限性，我们引入了一种新颖、自动化且可扩展的流程，用于持续从不同的GitHub仓库中提取真实世界的交互式SWE任务。利用该流程，我们构建了SWE-rebench，这是一个包含超过21,000个基于Python的交互式SWE任务的公开数据集，适用于大规模SWE Agent的强化学习。此外，我们利用通过SWE-rebench方法收集的持续涌现的新鲜任务，构建了一个用于Agent化软件工程的无污染基准测试。我们将各种LLM在该基准测试上的结果与SWE-bench Verified上的结果进行比较，并发现某些语言模型的性能可能由于污染问题而被高估。",
    "keywords": [
      "SWE-rebench",
      "软件工程Agent",
      "自动化流程",
      "去污染评估",
      "交互式任务数据集"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "content": "基于LLM的Agent在日益增长的软件工程（SWE）任务中展现出令人瞩目的能力。然而，推进该领域面临两个关键挑战。首先，高质量的训练数据稀缺，特别是反映真实世界SWE场景的数据，Agent必须在此场景下与开发环境交互，执行代码并根据其行动结果调整行为。现有数据集要么仅限于一次性代码生成，要么由小规模、手动整理的交互式任务集合组成，缺乏规模和多样性。其次，新颖的交互式SWE任务的缺乏影响了对快速改进模型的评估，因为静态基准测试由于污染问题很快就会过时。为了解决这些局限性，我们引入了一种新颖、自动化且可扩展的流程，用于持续从不同的GitHub仓库中提取真实世界的交互式SWE任务。利用该流程，我们构建了SWE-rebench，这是一个包含超过21,000个基于Python的交互式SWE任务的公开数据集，适用于大规模SWE Agent的强化学习。此外，我们利用通过SWE-rebench方法收集的持续涌现的新鲜任务，构建了一个用于Agent化软件工程的无污染基准测试。我们将各种LLM在该基准测试上的结果与SWE-bench Verified上的结果进行比较，并发现某些语言模型的性能可能由于污染问题而被高估。",
    "published_time": "2025-05-26T18:01:00.000Z",
    "download_time": "2025-05-29 07:05:14",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20411.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20411",
      "arxiv_url": "https://arxiv.org/abs/2505.20411"
    }
  },
  {
    "id": "2505.21136",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21136",
    "title": "SageAttention2++：SageAttention2 的更高效率实现",
    "summary": "注意力机制的效率至关重要，因为其时间复杂度随序列长度呈二次方增长。SageAttention2 通过利用量化技术来加速注意力机制中的矩阵乘法（Matmul）解决了这一问题。为了进一步加速 SageAttention2，我们提出利用在 FP16 中累积的 FP8 Matmul 更快的指令。该指令比 SageAttention2 中使用的 FP8 Matmul 快 2 倍。我们的实验表明，SageAttention2++ 在保持与 SageAttention2 相同注意力精度的同时，比 FlashAttention 实现了 3.9 倍的加速。这意味着 SageAttention2++ 可以有效加速包括语言、图像和视频生成在内的各种模型，且端到端指标损失可以忽略不计。代码将在 https://github.com/thu-ml/SageAttention 上提供。",
    "keywords": [
      "SageAttention2++",
      "注意力机制",
      "量化",
      "FP8 Matmul",
      "加速"
    ],
    "area": [
      "人工智能",
      "深度学习",
      "生成式AI"
    ],
    "content": "注意力机制的效率至关重要，因为其时间复杂度随序列长度呈二次方增长。SageAttention2 通过利用量化技术来加速注意力机制中的矩阵乘法（Matmul）解决了这一问题。为了进一步加速 SageAttention2，我们提出利用在 FP16 中累积的 FP8 Matmul 更快的指令。该指令比 SageAttention2 中使用的 FP8 Matmul 快 2 倍。我们的实验表明，SageAttention2++ 在保持与 SageAttention2 相同注意力精度的同时，比 FlashAttention 实现了 3.9 倍的加速。这意味着 SageAttention2++ 可以有效加速包括语言、图像和视频生成在内的各种模型，且端到端指标损失可以忽略不计。代码将在 https://github.com/thu-ml/SageAttention 上提供。",
    "published_time": "2025-05-27T12:50:36.000Z",
    "download_time": "2025-05-29 07:05:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21136.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21136",
      "arxiv_url": "https://arxiv.org/abs/2505.21136"
    }
  },
  {
    "id": "2505.22334",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22334",
    "title": "通过带冷启动的强化学习提升多模态推理能力",
    "summary": "近期大型语言模型（LLMs）的进展展示了令人印象深刻的思维链推理能力，其中强化学习（RL）在此进展中发挥了关键作用。虽然模型通过反思展现自我纠正的“顿悟时刻”模式通常被归因于RL的涌现特性，但我们首次证明这些模式在RL训练前已存在于多模态LLMs（MLLMs）中，且可能不一定与推理性能的提升相关。基于这些见解，我们提出了一项关于通过两阶段方法增强多模态推理的综合研究：(1) 作为冷启动的监督微调（SFT），采用结构化思维链推理模式，随后 (2) 通过GRPO进行强化学习，以进一步提升这些能力。我们的大量实验表明，这种组合方法在具有挑战性的多模态推理基准上，始终优于仅使用SFT和仅使用RL的方法。所得模型在3十亿和7十亿参数规模的开源MLLMs中均达到了最先进的性能，其中我们的7十亿模型相比基础模型显示出显著提升（例如，MathVista上从66.3%到73.4%，We-Math上从62.9%到70.4%），而我们的3十亿模型性能可与多个7十亿模型媲美。总而言之，这项工作为构建先进的多模态推理模型提供了实用指导。我们的代码可在 https://github.com/waltonfuture/RL-with-Cold-Start 查看。",
    "keywords": [
      "多模态推理",
      "强化学习",
      "冷启动",
      "MLLMs",
      "SFT"
    ],
    "area": [
      "多模态",
      "大模型",
      "深度学习"
    ],
    "content": "近期大型语言模型（LLMs）的进展展示了令人印象深刻的思维链推理能力，其中强化学习（RL）在此进展中发挥了关键作用。虽然模型通过反思展现自我纠正的“顿悟时刻”模式通常被归因于RL的涌现特性，但我们首次证明这些模式在RL训练前已存在于多模态LLMs（MLLMs）中，且可能不一定与推理性能的提升相关。基于这些见解，我们提出了一项关于通过两阶段方法增强多模态推理的综合研究：(1) 作为冷启动的监督微调（SFT），采用结构化思维链推理模式，随后 (2) 通过GRPO进行强化学习，以进一步提升这些能力。我们的大量实验表明，这种组合方法在具有挑战性的多模态推理基准上，始终优于仅使用SFT和仅使用RL的方法。所得模型在3十亿和7十亿参数规模的开源MLLMs中均达到了最先进的性能，其中我们的7十亿模型相比基础模型显示出显著提升（例如，MathVista上从66.3%到73.4%，We-Math上从62.9%到70.4%），而我们的3十亿模型性能可与多个7十亿模型媲美。总而言之，这项工作为构建先进的多模态推理模型提供了实用指导。我们的代码可在 https://github.com/waltonfuture/RL-with-Cold-Start 查看。",
    "published_time": "2025-05-28T13:21:38.000Z",
    "download_time": "2025-05-29 07:05:42",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22334.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22334",
      "arxiv_url": "https://arxiv.org/abs/2505.22334"
    }
  },
  {
    "id": "2505.22457",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22457",
    "title": "通过下一事件预测增强视频推理",
    "summary": "下一词元预测作为使大语言模型（LLMs）能够进行推理的基础学习任务。但是，当旨在赋予多模态大语言模型（MLLMs）处理视频输入的时间推理能力时，学习任务应该是什么？现有的任务，如视频问答，通常依赖于人类或更强的多模态大语言模型的标注，而视频字幕则倾向于将时间推理与空间信息纠缠在一起。为了弥补这一空白，我们提出了下一事件预测（NEP），这是一个利用未来视频片段作为丰富、自监督信号以促进时间推理的学习任务。我们将每个视频分割成过去和未来的帧：多模态大语言模型将过去的帧作为输入，并预测从未来帧提取的事件摘要，从而鼓励模型进行时间推理以完成任务。为了支持这项任务，我们整理了V1-33K数据集，该数据集包含33,000个自动提取的视频片段，涵盖了各种现实场景。我们进一步探索了一系列视频指令微调策略，以研究它们对时间推理的影响。为了评估进展，我们引入了FutureBench基准测试，用于评估预测未见过的未来事件的连贯性。实验验证了下一事件预测为增强多模态大语言模型的时间推理提供了一个可扩展且有效的训练范式。",
    "keywords": [
      "下一事件预测",
      "时间推理",
      "多模态大模型",
      "视频理解",
      "自监督学习"
    ],
    "area": [
      "视频理解",
      "多模态",
      "大模型"
    ],
    "content": "下一词元预测作为使大语言模型（LLMs）能够进行推理的基础学习任务。但是，当旨在赋予多模态大语言模型（MLLMs）处理视频输入的时间推理能力时，学习任务应该是什么？现有的任务，如视频问答，通常依赖于人类或更强的多模态大语言模型的标注，而视频字幕则倾向于将时间推理与空间信息纠缠在一起。为了弥补这一空白，我们提出了下一事件预测（NEP），这是一个利用未来视频片段作为丰富、自监督信号以促进时间推理的学习任务。我们将每个视频分割成过去和未来的帧：多模态大语言模型将过去的帧作为输入，并预测从未来帧提取的事件摘要，从而鼓励模型进行时间推理以完成任务。为了支持这项任务，我们整理了V1-33K数据集，该数据集包含33,000个自动提取的视频片段，涵盖了各种现实场景。我们进一步探索了一系列视频指令微调策略，以研究它们对时间推理的影响。为了评估进展，我们引入了FutureBench基准测试，用于评估预测未见过的未来事件的连贯性。实验验证了下一事件预测为增强多模态大语言模型的时间推理提供了一个可扩展且有效的训练范式。",
    "published_time": "2025-05-28T15:13:34.000Z",
    "download_time": "2025-05-29 07:06:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22457.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22457",
      "arxiv_url": "https://arxiv.org/abs/2505.22457"
    }
  },
  {
    "id": "2505.21925",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21925",
    "title": "RenderFormer：基于 Transformer 的带全局光照三角网格神经渲染",
    "summary": "我们提出了 RenderFormer，一种神经渲染管线，它可以直接从包含完整全局光照效果的基于三角网格的场景表示渲染图像，且无需针对每个场景进行训练或微调。与采用以物理为中心的方法进行渲染不同，我们将渲染表述为一种序列到序列的转换：将代表带有材质属性的三角形的 token 序列转换为代表小块像素的输出 token 序列。RenderFormer 遵循一个两阶段管线：一个与视角无关的阶段，用于建模三角形到三角形的光传输；以及一个与视角相关的阶段，该阶段将代表光线束的 token 转换为相应的像素值，并由第一阶段的三角形序列引导。这两个阶段都基于 Transformer 架构，且以最小的先验约束进行学习。我们在具有不同形状和光传输复杂度的场景上演示并评估了 RenderFormer。",
    "keywords": [
      "神经渲染",
      "Transformer",
      "全局光照",
      "三角网格",
      "无需场景训练"
    ],
    "area": [
      "深度学习",
      "计算机视觉",
      "生成式AI"
    ],
    "content": "我们提出了 RenderFormer，一种神经渲染管线，它可以直接从包含完整全局光照效果的基于三角网格的场景表示渲染图像，且无需针对每个场景进行训练或微调。与采用以物理为中心的方法进行渲染不同，我们将渲染表述为一种序列到序列的转换：将代表带有材质属性的三角形的 token 序列转换为代表小块像素的输出 token 序列。RenderFormer 遵循一个两阶段管线：一个与视角无关的阶段，用于建模三角形到三角形的光传输；以及一个与视角相关的阶段，该阶段将代表光线束的 token 转换为相应的像素值，并由第一阶段的三角形序列引导。这两个阶段都基于 Transformer 架构，且以最小的先验约束进行学习。我们在具有不同形状和光传输复杂度的场景上演示并评估了 RenderFormer。",
    "published_time": "2025-05-28T03:20:46.000Z",
    "download_time": "2025-05-29 07:06:17",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21925.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21925",
      "arxiv_url": "https://arxiv.org/abs/2505.21925"
    }
  },
  {
    "id": "2505.19253",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19253",
    "title": "DeepResearchGym：一个免费、透明、可复现的深度研究评估沙箱",
    "summary": "深度研究系统代表了一种新兴的智能信息检索方法，能够针对复杂查询生成全面且有充分依据的报告。然而，大多数现有框架依赖于动态商业搜索API，这除了成本之外，还带来了可复现性和透明性方面的挑战。为了解决这些局限性，我们引入了 DeepResearchGym，这是一个开源沙箱，它结合了可复现的搜索API和严格的评估协议，用于对深度研究系统进行基准测试。该API使用先进的密集检索器和通过 DiskANN 实现的近似最近邻搜索，索引了大规模公共网络语料库，即 ClueWeb22 和 FineWeb。它比流行的商业API具有更低的延迟，同时确保跨运行文档排名的稳定性，并免费用于研究用途。为了评估深度研究系统的输出，我们通过“大模型即评委”（LLM-as-a-judge）评估，将自动指标扩展到 Researchy Questions 基准测试中，以衡量其与用户信息需求的匹配度、检索的忠实度以及报告质量。实验结果表明，与 DeepResearchGym集成的系统取得了与使用商业API的系统相当的性能，且性能排名在各评估指标上保持一致。人工评估研究进一步证实了我们的自动评估协议与人工偏好一致，验证了该框架支持对深度研究系统进行受控评估的能力。我们的代码和API文档可在 https://www.deepresearchgym.ai 获取。",
    "keywords": [
      "Deep Research",
      "Evaluation",
      "Reproducibility",
      "Information Retrieval",
      "LLM-as-a-judge"
    ],
    "area": [
      "智能体",
      "大模型",
      "自然语言处理"
    ],
    "content": "深度研究系统代表了一种新兴的智能信息检索方法，能够针对复杂查询生成全面且有充分依据的报告。然而，大多数现有框架依赖于动态商业搜索API，这除了成本之外，还带来了可复现性和透明性方面的挑战。为了解决这些局限性，我们引入了 DeepResearchGym，这是一个开源沙箱，它结合了可复现的搜索API和严格的评估协议，用于对深度研究系统进行基准测试。该API使用先进的密集检索器和通过 DiskANN 实现的近似最近邻搜索，索引了大规模公共网络语料库，即 ClueWeb22 和 FineWeb。它比流行的商业API具有更低的延迟，同时确保跨运行文档排名的稳定性，并免费用于研究用途。为了评估深度研究系统的输出，我们通过“大模型即评委”（LLM-as-a-judge）评估，将自动指标扩展到 Researchy Questions 基准测试中，以衡量其与用户信息需求的匹配度、检索的忠实度以及报告质量。实验结果表明，与 DeepResearchGym集成的系统取得了与使用商业API的系统相当的性能，且性能排名在各评估指标上保持一致。人工评估研究进一步证实了我们的自动评估协议与人工偏好一致，验证了该框架支持对深度研究系统进行受控评估的能力。我们的代码和API文档可在 https://www.deepresearchgym.ai 获取。",
    "published_time": "2025-05-25T18:16:13.000Z",
    "download_time": "2025-05-29 07:06:30",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19253.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19253",
      "arxiv_url": "https://arxiv.org/abs/2505.19253"
    }
  },
  {
    "id": "2505.18600",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18600",
    "title": "Chain-of-Zoom：通过尺度自回归与偏好对齐实现极端超分辨率",
    "summary": "现代单幅图像超分辨率 (SISR) 模型在其训练的尺度因子下能产生逼真的结果，但在远超该范围进行放大时性能会急剧下降。我们通过 Chain-of-Zoom (CoZ) 解决了这一可扩展性瓶颈。CoZ 是一个与具体模型无关的框架，它将 SISR 分解为一个由中间尺度状态组成的自回归链，并辅以多尺度感知的提示。CoZ 反复重用一个骨干 SR 模型，将条件概率分解为可处理的子问题，从而在无需额外训练的情况下实现极端分辨率。由于在高放大倍数下视觉线索会减少，我们在每个缩放步骤中都通过一个视觉-语言模型 (VLM) 生成多尺度感知的文本提示进行增强。提示提取器本身使用广义奖励策略优化 (GRPO) 和一个评论家 VLM 进行微调，以便将文本指导与人类偏好对齐。实验表明，一个用 CoZ 包装的标准 4 倍扩散 SR 模型可以实现超过 256 倍的放大，同时保持高感知质量和保真度。项目页面：https://bryanswkim.github.io/chain-of-zoom/ 。",
    "keywords": [
      "Extreme Super-Resolution",
      "尺度自回归",
      "偏好对齐",
      "Chain-of-Zoom",
      "视觉-语言模型"
    ],
    "area": [
      "计算机视觉",
      "多模态",
      "深度学习"
    ],
    "content": "现代单幅图像超分辨率 (SISR) 模型在其训练的尺度因子下能产生逼真的结果，但在远超该范围进行放大时性能会急剧下降。我们通过 Chain-of-Zoom (CoZ) 解决了这一可扩展性瓶颈。CoZ 是一个与具体模型无关的框架，它将 SISR 分解为一个由中间尺度状态组成的自回归链，并辅以多尺度感知的提示。CoZ 反复重用一个骨干 SR 模型，将条件概率分解为可处理的子问题，从而在无需额外训练的情况下实现极端分辨率。由于在高放大倍数下视觉线索会减少，我们在每个缩放步骤中都通过一个视觉-语言模型 (VLM) 生成多尺度感知的文本提示进行增强。提示提取器本身使用广义奖励策略优化 (GRPO) 和一个评论家 VLM 进行微调，以便将文本指导与人类偏好对齐。实验表明，一个用 CoZ 包装的标准 4 倍扩散 SR 模型可以实现超过 256 倍的放大，同时保持高感知质量和保真度。项目页面：https://bryanswkim.github.io/chain-of-zoom/ 。",
    "published_time": "2025-05-24T08:50:08.000Z",
    "download_time": "2025-05-29 07:06:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18600.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18600",
      "arxiv_url": "https://arxiv.org/abs/2505.18600"
    }
  },
  {
    "id": "2505.22232",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22232",
    "title": "跨语言质量评估：一种利用语言模型进行预训练数据过滤的多语言方法",
    "summary": "高质量的多语言训练数据对于有效预训练大型语言模型（LLM）至关重要。然而，合适的开源多语言数据集仍然有限。现有的最先进数据集主要依赖启发式过滤方法，这限制了它们的跨语言迁移能力和可扩展性。\n本文介绍 JQL，这是一种系统性方法，能够高效地大规模筛选多样化的高质量多语言数据，同时显著降低计算需求。JQL 将 LLM 的标注能力提炼到基于预训练多语言嵌入的轻量级标注器中。这些模型表现出强大的多语言和跨语言性能，甚至对于训练期间未见的语言和脚本也是如此。通过对 35 种语言的实证评估，由此产生的标注流程显著优于当前的启发式过滤方法，例如 Fineweb2。JQL 显著提高了下游模型训练质量，并提高了数据保留率。我们的研究为多语言数据管理提供了实用的见解和宝贵的资源，提升了多语言数据集开发的标准。",
    "keywords": [
      "多语言数据",
      "数据过滤",
      "LLMs",
      "预训练",
      "跨语言"
    ],
    "area": [
      "自然语言处理",
      "大模型",
      "机器学习"
    ],
    "content": "高质量的多语言训练数据对于有效预训练大型语言模型（LLM）至关重要。然而，合适的开源多语言数据集仍然有限。现有的最先进数据集主要依赖启发式过滤方法，这限制了它们的跨语言迁移能力和可扩展性。\n本文介绍 JQL，这是一种系统性方法，能够高效地大规模筛选多样化的高质量多语言数据，同时显著降低计算需求。JQL 将 LLM 的标注能力提炼到基于预训练多语言嵌入的轻量级标注器中。这些模型表现出强大的多语言和跨语言性能，甚至对于训练期间未见的语言和脚本也是如此。通过对 35 种语言的实证评估，由此产生的标注流程显著优于当前的启发式过滤方法，例如 Fineweb2。JQL 显著提高了下游模型训练质量，并提高了数据保留率。我们的研究为多语言数据管理提供了实用的见解和宝贵的资源，提升了多语言数据集开发的标准。",
    "published_time": "2025-05-28T11:06:54.000Z",
    "download_time": "2025-05-29 07:06:58",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22232.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22232",
      "arxiv_url": "https://arxiv.org/abs/2505.22232"
    }
  },
  {
    "id": "2505.19075",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19075",
    "title": "通用推理器：用于冻结大型语言模型的单一、可组合的即插即用推理器",
    "summary": "大型语言模型（LLMs）展示了非凡的通用能力，但增强推理等技能通常需要大量的计算资源，并可能影响其泛化能力。虽然参数高效微调（PEFT）方法提供了一种更资源节约的替代方案，但由于架构依赖性，它们通常需要针对每个LLM骨干模型重新训练。为了解决这些挑战，我们提出通用推理器（UniR）——一个单一、轻量级、可组合、即插即用的推理模块，可与任何冻结的LLM结合使用，赋予其专门的推理能力。具体来说，UniR将奖励分解为一个独立的推理模块，该模块使用预定义奖励独立训练，有效地将轨迹级信号转化为令牌级指导。训练完成后，UniR可以在推理时通过简单地将其输出logits叠加到LLM骨干模型的logits上，与任何冻结的LLM结合使用。这种叠加结构自然支持模块化组合：多个针对不同任务训练的UniR模块可以通过叠加其logits联合应用，通过组合实现复杂的推理。在数学推理和机器翻译任务上的实验结果表明，使用Llama3.2模型时，UniR显著优于现有的基线微调方法。此外，UniR展示了强大的弱监督到强监督泛化能力：在较小模型上训练的推理模块能有效地指导大得多的LLMs。这使得UniR成为一种经济高效、适应性强且稳健的解决方案，可在不损害其核心能力的情况下增强LLMs的推理能力。代码已在https://github.com/hangeol/UniR 开源。",
    "keywords": [
      "Universal Reasoner",
      "Frozen LLMs",
      "Reasoning",
      "Plug-and-Play",
      "Composable"
    ],
    "area": [
      "自然语言处理",
      "大模型",
      "机器学习"
    ],
    "content": "大型语言模型（LLMs）展示了非凡的通用能力，但增强推理等技能通常需要大量的计算资源，并可能影响其泛化能力。虽然参数高效微调（PEFT）方法提供了一种更资源节约的替代方案，但由于架构依赖性，它们通常需要针对每个LLM骨干模型重新训练。为了解决这些挑战，我们提出通用推理器（UniR）——一个单一、轻量级、可组合、即插即用的推理模块，可与任何冻结的LLM结合使用，赋予其专门的推理能力。具体来说，UniR将奖励分解为一个独立的推理模块，该模块使用预定义奖励独立训练，有效地将轨迹级信号转化为令牌级指导。训练完成后，UniR可以在推理时通过简单地将其输出logits叠加到LLM骨干模型的logits上，与任何冻结的LLM结合使用。这种叠加结构自然支持模块化组合：多个针对不同任务训练的UniR模块可以通过叠加其logits联合应用，通过组合实现复杂的推理。在数学推理和机器翻译任务上的实验结果表明，使用Llama3.2模型时，UniR显著优于现有的基线微调方法。此外，UniR展示了强大的弱监督到强监督泛化能力：在较小模型上训练的推理模块能有效地指导大得多的LLMs。这使得UniR成为一种经济高效、适应性强且稳健的解决方案，可在不损害其核心能力的情况下增强LLMs的推理能力。代码已在https://github.com/hangeol/UniR 开源。",
    "published_time": "2025-05-25T10:19:10.000Z",
    "download_time": "2025-05-29 07:07:06",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19075.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19075",
      "arxiv_url": "https://arxiv.org/abs/2505.19075"
    }
  },
  {
    "id": "2505.21887",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21887",
    "title": "SVRPBench：一项面向随机车辆路径问题的现实基准测试",
    "summary": "在不确定性下实现鲁棒路径规划是现实物流的核心，然而大多数基准测试假定静态、理想化的设置。我们提出 SVRPBench，这是首个捕获城市规模车辆路径高保真随机动态的开放基准测试。它涵盖了 500 多个算例，服务客户数量高达 1000 个，模拟了现实的配送条件：时变拥堵、对数正态分布的延误、概率性事故以及基于实证数据的住宅和商业客户时间窗。我们的流程生成了多样化、约束丰富的场景，包括多仓库和多车辆设置。基准测试结果显示，POMO 和 AM 等最先进的强化学习求解器在分布迁移下性能下降超过 20%，而经典和元启发式方法则保持鲁棒性。为了促进可复现研究，我们发布了数据集和评估套件。SVRPBench 挑战社区设计能够超越合成假设、适应现实不确定性的求解器。",
    "keywords": [
      "SVRP",
      "Benchmark",
      "Uncertainty",
      "Logistics",
      "Reinforcement Learning"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "深度学习"
    ],
    "content": "在不确定性下实现鲁棒路径规划是现实物流的核心，然而大多数基准测试假定静态、理想化的设置。我们提出 SVRPBench，这是首个捕获城市规模车辆路径高保真随机动态的开放基准测试。它涵盖了 500 多个算例，服务客户数量高达 1000 个，模拟了现实的配送条件：时变拥堵、对数正态分布的延误、概率性事故以及基于实证数据的住宅和商业客户时间窗。我们的流程生成了多样化、约束丰富的场景，包括多仓库和多车辆设置。基准测试结果显示，POMO 和 AM 等最先进的强化学习求解器在分布迁移下性能下降超过 20%，而经典和元启发式方法则保持鲁棒性。为了促进可复现研究，我们发布了数据集和评估套件。SVRPBench 挑战社区设计能够超越合成假设、适应现实不确定性的求解器。",
    "published_time": "2025-05-28T02:03:31.000Z",
    "download_time": "2025-05-29 07:07:22",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21887.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21887",
      "arxiv_url": "https://arxiv.org/abs/2505.21887"
    }
  },
  {
    "id": "2505.22129",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22129",
    "title": "基于 Stable Diffusion 的文本到 360 度全景图生成是如何实现的？",
    "summary": "近年来，文本到图像扩散模型（例如 Stable Diffusion）的繁荣激发了将其应用于 360 度全景图生成的研究。先前的工作展示了在预训练扩散模型上使用传统的低秩适应技术生成全景图的可行性。然而，透视图和全景图之间巨大的领域差异，引发了对促成这种经验成功的底层机制的疑问。我们假设并研究了在全景数据上微调时，可训练对应部分表现出不同的行为，并且这种适应隐藏了某种利用预训练扩散模型中先验知识的内在机制。我们的分析揭示如下：1）注意力模块中的查询（query）和键（key）矩阵负责可在全景和透视域之间共享的通用信息，因此与全景图生成的相关性较低；2）值（value）和输出权重矩阵专门用于将预训练知识适应到全景域，在全景图生成的微调过程中起着更关键的作用。我们通过引入一个名为 UniPano 的简单框架，对这些见解进行了实证验证，旨在为未来的研究提供一个优雅的基线。UniPano 不仅优于现有方法，而且与先前的双分支方法相比，显著减少了内存使用和训练时间，使其能够扩展到更高分辨率的端到端全景图生成。代码将会发布。",
    "keywords": [
      "文本到全景图生成",
      "Stable Diffusion",
      "扩散模型",
      "微调",
      "UniPano"
    ],
    "area": [
      "生成式AI",
      "计算机视觉",
      "深度学习"
    ],
    "content": "近年来，文本到图像扩散模型（例如 Stable Diffusion）的繁荣激发了将其应用于 360 度全景图生成的研究。先前的工作展示了在预训练扩散模型上使用传统的低秩适应技术生成全景图的可行性。然而，透视图和全景图之间巨大的领域差异，引发了对促成这种经验成功的底层机制的疑问。我们假设并研究了在全景数据上微调时，可训练对应部分表现出不同的行为，并且这种适应隐藏了某种利用预训练扩散模型中先验知识的内在机制。我们的分析揭示如下：1）注意力模块中的查询（query）和键（key）矩阵负责可在全景和透视域之间共享的通用信息，因此与全景图生成的相关性较低；2）值（value）和输出权重矩阵专门用于将预训练知识适应到全景域，在全景图生成的微调过程中起着更关键的作用。我们通过引入一个名为 UniPano 的简单框架，对这些见解进行了实证验证，旨在为未来的研究提供一个优雅的基线。UniPano 不仅优于现有方法，而且与先前的双分支方法相比，显著减少了内存使用和训练时间，使其能够扩展到更高分辨率的端到端全景图生成。代码将会发布。",
    "published_time": "2025-05-28T08:54:04.000Z",
    "download_time": "2025-05-29 07:07:36",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22129.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22129",
      "arxiv_url": "https://arxiv.org/abs/2505.22129"
    }
  },
  {
    "id": "2505.22648",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22648",
    "title": "WebDancer：迈向自主信息搜索智能体",
    "summary": "解决复杂的实际问题需要深入的信息搜索和多步推理。智能体系统的最新进展，例如 Deep Research，强调了自主多步研究的潜力。在本文中，我们从以数据为中心和训练阶段的角度，提出了构建端到端智能信息搜索智能体的统一范式。我们的方法包含四个关键阶段：(1) 浏览数据构建，(2) 轨迹采样，(3) 用于有效冷启动的监督微调，以及 (4) 用于增强泛化能力的强化学习。我们在基于 ReAct 的 Web 智能体 WebDancer 中实例化了这一框架。在具有挑战性的信息搜索基准 GAIA 和 WebWalkerQA 上的实证评估表明了 WebDancer 的强大性能，取得了可观的结果，并凸显了我们训练范式的有效性。对智能体训练的进一步分析提供了宝贵的见解和可行的、系统的路径，用于开发更强大的智能体模型。代码和演示将在 https://github.com/Alibaba-NLP/WebAgent 上发布。",
    "keywords": [
      "信息搜索智能体",
      "训练范式",
      "强化学习",
      "Web Agent",
      "ReAct"
    ],
    "area": [
      "智能体",
      "强化学习",
      "自然语言处理"
    ],
    "content": "解决复杂的实际问题需要深入的信息搜索和多步推理。智能体系统的最新进展，例如 Deep Research，强调了自主多步研究的潜力。在本文中，我们从以数据为中心和训练阶段的角度，提出了构建端到端智能信息搜索智能体的统一范式。我们的方法包含四个关键阶段：(1) 浏览数据构建，(2) 轨迹采样，(3) 用于有效冷启动的监督微调，以及 (4) 用于增强泛化能力的强化学习。我们在基于 ReAct 的 Web 智能体 WebDancer 中实例化了这一框架。在具有挑战性的信息搜索基准 GAIA 和 WebWalkerQA 上的实证评估表明了 WebDancer 的强大性能，取得了可观的结果，并凸显了我们训练范式的有效性。对智能体训练的进一步分析提供了宝贵的见解和可行的、系统的路径，用于开发更强大的智能体模型。代码和演示将在 https://github.com/Alibaba-NLP/WebAgent 上发布。",
    "published_time": "2025-05-28T17:57:07.000Z",
    "download_time": "2025-05-29 07:07:56",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22648.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22648",
      "arxiv_url": "https://arxiv.org/abs/2505.22648"
    }
  },
  {
    "id": "2505.19187",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19187",
    "title": "LIMOPro：用于高效有效测试时扩展的推理精炼",
    "summary": "大型语言模型（LLM）通过测试时扩展方法展现出卓越的推理能力，尤其是在使用从更强大的大型推理模型（LRM）中提炼出的思维链（CoT）数据进行微调时。然而，这些推理链通常包含冗余元素，反映了人类解决问题的过程，这些元素可分为渐进式推理（必需的解决方案发展路径）和功能性元素（验证过程、替代解决方案方法和错误修正）。虽然渐进式推理至关重要，但功能性元素在测试时推理过程中显著增加了计算需求。我们引入了 PIR（基于困惑度的重要性精炼），这是一个基于原则的框架，它根据每个推理步骤对答案预测置信度的影响来定量评估其重要性。PIR 系统地识别并选择性地剪枝仅具有较低重要性的功能性步骤，同时保留渐进式推理组成部分，从而创建优化的训练数据，在减少冗余的同时保持核心解决方案路径的完整性。在 PIR 优化数据上微调的模型表现出卓越的测试时扩展特性，生成更简洁的推理链，同时在具有挑战性的推理基准（AIME、AMC 和 GPQA Diamond）上实现了更高的准确率（+0.9% 至 +6.6%）和显著降低的令牌使用量（-3% 至 -41%）。我们的方法在不同模型尺寸、数据源和令牌预算下都展现出强大的泛化能力，为在高效测试时扩展、响应时间和计算效率是重要限制的场景中部署具有推理能力的 LLM 提供了一种实用的解决方案。",
    "keywords": [
      "大型语言模型",
      "推理精炼",
      "测试时扩展",
      "思维链",
      "PIR"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "人工智能"
    ],
    "content": "大型语言模型（LLM）通过测试时扩展方法展现出卓越的推理能力，尤其是在使用从更强大的大型推理模型（LRM）中提炼出的思维链（CoT）数据进行微调时。然而，这些推理链通常包含冗余元素，反映了人类解决问题的过程，这些元素可分为渐进式推理（必需的解决方案发展路径）和功能性元素（验证过程、替代解决方案方法和错误修正）。虽然渐进式推理至关重要，但功能性元素在测试时推理过程中显著增加了计算需求。我们引入了 PIR（基于困惑度的重要性精炼），这是一个基于原则的框架，它根据每个推理步骤对答案预测置信度的影响来定量评估其重要性。PIR 系统地识别并选择性地剪枝仅具有较低重要性的功能性步骤，同时保留渐进式推理组成部分，从而创建优化的训练数据，在减少冗余的同时保持核心解决方案路径的完整性。在 PIR 优化数据上微调的模型表现出卓越的测试时扩展特性，生成更简洁的推理链，同时在具有挑战性的推理基准（AIME、AMC 和 GPQA Diamond）上实现了更高的准确率（+0.9% 至 +6.6%）和显著降低的令牌使用量（-3% 至 -41%）。我们的方法在不同模型尺寸、数据源和令牌预算下都展现出强大的泛化能力，为在高效测试时扩展、响应时间和计算效率是重要限制的场景中部署具有推理能力的 LLM 提供了一种实用的解决方案。",
    "published_time": "2025-05-25T15:17:57.000Z",
    "download_time": "2025-05-29 07:08:11",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19187.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19187",
      "arxiv_url": "https://arxiv.org/abs/2505.19187"
    }
  },
  {
    "id": "2505.17663",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17663",
    "title": "迈向动态心智理论：评估大型语言模型对人类状态时间演变的适应能力",
    "summary": "随着大型语言模型（LLMs）越来越多地参与到人机交互中，评估其心智理论（ToM）能力——特别是其跟踪动态心理状态的能力——变得至关重要。虽然现有基准评估基本的ToM能力，但它们主要关注心理状态的静态快照，忽略了现实社交互动中特有的时间演变。我们提出了DynToM，这是一个新颖的基准，专门设计用于评估LLMs在相互关联的场景中理解和跟踪心理状态时间进展的能力。通过一个系统的四步框架，我们生成了1,100个社交情境，包含5,500个场景和78,100个问题，每个情境都经过真实性和质量验证。我们对十个最先进LLMs的综合评估表明，它们的平均性能比人类低44.7%，并且在跟踪和推理心理状态变化时性能显著下降。这一性能差距突显了当前LLMs在建模人类心理状态动态性方面的根本性局限。",
    "keywords": [
      "大模型",
      "心智理论 (ToM)",
      "动态心理状态",
      "基准",
      "评估"
    ],
    "area": [
      "人工智能",
      "自然语言处理",
      "大模型"
    ],
    "content": "随着大型语言模型（LLMs）越来越多地参与到人机交互中，评估其心智理论（ToM）能力——特别是其跟踪动态心理状态的能力——变得至关重要。虽然现有基准评估基本的ToM能力，但它们主要关注心理状态的静态快照，忽略了现实社交互动中特有的时间演变。我们提出了DynToM，这是一个新颖的基准，专门设计用于评估LLMs在相互关联的场景中理解和跟踪心理状态时间进展的能力。通过一个系统的四步框架，我们生成了1,100个社交情境，包含5,500个场景和78,100个问题，每个情境都经过真实性和质量验证。我们对十个最先进LLMs的综合评估表明，它们的平均性能比人类低44.7%，并且在跟踪和推理心理状态变化时性能显著下降。这一性能差距突显了当前LLMs在建模人类心理状态动态性方面的根本性局限。",
    "published_time": "2025-05-23T09:27:40.000Z",
    "download_time": "2025-05-29 07:08:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17663.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17663",
      "arxiv_url": "https://arxiv.org/abs/2505.17663"
    }
  },
  {
    "id": "2505.22019",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22019",
    "title": "VRAG-RL：通过强化学习迭代推理增强基于视觉感知的RAG，以理解视觉丰富信息",
    "summary": "有效检索、推理和理解视觉丰富信息对于RAG方法来说仍然是一个挑战。传统的基于文本的方法无法处理视觉相关信息。另一方面，当前基于视觉的RAG方法常受限于固定流程，并且由于未能充分激活模型的基本能力，在有效推理方面常常遇到困难。由于强化学习已被证明有利于模型推理，我们引入了VRAG-RL，这是一个专为视觉丰富信息上的复杂推理量身定制的新型强化学习框架。在此框架下，VLMs与搜索引擎交互，借助视觉感知tokens自主采样单轮或多轮推理轨迹，并基于这些样本进行持续优化。我们的方法揭示了在RAG领域处理视觉信息的关键不足：（i）先前的多模态RAG方法倾向于仅将图像整合到上下文中，导致推理tokens分配不足，并忽略了视觉特有的感知；（ii）当模型与搜索引擎交互时，由于无法清晰表达需求，其查询常常无法检索到相关信息，从而导致性能欠佳。为了应对这些挑战，我们定义了一个专为视觉丰富输入量身定制的动作空间，动作包括裁剪和缩放，使模型能够从粗粒度到细粒度地收集信息。此外，为了弥合用户原始查询与检索器之间的差距，我们采用了一种简单而有效的奖励机制，该机制将查询重写、检索性能与基于模型的奖励相结合。我们的VRAG-RL利用专门设计的强化学习策略优化VLMs以执行RAG任务，使模型与实际应用场景对齐。代码可在 https://github.com/Alibaba-NLP/VRAG 获取。",
    "keywords": [
      "VRAG-RL",
      "RAG",
      "强化学习",
      "视觉语言模型",
      "迭代推理"
    ],
    "area": [
      "多模态",
      "强化学习",
      "计算机视觉"
    ],
    "content": "有效检索、推理和理解视觉丰富信息对于RAG方法来说仍然是一个挑战。传统的基于文本的方法无法处理视觉相关信息。另一方面，当前基于视觉的RAG方法常受限于固定流程，并且由于未能充分激活模型的基本能力，在有效推理方面常常遇到困难。由于强化学习已被证明有利于模型推理，我们引入了VRAG-RL，这是一个专为视觉丰富信息上的复杂推理量身定制的新型强化学习框架。在此框架下，VLMs与搜索引擎交互，借助视觉感知tokens自主采样单轮或多轮推理轨迹，并基于这些样本进行持续优化。我们的方法揭示了在RAG领域处理视觉信息的关键不足：（i）先前的多模态RAG方法倾向于仅将图像整合到上下文中，导致推理tokens分配不足，并忽略了视觉特有的感知；（ii）当模型与搜索引擎交互时，由于无法清晰表达需求，其查询常常无法检索到相关信息，从而导致性能欠佳。为了应对这些挑战，我们定义了一个专为视觉丰富输入量身定制的动作空间，动作包括裁剪和缩放，使模型能够从粗粒度到细粒度地收集信息。此外，为了弥合用户原始查询与检索器之间的差距，我们采用了一种简单而有效的奖励机制，该机制将查询重写、检索性能与基于模型的奖励相结合。我们的VRAG-RL利用专门设计的强化学习策略优化VLMs以执行RAG任务，使模型与实际应用场景对齐。代码可在 https://github.com/Alibaba-NLP/VRAG 获取。",
    "published_time": "2025-05-28T06:30:51.000Z",
    "download_time": "2025-05-29 07:08:44",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22019.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22019",
      "arxiv_url": "https://arxiv.org/abs/2505.22019"
    }
  },
  {
    "id": "2505.22202",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22202",
    "title": "逐句预测",
    "summary": "自回归语言模型（LMs）每次生成一个词元（token），然而人类推理则是在句子、命题、概念等更高层次的抽象单位上进行的。这种对比引出了一个核心问题：LMs能否也像人类一样，学习在结构化的语义单元而非原始词元序列上进行推理？在本文中，我们基于预训练LMs学到的表示，探究它们是否能够被提升到这种抽象的推理空间。我们提出一个框架，该框架通过自回归地预测下一个句子的连续嵌入，将一个预训练的词元级LM调整到句子空间中运行。我们探索了两种受经典表示学习启发的嵌入范式：1）语义嵌入，通过自编码学习以保留表面含义；2）上下文嵌入，通过下一句预测训练以编码预期结构。我们在两种推理机制下评估了这两种范式：离散式（Discretized），它在重新编码前将每个预测的嵌入解码成文本；以及连续式（Continuous），它为了提高效率完全在嵌入空间中进行推理。在数学、逻辑、常识和规划这四个领域中，连续推理下的上下文嵌入展现出与思维链（Chain-of-Thought, CoT）相当的性能，同时将推理时的浮点运算次数（FLOPs）平均降低了一半。我们还展示了可扩展性和模块化适应性的初步迹象。最后，为了可视化潜在轨迹，我们引入了SentenceLens，这是一个将中间模型状态解码成可解释句子的诊断工具。总的来说，我们的结果表明，预训练LMs能够有效地在潜在嵌入空间中转化到抽象的、结构化的推理。",
    "keywords": [
      "逐句预测",
      "句子嵌入",
      "抽象推理",
      "连续推理",
      "Chain-of-Thought"
    ],
    "area": [
      "自然语言处理",
      "机器学习",
      "人工智能"
    ],
    "content": "自回归语言模型（LMs）每次生成一个词元（token），然而人类推理则是在句子、命题、概念等更高层次的抽象单位上进行的。这种对比引出了一个核心问题：LMs能否也像人类一样，学习在结构化的语义单元而非原始词元序列上进行推理？在本文中，我们基于预训练LMs学到的表示，探究它们是否能够被提升到这种抽象的推理空间。我们提出一个框架，该框架通过自回归地预测下一个句子的连续嵌入，将一个预训练的词元级LM调整到句子空间中运行。我们探索了两种受经典表示学习启发的嵌入范式：1）语义嵌入，通过自编码学习以保留表面含义；2）上下文嵌入，通过下一句预测训练以编码预期结构。我们在两种推理机制下评估了这两种范式：离散式（Discretized），它在重新编码前将每个预测的嵌入解码成文本；以及连续式（Continuous），它为了提高效率完全在嵌入空间中进行推理。在数学、逻辑、常识和规划这四个领域中，连续推理下的上下文嵌入展现出与思维链（Chain-of-Thought, CoT）相当的性能，同时将推理时的浮点运算次数（FLOPs）平均降低了一半。我们还展示了可扩展性和模块化适应性的初步迹象。最后，为了可视化潜在轨迹，我们引入了SentenceLens，这是一个将中间模型状态解码成可解释句子的诊断工具。总的来说，我们的结果表明，预训练LMs能够有效地在潜在嵌入空间中转化到抽象的、结构化的推理。",
    "published_time": "2025-05-28T10:28:35.000Z",
    "download_time": "2025-05-29 07:08:59",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22202.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22202",
      "arxiv_url": "https://arxiv.org/abs/2505.22202"
    }
  },
  {
    "id": "2505.22613",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22613",
    "title": "RICO：通过视觉重构提高图像重标注的准确性和完整性",
    "summary": "图像重标注被广泛用于为各种多模态任务生成质量增强的训练数据集。现有重标注方法通常依赖强大的多模态大语言模型（MLLM）来增强文本描述，但由于幻觉和缺失细粒度细节导致的不完整性，常常存在不准确性。为了解决这些局限性，我们提出了RICO，一个通过视觉重构来优化标注的新颖框架。具体而言，我们利用文本到图像模型将标注重构为参考图像，并提示一个MLLM识别原始图像和重构图像之间的差异来完善标注。这一过程迭代执行，进一步逐步促进生成更忠实和全面的描述。为了减轻迭代过程带来的额外计算成本，我们引入了RICO-Flash，它使用DPO学习像RICO一样生成标注。大量实验表明，我们的方法在标注准确性和完整性方面取得了显著提升，在CapsBench和CompreCap上均比大多数基线模型高出约10%。代码已发布于 https://github.com/wangyuchi369/RICO。",
    "keywords": [
      "图像重标注",
      "视觉重构",
      "多模态大语言模型",
      "文本到图像模型",
      "迭代优化"
    ],
    "area": [
      "多模态",
      "计算机视觉",
      "生成式AI"
    ],
    "content": "图像重标注被广泛用于为各种多模态任务生成质量增强的训练数据集。现有重标注方法通常依赖强大的多模态大语言模型（MLLM）来增强文本描述，但由于幻觉和缺失细粒度细节导致的不完整性，常常存在不准确性。为了解决这些局限性，我们提出了RICO，一个通过视觉重构来优化标注的新颖框架。具体而言，我们利用文本到图像模型将标注重构为参考图像，并提示一个MLLM识别原始图像和重构图像之间的差异来完善标注。这一过程迭代执行，进一步逐步促进生成更忠实和全面的描述。为了减轻迭代过程带来的额外计算成本，我们引入了RICO-Flash，它使用DPO学习像RICO一样生成标注。大量实验表明，我们的方法在标注准确性和完整性方面取得了显著提升，在CapsBench和CompreCap上均比大多数基线模型高出约10%。代码已发布于 https://github.com/wangyuchi369/RICO。",
    "published_time": "2025-05-28T17:29:34.000Z",
    "download_time": "2025-05-29 07:09:12",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22613.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22613",
      "arxiv_url": "https://arxiv.org/abs/2505.22613"
    }
  },
  {
    "id": "2505.22525",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22525",
    "title": "基于生成图像的思考",
    "summary": "我们提出了“基于生成图像的思考”，这是一种新颖的范式，它通过使大型多模态模型（LMM）能够通过自发生成中间视觉思维步骤，以原生方式跨文本和视觉模态进行思考，从而从根本上改变了它们进行视觉推理的方式。当前大型多模态模型的视觉推理受限于处理固定的用户提供图像，或仅通过基于文本的思维链（CoT）进行推理。“基于生成图像的思考”开启了认知能力的新维度，模型可以在其中主动构建中间视觉思维，批判自身的视觉假设，并将其作为推理过程的组成部分进行完善。我们通过两种互补的机制展示了我们方法的有效性：（1）带有中间视觉子目标的视觉生成，其中模型将复杂的视觉任务分解为可管理的组成部分，这些部分被逐步生成和整合；（2）带有自我批判的视觉生成，其中模型生成初步的视觉假设，通过文本推理分析其不足，并根据自身的批判生成优化的输出。我们在视觉生成基准上的实验表明，与基准方法相比，我们的方法取得了显著改进，在处理复杂的多对象场景时，模型的相对改进高达50%（从38%提高到57%）。从探索新型蛋白质结构的生物化学家、迭代空间设计的建筑师，到重建犯罪现场的法医分析师以及构思战略性比赛的篮球运动员，我们的方法使人工智能模型能够进行那种类似于人类创造性、分析性和战略性思维的视觉想象和迭代完善。我们在 https://github.com/GAIR-NLP/thinking-with-generated-images 上发布了我们的开源套件。",
    "keywords": [
      "多模态",
      "大模型",
      "视觉推理",
      "生成图像",
      "自我批判"
    ],
    "area": [
      "多模态",
      "大模型",
      "生成式AI"
    ],
    "content": "我们提出了“基于生成图像的思考”，这是一种新颖的范式，它通过使大型多模态模型（LMM）能够通过自发生成中间视觉思维步骤，以原生方式跨文本和视觉模态进行思考，从而从根本上改变了它们进行视觉推理的方式。当前大型多模态模型的视觉推理受限于处理固定的用户提供图像，或仅通过基于文本的思维链（CoT）进行推理。“基于生成图像的思考”开启了认知能力的新维度，模型可以在其中主动构建中间视觉思维，批判自身的视觉假设，并将其作为推理过程的组成部分进行完善。我们通过两种互补的机制展示了我们方法的有效性：（1）带有中间视觉子目标的视觉生成，其中模型将复杂的视觉任务分解为可管理的组成部分，这些部分被逐步生成和整合；（2）带有自我批判的视觉生成，其中模型生成初步的视觉假设，通过文本推理分析其不足，并根据自身的批判生成优化的输出。我们在视觉生成基准上的实验表明，与基准方法相比，我们的方法取得了显著改进，在处理复杂的多对象场景时，模型的相对改进高达50%（从38%提高到57%）。从探索新型蛋白质结构的生物化学家、迭代空间设计的建筑师，到重建犯罪现场的法医分析师以及构思战略性比赛的篮球运动员，我们的方法使人工智能模型能够进行那种类似于人类创造性、分析性和战略性思维的视觉想象和迭代完善。我们在 https://github.com/GAIR-NLP/thinking-with-generated-images 上发布了我们的开源套件。",
    "published_time": "2025-05-28T16:12:45.000Z",
    "download_time": "2025-05-29 07:09:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22525.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22525",
      "arxiv_url": "https://arxiv.org/abs/2505.22525"
    }
  },
  {
    "id": "2505.20779",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20779",
    "title": "CHIMERA：科学文献中的思想重组知识库",
    "summary": "人类创新的一个显著特征是重组过程——通过整合现有机制和概念的元素来创造原创思想。在这项工作中，我们自动挖掘科学文献并构建了CHIMERA：一个大规模的重组实例知识库（KB）。CHIMERA可用于大规模实证探索科学家如何重组概念并从不同领域汲取灵感，或用于训练监督机器学习模型，这些模型学习预测新的跨领域创新方向。为了构建这个知识库，我们提出了一个从科学论文摘要中提取重组的新颖信息提取任务，收集了一个包含数百个手动标注摘要的高质量语料库，并用它来训练一个基于LLM的提取模型。该模型应用于AI领域的大量论文语料库，生成了一个包含超过28K个重组实例的知识库。我们分析CHIMERA，以探索AI不同子领域中重组的属性。最后，我们利用该知识库训练了一个科学假设生成模型，该模型预测了现实世界研究人员认为具有启发性的新重组方向。我们的数据和代码可在 https://github.cs.huji.ac.il/tomhope-lab/CHIMERA 获取。",
    "keywords": [
      "Recombination",
      "Knowledge Base",
      "Information Extraction",
      "LLM",
      "AI"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "content": "人类创新的一个显著特征是重组过程——通过整合现有机制和概念的元素来创造原创思想。在这项工作中，我们自动挖掘科学文献并构建了CHIMERA：一个大规模的重组实例知识库（KB）。CHIMERA可用于大规模实证探索科学家如何重组概念并从不同领域汲取灵感，或用于训练监督机器学习模型，这些模型学习预测新的跨领域创新方向。为了构建这个知识库，我们提出了一个从科学论文摘要中提取重组的新颖信息提取任务，收集了一个包含数百个手动标注摘要的高质量语料库，并用它来训练一个基于LLM的提取模型。该模型应用于AI领域的大量论文语料库，生成了一个包含超过28K个重组实例的知识库。我们分析CHIMERA，以探索AI不同子领域中重组的属性。最后，我们利用该知识库训练了一个科学假设生成模型，该模型预测了现实世界研究人员认为具有启发性的新重组方向。我们的数据和代码可在 https://github.cs.huji.ac.il/tomhope-lab/CHIMERA 获取。",
    "published_time": "2025-05-27T06:36:04.000Z",
    "download_time": "2025-05-29 07:09:44",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20779.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20779",
      "arxiv_url": "https://arxiv.org/abs/2505.20779"
    }
  },
  {
    "id": "2505.22523",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22523",
    "title": "PrismLayers：高质量多层透明图像生成模型的开放数据",
    "summary": "从文本提示生成高质量的多层透明图像可以开启新的创意控制层面，使用户能够像编辑大模型输出的文本一样轻松地编辑每个图层。然而，由于缺乏大型、高质量的多层透明数据语料库，多层生成模型的发展落后于传统的文本到图像模型。在本文中，我们通过以下方式解决了这一根本挑战：(i) 发布了首个开放、超高保真度的 PrismLayers (PrismLayersPro) 数据集，包含 200K (20K) 张具有精确 alpha 遮罩的多层透明图像，(ii) 引入了一种免训练的合成流程，该流程使用现有的扩散模型按需生成此类数据，以及 (iii) 提供了一个强大的开源多层生成模型 ART+，其美学效果与现代文本到图像生成模型相媲美。主要的技术贡献包括：LayerFLUX，它擅长生成具有精确 alpha 遮罩的高质量单层透明图像，以及 MultiLayerFLUX，它在人工标注的语义布局指导下，将多个 LayerFLUX 的输出合成为完整图像。为了确保更高质量，我们应用了严格的过滤阶段来去除伪影和语义不匹配，随后进行人工筛选。在合成的 PrismLayersPro 上对最先进的 ART 模型进行微调，得到了 ART+，在头对头用户研究比较中，ART+ 在 60% 的情况下优于原始 ART，甚至可与 FLUX.1-[dev] 模型生成的图像达到相同的视觉质量。我们预计，我们的工作将为多层透明图像生成任务奠定坚实的数据集基础，从而推动需要精确、可编辑且具有视觉吸引力的分层图像的研究和应用。",
    "keywords": [
      "多层透明图像",
      "数据集",
      "生成模型",
      "扩散模型",
      "LayerFLUX"
    ],
    "area": [
      "生成式AI",
      "计算机视觉",
      "深度学习"
    ],
    "content": "从文本提示生成高质量的多层透明图像可以开启新的创意控制层面，使用户能够像编辑大模型输出的文本一样轻松地编辑每个图层。然而，由于缺乏大型、高质量的多层透明数据语料库，多层生成模型的发展落后于传统的文本到图像模型。在本文中，我们通过以下方式解决了这一根本挑战：(i) 发布了首个开放、超高保真度的 PrismLayers (PrismLayersPro) 数据集，包含 200K (20K) 张具有精确 alpha 遮罩的多层透明图像，(ii) 引入了一种免训练的合成流程，该流程使用现有的扩散模型按需生成此类数据，以及 (iii) 提供了一个强大的开源多层生成模型 ART+，其美学效果与现代文本到图像生成模型相媲美。主要的技术贡献包括：LayerFLUX，它擅长生成具有精确 alpha 遮罩的高质量单层透明图像，以及 MultiLayerFLUX，它在人工标注的语义布局指导下，将多个 LayerFLUX 的输出合成为完整图像。为了确保更高质量，我们应用了严格的过滤阶段来去除伪影和语义不匹配，随后进行人工筛选。在合成的 PrismLayersPro 上对最先进的 ART 模型进行微调，得到了 ART+，在头对头用户研究比较中，ART+ 在 60% 的情况下优于原始 ART，甚至可与 FLUX.1-[dev] 模型生成的图像达到相同的视觉质量。我们预计，我们的工作将为多层透明图像生成任务奠定坚实的数据集基础，从而推动需要精确、可编辑且具有视觉吸引力的分层图像的研究和应用。",
    "published_time": "2025-05-28T16:09:33.000Z",
    "download_time": "2025-05-29 07:10:01",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22523.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22523",
      "arxiv_url": "https://arxiv.org/abs/2505.22523"
    }
  },
  {
    "id": "2505.22338",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22338",
    "title": "Text2Grad：基于自然语言反馈的强化学习",
    "summary": "传统的RLHF（人类反馈强化学习）通过粗粒度的标量奖励来优化语言模型，这种奖励掩盖了成功或失败背后的细粒度原因，导致学习缓慢且不透明。近期工作通过提示或反思的方式用文本批评增强了强化学习，提高了可解释性，但没有触及模型参数。我们引入了 Text2Grad，这是一种将自由形式文本反馈转化为跨度级梯度的强化学习范式。Text2Grad 接收人类（或程序化）批评，将每个反馈短语与相关的 token 跨度对齐，将这些对齐转换为可微分的奖励信号，并执行梯度更新，直接改进模型策略中存在问题的部分。这产生了精确的、基于反馈的调整，而非全局的微调。Text2Grad 通过三个组件实现：（1）一个高质量的反馈标注流程，将批评与 token 跨度配对；（2）一个细粒度奖励模型，在生成解释性批评的同时预测答案的跨度级奖励；（3）一个跨度级策略优化器，反向传播自然语言梯度。在摘要生成、代码生成和问答任务中，Text2Grad 持续超越标量奖励强化学习和仅提示的基线方法，提供了更高的任务指标和更丰富的可解释性。我们的结果表明，自然语言反馈在转化为梯度后，是进行细粒度策略优化的有力信号。",
    "keywords": [
      "Text2Grad",
      "自然语言反馈",
      "强化学习",
      "跨度级梯度",
      "细粒度优化"
    ],
    "area": [
      "自然语言处理",
      "机器学习",
      "生成式AI"
    ],
    "content": "传统的RLHF（人类反馈强化学习）通过粗粒度的标量奖励来优化语言模型，这种奖励掩盖了成功或失败背后的细粒度原因，导致学习缓慢且不透明。近期工作通过提示或反思的方式用文本批评增强了强化学习，提高了可解释性，但没有触及模型参数。我们引入了 Text2Grad，这是一种将自由形式文本反馈转化为跨度级梯度的强化学习范式。Text2Grad 接收人类（或程序化）批评，将每个反馈短语与相关的 token 跨度对齐，将这些对齐转换为可微分的奖励信号，并执行梯度更新，直接改进模型策略中存在问题的部分。这产生了精确的、基于反馈的调整，而非全局的微调。Text2Grad 通过三个组件实现：（1）一个高质量的反馈标注流程，将批评与 token 跨度配对；（2）一个细粒度奖励模型，在生成解释性批评的同时预测答案的跨度级奖励；（3）一个跨度级策略优化器，反向传播自然语言梯度。在摘要生成、代码生成和问答任务中，Text2Grad 持续超越标量奖励强化学习和仅提示的基线方法，提供了更高的任务指标和更丰富的可解释性。我们的结果表明，自然语言反馈在转化为梯度后，是进行细粒度策略优化的有力信号。",
    "published_time": "2025-05-28T13:23:49.000Z",
    "download_time": "2025-05-29 07:10:15",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22338.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22338",
      "arxiv_url": "https://arxiv.org/abs/2505.22338"
    }
  },
  {
    "id": "2505.22203",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22203",
    "title": "基于规则和基于模型的验证器的弊端——以数学推理为例的案例研究",
    "summary": "可信的验证器对于带有可验证奖励的强化学习（RLVR）的成功至关重要，RLVR 是 DeepSeek-R1 等各种大型推理模型背后的核心方法。在数学推理等复杂领域，基于规则的验证器在以往工作中被广泛采用，用于训练强大的推理模型。然而，这些验证器的可靠性及其对强化学习训练过程的影响仍缺乏深入了解。在这项工作中，我们以数学推理为例，在静态评估和强化学习训练场景下对各种验证器进行了全面分析。首先，我们发现当前开源的基于规则的验证器在多个常用数学数据集中常常无法识别以不同格式表达的等价答案，导致不可忽略的假阴性率。这一局限性对强化学习训练性能产生不利影响，并且随着策略模型的增强而变得更加突出。随后，我们探讨基于模型的验证器作为解决这些局限性的潜在方案。虽然静态评估表明基于模型的验证器取得了显著更高的验证准确率，但进一步分析和强化学习训练结果表明它们极易被“攻击”（hacking），即将回复中的某些特定模式错误地分类为正确（即假阳性）。这种漏洞在策略模型优化过程中被利用，导致奖励被人为夸大。我们的研究结果强调了基于规则和基于模型的验证器各自固有的独特风险，旨在为开发更稳健的强化学习奖励系统提供宝贵见解。",
    "keywords": [
      "强化学习",
      "验证器",
      "数学推理",
      "可信度",
      "假阳性"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "大模型"
    ],
    "content": "可信的验证器对于带有可验证奖励的强化学习（RLVR）的成功至关重要，RLVR 是 DeepSeek-R1 等各种大型推理模型背后的核心方法。在数学推理等复杂领域，基于规则的验证器在以往工作中被广泛采用，用于训练强大的推理模型。然而，这些验证器的可靠性及其对强化学习训练过程的影响仍缺乏深入了解。在这项工作中，我们以数学推理为例，在静态评估和强化学习训练场景下对各种验证器进行了全面分析。首先，我们发现当前开源的基于规则的验证器在多个常用数学数据集中常常无法识别以不同格式表达的等价答案，导致不可忽略的假阴性率。这一局限性对强化学习训练性能产生不利影响，并且随着策略模型的增强而变得更加突出。随后，我们探讨基于模型的验证器作为解决这些局限性的潜在方案。虽然静态评估表明基于模型的验证器取得了显著更高的验证准确率，但进一步分析和强化学习训练结果表明它们极易被“攻击”（hacking），即将回复中的某些特定模式错误地分类为正确（即假阳性）。这种漏洞在策略模型优化过程中被利用，导致奖励被人为夸大。我们的研究结果强调了基于规则和基于模型的验证器各自固有的独特风险，旨在为开发更稳健的强化学习奖励系统提供宝贵见解。",
    "published_time": "2025-05-28T10:28:41.000Z",
    "download_time": "2025-05-29 07:10:33",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22203.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22203",
      "arxiv_url": "https://arxiv.org/abs/2505.22203"
    }
  },
  {
    "id": "2505.21876",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21876",
    "title": "EPiC：基于精确锚点视频引导的高效视频相机控制学习",
    "summary": "视频扩散模型（VDMs）中用于3D相机控制的最新方法通常通过从估计的点云中根据标注的相机轨迹进行渲染来创建锚点视频，以此作为结构化先验信息来引导扩散模型。然而，点云估计中固有的误差经常导致锚点视频不准确。此外，对大量相机轨迹标注的需求进一步增加了资源投入。为了解决这些限制，我们引入了 EPiC，一种高效精确的相机控制学习框架，它无需昂贵的相机轨迹标注即可自动构建高质量的锚点视频。具体而言，我们通过基于首帧可见性来遮罩源视频，从而为训练创建高精度的锚点视频。这种方法确保了高对齐度，消除了相机轨迹标注的需求，因此可以轻松应用于任意“野外”视频以生成图像到视频（I2V）训练对。此外，我们引入了 Anchor-ControlNet，一个轻量级条件模块，它将锚点视频的指导信息集成到预训练的VDMs中，仅使用不到骨干模型参数的1%。通过结合所提出的锚点视频数据和 ControlNet 模块，EPiC 实现了高效训练，显著减少了参数、训练步骤和数据需求，并且无需像通常那样为了减轻渲染不对齐问题而修改扩散模型骨干。尽管在基于遮罩的锚点视频上进行训练，我们的方法在推理时对使用点云制作的锚点视频表现出强大的泛化能力，从而实现了精确的3D感知相机控制。EPiC 在 RealEstate10K 和 MiraData 数据集上的 I2V 相机控制任务中取得了 SOTA 性能，定量和定性地展示了精确而稳健的相机控制能力。值得注意的是，EPiC 还对视频到视频场景表现出极强的零样本泛化能力。",
    "keywords": [
      "视频相机控制",
      "扩散模型",
      "锚点视频",
      "ControlNet",
      "图像到视频 (I2V)"
    ],
    "area": [
      "计算机视觉",
      "深度学习",
      "生成式AI"
    ],
    "content": "视频扩散模型（VDMs）中用于3D相机控制的最新方法通常通过从估计的点云中根据标注的相机轨迹进行渲染来创建锚点视频，以此作为结构化先验信息来引导扩散模型。然而，点云估计中固有的误差经常导致锚点视频不准确。此外，对大量相机轨迹标注的需求进一步增加了资源投入。为了解决这些限制，我们引入了 EPiC，一种高效精确的相机控制学习框架，它无需昂贵的相机轨迹标注即可自动构建高质量的锚点视频。具体而言，我们通过基于首帧可见性来遮罩源视频，从而为训练创建高精度的锚点视频。这种方法确保了高对齐度，消除了相机轨迹标注的需求，因此可以轻松应用于任意“野外”视频以生成图像到视频（I2V）训练对。此外，我们引入了 Anchor-ControlNet，一个轻量级条件模块，它将锚点视频的指导信息集成到预训练的VDMs中，仅使用不到骨干模型参数的1%。通过结合所提出的锚点视频数据和 ControlNet 模块，EPiC 实现了高效训练，显著减少了参数、训练步骤和数据需求，并且无需像通常那样为了减轻渲染不对齐问题而修改扩散模型骨干。尽管在基于遮罩的锚点视频上进行训练，我们的方法在推理时对使用点云制作的锚点视频表现出强大的泛化能力，从而实现了精确的3D感知相机控制。EPiC 在 RealEstate10K 和 MiraData 数据集上的 I2V 相机控制任务中取得了 SOTA 性能，定量和定性地展示了精确而稳健的相机控制能力。值得注意的是，EPiC 还对视频到视频场景表现出极强的零样本泛化能力。",
    "published_time": "2025-05-28T01:45:26.000Z",
    "download_time": "2025-05-29 07:10:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21876.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21876",
      "arxiv_url": "https://arxiv.org/abs/2505.21876"
    }
  },
  {
    "id": "2505.18700",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18700",
    "title": "GRE 套件：基于微调视觉-语言模型和增强推理链的地理定位推断",
    "summary": "视觉语言模型 (VLM) 近期在视觉推理任务中展现出卓越性能。然而，地理定位任务具有独特挑战，需要从图像中提取多粒度视觉线索，并将其与外部世界知识结合以进行系统推理。当前地理定位方法常缺乏鲁棒推理机制和可解释性，限制了其有效性。为解决这些局限，我们提出地理推理增强 (GRE) 套件，这是一个新颖框架，通过结构化推理链增强 VLM，实现准确且可解释的位置推断。GRE 套件在数据集、模型和基准三个关键维度上进行系统开发。首先，我们引入高质量地理定位推理数据集 GRE30K，旨在促进细粒度视觉和上下文分析。其次，我们提出 GRE 模型，采用多阶段推理策略，逐步推断场景属性、局部细节和语义特征，以更高精度缩小潜在地理区域。最后，我们构建地理推理评估基准 (GREval-Bench)，该综合框架评估 VLM 在多样城市、自然和地标场景中的表现，衡量粗粒度（如国家、大陆）和细粒度（如城市、街道）定位性能。实验结果表明，GRE 在所有粒度的地理定位任务上均显著优于现有方法，凸显了推理增强 VLM 在复杂地理推断中的有效性。代码和数据将在 https://github.com/Thorin215/GRE 发布。",
    "keywords": [
      "Geo-localization",
      "Vision-Language Models",
      "Reasoning",
      "GRE30K",
      "GREval-Bench"
    ],
    "area": [
      "多模态",
      "计算机视觉",
      "深度学习"
    ],
    "content": "视觉语言模型 (VLM) 近期在视觉推理任务中展现出卓越性能。然而，地理定位任务具有独特挑战，需要从图像中提取多粒度视觉线索，并将其与外部世界知识结合以进行系统推理。当前地理定位方法常缺乏鲁棒推理机制和可解释性，限制了其有效性。为解决这些局限，我们提出地理推理增强 (GRE) 套件，这是一个新颖框架，通过结构化推理链增强 VLM，实现准确且可解释的位置推断。GRE 套件在数据集、模型和基准三个关键维度上进行系统开发。首先，我们引入高质量地理定位推理数据集 GRE30K，旨在促进细粒度视觉和上下文分析。其次，我们提出 GRE 模型，采用多阶段推理策略，逐步推断场景属性、局部细节和语义特征，以更高精度缩小潜在地理区域。最后，我们构建地理推理评估基准 (GREval-Bench)，该综合框架评估 VLM 在多样城市、自然和地标场景中的表现，衡量粗粒度（如国家、大陆）和细粒度（如城市、街道）定位性能。实验结果表明，GRE 在所有粒度的地理定位任务上均显著优于现有方法，凸显了推理增强 VLM 在复杂地理推断中的有效性。代码和数据将在 https://github.com/Thorin215/GRE 发布。",
    "published_time": "2025-05-24T13:48:57.000Z",
    "download_time": "2025-05-29 07:11:06",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18700.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18700",
      "arxiv_url": "https://arxiv.org/abs/2505.18700"
    }
  },
  {
    "id": "2505.17870",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17870",
    "title": "正如人类需要疫苗，模型亦然：模型免疫以对抗谬误",
    "summary": "生成式AI模型通常会学习并复制其训练语料库中存在的虚假信息。本立场文件认为，类比于生物免疫（通过受控接触弱化的病原体来建立免疫力），AI模型应该在一个小型的、隔离的、包含明确标记的谬误集合上进行微调，以此作为对抗错误信息的“疫苗”。这些精心策划的虚假示例在微调过程中周期性地注入，增强了模型识别和拒绝误导性主张的能力，同时保持了其在真实输入上的准确性。一个说明性案例研究表明，经过免疫处理的模型产生的错误信息显著少于基线模型。据我们所知，这是第一个将经过事实核查的谬误本身视为监督式疫苗的训练框架，而不是依赖于输入扰动或通用的人类反馈信号，旨在增强模型抵抗未来错误信息的能力。我们还概述了伦理保障和治理控制措施，以确保虚假数据的安全使用。模型免疫为使AI系统符合事实提供了一种主动范式。",
    "keywords": [
      "Model Immunization",
      "虚假信息",
      "生成式AI",
      "微调",
      "事实核查谬误"
    ],
    "area": [
      "生成式AI",
      "机器学习",
      "自然语言处理"
    ],
    "content": "生成式AI模型通常会学习并复制其训练语料库中存在的虚假信息。本立场文件认为，类比于生物免疫（通过受控接触弱化的病原体来建立免疫力），AI模型应该在一个小型的、隔离的、包含明确标记的谬误集合上进行微调，以此作为对抗错误信息的“疫苗”。这些精心策划的虚假示例在微调过程中周期性地注入，增强了模型识别和拒绝误导性主张的能力，同时保持了其在真实输入上的准确性。一个说明性案例研究表明，经过免疫处理的模型产生的错误信息显著少于基线模型。据我们所知，这是第一个将经过事实核查的谬误本身视为监督式疫苗的训练框架，而不是依赖于输入扰动或通用的人类反馈信号，旨在增强模型抵抗未来错误信息的能力。我们还概述了伦理保障和治理控制措施，以确保虚假数据的安全使用。模型免疫为使AI系统符合事实提供了一种主动范式。",
    "published_time": "2025-05-23T13:20:23.000Z",
    "download_time": "2025-05-29 07:11:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17870.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17870",
      "arxiv_url": "https://arxiv.org/abs/2505.17870"
    }
  },
  {
    "id": "2505.15813",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.15813",
    "title": "元学习一种人类高级视觉皮层的上下文学习Transformer模型",
    "summary": "理解高级视觉皮层内的功能表征是计算神经科学中的一个基本问题。尽管在大型数据集上预训练的人工神经网络在表征上与人类神经反应表现出显著的一致性，但学习视觉皮层的图像可计算模型依赖于个体层面的大型fMRI数据集。昂贵、耗时且通常不切实际的数据采集的必要性限制了编码器对新受试者和刺激的泛化能力。BraInCoRL利用上下文学习，从少样本示例中预测体素级神经反应，无需对新受试者和刺激进行额外的微调。我们利用一种Transformer架构，该架构可以灵活地基于可变数量的上下文图像刺激进行条件化，并在多个受试者上学习一种归纳偏置。在训练过程中，我们明确地针对上下文学习优化模型。通过联合基于图像特征和体素激活进行条件化，我们的模型学会直接生成性能更好的人类高级视觉皮层体素级模型。我们证明了BraInCoRL在低数据量条件下评估全新图像时，始终优于现有的体素级编码器设计，同时还表现出强大的测试时扩展行为。该模型还可以泛化到一个全新的视觉fMRI数据集，该数据集使用了不同的受试者和fMRI数据采集参数。此外，BraInCoRL通过关注语义相关的刺激，有助于更好地解释高级视觉皮层中的神经信号。最后，我们展示了我们的框架能够实现从自然语言查询到体素选择性的可解释映射。",
    "keywords": [
      "元学习",
      "上下文学习",
      "Transformer",
      "人类高级视觉皮层",
      "fMRI"
    ],
    "area": [
      "机器学习",
      "深度学习",
      "计算机视觉"
    ],
    "content": "理解高级视觉皮层内的功能表征是计算神经科学中的一个基本问题。尽管在大型数据集上预训练的人工神经网络在表征上与人类神经反应表现出显著的一致性，但学习视觉皮层的图像可计算模型依赖于个体层面的大型fMRI数据集。昂贵、耗时且通常不切实际的数据采集的必要性限制了编码器对新受试者和刺激的泛化能力。BraInCoRL利用上下文学习，从少样本示例中预测体素级神经反应，无需对新受试者和刺激进行额外的微调。我们利用一种Transformer架构，该架构可以灵活地基于可变数量的上下文图像刺激进行条件化，并在多个受试者上学习一种归纳偏置。在训练过程中，我们明确地针对上下文学习优化模型。通过联合基于图像特征和体素激活进行条件化，我们的模型学会直接生成性能更好的人类高级视觉皮层体素级模型。我们证明了BraInCoRL在低数据量条件下评估全新图像时，始终优于现有的体素级编码器设计，同时还表现出强大的测试时扩展行为。该模型还可以泛化到一个全新的视觉fMRI数据集，该数据集使用了不同的受试者和fMRI数据采集参数。此外，BraInCoRL通过关注语义相关的刺激，有助于更好地解释高级视觉皮层中的神经信号。最后，我们展示了我们的框架能够实现从自然语言查询到体素选择性的可解释映射。",
    "published_time": "2025-05-21T17:59:41.000Z",
    "download_time": "2025-05-29 07:11:43",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15813.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.15813",
      "arxiv_url": "https://arxiv.org/abs/2505.15813"
    }
  },
  {
    "id": "2505.21960",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21960",
    "title": "单程票：用于蒸馏文本到图像扩散模型的时间独立统一编码器",
    "summary": "文本到图像 (T2I) 扩散模型在生成建模方面取得了显著进展；然而，它们面临推理速度和图像质量之间的权衡，给高效部署带来了挑战。现有的蒸馏T2I模型虽然能以更少的采样步生成高保真图像，但往往在多样性和质量方面表现不佳，特别是在一步模型中。通过我们的分析，我们观察到UNet编码器存在冗余计算。我们的研究结果表明，对于T2I扩散模型而言，解码器更善于捕捉更丰富、更显式的语义信息，而编码器可以有效地在来自不同时间步的解码器之间共享。基于这些观察，我们引入了第一个时间独立统一编码器 TiUE，用于学生模型UNet架构，这是一种用于蒸馏T2I扩散模型的无循环图像生成方法。通过单次通过方案，TiUE在多个解码器时间步之间共享编码器特征，从而实现并行采样并显著降低推理时间复杂度。此外，我们引入了一个KL散度项来正则化噪声预测，从而增强生成图像的感知真实性和多样性。实验结果表明，TiUE优于现有的最先进方法，包括LCM、SD-Turbo和SwiftBrushv2，在保持计算效率的同时，产生了更多样化和更真实的结果。",
    "keywords": [
      "T2I扩散模型",
      "模型蒸馏",
      "高效推理",
      "时间独立统一编码器 (TiUE)",
      "单次采样"
    ],
    "area": [
      "生成式AI",
      "深度学习",
      "计算机视觉"
    ],
    "content": "文本到图像 (T2I) 扩散模型在生成建模方面取得了显著进展；然而，它们面临推理速度和图像质量之间的权衡，给高效部署带来了挑战。现有的蒸馏T2I模型虽然能以更少的采样步生成高保真图像，但往往在多样性和质量方面表现不佳，特别是在一步模型中。通过我们的分析，我们观察到UNet编码器存在冗余计算。我们的研究结果表明，对于T2I扩散模型而言，解码器更善于捕捉更丰富、更显式的语义信息，而编码器可以有效地在来自不同时间步的解码器之间共享。基于这些观察，我们引入了第一个时间独立统一编码器 TiUE，用于学生模型UNet架构，这是一种用于蒸馏T2I扩散模型的无循环图像生成方法。通过单次通过方案，TiUE在多个解码器时间步之间共享编码器特征，从而实现并行采样并显著降低推理时间复杂度。此外，我们引入了一个KL散度项来正则化噪声预测，从而增强生成图像的感知真实性和多样性。实验结果表明，TiUE优于现有的最先进方法，包括LCM、SD-Turbo和SwiftBrushv2，在保持计算效率的同时，产生了更多样化和更真实的结果。",
    "published_time": "2025-05-28T04:23:22.000Z",
    "download_time": "2025-05-29 07:11:54",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21960.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21960",
      "arxiv_url": "https://arxiv.org/abs/2505.21960"
    }
  },
  {
    "id": "2505.21191",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21191",
    "title": "揭示指令专用神经元与专家：大型语言模型指令遵循能力的分析框架",
    "summary": "大型语言模型（LLMs）的微调显著提升了其遵循指令的能力，然而驱动这些改进的底层计算机制仍不为人知。本研究通过分离和分析指令专用的稀疏组件，即密集模型中的神经元以及专家混合模型（MoE）架构中的神经元和专家，系统地考察了微调如何重新配置LLM的计算过程。特别地，我们引入了HexaInst，一个精心策划且平衡的、涵盖六个不同类别的指令数据集；并提出了SPARCOM，一个新颖的分析框架，包含三个主要贡献：（1）一种识别这些稀疏组件的方法，（2）评估其功能通用性和唯一性，以及（3）系统比较其变化。通过实验，我们证明了这些组件的功能通用性、唯一性及其在指令执行中的关键作用。通过阐明微调引起的适应性与稀疏计算基底之间的关系，这项工作为可信赖的大型语言模型社区深入了解LLMs如何内化指令遵循行为提供了深刻见解。",
    "keywords": [
      "大模型",
      "指令遵循",
      "微调",
      "稀疏组件",
      "分析框架"
    ],
    "area": [
      "大模型",
      "深度学习",
      "自然语言处理"
    ],
    "content": "大型语言模型（LLMs）的微调显著提升了其遵循指令的能力，然而驱动这些改进的底层计算机制仍不为人知。本研究通过分离和分析指令专用的稀疏组件，即密集模型中的神经元以及专家混合模型（MoE）架构中的神经元和专家，系统地考察了微调如何重新配置LLM的计算过程。特别地，我们引入了HexaInst，一个精心策划且平衡的、涵盖六个不同类别的指令数据集；并提出了SPARCOM，一个新颖的分析框架，包含三个主要贡献：（1）一种识别这些稀疏组件的方法，（2）评估其功能通用性和唯一性，以及（3）系统比较其变化。通过实验，我们证明了这些组件的功能通用性、唯一性及其在指令执行中的关键作用。通过阐明微调引起的适应性与稀疏计算基底之间的关系，这项工作为可信赖的大型语言模型社区深入了解LLMs如何内化指令遵循行为提供了深刻见解。",
    "published_time": "2025-05-27T13:40:28.000Z",
    "download_time": "2025-05-29 07:12:08",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21191.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21191",
      "arxiv_url": "https://arxiv.org/abs/2505.21191"
    }
  },
  {
    "id": "2505.20715",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20715",
    "title": "MUSEG：通过时间戳感知多片段对齐增强视频时间理解",
    "summary": "视频时间理解对于多模态大型语言模型（MLLMs）推理视频中的事件至关重要。尽管通用视频理解取得了最新进展，但当前的多模态大模型在细粒度时间推理方面仍然存在困难。尽管最近探索了强化学习（RL）来解决此问题，但现有的强化学习方法效果仍然有限。在这项工作中，我们提出了 MUSEG，一种新颖的基于强化学习的方法，通过引入时间戳感知多片段对齐来增强时间理解。MUSEG 使多模态大模型能够将查询与多个相关的视频片段对齐，从而促进更全面的时间推理。为了促进有效学习，我们设计了一种定制的强化学习训练策略，采用分阶段奖励，逐步引导模型进行时间定位推理。在时间对齐和时间敏感的视频问答任务上的大量实验表明，MUSEG 显著优于现有方法，并在不同的时间理解场景中表现出良好的泛化能力。",
    "keywords": [
      "多片段对齐",
      "视频时间理解",
      "MLLMs",
      "强化学习",
      "时间推理"
    ],
    "area": [
      "多模态",
      "大模型",
      "视频理解"
    ],
    "content": "视频时间理解对于多模态大型语言模型（MLLMs）推理视频中的事件至关重要。尽管通用视频理解取得了最新进展，但当前的多模态大模型在细粒度时间推理方面仍然存在困难。尽管最近探索了强化学习（RL）来解决此问题，但现有的强化学习方法效果仍然有限。在这项工作中，我们提出了 MUSEG，一种新颖的基于强化学习的方法，通过引入时间戳感知多片段对齐来增强时间理解。MUSEG 使多模态大模型能够将查询与多个相关的视频片段对齐，从而促进更全面的时间推理。为了促进有效学习，我们设计了一种定制的强化学习训练策略，采用分阶段奖励，逐步引导模型进行时间定位推理。在时间对齐和时间敏感的视频问答任务上的大量实验表明，MUSEG 显著优于现有方法，并在不同的时间理解场景中表现出良好的泛化能力。",
    "published_time": "2025-05-27T04:50:07.000Z",
    "download_time": "2025-05-29 07:12:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20715.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20715",
      "arxiv_url": "https://arxiv.org/abs/2505.20715"
    }
  },
  {
    "id": "2505.17507",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17507",
    "title": "基于Hugging Face知识图谱的推荐、分类与追踪基准测试",
    "summary": "开源机器学习(ML)资源的快速增长，如模型和数据集，加速了信息检索(IR)研究。然而，Hugging Face等现有平台并未明确利用结构化表示，限制了诸如追踪模型演变、推荐相关数据集等高级查询和分析。为了弥补这一空白，我们构建了HuggingKG，这是首个为ML资源管理而从Hugging Face社区构建的大规模知识图谱。HuggingKG拥有260万个节点和620万条边，捕获了领域特定关系和丰富的文本属性。基于此，我们进一步提出了HuggingBench，一个包含三个新颖测试集的多任务基准，用于IR任务，包括资源推荐、分类和追踪。我们的实验揭示了HuggingKG和衍生任务的独特特性。这两个资源均已公开发布，有望推动开源资源共享与管理领域的研究。",
    "keywords": [
      "Hugging Face",
      "知识图谱",
      "基准测试",
      "推荐",
      "分类"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "自然语言处理"
    ],
    "content": "开源机器学习(ML)资源的快速增长，如模型和数据集，加速了信息检索(IR)研究。然而，Hugging Face等现有平台并未明确利用结构化表示，限制了诸如追踪模型演变、推荐相关数据集等高级查询和分析。为了弥补这一空白，我们构建了HuggingKG，这是首个为ML资源管理而从Hugging Face社区构建的大规模知识图谱。HuggingKG拥有260万个节点和620万条边，捕获了领域特定关系和丰富的文本属性。基于此，我们进一步提出了HuggingBench，一个包含三个新颖测试集的多任务基准，用于IR任务，包括资源推荐、分类和追踪。我们的实验揭示了HuggingKG和衍生任务的独特特性。这两个资源均已公开发布，有望推动开源资源共享与管理领域的研究。",
    "published_time": "2025-05-23T06:00:20.000Z",
    "download_time": "2025-05-29 07:12:37",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17507.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17507",
      "arxiv_url": "https://arxiv.org/abs/2505.17507"
    }
  },
  {
    "id": "2505.12667",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.12667",
    "title": "Safe-Sora：基于图形水印的安全文本到视频生成",
    "summary": "生成式视频模型的爆炸性增长加剧了对人工智能生成内容可靠版权保护的需求。尽管隐形生成水印在图像合成领域广受欢迎，但在视频生成领域的研究却相对不足。为弥合这一空白，我们提出了 Safe-Sora，这是首个将图形水印直接嵌入视频生成过程的框架。基于水印性能与水印和载体内容（即视频）之间视觉相似度密切相关的观察，我们引入了一种层级式由粗到精的自适应匹配机制。具体而言，水印图像被分割成块，每个块被分配给最相似的视频帧，并进一步定位到最佳空间区域以实现无缝嵌入。为了实现水印块在视频帧间的时空融合，我们开发了一种结合 3D 小波变换增强的 Mamba 架构，并采用了新颖的时空局部扫描策略，在水印嵌入和提取过程中有效建模长程依赖关系。据我们所知，这是首次将状态空间模型应用于水印技术，为高效且鲁棒的水印保护开辟了新途径。广泛的实验表明，Safe-Sora 在视频质量、水印保真度及鲁棒性方面取得了最先进的性能，这主要归功于我们提出的方法。我们的代码将在论文发表后发布。",
    "keywords": [
      "Safe-Sora",
      "图形水印",
      "生成式视频",
      "Mamba 架构",
      "鲁棒性"
    ],
    "area": [
      "生成式AI",
      "计算机视觉",
      "多模态"
    ],
    "content": "生成式视频模型的爆炸性增长加剧了对人工智能生成内容可靠版权保护的需求。尽管隐形生成水印在图像合成领域广受欢迎，但在视频生成领域的研究却相对不足。为弥合这一空白，我们提出了 Safe-Sora，这是首个将图形水印直接嵌入视频生成过程的框架。基于水印性能与水印和载体内容（即视频）之间视觉相似度密切相关的观察，我们引入了一种层级式由粗到精的自适应匹配机制。具体而言，水印图像被分割成块，每个块被分配给最相似的视频帧，并进一步定位到最佳空间区域以实现无缝嵌入。为了实现水印块在视频帧间的时空融合，我们开发了一种结合 3D 小波变换增强的 Mamba 架构，并采用了新颖的时空局部扫描策略，在水印嵌入和提取过程中有效建模长程依赖关系。据我们所知，这是首次将状态空间模型应用于水印技术，为高效且鲁棒的水印保护开辟了新途径。广泛的实验表明，Safe-Sora 在视频质量、水印保真度及鲁棒性方面取得了最先进的性能，这主要归功于我们提出的方法。我们的代码将在论文发表后发布。",
    "published_time": "2025-05-19T03:31:31.000Z",
    "download_time": "2025-05-29 07:12:56",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12667.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.12667",
      "arxiv_url": "https://arxiv.org/abs/2505.12667"
    }
  },
  {
    "id": "2505.22645",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22645",
    "title": "表征偏见：简体中文与繁体中文大语言模型基准测试",
    "summary": "尽管大语言模型（LLM）在简体中文和繁体中文环境下的能力已有所研究，但当使用这两种书面中文变体进行提示时，LLM是否表现出差异化的性能尚不明确。了解这一点至关重要，因为LLM响应质量的差异可能会忽略简体中文与繁体中文背后不同的文化背景，从而持续存在表征性危害，并可能加剧LLM驱动的决策（例如教育或招聘领域）所带来的下游危害。为了探究潜在的LLM性能差异，我们设计了两个反映真实场景的基准任务：区域术语选择（提示LLM命名在大陆和台湾地区称谓不同的物品）和区域名称选择（提示LLM从包含简体和繁体中文姓名列表中选择招聘对象）。在这两个任务中，我们评估了11种领先的商业LLM服务和开源模型的性能——这些模型主要训练于英文、简体中文或繁体中文。我们的分析表明，LLM响应中的偏见取决于任务和提示语言：在区域术语选择任务中，大多数LLM不成比例地偏向于简体中文响应；而在区域名称选择任务中，它们却出人意料地偏向于繁体中文姓名。我们发现，这些差异可能源于训练数据表示、书写字符偏好以及简体中文和繁体中文的分词（tokenization）方式的不同。这些发现强调了进一步分析LLM偏见的必要性；为此，我们提供了一个开源的基准数据集，以促进未来LLM在中国语言变体中的行为进行可复现的评估（https://github.com/brucelyu17/SC-TC-Bench）。",
    "keywords": [
      "大语言模型",
      "简体中文",
      "繁体中文",
      "偏见",
      "基准测试"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "人工智能"
    ],
    "content": "尽管大语言模型（LLM）在简体中文和繁体中文环境下的能力已有所研究，但当使用这两种书面中文变体进行提示时，LLM是否表现出差异化的性能尚不明确。了解这一点至关重要，因为LLM响应质量的差异可能会忽略简体中文与繁体中文背后不同的文化背景，从而持续存在表征性危害，并可能加剧LLM驱动的决策（例如教育或招聘领域）所带来的下游危害。为了探究潜在的LLM性能差异，我们设计了两个反映真实场景的基准任务：区域术语选择（提示LLM命名在大陆和台湾地区称谓不同的物品）和区域名称选择（提示LLM从包含简体和繁体中文姓名列表中选择招聘对象）。在这两个任务中，我们评估了11种领先的商业LLM服务和开源模型的性能——这些模型主要训练于英文、简体中文或繁体中文。我们的分析表明，LLM响应中的偏见取决于任务和提示语言：在区域术语选择任务中，大多数LLM不成比例地偏向于简体中文响应；而在区域名称选择任务中，它们却出人意料地偏向于繁体中文姓名。我们发现，这些差异可能源于训练数据表示、书写字符偏好以及简体中文和繁体中文的分词（tokenization）方式的不同。这些发现强调了进一步分析LLM偏见的必要性；为此，我们提供了一个开源的基准数据集，以促进未来LLM在中国语言变体中的行为进行可复现的评估（https://github.com/brucelyu17/SC-TC-Bench）。",
    "published_time": "2025-05-28T17:56:49.000Z",
    "download_time": "2025-05-29 07:13:12",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22645.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22645",
      "arxiv_url": "https://arxiv.org/abs/2505.22645"
    }
  },
  {
    "id": "2505.21582",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21582",
    "title": "AITEE -- 电气工程智能体导师",
    "summary": "智能辅导系统结合大型语言模型为满足学生的多元化需求和促进自主高效学习提供了一种有前景的方法。尽管大型语言模型具备良好的电气工程基础知识，但在解决与电路相关的具体问题方面能力仍显不足。在本文中，我们介绍了 AITEE，一个基于智能体的电气工程辅导系统，旨在全程陪伴学生学习过程，提供个性化支持，并促进自主学习。AITEE 通过一种改进的电路重建过程支持手绘和数字电路，从而实现与学生的自然交互。我们新颖的基于图的相似性度量通过检索增强生成方法从讲义中识别相关上下文，而并行 Spice 仿真进一步提高了应用解题方法的准确性。该系统采用苏格拉底式对话来通过引导式提问培养学习者的自主性。实验评估表明，AITEE 在领域特定知识应用方面显著优于基线方法，即使是中等规模的 LLM 模型也表现出可接受的性能。我们的结果突出了智能体导师在电气工程教育中提供可扩展、个性化和有效学习环境的潜力。",
    "keywords": [
      "智能辅导系统",
      "大模型",
      "智能体",
      "电气工程",
      "检索增强生成 (RAG)"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "content": "智能辅导系统结合大型语言模型为满足学生的多元化需求和促进自主高效学习提供了一种有前景的方法。尽管大型语言模型具备良好的电气工程基础知识，但在解决与电路相关的具体问题方面能力仍显不足。在本文中，我们介绍了 AITEE，一个基于智能体的电气工程辅导系统，旨在全程陪伴学生学习过程，提供个性化支持，并促进自主学习。AITEE 通过一种改进的电路重建过程支持手绘和数字电路，从而实现与学生的自然交互。我们新颖的基于图的相似性度量通过检索增强生成方法从讲义中识别相关上下文，而并行 Spice 仿真进一步提高了应用解题方法的准确性。该系统采用苏格拉底式对话来通过引导式提问培养学习者的自主性。实验评估表明，AITEE 在领域特定知识应用方面显著优于基线方法，即使是中等规模的 LLM 模型也表现出可接受的性能。我们的结果突出了智能体导师在电气工程教育中提供可扩展、个性化和有效学习环境的潜力。",
    "published_time": "2025-05-27T10:07:05.000Z",
    "download_time": "2025-05-29 07:13:28",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21582.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21582",
      "arxiv_url": "https://arxiv.org/abs/2505.21582"
    }
  },
  {
    "id": "2505.20444",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20444",
    "title": "HoPE：用于视觉-语言模型长度泛化的混合位置编码",
    "summary": "视觉-语言模型（VLM）在多模态任务中取得了显著进展。然而，在长上下文场景，特别是长视频中，其性能常常下降。尽管旋转位置编码（RoPE）已广泛用于大型语言模型（LLM）的长度泛化，但将传统的RoPE扩展以捕获视频中复杂的时空依赖性仍然是一个未解决的挑战。现有方法通常在RoPE内分配不同频率来编码3D位置信息。然而，这些分配策略主要依靠启发式方法，缺乏深入的理论分析。在本文中，我们首先研究了不同的分配策略如何影响VLM的长上下文能力。我们的分析表明，当前的多模态RoPE无法在扩展上下文中可靠地捕获语义相似性。为了解决这个问题，我们提出了HoPE，一种混合位置编码，旨在提高VLM的长上下文能力。HoPE引入了一种混合频率分配策略，用于在任意长上下文上进行可靠的语义建模，以及一种动态时间尺度机制，以促进在不同上下文长度下的鲁棒学习和灵活推理。在四个视频基准上进行的关于长视频理解和检索任务的广泛实验表明，HoPE持续优于现有方法，证实了其有效性。代码可在 https://github.com/hrlics/HoPE 获取。",
    "keywords": [
      "Vision-Language Models",
      "Length Generalization",
      "HoPE",
      "Long Video Understanding",
      "Position Embedding"
    ],
    "area": [
      "深度学习",
      "多模态",
      "视频理解"
    ],
    "content": "视觉-语言模型（VLM）在多模态任务中取得了显著进展。然而，在长上下文场景，特别是长视频中，其性能常常下降。尽管旋转位置编码（RoPE）已广泛用于大型语言模型（LLM）的长度泛化，但将传统的RoPE扩展以捕获视频中复杂的时空依赖性仍然是一个未解决的挑战。现有方法通常在RoPE内分配不同频率来编码3D位置信息。然而，这些分配策略主要依靠启发式方法，缺乏深入的理论分析。在本文中，我们首先研究了不同的分配策略如何影响VLM的长上下文能力。我们的分析表明，当前的多模态RoPE无法在扩展上下文中可靠地捕获语义相似性。为了解决这个问题，我们提出了HoPE，一种混合位置编码，旨在提高VLM的长上下文能力。HoPE引入了一种混合频率分配策略，用于在任意长上下文上进行可靠的语义建模，以及一种动态时间尺度机制，以促进在不同上下文长度下的鲁棒学习和灵活推理。在四个视频基准上进行的关于长视频理解和检索任务的广泛实验表明，HoPE持续优于现有方法，证实了其有效性。代码可在 https://github.com/hrlics/HoPE 获取。",
    "published_time": "2025-05-26T18:37:40.000Z",
    "download_time": "2025-05-29 07:13:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20444.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20444",
      "arxiv_url": "https://arxiv.org/abs/2505.20444"
    }
  },
  {
    "id": "2505.20298",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20298",
    "title": "MangaVQA 和 MangaLMM: 一个用于多模态漫画理解的基准和专用模型",
    "summary": "漫画，即日本漫画，是一种丰富的多模态叙事形式，它以复杂的方式融合了图像和文本。教导大型多模态模型（LMMs）达到人类水平理解此类叙事，有助于漫画创作者反思和改进他们的故事。为此，我们引入了两个用于多模态漫画理解的基准：MangaOCR，用于页面内文本识别；以及 MangaVQA，一个旨在通过视觉问答评估上下文理解的新基准。MangaVQA 包含 526 对高质量、手动构建的问答对，可在不同的叙事和视觉情境下实现可靠评估。基于这些基准，我们开发了 MangaLMM，一个从开源 LMM Qwen2.5-VL 进行微调的、漫画专用的模型，用以联合处理这两项任务。通过广泛的实验，包括与 GPT-4o 和 Gemini 2.5 等专有模型的比较，我们评估了 LMMs 理解漫画的能力。我们的基准和模型为评估和推进 LMMs 在漫画这一丰富的叙事领域提供了全面的基础。",
    "keywords": [
      "多模态",
      "漫画理解",
      "LMMs",
      "基准",
      "视觉问答"
    ],
    "area": [
      "人工智能",
      "多模态",
      "大模型"
    ],
    "content": "漫画，即日本漫画，是一种丰富的多模态叙事形式，它以复杂的方式融合了图像和文本。教导大型多模态模型（LMMs）达到人类水平理解此类叙事，有助于漫画创作者反思和改进他们的故事。为此，我们引入了两个用于多模态漫画理解的基准：MangaOCR，用于页面内文本识别；以及 MangaVQA，一个旨在通过视觉问答评估上下文理解的新基准。MangaVQA 包含 526 对高质量、手动构建的问答对，可在不同的叙事和视觉情境下实现可靠评估。基于这些基准，我们开发了 MangaLMM，一个从开源 LMM Qwen2.5-VL 进行微调的、漫画专用的模型，用以联合处理这两项任务。通过广泛的实验，包括与 GPT-4o 和 Gemini 2.5 等专有模型的比较，我们评估了 LMMs 理解漫画的能力。我们的基准和模型为评估和推进 LMMs 在漫画这一丰富的叙事领域提供了全面的基础。",
    "published_time": "2025-05-26T17:59:59.000Z",
    "download_time": "2025-05-29 07:14:00",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20298.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20298",
      "arxiv_url": "https://arxiv.org/abs/2505.20298"
    }
  },
  {
    "id": "2505.19051",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19051",
    "title": "通过影响力蒸馏实现大规模高效数据选择",
    "summary": "有效的数据选择对于现代大型语言模型 (LLMs) 的高效训练至关重要。本文介绍了影响力蒸馏（Influence Distillation），这是一种新颖且具有数学依据的数据选择框架，它利用二阶信息来最优地加权训练样本。通过蒸馏每个样本对目标分布的影响，我们的方法分配模型特定的权重，这些权重用于选择 LLM 微调的训练数据，从而引导模型在目标领域实现优异性能。我们推导了针对梯度下降（Gradient Descent）和 Adam 优化器的最优权重。为了确保可扩展性并降低计算成本，我们提出了一种基于地标的近似方法：精确计算一小部分“地标”样本的影响，然后将其高效地传播到所有其他样本，以确定它们的权重。我们通过将影响力蒸馏应用于 Tulu V2 数据集上的指令微调来验证其有效性，目标任务包括 GSM8k、SQuAD 和 MMLU，涵盖了 Llama 和 Qwen 系列的多种模型。实验表明，影响力蒸馏在实现高达 3.5 倍更快的选择速度的同时，能达到或超越现有最佳性能。",
    "keywords": [
      "大型语言模型",
      "数据选择",
      "影响力蒸馏",
      "模型微调",
      "高效训练"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "深度学习"
    ],
    "content": "有效的数据选择对于现代大型语言模型 (LLMs) 的高效训练至关重要。本文介绍了影响力蒸馏（Influence Distillation），这是一种新颖且具有数学依据的数据选择框架，它利用二阶信息来最优地加权训练样本。通过蒸馏每个样本对目标分布的影响，我们的方法分配模型特定的权重，这些权重用于选择 LLM 微调的训练数据，从而引导模型在目标领域实现优异性能。我们推导了针对梯度下降（Gradient Descent）和 Adam 优化器的最优权重。为了确保可扩展性并降低计算成本，我们提出了一种基于地标的近似方法：精确计算一小部分“地标”样本的影响，然后将其高效地传播到所有其他样本，以确定它们的权重。我们通过将影响力蒸馏应用于 Tulu V2 数据集上的指令微调来验证其有效性，目标任务包括 GSM8k、SQuAD 和 MMLU，涵盖了 Llama 和 Qwen 系列的多种模型。实验表明，影响力蒸馏在实现高达 3.5 倍更快的选择速度的同时，能达到或超越现有最佳性能。",
    "published_time": "2025-05-25T09:08:00.000Z",
    "download_time": "2025-05-29 07:14:14",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19051.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19051",
      "arxiv_url": "https://arxiv.org/abs/2505.19051"
    }
  },
  {
    "id": "2505.21060",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21060",
    "title": "Styl3R：任意场景和风格的即时3D风格化重建",
    "summary": "即时对3D场景进行风格化，同时保持多视角一致性并忠实地再现风格图像，仍然是一个重大挑战。当前最先进的3D风格化方法通常涉及计算密集型的测试时优化，以便将艺术特征迁移到预训练的3D表示中，并且通常需要密集的带姿态输入图像。相比之下，利用前向重建模型的最新进展，我们展示了一种新颖的方法，可以使用无姿态的稀疏视角场景图像和任意风格图像在不到一秒钟内实现直接3D风格化。为了解决重建和风格化之间固有的解耦问题，我们引入了一种分支架构，将结构建模和外观着色分离，有效防止风格迁移扭曲底层的3D场景结构。此外，我们采用了一种恒等损失（identity loss），以便通过新颖视角合成任务来促进我们风格化模型的预训练。这种策略还使得我们的模型在针对风格化进行微调的同时，能够保留其原始的重建能力。使用域内和域外数据集进行的全面评估表明，我们的方法能够生成高质量的风格化3D内容，实现了风格和场景外观的优越融合，同时在多视角一致性和效率方面也优于现有方法。",
    "keywords": [
      "3D Stylization",
      "即时",
      "Multi-view consistency",
      "3D Reconstruction",
      "前向模型"
    ],
    "area": [
      "计算机视觉",
      "深度学习",
      "生成式AI"
    ],
    "content": "即时对3D场景进行风格化，同时保持多视角一致性并忠实地再现风格图像，仍然是一个重大挑战。当前最先进的3D风格化方法通常涉及计算密集型的测试时优化，以便将艺术特征迁移到预训练的3D表示中，并且通常需要密集的带姿态输入图像。相比之下，利用前向重建模型的最新进展，我们展示了一种新颖的方法，可以使用无姿态的稀疏视角场景图像和任意风格图像在不到一秒钟内实现直接3D风格化。为了解决重建和风格化之间固有的解耦问题，我们引入了一种分支架构，将结构建模和外观着色分离，有效防止风格迁移扭曲底层的3D场景结构。此外，我们采用了一种恒等损失（identity loss），以便通过新颖视角合成任务来促进我们风格化模型的预训练。这种策略还使得我们的模型在针对风格化进行微调的同时，能够保留其原始的重建能力。使用域内和域外数据集进行的全面评估表明，我们的方法能够生成高质量的风格化3D内容，实现了风格和场景外观的优越融合，同时在多视角一致性和效率方面也优于现有方法。",
    "published_time": "2025-05-27T11:47:15.000Z",
    "download_time": "2025-05-29 07:14:32",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21060.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21060",
      "arxiv_url": "https://arxiv.org/abs/2505.21060"
    }
  },
  {
    "id": "2505.18149",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18149",
    "title": "先完成搜索：大型语言模型中的高效测试时缩放",
    "summary": "测试时缩放（TTS）涉及在推理过程中动态分配计算资源，提供了一种改进大型语言模型推理能力的有前景的方法。尽管现有的 TTS 方法效果不错，但它们通常依赖于长解码路径或需要生成大量样本，这增加了 token 用量和推理延迟。我们观察到一个令人惊讶的事实：对于推理任务，较短的轨迹（traces）比更长的轨迹更有可能正确。受此启发，我们引入了先完成搜索（FFS），这是一种无需训练的并行解码策略，它并行启动 n 个独立样本，并在任何一个完成时立即返回。我们在四种推理模型（DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B 和 Phi-4-Reasoning-Plus）和四个数据集（AIME24, AIME25-I, AIME25-II 和 GPQA Diamond）上，将 FFS 与简单解码、集束搜索、多数投票和预算强制等方法进行了评估。使用 DeepSeek-R1 模型时，FFS 在 AIME 数据集上达到了 82.23% 的准确率，比 DeepSeek-R1 的独立准确率提高了 15%，几乎与 OpenAI 的 o4-mini 性能相当。我们的理论分析解释了为什么在最短轨迹处停止更可能获得正确答案，并指出了早期停止可能不是最优的情况。FFS 的优雅和简洁表明，简单有效的 TTS 策略也能表现出色，揭示了推理时简单方法的巨大潜力。",
    "keywords": [
      "Test-Time Scaling",
      "First Finish Search",
      "Parallel Decoding",
      "大语言模型",
      "推理"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "人工智能"
    ],
    "content": "测试时缩放（TTS）涉及在推理过程中动态分配计算资源，提供了一种改进大型语言模型推理能力的有前景的方法。尽管现有的 TTS 方法效果不错，但它们通常依赖于长解码路径或需要生成大量样本，这增加了 token 用量和推理延迟。我们观察到一个令人惊讶的事实：对于推理任务，较短的轨迹（traces）比更长的轨迹更有可能正确。受此启发，我们引入了先完成搜索（FFS），这是一种无需训练的并行解码策略，它并行启动 n 个独立样本，并在任何一个完成时立即返回。我们在四种推理模型（DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B 和 Phi-4-Reasoning-Plus）和四个数据集（AIME24, AIME25-I, AIME25-II 和 GPQA Diamond）上，将 FFS 与简单解码、集束搜索、多数投票和预算强制等方法进行了评估。使用 DeepSeek-R1 模型时，FFS 在 AIME 数据集上达到了 82.23% 的准确率，比 DeepSeek-R1 的独立准确率提高了 15%，几乎与 OpenAI 的 o4-mini 性能相当。我们的理论分析解释了为什么在最短轨迹处停止更可能获得正确答案，并指出了早期停止可能不是最优的情况。FFS 的优雅和简洁表明，简单有效的 TTS 策略也能表现出色，揭示了推理时简单方法的巨大潜力。",
    "published_time": "2025-05-23T17:57:43.000Z",
    "download_time": "2025-05-29 07:14:44",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18149.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18149",
      "arxiv_url": "https://arxiv.org/abs/2505.18149"
    }
  },
  {
    "id": "2505.18227",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18227",
    "title": "令牌缩减应超越生成模型中的效率考量 -- 从视觉、语言到多模态",
    "summary": "在Transformer架构中，tokens（记号）——源自原始数据的离散单元——通过将输入分割成定长块来形成。每个记号随后被映射到一个嵌入向量，从而在保留输入基本信息的同时，实现并行的注意力计算。由于Transformer自注意力机制的二次方计算复杂度，记号缩减主要被用作一种效率策略。这在单一的视觉和语言领域尤为如此，它有助于平衡计算成本、内存使用和推理延迟。尽管取得了这些进展，本文认为在大型生成模型时代，记号缩减应超越其传统的效率导向作用。相反，我们将其定位为生成建模中的一个基本原则，对模型架构和更广泛的应用产生关键影响。具体而言，我们主张在跨越视觉、语言和多模态系统中，记号缩减能够：(i) 促进更深度的多模态融合与对齐，(ii) 减轻“过度思考”和幻觉，(iii) 在长输入序列中保持连贯性，以及(iv) 增强训练稳定性等。我们将记号缩减重新定义为不仅仅是一种效率衡量。通过这样做，我们概述了有前景的未来方向，包括算法设计、强化学习引导的记号缩减、用于上下文学习的记号优化，以及更广泛的机器学习和科学领域。我们强调了它在推动新的模型架构和学习策略方面的潜力，这些架构和策略能够提高鲁棒性、增强可解释性，并更好地与生成建模的目标对齐。",
    "keywords": [
      "令牌缩减",
      "生成模型",
      "Transformer",
      "多模态",
      "基本原则"
    ],
    "area": [
      "深度学习",
      "多模态",
      "生成式AI"
    ],
    "content": "在Transformer架构中，tokens（记号）——源自原始数据的离散单元——通过将输入分割成定长块来形成。每个记号随后被映射到一个嵌入向量，从而在保留输入基本信息的同时，实现并行的注意力计算。由于Transformer自注意力机制的二次方计算复杂度，记号缩减主要被用作一种效率策略。这在单一的视觉和语言领域尤为如此，它有助于平衡计算成本、内存使用和推理延迟。尽管取得了这些进展，本文认为在大型生成模型时代，记号缩减应超越其传统的效率导向作用。相反，我们将其定位为生成建模中的一个基本原则，对模型架构和更广泛的应用产生关键影响。具体而言，我们主张在跨越视觉、语言和多模态系统中，记号缩减能够：(i) 促进更深度的多模态融合与对齐，(ii) 减轻“过度思考”和幻觉，(iii) 在长输入序列中保持连贯性，以及(iv) 增强训练稳定性等。我们将记号缩减重新定义为不仅仅是一种效率衡量。通过这样做，我们概述了有前景的未来方向，包括算法设计、强化学习引导的记号缩减、用于上下文学习的记号优化，以及更广泛的机器学习和科学领域。我们强调了它在推动新的模型架构和学习策略方面的潜力，这些架构和策略能够提高鲁棒性、增强可解释性，并更好地与生成建模的目标对齐。",
    "published_time": "2025-05-23T11:30:30.000Z",
    "download_time": "2025-05-29 07:15:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18227.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18227",
      "arxiv_url": "https://arxiv.org/abs/2505.18227"
    }
  }
]