[
  {
    "id": "Twitterd727e084-f40d-4373-93c4-24a641d96f51",
    "source": "Twitter",
    "url": "https://x.com/omarsar0/status/1927368367075197179",
    "title": "Agents Basics",
    "content": "Agents Basics It's super simple to create an agent by giving it a description, name, instructions, and tools.",
    "summary": "代理基础知识:创建代理非常简单，只需提供描述、名称、说明和工具。",
    "keywords": "代理,创建,描述,工具,说明",
    "area": "人工智能,多模态,智能体",
    "published_time": "2025-05-27T22:16:00Z",
    "download_time": "2023-10-03 10:56:00",
    "visual_resource": [
      "screenshot/twitter_Twitterd727e084-f40d-4373-93c4-24a641d96f51.png"
    ]
  },
  {
    "id": "Twittercd08f6e0-b7ce-406a-bf37-c7a885e4697f",
    "source": "Twitter",
    "url": "https://x.com/omarsar0/status/1927372457578483828",
    "title": "Handoff",
    "content": "The handoff feature enables agents to call other agents to complete tasks or hand over a conversation mid-action. Handoff enables complex multi-agent orchestration to support even more advanced and capable agentic systems. Lots of great cookbooks and agent examples",
    "summary": "交接功能允许代理在执行任务中途呼叫其他代理完成任务或移交会话。交接还能实现复杂多代理编排,支持更先进和强大的代理系统。",
    "keywords": "交接功能,代理,多代理编排,会话移交,代理系统",
    "area": "多模态,智能体,生成式AI",
    "published_time": "",
    "download_time": "2023-10-11 13:23:45",
    "visual_resource": [
      "screenshot/twitter_Twittercd08f6e0-b7ce-406a-bf37-c7a885e4697f.png"
    ]
  },
  {
    "id": "Twitter0666b196-9aeb-4af9-a4bd-cd879d386b49",
    "source": "Twitter",
    "url": "https://x.com/LangChainAI/status/1927413238733681027",
    "title": "Ready to deploy your own Open Agent Platform (OAP) instance? In our latest video, we show you how to self-host OAP in production—no managed instance required. OAP is an open-source, no-code platform for building, prototyping, and deploying intelligent agents. With its",
    "content": "Ready to deploy your own Open Agent Platform (OAP) instance? In our latest video, we show you how to self-host OAP in production—no managed instance required. OAP is an open-source, no-code platform for building, prototyping, and deploying intelligent agents. With its intuitive web UI, you can: Set up Tools and Supervisor agents out of the box Plug in your own RAG server for domain-specific knowledge Connect to your own MCP server to extend agent capabilities Build and manage custom agents—all in the browser Whether you’re a developer, product manager, or analyst, OAP gives you a full-stack agent platform, powered by LangGraph, with minimal setup. Watch the walkthrough: youtube.com/watch?v=GQCGwn Explore the platform: oap.langchain.com Docs: docs.oap.langchain.com GitHub: github.com/langchain-ai/o",
    "summary": "LangChain最新视频展示如何自托管Open Agent Platform (OAP)以进行生产。OAP是一个开源的无代码平台，适合构建、原型设计和部署智能代理。在直观的Web UI中，您可以设置现成的工具和监督代理，插入自己的RAG服务器以获取特定领域知识，连接到自己的MCP服务器以扩展代理功能，构建和管理自定义代理。平台由LangGraph支持，设置简单。这是一个适合开发者、产品经理和分析师的全栈代理平台。",
    "keywords": "LangChain,Open Agent Platform,智能代理,无代码平台,LangGraph",
    "area": "人工智能,大模型,智能体",
    "published_time": "2025-05-28T01:15:00Z",
    "download_time": "2025-05-28 03:15:00",
    "visual_resource": [
      "screenshot/twitter_Twitter0666b196-9aeb-4af9-a4bd-cd879d386b49.png"
    ]
  },
  {
    "id": "Twitterc0eb231e-1dbc-485a-a660-3d4b04a07428",
    "source": "Twitter",
    "url": "https://x.com/scaling01/status/1926788548155293978",
    "title": "On OpenAI's product strategy - super assistants, their largest competitors and their moats",
    "content": "On OpenAI's product strategy - super assistants, their largest competitors and their moats, based on recently revealed court exhibits. In H1 2025 OpenAI will start evolving ChatGPT into super-assistant, as models like o2 and o3 (now o3 and o4) are finally smart enough to perform agentic tasks. A super-assistant is an intelligent entitiy with T-shaped skills. It has broad skills for tedious daily tasks and deep expertise for tasks that most people find impossible. OpenAI recognizes that \"growth and revenue won't line up forever\", so they need to focus their efforts first on super-assistants to generate enough monetizable demand, to pursue more expensive models in H2. Interestingly, OpenAI sees Meta as their biggest competitor, since Google is at risk of cannibalizing their own core search business (as of Dec 2024).",
    "summary": "OpenAI计划将ChatGPT演变为超级助手，应对主要竞争对手和市场需求。超级助手将具备执行智能任务的能力，整合日常和专业技能。公司预计2025年初通过这种方式提升需求，并在2025年下半年开发更复杂的模型。不同于Google面临的自我产品侵蚀风险，Meta被视为主要竞争者。",
    "keywords": "OpenAI,产品战略,超级助手,竞争对手,智能实体",
    "area": "人工智能,自然语言处理,多模态",
    "published_time": "2025-05-26T07:52:00Z",
    "download_time": "2025-05-14 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitterc0eb231e-1dbc-485a-a660-3d4b04a07428.png"
    ]
  },
  {
    "id": "Twitterce921784-5cf7-42ef-be2d-b3dd5a71ef44",
    "source": "Twitter",
    "url": "https://x.com/teortaxesTex/status/1927459880341782700",
    "title": "\"Kicking the Qwen randomly makes it work better\" like old TVs. I'm not reading any of it at this point.",
    "content": "\"kicking the qwen randomly makes it work better\" like old TVs. I'm not reading any of it at this point",
    "summary": "网友指出像老电视一样，用随机刺激可以改善Qwen的表现，但表示对内容失去兴趣。",
    "keywords": "Qwen, 老电视, 随机刺激, 改善表现, 失去兴趣",
    "area": "人工智能, 深度学习, 其他",
    "published_time": "2025-05-14T12:34:56Z",
    "download_time": "2025-05-14 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitterce921784-5cf7-42ef-be2d-b3dd5a71ef44.png"
    ]
  },
  {
    "id": "Twitter1ac5dae4-aa58-43db-9c47-ea11235149af",
    "source": "Twitter",
    "url": "https://x.com/lateinteraction/status/1927392900632985694",
    "title": "What I’m trying to say is this conceptual ablation: If X + just about any technique -> huge improvements, for only a limited set of recent base models X, then the discovery is about the class X, not any of the techniques (yet).",
    "content": "What I’m trying to say is this conceptual ablation: If X + just about any technique -> huge improvements, for only a limited set of recent base models X, then the discovery is about the class X, not any of the techniques (yet).",
    "summary": "本文探讨了一种概念性消融：如果将X与几乎任何技术结合，可以在有限的一组最新基础模型X中获得显著提升，那么这种发现是关于X类的，而不是任何具体的技术。",
    "keywords": "概念性消融,技术结合,基础模型,显著提升,发现",
    "area": "机器学习,大模型,其他",
    "published_time": "2025-05-27T23:54:00Z",
    "download_time": "2023-10-07 16:42:38",
    "visual_resource": [
      "screenshot/twitter_Twitter1ac5dae4-aa58-43db-9c47-ea11235149af.png"
    ]
  },
  {
    "id": "Twitter048219eb-7cb5-4492-80a6-92a9f8a251d3",
    "source": "Twitter",
    "url": "https://x.com/scaling01/status/1927418304718623180",
    "title": "Claude 4 Sonnet beating o3-preview on ARC-AGI 2 while being <1/400th of the price",
    "content": "Claude 4 Sonnet beating o3-preview on ARC-AGI 2 while being <1/400th of the price",
    "summary": "Claude 4 Sonnet以低于o3-preview的1/400价格在ARC-AGI 2上表现出色。",
    "keywords": "Claude,Sonnet,o3-preview,ARC-AGI,价格",
    "area": "人工智能,机器学习,大模型",
    "published_time": "2025-05-28T01:35:00Z",
    "download_time": "2025-10-09 13:14:32",
    "visual_resource": [
      "screenshot/twitter_Twitter048219eb-7cb5-4492-80a6-92a9f8a251d3.png"
    ]
  },
  {
    "id": "Twitterffa3d7b1-6647-40f6-9ba3-e799fc38d508",
    "source": "Twitter",
    "url": "https://x.com/cto_junior/status/1926879933957038176",
    "title": "One possible explanation is Claude-4 is really not designed for 0-shotting code it works better in an agentic setup with feedback loop built in to gradually lead to an optimized code",
    "content": "One possible explanation is Claude-4 is really not designed for 0-shotting code it works better in an agentic setup with feedback loop built in to gradually lead to an optimized code",
    "summary": "Claude-4 可能并不适合零样本代码生成，在结合反馈循环的智能体设置中效果更佳，逐渐优化代码。",
    "keywords": "Claude-4,零样本,反馈循环,智能体优化,代码生成",
    "area": "人工智能, 生成式AI, 智能体",
    "published_time": "2025-05-26T13:55:00Z",
    "download_time": "2023-11-29 14:26:01",
    "visual_resource": [
      "screenshot/twitter_Twitterffa3d7b1-6647-40f6-9ba3-e799fc38d508.png"
    ]
  },
  {
    "id": "Twitter383e076d-fc7a-4371-a3dd-784346298e26",
    "source": "Twitter",
    "url": "https://x.com/SakanaAILabs/status/1926798125060002243",
    "title": "Following our Sudoku-based reasoning benchmark announcement, we've been evaluating the latest models to track improvements in their reasoning capabilities. Today, we’re launching the Sudoku-Bench Leaderboard",
    "content": "Following our Sudoku-based reasoning benchmark announcement, we've been evaluating the latest models to track improvements in their reasoning capabilities. Today, we’re launching the Sudoku-Bench Leaderboard: pub.sakana.ai/sudoku/ New technical report: arxiv.org/abs/2505.16135 You can now track new model progress on our live Leaderboard. Of the models we’ve benchmarked so far: OpenAI’s o3 Mini High leads overall. Interestingly, Gemini 2.5 Pro does better on the harder 6x6 puzzles! However, o3 is the only model that solves any of the 9x9 Sudokus, but only 2.9% and only the vanilla Sudoku’s. Crucially, NO model tested can yet conquer 9x9s requiring strong, creative reasoning. This benchmark remains a grand challenge! For a deeper dive into the benchmark, methodology, and our findings, check out our technical report. Want to test a model on Sudoku-Bench? It's simple! Visit the leaderboard. Choose a puzzle. We generate a prompt (puzzle + instructions) to paste into any model. Explore sample reasoning traces from our tests too!",
    "summary": "我们推出了Sudoku-Bench排名榜，可以跟踪最新模型在推理能力上的进展。OpenAI的o3 Mini High在整体上领先，而Gemini 2.5 Pro在较难的6x6谜题中表现更好。然而，没有任何模型能够成功解决需要强大创意推理的9x9谜题，这仍是一个巨大挑战。",
    "keywords": "Sudoku-Bench,推理能力,OpenAI,o3 Mini High,Gemini 2.5 Pro",
    "area": "人工智能,多模态,生成式AI",
    "published_time": "2025-05-26T08:30:00Z",
    "download_time": "2023-10-08 07:48:35",
    "visual_resource": [
      "screenshot/twitter_Twitter383e076d-fc7a-4371-a3dd-784346298e26.png"
    ]
  },
  {
    "id": "Twitter7ec69832-3598-432f-899e-e43487de5037",
    "source": "Twitter",
    "url": "https://x.com/_lewtun/status/1927043160275923158",
    "title": "Happy to share 💭 Mixture of Thoughts 💭",
    "content": "Happy to share 💭 Mixture of Thoughts 💭 A curated, general reasoning dataset that trims down over 1M samples from public datasets to ~350k through an extensive set of ablations 🧑‍🍳 Models trained on this mix match or exceed the performance of DeepSeek's distilled models -- not just on math/code but also on scientific benchmarks like GPQA We also validate that the \"additive\" methodology from Phi-4-reasoning really works! You can optimise the data mixture independently per reasoning domain and then bring it all together for the final run 🔥 Link to the dataset ⤵️",
    "summary": "分享了一种叫做\"思维混合\"的通用推理数据集，通过广泛的消融研究将100多万个样本精简到约35万。训练在此混合数据上的模型不仅在数学/代码方面，而且在GPQA等科学基准测试中，与DeepSeek的蒸馏模型匹配或超过其性能。验证了Phi-4推理中的\"附加\"方法确实有效。数据混合可以根据每个推理领域独立优化，然后汇总进行最终运行。",
    "keywords": "通用推理,深度学习,DeepSeek,数据集,科学基准",
    "area": "人工智能,机器学习,大模型",
    "published_time": "2025-05-27T00:44:00Z",
    "download_time": "2023-11-03 10:00:00",
    "visual_resource": [
      "screenshot/twitter_Twitter7ec69832-3598-432f-899e-e43487de5037.png"
    ]
  },
  {
    "id": "Twitter8e30306c-7c1a-4311-b78e-8d5bd35d5b54",
    "source": "Twitter",
    "url": "https://x.com/GoogleDeepMind/status/1927375853551235160",
    "title": "We're thrilled to announce SignGemma, our most capable model for translating sign language into spoken text.",
    "content": "We're thrilled to announce SignGemma, our most capable model for translating sign language into spoken text.\n🧏\nThis open model is coming to the Gemma model family later this year, opening up new possibilities for inclusive tech. Share your feedback and interest in early testing → goo.gle/SignGemma",
    "summary": "谷歌深度思维团队公布了他们最新的手语翻译模型SignGemma，这一开放模型即将加入Gemma模型家族，旨在为包容性技术带来新的可能性。",
    "keywords": "手语,翻译模型,SignGemma,包容性,技术",
    "area": "人工智能,自然语言处理,多模态",
    "published_time": "2025-05-27T22:46:00Z",
    "download_time": "2023-10-27 14:56:00",
    "visual_resource": [
      "screenshot/twitter_Twitter8e30306c-7c1a-4311-b78e-8d5bd35d5b54.png"
    ]
  },
  {
    "id": "Twitter2e898284-7745-4b4b-8030-b7daa6678457",
    "source": "Twitter",
    "url": "https://x.com/c_valenzuelab/status/1927149229966766373",
    "title": "Ensure models have infinite use cases",
    "content": "This is pretty wild. We wanted to ensure our models have infinite use cases that are less prescriptive and linear than the simplistic \"text-to-X\" approach. Which means that are still plenty of uses cases we have not yet discovered. Gen-4 and References feel like a step toward universality.",
    "summary": "这项努力是为了确保我们的模型具有无穷的使用场景，这些场景比简单的\"text-to-X\"方法更具创新性和包容性。虽然我们还没有发掘出所有可能性，但Gen-4和参考文献是朝着普适化的一步。",
    "keywords": "模型,使用场景,创新性,包容性,普适化",
    "area": "人工智能,多模态,生成式AI",
    "published_time": "2025-05-27T07:45:00Z",
    "download_time": "2023-10-02 14:30:00",
    "visual_resource": [
      "screenshot/twitter_Twitter2e898284-7745-4b4b-8030-b7daa6678457.png"
    ]
  },
  {
    "id": "Twitter395ee417-8271-45cb-a90a-f0e2e1ea560f",
    "source": "Twitter",
    "url": "https://x.com/TheTuringPost/status/1927123359969468420",
    "title": "A new recipe for training multimodal models",
    "content": "A new recipe for training multimodal models 👉 Mixed together various data types: text next to images, video frames after captions, then webpages, etc. This way the model learns to connect what it reads with what it sees. ByteDance proposed and implemented this idea in their BAGEL, a new open-source multimodal model. Here's how it works:",
    "summary": "一个新的多模态模型训练方法被提出，通过结合文本、图像、视频帧等多种数据类型，模型可以学会连接阅读与视觉。此想法由字节跳动提出并在其开源多模态模型BAGEL中实现。",
    "keywords": "多模态模型,字节跳动,BAGEL,数据整合,模型训练",
    "area": "多模态,深度学习,自然语言处理",
    "published_time": "6:03 AM · May 27, 2025",
    "download_time": "2023-10-02 10:00:00",
    "visual_resource": [
      "screenshot/twitter_Twitter395ee417-8271-45cb-a90a-f0e2e1ea560f.png"
    ]
  },
  {
    "id": "Twitter147bc4e1-87bd-48b5-9092-2eea33f11d52",
    "source": "Twitter",
    "url": "https://x.com/mervenoyann/status/1926987808360509636",
    "title": "X",
    "content": "what happened in open AI past week? so many vision LM & omni releases\nhere's our picks\nmultimodal\n > new moondream (VLM) is out: it's 4-bit quantized (with QAT) version of moondream-2b, runs on 2.5GB VRAM at 184 tps with only 0.6% drop in accuracy (OS)\n > ByteDance released BAGEL-7B, an omni model that understands and generates both image + text. they also released Dolphin, a document parsing VLM\nThe tweet is truncated. To view the full content, please visit the tweet URL.",
    "summary": "最近一周，Open AI 发布了多个视觉语言模型和全能模型的新版本。其中，MoonDream 推出了经过 QAT 4-bit 量化的 VLM，仅在 184 tps 下就能在 2.5GB VRAM 上运行，精确度仅下降 0.6%。同时，字节跳动发布了 BAGEL-7B，可以理解和生成图文内容，还推出了用于文档解析的 Dolphin VLM。",
    "keywords": "Open AI,视觉语言模型,全能模型,ByteDance,VLM",
    "area": "多模态,大模型,自然语言处理",
    "published_time": "2025-05-26T21:04:00Z",
    "download_time": "2023-10-04 15:26:34",
    "visual_resource": [
      "screenshot/twitter_Twitter147bc4e1-87bd-48b5-9092-2eea33f11d52.png"
    ]
  },
  {
    "id": "2CuZyE4YMs7xD8HK7cf5xQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/2CuZyE4YMs7xD8HK7cf5xQ",
    "title": "大模型玩不好数独？！Transformer作者初创公司公布排行榜：o3 Mini High“变异数独”正确率仅2.9%",
    "summary": "由Transformer作者Llion Jones创立的Sakana AI公司，发布了AI数独能力基准测试Sudoku-Bench，旨在评估大模型的创造性推理能力。测试结果揭示，大模型整体数独正确率仅15%，在更具挑战性的9x9变异数独中，即使是高性能模型o3 Mini High，正确率也仅有2.9%。这凸显了当前大模型普遍存在的“记忆依赖症”缺陷，即它们倾向于记忆而非真正进行逻辑推理。该基准通过引入无法依靠记忆解决的“变异数独”，为促进AI推理能力的实质性提升提供了新方向，NVIDIA CEO黄仁勋亦肯定此类谜题对提升AI推理能力的重要性。",
    "keywords": [
      "大模型",
      "数独",
      "推理能力",
      "Sudoku-Bench",
      "SakanaAI",
      "变异数独",
      "Transformer作者",
      "记忆依赖症"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器学习"
    ],
    "published_time": "2025-05-28T04:23:13.000Z",
    "download_time": "2025-05-30T00:28:47.698150",
    "visual_resource": [
      "screenshot/wechat_wx_bbaaa987.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T04:23:13.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAHkwSrvicgK1yjPVBOuCG2fPpys2kE35ctegAwrfibERAZ5icoA65PQuCEGMH9B4X0yVyYZlP5f4ueA/0?wx_fmt=jpeg\", \"id\": \"2CuZyE4YMs7xD8HK7cf5xQ\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/2CuZyE4YMs7xD8HK7cf5xQ.txt\"}}"
  },
  {
    "id": "pC84-TMj94gb5n5PpaDolg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/pC84-TMj94gb5n5PpaDolg",
    "title": "Claude 4破解困扰人类4年系统bug，30年码龄程序员200小时没搞定，GPT-4.1/Gemini-2.5也做不到",
    "summary": "Anthropic旗舰大模型Claude Opus 4成功破解了一个困扰资深C++程序员长达四年、耗时200小时未解的复杂系统顽固性bug，而此前GPT-4.1和Gemini-2.5等模型均未能解决。这位拥有30年开发经验的程序员在33个prompt和一次重启后，Claude Opus 4便清晰定位并提供了解决方案。此次事件凸显了Claude 4系列模型在编程和推理能力上的显著提升，特别是其Code模式作为智能代码助手，可高效处理代码重构、bug修复等工程任务，展示了AI在复杂软件开发领域日益强大的辅助与问题解决潜力，预示着AI辅助编程的未来趋势。",
    "keywords": [
      "ClaudeOpus4",
      "bug修复",
      "大模型",
      "编程能力",
      "AI辅助编程",
      "代码重构",
      "生成式AI"
    ],
    "area": [
      "人工智能",
      "大模型",
      "生成式AI"
    ],
    "published_time": "2025-05-28T04:23:13.000Z",
    "download_time": "2025-05-30T00:28:59.412368",
    "visual_resource": [
      "screenshot/wechat_wx_ba2d50c1.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T04:23:13.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtByts5aLdmL34037Zsk2h6Jeh0rHXK5PIteEOPFGDI8tteia9wNqmdTZq69BsuLzLUySFlakM1iaU0w/0?wx_fmt=jpeg\", \"id\": \"pC84-TMj94gb5n5PpaDolg\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/pC84-TMj94gb5n5PpaDolg.txt\"}}"
  },
  {
    "id": "qcGrNjIqU1cLSg_31wijJg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/qcGrNjIqU1cLSg_31wijJg",
    "title": "一个省略号提示+强化学习搞定大模型“过度思考”，中科院自动化所新方法：从强制推理到自主选择",
    "summary": "中科院自动化研究所与鹏城实验室联合提出AutoThink新方法，旨在解决大语言模型“过度思考”问题。该方法通过在提示词中加入省略号，并结合三阶段强化学习，赋予大模型自主决定何时以及如何深度思考的能力，使其能根据题目难度动态切换思考模式，实现“按需思考”。实验结果显示，AutoThink不仅能有效提升模型在复杂任务上的准确率，同时大幅减少Token消耗，显著提高推理效率，并已集成至ScienceOne平台。此创新范式为大模型实现性能与算力的双重优化提供了新思路，推动了模型向更智能、更经济的通用智能方向发展。",
    "keywords": [
      "大语言模型",
      "过度思考",
      "强化学习",
      "AutoThink",
      "推理优化",
      "Token效率",
      "任务感知",
      "省略号提示"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "强化学习"
    ],
    "published_time": "2025-05-28T04:23:13.000Z",
    "download_time": "2025-05-30T00:29:09.851460",
    "visual_resource": [
      "screenshot/wechat_wx_438670d1.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T04:23:13.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBHYQjMiappg4LGkN8wRtPFmWuXTHcWbOcGrQugicwyB9rV5naXFicWp1ryNicFQTfgmfkxxr48zbnVbw/0?wx_fmt=jpeg\", \"id\": \"qcGrNjIqU1cLSg_31wijJg\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/qcGrNjIqU1cLSg_31wijJg.txt\"}}"
  },
  {
    "id": "tukh12k0iG-b3WbysI_15w",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/tukh12k0iG-b3WbysI_15w",
    "title": "阿里通义发布并行计算新策略：1.6B等效4.4B，内存消耗骤降95%",
    "summary": "阿里通义团队提出大模型（LLM）“计算缩放”新范式PARSCALE，旨在在不显著增加内存与时间成本的前提下，提升模型能力。该策略受CFG（无分类器引导）启发，将并行计算从生成阶段扩展至训练与推理全流程，通过P条可学习并行路径、差异化输入变换与动态聚合实现。PARSCALE使得1.6B模型性能可接近4.4B模型，但内存占用仅为后者1/22，并可直接应用于现有模型，显著降低训练成本。其突破了传统参数或推理时间扩展瓶颈，为大模型的高效发展提供了全新优化路径。",
    "keywords": [
      "PARSCALE",
      "大模型",
      "并行计算",
      "计算缩放",
      "内存优化",
      "CFG",
      "推理优化",
      "参数高效微调"
    ],
    "area": [
      "大模型",
      "机器学习",
      "自然语言处理"
    ],
    "published_time": "2025-05-28T04:23:13.000Z",
    "download_time": "2025-05-30T00:29:19.086251",
    "visual_resource": [
      "screenshot/wechat_wx_6f5fafc2.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T04:23:13.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtArogqibFfK994adaJfw3uicc4z1j8pmSQMVFPm97Hiaq2RopTm80F5vvwue4PDwZq4DDcSDMFuREDGA/0?wx_fmt=jpeg\", \"id\": \"tukh12k0iG-b3WbysI_15w\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/tukh12k0iG-b3WbysI_15w.txt\"}}"
  },
  {
    "id": "VKsZftqpNCtKQsQEb5g_Mg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/VKsZftqpNCtKQsQEb5g_Mg",
    "title": "港科大Apple新研究：Tokens使用量减少，模型推理还更强了",
    "summary": "香港科技大学与苹果等机构研究人员提出Laser系列创新方法，显著提升大推理模型（LRMs）的效率与准确率。针对现有LRMs在复杂推理任务中Tokens使用量巨大的问题，Laser通过统一的长度奖励设计和动态难度感知机制，实现了Tokens使用量大幅减少（如在AIME24上降低63%），同时推理准确率不降反升（提升6.1%）。研究表明，该方法促使模型形成更健康的思考模式，减少冗余的“自我反思”。Laser系列在多个基准测试和不同规模模型上展现卓越性能与泛化能力，为构建更高效、更智能的AI模型提供了新路径，强调准确性与简洁性并存的高级智能。",
    "keywords": [
      "大推理模型",
      "Tokens效率",
      "推理能力",
      "准确率",
      "Laser方法",
      "奖励函数",
      "模型优化",
      "思维模式"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "大模型"
    ],
    "published_time": "2025-05-28T04:23:13.000Z",
    "download_time": "2025-05-30T00:29:30.403526",
    "visual_resource": [
      "screenshot/wechat_wx_53c6a818.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T04:23:13.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAHkwSrvicgK1yjPVBOuCG2fmGoxkB62QWsMJV2cyzXyxb3hD6gS0McJgdcVnCmWoCrBXwNqx1muwA/0?wx_fmt=jpeg\", \"id\": \"VKsZftqpNCtKQsQEb5g_Mg\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/VKsZftqpNCtKQsQEb5g_Mg.txt\"}}"
  },
  {
    "id": "_3GnRpcgD_c5_nkzE_Fkyg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/_3GnRpcgD_c5_nkzE_Fkyg",
    "title": "新智元十年，ASI降临，诚邀你加入！",
    "summary": "新智元即将迎来其十年里程碑，同时传达了“ASI降临”的前瞻性展望，这预示着人工智能领域正迈向超越人类智能的奇点时刻。该标题不仅是新智元媒体自身发展的重要节点，更强调了通用人工智能（AGI）乃至超级人工智能（ASI）未来可能带来的深远影响，暗示了技术突破和业界发展的新篇章。此次发布可能伴随着某项重大活动或合作倡议，旨在邀请各界共同探讨并见证AI的未来发展趋势。此举体现了业界对高级AI形态的持续关注与探索。",
    "keywords": [
      "新智元",
      "ASI",
      "人工智能",
      "超级智能",
      "AI发展",
      "科技媒体",
      "十周年"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-05-28T05:06:10.000Z",
    "download_time": "2025-05-30T00:27:28.308284",
    "visual_resource": [
      "screenshot/wechat_wx_b14aed7a.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T05:06:10.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb0UW7sJXoiaeUGtFg6Lh1gfPFfounjYicVzj0abXr6Q50yIQq326SJr1we8zc3c9D0S7WhOMiaJia1ibUg/0?wx_fmt=jpeg\", \"id\": \"_3GnRpcgD_c5_nkzE_Fkyg\"}, \"extraction_info\": {\"account\": \"新智元\", \"file_path\": \"./database/content/wechat/_3GnRpcgD_c5_nkzE_Fkyg.txt\"}}"
  },
  {
    "id": "3HqPGCvgvShv6pLeElVEAQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/3HqPGCvgvShv6pLeElVEAQ",
    "title": "AI狂飙100天，中国力量突起！顶流视频号10分钟看尽全球最强杀招",
    "summary": "文章回顾了自2025年初全球AI版图的剧烈重绘。中国DeepSeek以开源大模型DeepSeek-R1的卓越能力，突破硅谷技术壁垒，引发市场震动。面对挑战，OpenAI迅速通过价格战、发布GPT-4.5及GPT-4o进行反击，马斯克亦通过Grok抢占算力话语权。在此期间，DeepSeek进一步推出本地可运行的多模态模型V3，推动AI从实验室走向普通用户。至4月，AI发展已从“参数至上”转向“实用主义”，国产大模型全面接入主流平台，标志着AI技术正加速走向全民普及，重塑社会生产方式。",
    "keywords": [
      "DeepSeek",
      "OpenAI",
      "大模型",
      "开源",
      "算力",
      "多模态",
      "AI竞争",
      "技术普惠"
    ],
    "area": [
      "人工智能",
      "大模型",
      "多模态"
    ],
    "published_time": "2025-05-28T05:06:10.000Z",
    "download_time": "2025-05-30T00:27:36.406015",
    "visual_resource": [
      "screenshot/wechat_wx_0e9dc156.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T05:06:10.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb15VnEUUFuicY1icOBIJEjH5m5UCefNjTgqjCXky1Pyf5JBlfAOiasssLmLluzBkmNsP7iaeEOcHUY7CA/0?wx_fmt=jpeg\", \"id\": \"3HqPGCvgvShv6pLeElVEAQ\"}, \"extraction_info\": {\"account\": \"新智元\", \"file_path\": \"./database/content/wechat/3HqPGCvgvShv6pLeElVEAQ.txt\"}}"
  },
  {
    "id": "hZqnANXmojmHv1gE9ISgfA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/hZqnANXmojmHv1gE9ISgfA",
    "title": "第二次Sora时刻来了！全球首款实时摄像头诞生，真人感拉满颠覆全行业",
    "summary": "新智元报道指出，硅谷公司AKOOL正式发布全球首款实时AI摄像头——AKOOL Live Camera，被誉为AI视频领域的“第二次Sora时刻”。该产品集虚拟数字人、实时翻译、实时换脸及实时视频生成四大功能于一体，以超低延迟和影视级画质，实现了“边拍边生”的突破性创新。AKOOL Live Camera具备情境感知与情感响应的智能交互能力，彻底改变了传统“文生视频”的预制化模式。通过4D面部映射、神经语音引擎等先进技术，AKOOL正推动数字交互从“预制化”迈向“智能化响应”时代，有望深刻革新视频制作与数字营销行业。",
    "keywords": [
      "AKOOLLiveCamera",
      "实时AI视频",
      "虚拟数字人",
      "实时视频生成",
      "低延迟",
      "Sora时刻",
      "情境感知",
      "生成式AI"
    ],
    "area": [
      "人工智能",
      "生成式AI",
      "多模态"
    ],
    "published_time": "2025-05-28T05:06:10.000Z",
    "download_time": "2025-05-30T00:27:48.544932",
    "visual_resource": [
      "screenshot/wechat_wx_6a4adf73.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T05:06:10.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb15VnEUUFuicY1icOBIJEjH5mQCLTeEvkOtm8t5xL0Q8LyEicUiayicCAjib3aKuMq6nJLicI2UIIg29xtDw/0?wx_fmt=jpeg\", \"id\": \"hZqnANXmojmHv1gE9ISgfA\"}, \"extraction_info\": {\"account\": \"新智元\", \"file_path\": \"./database/content/wechat/hZqnANXmojmHv1gE9ISgfA.txt\"}}"
  },
  {
    "id": "yzfNRAJyITdHN4xskNCSWg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/yzfNRAJyITdHN4xskNCSWg",
    "title": "刚刚2岁的Llama，「爸妈」都跑了！小扎手拆Meta AI，LeCun保持独立",
    "summary": "Meta正经历AI部门的深度重组，拆解旧有研发体系为AI产品与AGI基础团队，以应对Llama系列大模型表现不佳、核心人才大量流失以及外部竞争加剧的挑战。此前，Llama 4遭遇“翻车”被曝性能争议与跳票，而Llama首版多数作者已离职创业或加入竞对，加剧人才真空。尽管重组旨在吸引新研究者，但首席AI科学家LeCun坚持其与主流LLM发展相悖的“世界模型”路线，FAIR仍保持独立，这为Meta的AGI战略及团队协同带来不确定性。此次变革显示Meta在投入巨资后，正加紧调整AI战略以提升竞争力。",
    "keywords": [
      "Meta",
      "Llama",
      "AI重组",
      "人才流失",
      "大模型",
      "LeCun",
      "AGI",
      "多模态"
    ],
    "area": [
      "人工智能",
      "大模型",
      "多模态"
    ],
    "published_time": "2025-05-28T05:06:10.000Z",
    "download_time": "2025-05-30T00:27:56.724835",
    "visual_resource": [
      "screenshot/wechat_wx_18d42434.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T05:06:10.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb15VnEUUFuicY1icOBIJEjH5mvjzLeypwZ2xfXtOAHOxebBVHE0kCOu4jzBR3rwr77m5N46XCz2AvLQ/0?wx_fmt=jpeg\", \"id\": \"yzfNRAJyITdHN4xskNCSWg\"}, \"extraction_info\": {\"account\": \"新智元\", \"file_path\": \"./database/content/wechat/yzfNRAJyITdHN4xskNCSWg.txt\"}}"
  },
  {
    "id": "Q4Il_vDhKbncI-kiOPnikQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Q4Il_vDhKbncI-kiOPnikQ",
    "title": "SIGGRAPH 2025 | CLR-Wire：曲线框可生成？可交互？深大VCC带你见证魔法",
    "summary": "深圳大学黄惠团队独立推出CLR-Wire技术，首次实现将复杂三维曲线框结构统一编码至连续潜空间，解决了传统方法难以同时捕捉线框几何和拓扑信息的难题。该创新方案通过结合变分自编码器与流匹配方法，构建了能够进行高效生成与平滑插值的三维线框模型，并支持无条件及基于点云、图像的条件生成。实验证明，CLR-Wire在生成精度、多样性方面显著超越现有技术，尤其在复杂拓扑结构处理上表现出色。此项技术为工业设计、三维重建及内容创作等领域带来了高效可靠的解决方案，为未来交互式3D设计与语义驱动控制奠定基础。",
    "keywords": [
      "CLR-Wire",
      "三维曲线框",
      "连续潜空间",
      "三维生成",
      "计算机图形学",
      "流匹配",
      "变分自编码器"
    ],
    "area": [
      "生成式AI",
      "深度学习",
      "计算机视觉"
    ],
    "published_time": "2025-05-28T08:10:04.000Z",
    "download_time": "2025-05-30T00:26:18.310685",
    "visual_resource": [
      "screenshot/wechat_wx_633868d7.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T08:10:04.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic6dWJP4fbjQQ0J78sktTAYgFl0Yhk7icTzMFLkERUZYiaIh2BkxK7LXRpNeur0vvqtG78qsKPaEh6A/0?wx_fmt=jpeg\", \"id\": \"Q4Il_vDhKbncI-kiOPnikQ\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/Q4Il_vDhKbncI-kiOPnikQ.txt\"}}"
  },
  {
    "id": "sFK1ukPNmI89kLREdPbckg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/sFK1ukPNmI89kLREdPbckg",
    "title": "LLM加RL遭质疑：故意用错奖励，数学基准也显著提升，AI圈炸了",
    "summary": "一篇来自华盛顿大学等机构的开创性论文挑战了当前大型语言模型（LLM）与强化学习（RLVR）结合的训练范式。研究发现，即使采用随机或错误等缺乏监督信息的“虚假奖励”，也能显著提升Qwen2.5-Math-7B模型在数学推理基准MATH-500上的性能。作者指出，这种异常提升源于RLVR机制意外激发了Qwen模型预训练中固有的代码推理能力，而非模型真正学习到新技能，且该效果并非普遍适用于所有模型，特异性依赖于模型预存能力。该研究对高质量监督信号的必要性提出质疑，并强调未来RLVR研究需在更多样化的模型上进行验证，并深入理解模型预训练能力对后续训练行为的影响。",
    "keywords": [
      "LLM",
      "强化学习",
      "大语言模型",
      "虚假奖励",
      "Qwen模型",
      "数学推理",
      "代码推理",
      "模型泛化"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器学习"
    ],
    "published_time": "2025-05-28T08:10:04.000Z",
    "download_time": "2025-05-30T00:26:28.286036",
    "visual_resource": [
      "screenshot/wechat_wx_311fcc7a.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T08:10:04.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibPsWibzGOHYBMyw9VU4bSiaTicp2U1ibBmuzpqC0qGdrG76FicFAv7bNsq7hUotnTrSUtCtMJZeUuEvJQ/0?wx_fmt=jpeg\", \"id\": \"sFK1ukPNmI89kLREdPbckg\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/sFK1ukPNmI89kLREdPbckg.txt\"}}"
  },
  {
    "id": "vJlbt4snSxxNXqmmQc71uA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/vJlbt4snSxxNXqmmQc71uA",
    "title": "华为盘古首次露出，昇腾原生72B MoE架构，SuperCLUE千亿内模型并列国内第一",
    "summary": "华为盘古团队近日发布突破性混合专家模型（MoE）——分组混合专家模型（MoGE），旨在解决传统MoE中专家激活不均衡导致的计算效率瓶颈。该架构通过引入分组机制，在4K昇腾大规模集群上实现专家负载均匀分布，并构建出盘古Pro MoE大模型。该模型（总参数72B，激活参数16B）在昇腾300I/800I平台上展现出卓越推理效率，最高可达1528 tokens/s。在最新SuperCLUE榜单上，盘古Pro MoE在千亿参数以内大模型中并列国内第一。此次创新不仅标志着大模型技术从“参数军备竞赛”转向“实效主义”，更凭借软硬件协同优化，为企业级应用的高效部署提供了新范式。",
    "keywords": [
      "华为盘古",
      "混合专家模型",
      "MoGE",
      "昇腾",
      "大模型",
      "SuperCLUE",
      "负载均衡",
      "推理效率"
    ],
    "area": [
      "人工智能",
      "深度学习",
      "大模型"
    ],
    "published_time": "2025-05-28T08:10:04.000Z",
    "download_time": "2025-05-30T00:26:37.976630",
    "visual_resource": [
      "screenshot/wechat_wx_7b5c2ec1.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T08:10:04.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibPsWibzGOHYBMyw9VU4bSiaTj6gkAUia2Bq6PrC5QsFHibqsLicEVCGASJPXHjp8zPIjsVBRnx843SIVA/0?wx_fmt=jpeg\", \"id\": \"vJlbt4snSxxNXqmmQc71uA\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/vJlbt4snSxxNXqmmQc71uA.txt\"}}"
  },
  {
    "id": "Lcn4vdU_toUSMSWP-51Ovg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Lcn4vdU_toUSMSWP-51Ovg",
    "title": "爆火论文颠覆RL认知！「错误奖励」让LLM推理暴涨24.6%，学界惊了",
    "summary": "一项颠覆性研究发现，大语言模型（LLM）的增强学习（RL）训练中，即使是“伪奖励”（包括错误奖励、随机奖励或格式奖励）也能显著提升模型推理能力。华盛顿大学、AI2、UC伯克利团队实验证实，Qwen模型在伪奖励下数学推理性能飙升24.6%。研究揭示，这种提升并非模型真正“学会”新知识，而是通过GRPO算法的裁剪机制，促使其从自然语言推理转向更高效的“代码推理”策略。然而，该现象对Llama3、OLMo2等其他模型无效甚至有害，表明RL效果的有效性可能高度依赖模型预训练能力和特定算法特性，挑战了传统强化学习对高质量奖励信号的认知。",
    "keywords": [
      "伪奖励",
      "强化学习",
      "大语言模型",
      "数学推理",
      "代码推理",
      "GRPO",
      "Qwen",
      "AI训练"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器学习"
    ],
    "published_time": "2025-05-28T23:00:49.000Z",
    "download_time": "2025-05-30T00:25:10.330509",
    "visual_resource": [
      "screenshot/wechat_wx_8482a433.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T23:00:49.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icogrJsxw5f4NIynCF7ds2Cp4NKGW7gic46CW9VlZX87Yn4znQrn5B6JwstAib4dud0bViaH2TnANZPOow/0?wx_fmt=jpeg\", \"id\": \"Lcn4vdU_toUSMSWP-51Ovg\"}, \"extraction_info\": {\"account\": \"AI生成未来\", \"file_path\": \"./database/content/wechat/Lcn4vdU_toUSMSWP-51Ovg.txt\"}}"
  },
  {
    "id": "OWw_xUXhxL7416tzF4c-7A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/OWw_xUXhxL7416tzF4c-7A",
    "title": "修图模型照妖镜上线！ImgEdit-Bench三维「死亡评测」曝光，谁在裸泳一测便知",
    "summary": "近期，图像编辑领域迎来重大进展，一项名为ImgEdit的统一数据集及其评估基准ImgEdit-Bench正式发布。该数据集包含百万级高保真图像编辑对，涵盖单轮与多轮复杂编辑任务，并利用GPT-4o生成高质量指令。基于此，前沿模型ImgEdit-E1展现出卓越的编辑能力。ImgEdit-Bench则构建了系统性评估框架，通过指令遵循度、编辑质量等维度，全面评测现有图像编辑模型。测试结果显示，闭源模型如GPT-4o-image表现突出，而ImgEdit-E1在开源模型中脱颖而出，显著缩小了与顶尖模型的差距。该研究不仅弥补了现有数据集的不足，更为未来图像编辑模型的数据和架构升级提供了关键洞察，有力推动了整个领域的进步。",
    "keywords": [
      "图像编辑",
      "数据集",
      "评估基准",
      "大模型",
      "多模态",
      "生成式AI",
      "ImgEdit-Bench"
    ],
    "area": [
      "计算机视觉",
      "多模态",
      "生成式AI"
    ],
    "published_time": "2025-05-28T23:00:49.000Z",
    "download_time": "2025-05-30T00:25:22.422431",
    "visual_resource": [
      "screenshot/wechat_wx_36507f16.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T23:00:49.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icogrJsxw5f4NIynCF7ds2Cp4Tia9iaYZ02C9hdiaeTp6EiaRnxsRQqOEWdSoQoiaXeibv8pVn4Pd21wEBTTw/0?wx_fmt=jpeg\", \"id\": \"OWw_xUXhxL7416tzF4c-7A\"}, \"extraction_info\": {\"account\": \"AI生成未来\", \"file_path\": \"./database/content/wechat/OWw_xUXhxL7416tzF4c-7A.txt\"}}"
  },
  {
    "id": "WeNdDVENi1DJciGjt1R97Q",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/WeNdDVENi1DJciGjt1R97Q",
    "title": "港科大Apple新研究：Tokens使用量减少，模型推理还更强了！",
    "summary": "港科大等研究团队提出Laser系列方法，旨在提升大语言模型（LLMs）推理任务的效率和准确率。针对现有LLMs在推理时消耗大量冗余Tokens的问题，Laser方法通过统一长度奖励框架、阶跃函数奖励机制及动态难度感知调整，有效平衡并显著提升了模型性能。实验证明，该方法能在大幅减少Tokens使用量的同时，提高模型推理准确率，例如在AIME24基准测试中，Tokens使用降低63%的同时性能提升6.1。研究揭示，Laser引导模型形成更简洁、高效的思考模式，减少冗余自省，实现准确性与简洁性之间的高级智能平衡，为未来高效AI推理奠定基础。",
    "keywords": [
      "大语言模型",
      "推理能力",
      "Tokens效率",
      "准确率",
      "Laser方法",
      "长度奖励",
      "动态调整"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "机器学习"
    ],
    "published_time": "2025-05-28T23:00:49.000Z",
    "download_time": "2025-05-30T00:25:35.922775",
    "visual_resource": [
      "screenshot/wechat_wx_e453ed51.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T23:00:49.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icogrJsxw5f4NIynCF7ds2Cp4JOK6e4g1icDxGK6mBiciajlM3ibnaFf1Je3k9mb3mSB2noMzjLSpkkNJ3Q/0?wx_fmt=jpeg\", \"id\": \"WeNdDVENi1DJciGjt1R97Q\"}, \"extraction_info\": {\"account\": \"AI生成未来\", \"file_path\": \"./database/content/wechat/WeNdDVENi1DJciGjt1R97Q.txt\"}}"
  },
  {
    "id": "GmHJ9VmoUk0u1ZSfHEbnjw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/GmHJ9VmoUk0u1ZSfHEbnjw",
    "title": "金融支付 × 实时推荐：Milvus 如何支撑全球百亿交易的“猜你喜欢”",
    "summary": "某全球金融科技巨头通过引入向量数据库Milvus，成功支撑其每年百亿级交易的支付后实时推荐系统。面对海量数据洪流及现有数据库性能瓶颈，该企业发现Milvus在数据导入速度、弹性架构及开发友好性上表现突出，成功满足了高吞吐、低延迟的严苛要求。Milvus的应用显著提升了转化率及用户满意度，并已被扩展至智能客服等其他AI场景。项目复盘指出，企业级AI落地关键在于稳固的系统基础设施而非模型本身，强调了基础设施的战略价值及托管服务对业务创新的推动作用。",
    "keywords": [
      "金融支付",
      "实时推荐",
      "向量数据库",
      "Milvus",
      "人工智能",
      "弹性架构",
      "智能客服"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "生成式AI"
    ],
    "published_time": "2025-05-28T11:22:11.000Z",
    "download_time": "2025-05-30T00:24:31.417200",
    "visual_resource": [
      "screenshot/wechat_wx_03c679f2.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T11:22:11.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/MqgA8Ylgeh6yJ4KpzElDOdIFxY3VlrlQZicgQbR2kgJblTvm8RhFgnpYO6Y4NOiba6TroyxDmb1DmjuibRtiaVicfUQ/0?wx_fmt=jpeg\", \"id\": \"GmHJ9VmoUk0u1ZSfHEbnjw\"}, \"extraction_info\": {\"account\": \"Zilliz\", \"file_path\": \"./database/content/wechat/GmHJ9VmoUk0u1ZSfHEbnjw.txt\"}}"
  },
  {
    "id": "56KREcz-xS2s0RCnlnQRYQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/56KREcz-xS2s0RCnlnQRYQ",
    "title": "完爆人物“一致性&风格化” | 支持Flux+任意风格LoRA，即插即用，效果媲美商业版GPT-4o!",
    "summary": "OmniConsistency系统旨在解决AI生成图像中人物“一致性”和“风格化”的关键挑战。该系统通过整合独特的风格学习与一致性学习阶段，动态利用预训练LoRA模块，实现了支持Flux及任意LoRA风格的即插即用功能，宣称其生成效果可媲美商用版GPT-4o。文章详细介绍了其核心优势、落地场景及操作指南，并提供了体验链接、代码与论文，展现了在提升专业图像内容生成（AIGC）方面的前沿进展。",
    "keywords": [
      "OmniConsistency",
      "人物一致性",
      "风格化",
      "LoRA",
      "AI生成图像",
      "AIGC",
      "Flux"
    ],
    "area": [
      "人工智能",
      "生成式AI",
      "计算机视觉"
    ],
    "published_time": "2025-05-28T23:44:21.000Z",
    "download_time": "2025-05-30T00:24:07.526220",
    "visual_resource": [
      "screenshot/wechat_wx_9d656490.jpg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-28T23:44:21.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/Qj1jLINT4WibXn9VMnR6rEBQ0OlqicUkXyMgpic0BFxBMNOuIeB1v0JdUpSJiaxGP21sUfJOeDTcXcJwDpc6umich6w/0?wx_fmt=jpeg\", \"id\": \"56KREcz-xS2s0RCnlnQRYQ\"}, \"extraction_info\": {\"account\": \"AI产品汇\", \"file_path\": \"./database/content/wechat/56KREcz-xS2s0RCnlnQRYQ.txt\"}}"
  },
  {
    "id": "2505.19897",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19897",
    "title": "ScienceBoard：评估多模态自主智能体在现实科学工作流中的表现",
    "summary": "大型语言模型（LLMs）已将其影响力扩展到自然语言处理领域之外，极大地促进了跨学科研究的发展。近年来，研究人员开发了各种基于LLM的智能体，以协助跨多个方面和领域的科学发现进展。其中，能够像人类一样与操作系统交互的计算机使用智能体，正在为自动化科学问题解决和处理研究人员工作流中的 routine 任务铺平道路。认识到这些智能体的变革潜力，我们引入了ScienceBoard，它包含两个互补的贡献：（i）一个逼真、多域的环境，具有集成专业软件的动态且视觉丰富的科学工作流，智能体可以通过不同的接口进行自主交互，以加速复杂的科研任务和实验；（ii）一个具有挑战性的基准测试，包含169个由人类策划的高质量、经过严格验证的真实世界任务，涵盖生化、天文、地理信息学等领域的科学发现工作流。对具有最先进骨干模型（如GPT-4o、Claude 3.7、UI-TARS）的智能体进行的广泛评估表明，尽管取得了一些有前景的结果，但它们在可靠地辅助科学家完成复杂工作流方面仍远未达到要求，总体成功率仅为15%。深入分析进一步为解决当前智能体局限性和制定更有效的设计原则提供了宝贵见解，为构建用于科学发现的更强大智能体铺平了道路。我们的代码、环境和基准可在https://qiushisun.github.io/ScienceBoard-Home/获取。",
    "keywords": [
      "ScienceBoard",
      "多模态智能体",
      "自主智能体",
      "科学工作流",
      "评估基准"
    ],
    "area": [
      "人工智能",
      "智能体",
      "多模态"
    ],
    "published_time": "2025-05-26T12:27:27.000Z",
    "download_time": "2025-05-29 07:03:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19897.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19897",
      "arxiv_url": "https://arxiv.org/abs/2505.19897"
    }
  },
  {
    "id": "2505.19641",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19641",
    "title": "SynLogic：大规模合成可验证的推理数据，用于学习逻辑推理及更广泛能力",
    "summary": "近期进展，例如 OpenAI-o1 和 DeepSeek R1，已展示了强化学习（RL）增强大型语言模型（LLMs）推理能力的潜力。尽管开源复现工作主要集中在数学和编程领域，但发展通用推理能力的方法和资源仍未得到充分探索。这种差距部分是由于收集适合RL的多样化且可验证推理数据的挑战造成的。我们假设逻辑推理对于发展通用推理能力至关重要，因为逻辑构成了推理的基本组成部分。在这项工作中，我们提出了 SynLogic，一个数据合成框架和数据集，能够大规模生成多样化的逻辑推理数据，涵盖 35 种不同的逻辑推理任务。SynLogic 方法能够对数据进行可控合成，可调节难度和数量。重要的是，所有示例都可以通过简单的规则进行验证，使其非常适合基于可验证奖励的RL训练。在我们的实验中，我们验证了基于 7B 和 32B 模型在 SynLogic 数据集上进行 RL 训练的有效性。SynLogic 在开源数据集中取得了最先进的逻辑推理性能，在 BBEH 基准上比 DeepSeek-R1-Distill-Qwen-32B 高出 6 分。此外，将 SynLogic 数据与数学和编程任务混合训练，提高了这些领域的训练效率，并显著增强了推理的泛化能力。值得注意的是，我们的混合训练模型在多个基准上优于 DeepSeek-R1-Zero-Qwen-32B。这些发现表明 SynLogic 是一个宝贵的资源，有助于提升 LLMs 更广泛的推理能力。我们在 https://github.com/MiniMax-AI/SynLogic 上开源了数据合成管道和 SynLogic 数据集。",
    "keywords": [
      "LLMs",
      "强化学习",
      "逻辑推理",
      "数据合成",
      "可验证推理数据"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "大模型"
    ],
    "published_time": "2025-05-26T07:59:36.000Z",
    "download_time": "2025-05-29 07:03:19",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19641.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19641",
      "arxiv_url": "https://arxiv.org/abs/2505.19641"
    }
  },
  {
    "id": "2505.21189",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21189",
    "title": "探索大型语言模型单步文本生成的潜在能力",
    "summary": "近期一项研究表明，大型语言模型（LLMs）可以通过仅使用一个经过特殊训练的输入嵌入进行自回归生成，重建惊人长度的文本——长达数千个词元。在这项工作中，我们探索了在没有自回归的情况下是否也能实现这种重建。我们发现，当仅提供两个学习到的嵌入时，冻结的LLMs可以在仅一次前向传播中生成数百个准确的词元。这揭示了LLMs一个令人惊讶但尚未得到充分探索的能力——无需迭代解码的多词元生成。我们研究了这些嵌入的行为，并深入探讨了它们编码的信息类型。我们还通过实证表明，尽管这些表示对于给定文本并非独一无二，但它们在嵌入空间中形成相互连接的局部区域——这一特性预示着学习一个专用编码器映射到该空间的潜力。",
    "keywords": [
      "大型语言模型",
      "单步生成",
      "嵌入",
      "非自回归",
      "潜在能力"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "生成式AI"
    ],
    "published_time": "2025-05-27T13:39:24.000Z",
    "download_time": "2025-05-29 07:03:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21189.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21189",
      "arxiv_url": "https://arxiv.org/abs/2505.21189"
    }
  },
  {
    "id": "2505.20325",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20325",
    "title": "凭直觉指引：基于强化内在置信的高效测试时缩放",
    "summary": "用于增强大语言模型 (LLM) 推理能力的测试时缩放 (TTS) 方法通常会产生高昂的计算成本，这主要是由于它们过度依赖外部过程奖励模型 (PRM) 或诸如优中选N (BoN) 之类的采样方法。本文介绍了 Guided by Gut (GG)，这是一种高效的自引导 TTS 框架，它无需昂贵的外部验证器模型即可达到 PRM 级别的性能。我们的方法采用了一种轻量级树搜索，该搜索仅由 LLM 的内在信号——标记级置信度和步骤新颖性——引导。一个关键创新是通过一个有针对性的强化学习微调阶段来提高内部置信度估计的可靠性。在具有挑战性的数学推理基准上的实证评估表明，GG 使小型模型（例如 15亿 参数）能够达到或超越显著更大的模型（例如 320亿-700亿 参数）的准确性，同时将 GPU 内存使用量降低多达 10 倍。与基于 PRM 的方法相比，GG 在实现可比准确性的同时，推理速度提高了 8 倍，内存使用量降低了 4-5 倍。此外，与 BoN 策略相比，GG 将 KV cache 内存使用量减少了大约 50%，从而促进了 TTS 技术的更高效和实际部署。",
    "keywords": [
      "LLM",
      "Test-Time Scaling",
      "强化学习",
      "内在置信",
      "效率"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "深度学习"
    ],
    "published_time": "2025-05-23T18:19:09.000Z",
    "download_time": "2025-05-29 07:03:44",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20325.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20325",
      "arxiv_url": "https://arxiv.org/abs/2505.20325"
    }
  },
  {
    "id": "2505.21496",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21496",
    "title": "UI-Genie：一种迭代提升基于多模态大语言模型的移动图形界面智能体的自改进方法",
    "summary": "本文介绍了 UI-Genie，这是一个自改进框架，旨在解决图形界面（GUI）智能体面临的两个关键挑战：轨迹结果验证困难和高质量训练数据难以扩展。这些挑战分别通过一个奖励模型和一个自改进流程来解决。奖励模型 UI-Genie-RM 采用图文交错架构，能够高效处理历史上下文，并统一了动作级和任务级奖励。为了支持 UI-Genie-RM 的训练，我们开发了精心设计的数据生成策略，包括基于规则的验证、受控轨迹破坏和难负例挖掘。为了解决第二个挑战，一个自改进流程通过在动态环境中进行奖励引导的探索和结果验证，逐步增强智能体和奖励模型，从而扩展了可解决的复杂 GUI 任务范围。为了训练模型，我们生成了 UI-Genie-RM-517k 和 UI-Genie-Agent-16k 数据集，建立了首个针对 GUI 智能体的奖励专用数据集，同时展示了无需人工标注即可生成高质量合成轨迹的能力。实验结果表明，通过三代数据-模型自改进，UI-Genie 在多个 GUI 智能体基准测试中取得了最先进的性能。我们开源了完整的框架代码和生成的数据集，以促进进一步研究，地址为 https://github.com/Euphoria16/UI-Genie。",
    "keywords": [
      "Mobile GUI Agents",
      "Self-improving",
      "MLLM",
      "Reward Model",
      "Data Generation"
    ],
    "area": [
      "智能体",
      "机器学习",
      "多模态"
    ],
    "published_time": "2025-05-27T17:58:06.000Z",
    "download_time": "2025-05-29 07:03:58",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21496.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21496",
      "arxiv_url": "https://arxiv.org/abs/2505.21496"
    }
  },
  {
    "id": "2505.21493",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21493",
    "title": "无需验证器的通用推理强化",
    "summary": "近期，利用DeepSeek-R1-Zero式、基于可验证奖励的强化学习（RL）来训练大型语言模型（LLMs）的新范式，已在代码和数学推理方面取得了令人瞩目的进展。然而，这种方法仅限于那些规则化回答验证可能的任务，并不能自然地推广到化学、医疗、工程、法律、生物、商业和经济等现实世界领域。当前实际的变通方法是使用额外的LLM作为基于模型的验证器；但这带来了对强大验证器LLM的依赖、易受奖励欺骗（reward hacking）以及训练期间在内存中维护验证器模型的实际负担等问题。为了解决这些问题并将DeepSeek-R1-Zero式训练扩展到通用推理领域，我们提出了一种无需验证器的方法（VeriFree），该方法绕过回答验证，转而使用RL直接最大化生成参考答案的概率。我们将VeriFree与基于验证器的方法进行比较，并证明了VeriFree除了具有显著的实际益处和降低的计算需求外，在MMLU-Pro、GPQA、SuperGPQA和数学相关基准上的广泛评估中达到甚至超越了基于验证器的方法。此外，我们从多个角度提供了对这种方法的见解：它是一个将策略和隐式验证器统一整合到单个模型中的优雅方法，也是一种变分优化方法。代码可在https://github.com/sail-sg/VeriFree获取。",
    "keywords": [
      "大型语言模型",
      "强化学习",
      "General Reasoning",
      "Verifier-Free",
      "变分优化"
    ],
    "area": [
      "大模型",
      "机器学习",
      "自然语言处理"
    ],
    "published_time": "2025-05-27T17:56:27.000Z",
    "download_time": "2025-05-29 07:04:06",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21493.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21493",
      "arxiv_url": "https://arxiv.org/abs/2505.21493"
    }
  },
  {
    "id": "2505.17952",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17952",
    "title": "超越蒸馏：采用简约规则基强化学习拓展医疗大模型推理能力的极限",
    "summary": "提高大型语言模型（LLMs）在复杂任务上的表现并使其具备可解释的决策能力，尤其是在临床应用中，需要有效的推理能力。然而，这在没有通过对从闭源模型（如 GPT-4o）蒸馏出的昂贵的思维链（CoT）数据进行监督微调（SFT）的情况下仍然具有挑战性。在 G 工作中，我们提出了 AlphaMed，这是第一个医疗大模型，证明了推理能力可以完全通过强化学习（RL）产生，使用公共多项选择问答数据集上的简约规则基奖励，而无需依赖 SFT 或蒸馏的 CoT 数据。AlphaMed 在六个医疗问答基准测试上取得了最先进的结果，优于采用传统 SFT+RL 管线训练的模型。在有挑战性的基准测试（如 MedXpert）上，AlphaMed 甚至超过了更大的或闭源模型，例如 DeepSeek-V3-671B 和 Claude-3.5-Sonnet。为了理解成功的因素，我们进行了全面的以数据为中心的分析，由三个问题指导：(i) 简约规则基 RL 是否能在没有蒸馏 CoT 监督的情况下激励推理？(ii) 数据集的数量和多样性如何影响推理？(iii) 问题难度如何塑造推理的产生和泛化？我们的发现表明，数据集的信息量是推理表现的关键驱动因素，并且在信息丰富的多项选择问答数据上应用的简约 RL 能有效诱导推理，而无需 CoT 监督。我们还观察到不同基准测试之间存在差异趋势，突显了当前评估的局限性以及需要更具挑战性、面向推理的医疗问答基准测试的需求。",
    "keywords": [
      "医疗大模型",
      "推理",
      "强化学习",
      "简约规则基奖励",
      "医疗问答"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "机器学习"
    ],
    "published_time": "2025-05-23T14:27:37.000Z",
    "download_time": "2025-05-29 07:04:21",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17952.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17952",
      "arxiv_url": "https://arxiv.org/abs/2505.17952"
    }
  },
  {
    "id": "2505.16901",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16901",
    "title": "代码图模型（CGM）：一种用于仓库级软件工程任务的图集成大语言模型",
    "summary": "近年来大语言模型（LLM）在函数级代码生成方面展现出潜力，然而仓库级软件工程任务仍然充满挑战。当前解决方案主要依赖于专有的LLM智能体，这带来了不可预测性和可访问性限制，并引发了对数据隐私和模型定制的担忧。本文探讨了开源LLM是否能在不依赖智能体方法的情况下有效解决仓库级任务。我们通过使LLM能够借助代码库中函数和文件的语义信息和结构依赖关系来理解它们，证明这是可能的。为此，我们引入了代码图模型（CGM），它将仓库代码图结构集成到LLM的注意力机制中，并使用专门的适配器将节点属性映射到LLM的输入空间。当与无智能体的图增强生成（RAG）框架结合时，我们的方法在使用开源Qwen2.5-72B模型在SWE-bench Lite基准测试上达到了43.00%的解决率。这一性能在开源权重模型中排名第一，在包含开源系统的系统方法中排名第二，整体排名第八，超过了先前最佳的基于开源模型的方法12.33%。",
    "keywords": [
      "大语言模型",
      "仓库级软件工程",
      "代码图",
      "图集成LLM",
      "RAG"
    ],
    "area": [
      "大模型",
      "深度学习",
      "生成式AI"
    ],
    "published_time": "2025-05-22T17:00:55.000Z",
    "download_time": "2025-05-29 07:04:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16901.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16901",
      "arxiv_url": "https://arxiv.org/abs/2505.16901"
    }
  },
  {
    "id": "2505.21334",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21334",
    "title": "HoliTom：面向快速视频大语言模型的整体 Token 合并",
    "summary": "视频大语言模型 (video LLMs) 在视频理解方面表现出色，但由于冗余的视频 Token，面临显著的计算效率问题。现有的 Token 剪枝方法提供了一些解决方案。然而，在 LLM 内部进行处理的方法 (inner-LLM 剪枝)，如 FastV，在浅层会产生固有的计算开销。相比之下，在 LLM 之前执行 Token 剪枝的方法 (outer-LLM 剪枝) 主要解决单个帧内或有限时间窗口内的空间冗余，而忽略了长视频序列中关键的全局时间动态和关联性。这导致次优的时空冗余减少，并且未能充分利用视频的可压缩性。至关重要的是，结合这些策略的协同潜力及相互影响尚未得到探索。为了进一步减少冗余，我们引入了 HoliTom，一种新颖的、无需训练的整体 Token 合并框架。HoliTom 通过全局冗余感知的时序分割实现 outer-LLM 剪枝，随后进行时空合并，将视觉 Token 减少超过 90%，显著减轻了 LLM 的计算负担。作为补充，我们引入了一种基于 Token 相似性的鲁棒 inner-LLM 合并方法，旨在实现优异的性能并与 outer-LLM 剪枝兼容。评估表明，我们的方法在 LLaVA-OneVision-7B 上展现出良好的效率-性能权衡，将计算成本降低到原始 FLOPs 的 6.9%，同时保持了 99.1% 的原始性能。此外，我们在 Time-To-First-Token (TTFT) 上实现了 2.28 倍的减少，并在解码吞吐量上加速了 1.32 倍，突显了我们集成剪枝方法对高效视频 LLMs 推理的实际益处。",
    "keywords": [
      "Video LLMs",
      "Token Merging",
      "Computational Efficiency",
      "Outer-LLM",
      "Inner-LLM"
    ],
    "area": [
      "视频理解",
      "大模型",
      "多模态"
    ],
    "published_time": "2025-05-27T15:28:45.000Z",
    "download_time": "2025-05-29 07:04:53",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21334.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21334",
      "arxiv_url": "https://arxiv.org/abs/2505.21334"
    }
  },
  {
    "id": "2505.20287",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20287",
    "title": "MotionPro：一个用于图到视频生成的精确运动控制器",
    "summary": "利用交互式运动控制对图像进行动画处理已在图到视频（I2V）生成领域越来越受欢迎。现代方法通常依赖大型高斯核来扩展运动轨迹作为条件，但没有明确定义运动区域，这导致运动控制粗糙，并且无法区分物体和相机的移动。为了缓解这些问题，我们提出了 MotionPro，一个精确的运动控制器，它创新地利用区域轨迹和运动掩码来分别调节细粒度运动合成和识别目标运动类别（即物体或相机移动）。技术上，MotionPro 首先通过跟踪模型估计每个训练视频上的光流图，然后采样区域轨迹来模拟推理场景。与通过大型高斯核扩展光流不同，我们的区域轨迹方法通过直接利用局部区域内的轨迹实现更精确的控制，从而有效表征细粒度运动。同时，从预测的光流图导出运动掩码，以捕捉运动区域的整体运动动态。为了追求自然的运动控制，MotionPro 通过特征调制结合区域轨迹和运动掩码，进一步增强了视频去噪能力。更值得注意的是，我们精心构建了一个基准，即 MC-Bench，包含 1.1K 对用户标注的图像-轨迹对，用于评估细粒度和物体级别的 I2V 运动控制。在 WebVid-10M 和 MC-Bench 上进行的广泛实验证明了 MotionPro 的有效性。请参阅我们的项目页面以获取更多结果：https://zhw-zhang.github.io/MotionPro-page/。",
    "keywords": [
      "Image-to-Video Generation",
      "Precise Motion Control",
      "Region-wise Trajectory",
      "Motion Mask",
      "MC-Bench"
    ],
    "area": [
      "生成式AI",
      "计算机视觉",
      "深度学习"
    ],
    "published_time": "2025-05-26T17:59:03.000Z",
    "download_time": "2025-05-29 07:05:08",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20287.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20287",
      "arxiv_url": "https://arxiv.org/abs/2505.20287"
    }
  },
  {
    "id": "2505.21505",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21505",
    "title": "对齐如何增强大语言模型的多语言能力？基于语言神经元的视角",
    "summary": "多语言对齐是一种有效且具有代表性的范式，用于增强大语言模型的多语言能力，它将高资源语言的能力迁移到低资源语言。同时，一些关于特定语言神经元的研究表明，当处理不同语言时，大语言模型中存在选择性激活的特定语言神经元。这为在多语言场景中更具体地分析和理解大语言模型的机制提供了新视角。在这项工作中，我们提出了一种新的更精细粒度的神经元识别算法，该算法可以检测语言神经元（包括特定语言神经元和语言相关神经元）以及语言无关神经元。此外，基于不同类型神经元的分布特性，我们将大语言模型的多语言推理内部过程划分为四个部分： (1) 多语言理解，(2) 共享语义空间推理， (3) 多语言输出空间转换，和 (4) 词汇空间输出。此外，我们系统地分析了对齐前后模型的不同类型神经元。我们还分析了“自发多语言对齐”现象。总的来说，本工作基于不同类型的神经元进行了全面研究，为更好地理解多语言对齐和大语言模型的多语言能力提供了实证结果和有价值的见解。",
    "keywords": [
      "大语言模型",
      "多语言能力",
      "对齐",
      "语言神经元",
      "神经元分析"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "深度学习"
    ],
    "published_time": "2025-05-27T17:59:52.000Z",
    "download_time": "2025-05-29 07:05:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21505.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21505",
      "arxiv_url": "https://arxiv.org/abs/2505.21505"
    }
  },
  {
    "id": "2505.14064",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.14064",
    "title": "NOVA：一项脑部MRI异常定位与临床推理的基准测试",
    "summary": "在许多实际应用中，部署的模型会遇到与训练数据不同的输入。分布外检测（Out-of-distribution detection）用于识别输入是否源于一个未曾见的分布，而开放世界识别（open-world recognition）则标记此类输入，以确保系统在不断出现先前未知的类别并需要处理时保持鲁棒性，而无需重新训练。基础模型和视觉-语言模型（Foundation and vision-language models）在庞大且多样化的数据集上进行预训练，期望在包括医学影像在内的各个领域实现广泛泛化。然而，仅使用少数常见异常类型对这些模型进行基准测试，会使得评估悄然退化为封闭集问题（closed-set problem），掩盖了在临床应用中遇到的罕见或真正新颖病症上的失效。\n因此，我们提出了 NOVA，这是一个具有挑战性的、源于真实世界、仅用于评估的基准数据集，包含大约900例脑部MRI扫描，涵盖281种罕见病理和异构采集协议。每个病例都包含丰富的临床描述和双盲专家标注的边界框。这些资源共同支持对异常定位、视觉描述和诊断推理进行联合评估。由于 NOVA 从未用于训练，它充当了分布外泛化的极端压力测试：模型必须弥合样本外观和语义空间上的分布差异。使用领先的视觉-语言模型（GPT-4o、Gemini 2.0 Flash和Qwen2.5-VL-72B）进行的基线测试结果显示，在所有任务上均出现显著的性能下降，从而将 NOVA 确立为一个严苛的测试平台，可用于推进能够检测、定位和推理真正未知异常的模型。",
    "keywords": [
      "脑部MRI",
      "异常定位",
      "临床推理",
      "分布外泛化",
      "NOVA 基准"
    ],
    "area": [
      "多模态",
      "计算机视觉",
      "大模型"
    ],
    "published_time": "2025-05-20T08:10:57.000Z",
    "download_time": "2025-05-29 07:05:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14064.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.14064",
      "arxiv_url": "https://arxiv.org/abs/2505.14064"
    }
  },
  {
    "id": "2505.17332",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17332",
    "title": "SweEval：LLM真的会说脏话吗？一项面向企业应用的安全性测试基准",
    "summary": "企业客户越来越多地采用大型语言模型（LLM）执行关键的通信任务，例如撰写电子邮件、准备销售说辞和编写非正式消息。在不同区域部署此类模型需要它们理解多样化的文化和语言背景，并生成安全且尊重的回复。对于企业应用而言，通过有效识别和处理不安全或冒犯性语言，规避声誉风险、维护信任和确保合规性至关重要。为解决这一问题，我们引入了 SweEval，这是一个模拟真实世界场景的基准，场景涵盖了不同语气（积极或消极）和语境（正式或非正式）。提示会明确指示模型在完成任务时包含特定的脏话。该基准评估 LLM 是遵守还是抵制此类不恰当的指令，并评估它们在道德框架、文化细微差异和语言理解能力方面的对齐情况。为了推进构建面向企业应用及其他领域的道德对齐的 AI 系统的研究，我们发布了数据集和代码：https://github.com/amitbcp/multilingual_profanity。",
    "keywords": [
      "LLM",
      "企业应用",
      "安全基准",
      "冒犯性语言",
      "道德对齐"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2025-05-22T22:56:58.000Z",
    "download_time": "2025-05-29 07:06:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17332.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17332",
      "arxiv_url": "https://arxiv.org/abs/2505.17332"
    }
  },
  {
    "id": "2505.20650",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20650",
    "title": "FinTagging：一个面向大语言模型的财务信息提取与结构化基准测试",
    "summary": "我们引入FinTagging，这是首个全面的、表格感知的XBRL基准测试，旨在评估大语言模型（LLMs）在基于XBRL的财务报告背景下结构化信息提取和语义对齐的能力。与先前将XBRL标注过度简化为平面多类别分类且仅关注叙述性文本的基准测试不同，FinTagging将XBRL标注问题分解为两个子任务：FinNI用于财务实体提取，以及FinCL用于分类体系驱动的概念对齐。它要求模型联合提取事实并将其与完整的10k+ US-GAAP分类体系对齐，覆盖非结构化文本和结构化表格，从而实现真实、细粒度的评估。我们在零样本设置下评估了一系列多样化的大语言模型，系统分析了它们在两个子任务和整体标注准确性上的表现。我们的结果表明，虽然大语言模型在信息提取方面表现出强大的泛化能力，但在细粒度的概念对齐方面存在困难，尤其是在区分密切相关的分类体系条目时。这些发现突出了现有大语言模型在完全自动化XBRL标注方面的局限性，并强调需要改进语义推理和模式感知建模，以满足准确财务披露的需求。代码已在我们的GitHub仓库中提供，数据在我们的Hugging Face仓库中提供。",
    "keywords": [
      "FinTagging",
      "XBRL",
      "大语言模型 (LLM)",
      "财务信息提取",
      "语义对齐"
    ],
    "area": [
      "自然语言处理",
      "大模型",
      "机器学习"
    ],
    "published_time": "2025-05-27T02:55:53.000Z",
    "download_time": "2025-05-29 07:06:21",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20650.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20650",
      "arxiv_url": "https://arxiv.org/abs/2505.20650"
    }
  },
  {
    "id": "2505.21097",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21097",
    "title": "思考者：学习快思与慢想",
    "summary": "近期研究表明，通过将强化学习（RL）应用于数学和编程等领域的问答（QA）任务，可以提升大型语言模型（LLMs）的推理能力。在长上下文长度下，LLMs 可能会学习执行搜索行为，DeepSeek R1 中观察到的自我修正行为即是例证。然而，这种搜索行为往往不够精确且缺乏信心，导致冗长冗余的回复，并突显出在直觉和验证方面的不足。受心理学中双过程理论的启发，我们对问答任务进行了简单修改，纳入了四个阶段：快思（Fast Thinking），要求 LLM 在严格的 token 预算内作答；验证（Verification），模型评估其初始回复；慢想（Slow Thinking），更审慎地细化初始回复；以及总结（Summarization），将前一阶段的细化提炼为精确的步骤。我们提出的任务将 Qwen2.5-1.5B 的平均准确率从 24.9% 提高到 27.9%，将 DeepSeek-R1-Qwen-1.5B 的平均准确率从 45.9% 提高到 49.8%。值得注意的是，对于 Qwen2.5-1.5B，仅快思模式就在使用少于 1000 个 token 的情况下达到了 26.8% 的准确率，显示出显著的推理效率提升。这些发现表明，直觉和审慎推理是不同的、互补的系统，可以通过有针对性的训练来获益。",
    "keywords": [
      "大型语言模型",
      "推理",
      "强化学习",
      "双过程理论",
      "快思慢想"
    ],
    "area": [
      "人工智能",
      "自然语言处理",
      "大模型"
    ],
    "published_time": "2025-05-27T12:22:46.000Z",
    "download_time": "2025-05-29 07:06:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21097.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21097",
      "arxiv_url": "https://arxiv.org/abs/2505.21097"
    }
  },
  {
    "id": "2505.20793",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20793",
    "title": "渲染感知型强化学习用于矢量图形生成",
    "summary": "可缩放矢量图形（SVG）提供了一种强大的格式，用于将视觉设计表示为可解释的代码。视觉-语言模型（VLMs）的最新进展通过将问题框架为代码生成任务并利用大规模预训练，实现了高质量的SVG生成。VLMs特别适合此任务，因为它们能够捕捉全局语义和细粒度视觉模式，同时在视觉、自然语言和代码领域之间迁移知识。然而，现有的VLMs方法通常难以生成忠实且高效的SVG，因为它们在训练过程中从未观察过渲染后的图像。尽管自回归SVG代码生成的微分渲染目前尚不可用，但渲染后的输出仍可以与原始输入进行比较，从而提供适合强化学习（RL）的评估反馈。我们引入了RLRF（基于渲染反馈的强化学习），这是一种RL方法，通过利用渲染后的SVG输出反馈来增强自回归VLMs中的SVG生成。给定输入图像，模型生成SVG序列，这些序列被渲染并与原始图像进行比较，以计算奖励。这种视觉保真度反馈指导模型生成更准确、高效且语义连贯的SVG。RLRF显著优于监督微调方法，解决了常见的失败模式，并实现了具有强大结构理解和泛化能力的高精度、高质量SVG生成。",
    "keywords": [
      "强化学习",
      "矢量图形生成",
      "视觉-语言模型",
      "渲染反馈",
      "SVG"
    ],
    "area": [
      "多模态",
      "计算机视觉",
      "生成式AI"
    ],
    "published_time": "2025-05-27T06:56:00.000Z",
    "download_time": "2025-05-29 07:06:49",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20793.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20793",
      "arxiv_url": "https://arxiv.org/abs/2505.20793"
    }
  },
  {
    "id": "2505.17613",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17613",
    "title": "MMMG：面向多任务多模态生成的综合可靠评估基准",
    "summary": "自动评估多模态生成是一个重大挑战，因为自动化指标往往难以可靠地与人类评估保持一致，特别是对于涉及多种模态的复杂任务。为解决此问题，我们提出了 MMMG，一个面向多模态生成的综合且与人类评估高度一致的基准，涵盖图像、音频、图文交错、音文交错等 4 种模态组合。该基准侧重于对生成模型构成重大挑战的任务，同时通过结合模型和程序实现了可靠的自动评估。MMMG 包含 49 项任务（其中 29 项为新开发），每项任务都设计了精心构建的评估流程，并包含 937 条指令，系统地评估多模态生成模型的推理、可控性以及其他关键能力。大量验证表明，MMMG 与人类评估高度一致，平均一致性达到 94.3%。对 24 个多模态生成模型的基准测试结果显示，尽管当前最先进的模型 GPT Image 在图像生成方面达到了 78.3% 的准确率，但在多模态推理和交错生成方面表现不足。此外，结果表明音频生成有很大的改进空间，这凸显了未来研究的一个重要方向。",
    "keywords": [
      "多模态生成",
      "评估基准",
      "多任务",
      "人类一致性",
      "自动评估"
    ],
    "area": [
      "多模态",
      "生成式AI",
      "人工智能"
    ],
    "published_time": "2025-05-23T08:21:28.000Z",
    "download_time": "2025-05-29 07:07:07",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17613.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17613",
      "arxiv_url": "https://arxiv.org/abs/2505.17613"
    }
  },
  {
    "id": "2505.20426",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20426",
    "title": "MMPerspective：MLLM 理解透视吗？一个关于透视感知、推理和鲁棒性的综合基准测试",
    "summary": "理解透视是人类视觉感知的基本能力，然而，多模态大型语言模型（MLLM）在多大程度上内化了透视几何仍然不明确。我们引入了 MMPerspective，这是首个专门设计用于系统评估 MLLM 透视理解能力的基准测试，通过覆盖透视感知、推理和鲁棒性这三个互补维度的 10 个精心设计的任务来实现。我们的基准测试包含 2,711 个真实世界和合成的图像实例，以及 5,083 个问答对，旨在探测量点感知和计数、透视类型推理、三维空间中的直线关系理解、透视保持变换下的不变性等关键能力。通过对 43 个最先进的 MLLM 进行全面评估，我们揭示了显著的局限性：虽然模型在表层感知任务上表现出能力，但在组合推理和扰动下保持空间一致性方面仍面临困难。我们的分析进一步揭示了模型架构、规模与透视能力之间有趣的模式，突出了鲁棒性瓶颈以及思维链提示（chain-of-thought prompting）的益处。MMPerspective 为诊断和提升视觉-语言系统中的空间理解能力提供了一个有价值的测试平台。",
    "keywords": [
      "MLLM",
      "透视理解",
      "基准测试",
      "鲁棒性",
      "空间理解"
    ],
    "area": [
      "多模态",
      "大模型",
      "计算机视觉"
    ],
    "published_time": "2025-05-26T18:20:22.000Z",
    "download_time": "2025-05-29 07:07:22",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20426.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20426",
      "arxiv_url": "https://arxiv.org/abs/2505.20426"
    }
  },
  {
    "id": "2505.18134",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18134",
    "title": "VideoGameBench：视频-语言模型能否完成流行的电子游戏？",
    "summary": "视觉-语言模型（VLM）在对人类具有挑战性的编程和数学基准测试中取得了显著成果，但它们执行对人类而言自然而然的任务（如感知、空间导航和记忆管理）的能力仍未得到充分研究。真实的电子游戏利用人类固有的归纳偏置，旨在让人们直观地学习和掌握，这使得它们成为评估VLM此类能力的理想测试平台。为此，我们引入了VideoGameBench，这是一个包含10款20世纪90年代流行电子游戏的基准测试，VLM可以直接与这些游戏进行实时交互。VideoGameBench挑战模型仅依靠原始视觉输入以及对目标和控制的高级描述来完成整个游戏，这与依赖于游戏特定脚手架和辅助信息的现有设置显著不同。我们将其中三款游戏保密，以鼓励开发能够泛化到未知环境的解决方案。我们的实验表明，前沿的视频-语言模型难以在每款游戏中取得进展，往往止步于开局阶段。我们发现推理延迟是前沿模型在实时设置下的主要限制；因此，我们引入了VideoGameBench Lite，在这种设置下，游戏会在等待语言模型执行下一步动作时暂停。表现最好的模型 Gemini 2.5 Pro 仅完成了 VideoGameBench 的 0.48% 和 VideoGameBench Lite 的 1.6%。我们希望将上述人类技能形式化为这一基准测试，能够推动这些研究方向的进展。",
    "keywords": [
      "Vision-Language Models",
      "VideoGameBench",
      "电子游戏",
      "基准测试",
      "智能体"
    ],
    "area": [
      "多模态",
      "智能体",
      "视频理解"
    ],
    "published_time": "2025-05-23T17:43:27.000Z",
    "download_time": "2025-05-29 07:07:43",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18134.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18134",
      "arxiv_url": "https://arxiv.org/abs/2505.18134"
    }
  },
  {
    "id": "2505.21471",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21471",
    "title": "通过多智能体协作扩展LLM的外部知识输入超越上下文窗口限制",
    "summary": "随着推理和信息检索后训练技术的快速发展，大语言模型（LLMs）可以整合大量检索到的知识来解决复杂任务。然而，LLM有限的上下文窗口阻碍了外部知识输入的规模扩展，限制了进一步的性能提升，尤其对于需要大量外部知识的任务。现有的上下文窗口扩展方法不可避免地会导致信息丢失。基于LLM的多智能体方法作为一种新的范式出现，以分布式方式处理海量输入，在现有知识同步和推理过程中，我们发现了两个核心瓶颈。在这项工作中，我们开发了一个多智能体框架ExtAgents，以克服这些瓶颈并在无需长上下文训练的情况下，实现推理时知识集成的更好的可扩展性。通过我们增强的多跳问答测试$inftyBench+$以及包括长问卷生成在内的其他公共测试集进行基准测试，无论外部知识输入是否在上下文窗口内或超出，ExtAgents在相同外部知识输入量下，与现有非训练方法相比，显著提升了性能。此外，由于高度并行性，该方法保持了高效率。对LLM智能体在处理增加的外部知识输入方面的协作进行进一步研究，有望使实际应用受益。",
    "keywords": [
      "大语言模型",
      "外部知识",
      "多智能体协作",
      "上下文窗口",
      "可扩展性"
    ],
    "area": [
      "大模型",
      "智能体",
      "自然语言处理"
    ],
    "published_time": "2025-05-27T17:45:04.000Z",
    "download_time": "2025-05-29 07:08:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21471.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21471",
      "arxiv_url": "https://arxiv.org/abs/2505.21471"
    }
  },
  {
    "id": "2505.21178",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21178",
    "title": "先学走路再学跑！通过强化学习实现大型语言模型简洁推理",
    "summary": "随着测试时扩展成为大型语言模型（LLMs）发展的关键研究前沿，当代先进的后训练方法日益关注延长长链式思维（CoT）响应的生成长度，以增强推理能力，使其达到类似DeepSeek R1的表现。然而，最近的研究揭示了最先进的推理模型中存在一种持续的“过度思考”现象，表现为长CoT响应中出现过度的冗余或重复的思维模式。为了解决这个问题，本文提出了一个简单而有效的两阶段强化学习框架，用于实现LLMs的简洁推理，名为ConciseR。具体而言，第一阶段使用更多训练步数，旨在通过具有clip-higher和动态采样组件的群组相对策略优化（GRPO++）来激发模型的推理能力；第二阶段使用较少训练步数，通过长度感知群组相对策略优化（L-GRPO）明确增强简洁性并提高效率。值得注意的是，ConciseR遵循“先学走路再学跑”的原则，只有当一个样本的所有推演步骤都正确时，才会优化响应长度。大量实验结果表明，我们的ConciseR模型能生成更简洁的CoT推理响应，在AIME 2024、MATH-500、AMC 2023、Minerva和Olympiad等基准测试中，其性能优于近期零强化学习范式的最先进推理模型。",
    "keywords": [
      "大型语言模型",
      "强化学习",
      "链式思维",
      "推理",
      "简洁性"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "机器学习"
    ],
    "published_time": "2025-05-27T13:29:51.000Z",
    "download_time": "2025-05-29 07:08:13",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21178.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21178",
      "arxiv_url": "https://arxiv.org/abs/2505.21178"
    }
  },
  {
    "id": "2505.20561",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20561",
    "title": "超越马尔可夫：基于贝叶斯自适应强化学习的大语言模型反思性探索",
    "summary": "通过强化学习（RL）训练的大语言模型（LLM）展现出强大的推理能力和涌现的反思性行为，例如回溯和纠错。然而，传统的马尔可夫强化学习将探索限于训练阶段，以学习最优确定性策略，并且仅通过当前状态依赖于历史上下文。因此，在马尔可夫强化学习训练过程中是否会涌现反思性推理，以及为什么它们在测试时有益，仍然不清楚。为了解决这个问题，我们将反思性探索重新构建在贝叶斯自适应强化学习框架内，该框架明确优化了在马尔可夫决策过程后验分布下的预期回报。这种贝叶斯公式通过信仰更新内在激励了最大化回报的利用和收集信息的探索。我们由此产生的算法BARL，指导LLM根据观察到的结果拼接和切换策略，为模型何时以及如何进行反思性探索提供了原则性指导。在合成任务和数学推理任务上的实验结果表明，BARL在测试时优于标准的马尔可夫强化学习方法，通过提高探索效率实现了卓越的标记效率。我们的代码可在 https://github.com/shenao-zhang/BARL 获取。",
    "keywords": [
      "大语言模型",
      "贝叶斯自适应强化学习",
      "反思性探索",
      "强化学习",
      "推理"
    ],
    "area": [
      "大模型",
      "强化学习",
      "自然语言处理"
    ],
    "published_time": "2025-05-26T22:51:00.000Z",
    "download_time": "2025-05-29 07:08:32",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20561.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20561",
      "arxiv_url": "https://arxiv.org/abs/2505.20561"
    }
  },
  {
    "id": "2505.20286",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20286",
    "title": "Alita：实现可扩展智能体推理的通用智能体，具有最小预定义与最大自演化能力",
    "summary": "大型语言模型（LLMs）的最新进展使得智能体能够自主执行复杂的、开放式的任务。然而，许多现有框架过度依赖手动预定义的工具和工作流程，这阻碍了它们跨领域的适应性、可扩展性和泛化能力。在这项工作中，我们引入了 Alita——一款遵循“大道至简”（Simplicity is the ultimate sophistication）原则设计的通用智能体，通过最少的预定义和最大的自演化来实现可扩展的智能体推理。为了实现最小预定义，Alita 只配备了一个直接解决问题的组件，这比以往依赖大量手工制作的复杂工具和工作流程的方法要简单得多、整洁得多。这种简洁的设计增强了其泛化到具有挑战性问题的潜力，不受工具的限制。为了实现最大自演化，我们通过提供一套通用的组件来增强 Alita 的创造力，使其能够自主构建、完善和重用外部能力，方法是从开源生成与任务相关的模型上下文协议（MCPs），这有助于实现可扩展的智能体推理。值得注意的是，在 GAIA 基准测试验证数据集上，Alita 在通用智能体中取得了领先地位，pass@1 准确率达到 75.15%，pass@3 达到 87.27%；在 Mathvista 和 PathVQA 上，pass@1 准确率分别为 74.00% 和 52.00%，优于许多复杂度远高于它的智能体系统。更多详情将更新至 https://github.com/CharlesQ9/Alita。",
    "keywords": [
      "通用智能体",
      "智能体推理",
      "大模型",
      "自演化",
      "可扩展性"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-05-26T17:58:53.000Z",
    "download_time": "2025-05-29 07:08:51",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20286.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20286",
      "arxiv_url": "https://arxiv.org/abs/2505.20286"
    }
  },
  {
    "id": "2505.19433",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19433",
    "title": "压缩后的大语言模型能真正具备智能体能力吗？LLM压缩中智能体能力的实证评估",
    "summary": "训练后压缩降低了大语言模型（LLMs）的计算和内存成本，从而实现资源高效部署。然而，现有的压缩基准仅关注语言建模（例如困惑度）和自然语言理解任务（例如GLUE准确率），忽略了智能体能力——工作流、工具使用/函数调用、长上下文理解以及真实世界应用。我们引入了智能体压缩基准测试（ACBench），这是第一个用于评估压缩如何影响LLM智能体能力的综合基准测试。ACBench涵盖 (1) 跨越4种能力的12项任务（例如，用于工作流生成的WorfBench，用于长上下文检索的Needle-in-Haystack），(2) 量化（GPTQ, AWQ）和剪枝（Wanda, SparseGPT）等压缩方法，以及 (3) 15种模型，包括小型模型（Gemma-2B）、标准模型（Qwen2.5 7B-32B）和知识蒸馏推理型LLMs（DeepSeek-R1-Distill）。我们的实验揭示了压缩的权衡：4比特量化保留了工作流生成和工具使用能力（下降1%-3%），但使真实世界应用准确率下降了10%-15%。我们引入了ERank、Top-k排序相关性（Top-k Ranking Correlation）和能量（Energy）等指标来系统化分析。ACBench为在智能体场景中优化LLM压缩提供了可操作的见解。代码可在 https://github.com/pprp/ACBench 获取。",
    "keywords": [
      "大语言模型",
      "压缩",
      "智能体能力",
      "基准测试",
      "量化"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "智能体"
    ],
    "published_time": "2025-05-26T02:49:07.000Z",
    "download_time": "2025-05-29 07:09:07",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19433.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19433",
      "arxiv_url": "https://arxiv.org/abs/2505.19433"
    }
  },
  {
    "id": "2505.20321",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20321",
    "title": "BiomedSQL：面向生物医学知识库的科学推理文本到SQL系统",
    "summary": "生物医学研究人员日益依赖大规模结构化数据库进行复杂的分析任务。然而，当前的文本到SQL系统在将定性科学问题映射到可执行SQL时往往遇到困难，尤其是在需要隐式领域推理的情况下。我们引入了BiomedSQL，这是第一个明确设计用于评估真实世界生物医学知识库上文本到SQL生成中科学推理能力的基准。BiomedSQL包含68,000个问题/SQL查询/答案三元组，这些三元组基于整合了基因-疾病关联、组学数据的因果推理和药物审批记录的统一BigQuery知识库。每个问题都需要模型推理特定领域的标准，例如全基因组显著性阈值、效应方向性或试验阶段过滤，而不是仅仅依赖于语法翻译。我们评估了多种开放和闭源大型语言模型（LLMs）在不同提示策略和交互范式下的表现。结果显示存在显著的性能差距：GPT-o3-mini实现了59.0%的执行准确率，而我们定制的多步智能体BMSQL达到了62.6%，两者都远低于90.0%的专家基线。BiomedSQL为推进能够通过对结构化生物医学知识库进行鲁棒推理来支持科学发现的文本到SQL系统奠定了新的基础。我们的数据集可在https://huggingface.co/datasets/NIH-CARD/BiomedSQL 公开获取，我们的代码在https://github.com/NIH-CARD/biomedsql 开源。",
    "keywords": [
      "BiomedSQL",
      "Text-to-SQL",
      "生物医学知识库",
      "科学推理",
      "Benchmark"
    ],
    "area": [
      "自然语言处理",
      "大模型",
      "人工智能"
    ],
    "published_time": "2025-05-23T17:58:07.000Z",
    "download_time": "2025-05-29 07:09:18",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20321.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20321",
      "arxiv_url": "https://arxiv.org/abs/2505.20321"
    }
  },
  {
    "id": "2505.17005",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17005",
    "title": "R1-Searcher++：通过强化学习激励大模型的动态知识获取",
    "summary": "大型语言模型（LLMs）功能强大，但由于知识静态而容易产生幻觉。检索增强生成（RAG）通过注入外部信息提供了帮助，但现有方法通常成本较高、泛化性差，或忽略了模型的内部知识。在本文中，我们提出了 R1-Searcher++，一个旨在训练大型语言模型自适应地利用内部和外部知识源的新颖框架。R1-Searcher++ 采用两阶段训练策略：首先进行用于初步格式学习的初始 SFT 冷启动阶段，随后通过强化学习进行动态知识获取。强化学习阶段使用结果监督来鼓励探索，引入奖励机制以利用内部知识，并融合记忆机制来持续吸收检索到的信息，从而丰富模型的内部知识。通过利用内部知识和外部搜索引擎，模型不断提升其能力，实现高效的检索增强推理。我们的实验表明，R1-Searcher++ 在性能上优于先前的检索增强生成和推理方法，并实现了高效的检索。代码可在 https://github.com/RUCAIBox/R1-Searcher-plus 获取。",
    "keywords": [
      "大模型 LLMs",
      "强化学习 RL",
      "检索增强生成 RAG",
      "动态知识获取",
      "内外部知识协同"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "机器学习"
    ],
    "published_time": "2025-05-22T17:58:26.000Z",
    "download_time": "2025-05-29 07:09:30",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17005.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17005",
      "arxiv_url": "https://arxiv.org/abs/2505.17005"
    }
  },
  {
    "id": "2505.11277",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.11277",
    "title": "思考过程中的搜索与精炼：大语言模型的自主检索增强推理",
    "summary": "大语言模型展示了令人印象深刻的推理能力，但其固有的知识储备存在局限性。检索增强推理通过允许大语言模型查询外部资源来缓解这一局限性，但现有方法经常检索到不相关或噪声信息，阻碍了准确推理。在本文中，我们提出了 AutoRefine，这是一个采用全新“在思考过程中搜索与精炼”范式的强化学习后训练框架。AutoRefine 在连续的搜索调用之间引入了显式的知识精炼步骤，使模型能够在生成答案之前迭代地过滤、提炼和组织证据。此外，我们使用组相对策略优化方法，将定制的检索特定奖励与答案正确性奖励相结合。在单跳和多跳问答基准上的实验表明，AutoRefine 显著优于现有方法，尤其是在复杂的多跳推理场景中。详细分析表明，AutoRefine 发出了更频繁、质量更高的搜索，并有效地综合了证据。",
    "keywords": [
      "大语言模型",
      "检索增强推理",
      "强化学习",
      "多跳推理",
      "AutoRefine"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "机器学习"
    ],
    "published_time": "2025-05-16T14:11:29.000Z",
    "download_time": "2025-05-29 07:09:45",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11277.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.11277",
      "arxiv_url": "https://arxiv.org/abs/2505.11277"
    }
  },
  {
    "id": "2505.18657",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18657",
    "title": "多模态大语言模型深受模态偏差的深刻影响",
    "summary": "多模态大语言模型（MLLMs）的最新进展在整合文本和图像等多种模态方面取得了可喜的成果。MLLMs深受模态偏差的严重影响，它们经常过度依赖语言，而对视觉等其他模态的利用不足。本立场论文认为，MLLMs深受模态偏差的深刻影响。首先，我们诊断了模态偏差的当前状况，强调了它在各种任务中的表现形式。其次，我们提出了关于 MLLMs 中模态偏差的系统研究路线图。第三，我们确定了 MLLMs 中模态偏差的关键因素，并为未来的研究提出了可操作的缓解建议。为了证实这些发现，我们进行了实验来证明每个因素的影响：1. 数据特性：语言数据紧凑且抽象，而视觉数据冗余且复杂，这在学习动态中造成了固有的不平衡。2. 不平衡的骨干能力：预训练语言模型在 MLLMs 中的主导地位导致对语言的过度依赖和对视觉信息的忽视。3. 训练目标：当前的训练目标往往未能促进平衡的跨模态对齐，导致偏向语言的捷径学习。这些发现突出了需要平衡的训练策略和模型架构，以便在 MLLMs 中更好地整合多种模态。我们呼吁跨学科合作来应对这些挑战，并推动 MLLM 研究的创新。我们的工作为 MLLMs 中的模态偏差提供了新的视角，并为开发更鲁棒和更具泛化性的多模态系统（推动通用人工智能的进展）提供了见解。",
    "keywords": [
      "MLLMs",
      "Modality Bias",
      "Multimodal Learning",
      "Cross-modal Alignment",
      "Training Objectives"
    ],
    "area": [
      "多模态",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2025-05-24T11:49:31.000Z",
    "download_time": "2025-05-29 07:10:15",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18657.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18657",
      "arxiv_url": "https://arxiv.org/abs/2505.18657"
    }
  },
  {
    "id": "2505.22172",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22172",
    "title": "用于复杂指令遵循的逆向偏好优化",
    "summary": "指令遵循 (IF) 是大型语言模型 (LLM) 的一项关键能力。然而，处理具有多个限制条件的复杂指令仍然具有挑战性。以前的方法通常根据满足的限制条件数量来选择偏好对，这引入了噪声，因为选定的例子可能未能遵循某些限制条件，而被拒绝的例子可能在某些方面优于选定的例子。为了应对与多重偏好对齐的挑战，我们提出了一种简单而有效的方法，称为逆向偏好优化 (RPO)。它通过动态地反转指令中的限制条件来减轻偏好对中的噪声，从而确保选定的响应是完美的，减轻了收集完美响应所需的广泛采样和过滤负担。此外，反转还扩大了选定响应与被拒绝响应之间的差距，从而明确了优化方向，并使其对噪声更具鲁棒性。我们在两个多轮 IF 基准测试 Sysbench 和 Multi-IF 上评估了 RPO，结果表明其相对于 DPO 基线分别平均提高了 4.6 和 2.5 分（在 Llama-3.1 8B 上）。此外，RPO 在不同模型尺寸（8B 到 70B 参数）上表现出有效的可扩展性，其中 70B RPO 模型甚至超越了 GPT-4o。",
    "keywords": [
      "逆向偏好优化 (RPO)",
      "复杂指令遵循",
      "大语言模型 (LLM)",
      "Preference Optimization",
      "噪声缓解"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "生成式AI"
    ],
    "published_time": "2025-05-28T09:44:27.000Z",
    "download_time": "2025-05-29 07:10:31",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22172.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22172",
      "arxiv_url": "https://arxiv.org/abs/2505.22172"
    }
  },
  {
    "id": "2505.20162",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20162",
    "title": "基于能力的LLM红队测试缩放定律",
    "summary": "随着大型语言模型的能力和能动性日益增强，通过红队测试识别漏洞对于安全部署至关重要。然而，一旦红队测试演变为一个从弱到强的问题，即目标模型的能力超出红队测试者，传统的提示工程方法可能失效。为了研究这种转变，我们从攻击者与目标之间能力差距的视角来构建红队测试问题。我们使用模仿人类红队测试者的基于LLM的越狱攻击，评估了涵盖不同系列、规模和能力水平的500多个攻击者-目标对。出现了三个显著趋势：(i) 能力更强的模型是更好的攻击者；(ii) 一旦目标的能力超过攻击者，攻击成功率会急剧下降；(iii) 攻击成功率与在MMLU-Pro基准测试（特别是社会科学分项）上的高性能相关。基于这些趋势，我们推导出了一个越狱攻击的缩放定律，该定律可以根据攻击者与目标之间的能力差距来预测针对固定目标的攻击成功率。这些发现表明，能力固定的攻击者（例如人类）可能对未来的模型无效，能力日益增强的开源模型会放大现有系统的风险，并且模型提供者必须准确测量和控制模型的说服和操纵能力，以限制其作为攻击者的有效性。",
    "keywords": [
      "LLM Red-Teaming",
      "能力差距",
      "缩放定律",
      "越狱攻击",
      "大模型安全"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "人工智能"
    ],
    "published_time": "2025-05-26T16:05:41.000Z",
    "download_time": "2025-05-29 07:10:45",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20162.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20162",
      "arxiv_url": "https://arxiv.org/abs/2505.20162"
    }
  },
  {
    "id": "2505.19973",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19973",
    "title": "DFIR-Metric：一个用于评估数字取证与事件响应领域大语言模型的评测数据集",
    "summary": "数字取证与事件响应 (DFIR) 涉及分析数字证据以支持法律调查。大语言模型 (LLMs) 在日志分析和内存取证等 DFIR 任务中提供了新的机遇，但它们容易出错和产生幻觉的特性在高风险场景中引发了担忧。尽管兴趣日益增长，但目前还没有一个全面的基准来评估 LLM 在理论和实践 DFIR 领域中的表现。为了弥补这一空白，我们提出了 DFIR-Metric，这是一个包含三个组成部分的基准：(1) 知识评估：一套由行业标准认证和官方文档中提取的 700 道经专家评审的多项选择题；(2) 逼真取证挑战：150 个 CTF 风格的任务，测试多步推理和证据关联能力；(3) 实践分析：来自 NIST 计算机取证工具测试项目 (CFTT) 的 500 个磁盘和内存取证案例。我们使用 DFIR-Metric 评估了 14 个 LLM，分析了它们在多次试验中的准确性和一致性。我们还引入了一种新的度量标准——任务理解得分 (TUS)，旨在更有效地评估在准确率接近零的场景中模型的表现。该基准为推动数字取证领域的人工智能发展提供了一个严格、可复现的基础。所有脚本、工件和结果均可在项目网站 https://github.com/DFIR-Metric 上获取。",
    "keywords": [
      "DFIR",
      "大语言模型 LLMs",
      "评测基准 Benchmark",
      "数字取证 Digital Forensics",
      "事件响应 Incident Response"
    ],
    "area": [
      "人工智能",
      "自然语言处理",
      "大模型"
    ],
    "published_time": "2025-05-26T13:35:37.000Z",
    "download_time": "2025-05-29 07:11:07",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19973.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19973",
      "arxiv_url": "https://arxiv.org/abs/2505.19973"
    }
  },
  {
    "id": "2505.19650",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19650",
    "title": "模态精选：构建用于高级多模态信息检索的通用嵌入",
    "summary": "多模态信息检索（MIR）由于数据源的异质性和跨模态对齐的复杂性而面临固有挑战。尽管先前的研究已经发现了特征空间中的模态鸿沟，但解决这些挑战的系统方法仍未得到探索。在本项目中，我们引入了UNITE，这是一个通用框架，通过数据精选和模态感知训练配置这两个关键但研究不足的方面来应对这些挑战。我们的工作首次全面分析了模态特定的数据属性如何影响不同场景下的下游任务性能。此外，我们提出了模态感知掩码对比学习（Modal-Aware Masked Contrastive Learning，MAMCL），以减轻不同模态实例之间的竞争关系。我们的框架在多个多模态检索基准上取得了最先进的结果，显著优于现有方法。通过广泛的实验，我们证明了策略性模态精选和定制的训练协议对于鲁棒的跨模态表征学习至关重要。这项工作不仅提高了MIR的性能，还为未来多模态系统的研究提供了基础蓝图。我们的项目位于 https://friedrichor.github.io/projects/UNITE。",
    "keywords": [
      "多模态信息检索",
      "模态精选",
      "通用嵌入",
      "跨模态学习",
      "对比学习"
    ],
    "area": [
      "多模态",
      "机器学习",
      "深度学习"
    ],
    "published_time": "2025-05-26T08:09:44.000Z",
    "download_time": "2025-05-29 07:11:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19650.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19650",
      "arxiv_url": "https://arxiv.org/abs/2505.19650"
    }
  },
  {
    "id": "2505.22633",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22633",
    "title": "空间知识图谱引导的多模态合成",
    "summary": "近期多模态大语言模型（MLLMs）的进展显著增强了其能力；然而，它们的空间感知能力仍是一个显著的局限。为了解决这一挑战，多模态数据合成提供了一种有前景的解决方案。然而，确保合成数据符合空间常识是一项不平凡的任务。在这项工作中，我们引入了 SKG2Data，一种由空间知识图谱引导的新型多模态合成方法，其基础是知识到数据生成的概念。SKG2Data 自动构建空间知识图谱（SKG），以模拟人类对空间方向和距离的感知，随后利用该图谱来指导多模态数据合成。大量实验表明，利用包括方向和距离在内的各种空间知识合成的数据，不仅增强了 MLLMs 的空间感知和推理能力，而且表现出强大的泛化能力。我们希望基于知识的数据合成这一思想能够推动空间智能的发展。",
    "keywords": [
      "空间知识图谱",
      "多模态合成",
      "MLLMs",
      "空间感知",
      "知识到数据生成"
    ],
    "area": [
      "多模态",
      "大模型",
      "生成式AI"
    ],
    "published_time": "2025-05-28T17:50:21.000Z",
    "download_time": "2025-05-29 07:11:38",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22633.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22633",
      "arxiv_url": "https://arxiv.org/abs/2505.22633"
    }
  },
  {
    "id": "2505.21499",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21499",
    "title": "AdInject：通过广告投送对网络智能体发起真实世界黑盒攻击",
    "summary": "基于视觉-语言模型（VLM）的网络智能体通过模拟人类与网站的交互，在自动化复杂任务方面迈出了重要一步。然而，它们在不受控的网络环境中部署带来了重大的安全漏洞。现有关于对抗性环境注入攻击的研究通常依赖于不切合实际的假设，例如直接操纵 HTML、了解用户意图或访问智能体模型参数，这限制了它们的实际适用性。在本文中，我们提出了 AdInject，一种新颖且真实世界的黑盒攻击方法，它利用互联网广告投送将恶意内容注入到网络智能体的环境中。 AdInject 在比先前工作更现实的威胁模型下运行，假设智能体是黑盒的，恶意内容具有静态约束，并且不了解用户意图。AdInject 包括旨在误导智能体点击的恶意广告内容设计策略，以及一种基于 VLM 的广告内容优化技术，该技术从目标网站的上下文中推断潜在的用户意图，并将这些意图整合到广告内容中，使其对智能体的任务看起来更相关或更关键，从而增强攻击的有效性。实验评估证明了 AdInject 的有效性，在大多数场景下的攻击成功率超过 60%，在某些情况下接近 100%。这有力地表明，普遍存在的广告投送是针对网络智能体发起环境注入攻击的强大且真实世界的载体。这项工作强调了源自真实世界环境操纵渠道的网络智能体安全面临的关键漏洞，并突显了迫切需要开发针对此类威胁的强大防御机制。我们的代码可在 https://github.com/NicerWang/AdInject 获取。",
    "keywords": [
      "AdInject",
      "网络智能体",
      "对抗性攻击",
      "广告投送",
      "视觉-语言模型"
    ],
    "area": [
      "智能体",
      "多模态",
      "人工智能"
    ],
    "published_time": "2025-05-27T17:59:05.000Z",
    "download_time": "2025-05-29 07:11:48",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21499.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21499",
      "arxiv_url": "https://arxiv.org/abs/2505.21499"
    }
  },
  {
    "id": "2505.20279",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20279",
    "title": "VLM-3R：指令对齐三维重建增强的视觉语言模型",
    "summary": "针对2D图像和视频的大型多模态模型（LMMs）的快速发展，促进了将这些模型扩展到理解3D场景，旨在实现类人的视觉空间智能。然而，实现与人类能力相当的深度空间理解在模型编码和数据获取方面带来了显著挑战。现有方法经常依赖外部深度传感器进行几何捕获，或利用现成的算法预构建3D地图，从而限制了其可扩展性，特别是对于普遍存在的单目视频输入和对时间敏感的应用。在这项工作中，我们引入了VLM-3R，一个统一的视觉语言模型（VLMs）框架，它集成了3D重建指令微调。VLM-3R通过使用几何编码器来生成代表空间理解的隐式3D tokens，从而处理单目视频帧。利用我们的空间-视觉-视图融合（Spatial-Visual-View Fusion）以及超过20万对精选的3D重建指令微调问答（QA）对，VLM-3R有效地将现实世界的空间语境与语言指令对齐，从而实现了单目3D空间辅助和具身推理。为了促进时间推理的评估，我们引入了视觉-空间-时间智能基准，包含五个不同任务的超过13.86万对问答对，专注于演变中的空间关系。大量实验表明，我们的模型VLM-3R不仅促进了鲁棒的视觉空间推理，而且能够理解时间性的3D语境变化，在准确性和可扩展性方面均表现出色。",
    "keywords": [
      "VLM-3R",
      "3D Reconstruction",
      "Vision-Language Models",
      "Instruction Tuning",
      "Spatial-Temporal Reasoning"
    ],
    "area": [
      "计算机视觉",
      "多模态",
      "大模型"
    ],
    "published_time": "2025-05-26T17:56:30.000Z",
    "download_time": "2025-05-29 07:12:03",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20279.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20279",
      "arxiv_url": "https://arxiv.org/abs/2505.20279"
    }
  },
  {
    "id": "2505.19094",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19094",
    "title": "SATORI-R1：通过空间锚定和可验证奖励激励多模态推理",
    "summary": "DeepSeek-R1 通过稳定的强化学习 (RL) 在文本领域展现了强大的推理能力。近年来，在多模态领域，研究开始直接应用强化学习生成类似 R1 的自由形式推理来完成视觉问答 (VQA) 任务。然而，多模态任务与文本任务本质上存在差异，它们解决问题严重依赖于对输入图像的理解。因此，这种自由形式推理在 VQA 任务中面临两个关键局限：(1) 过长的推理链会使视觉焦点从任务关键区域扩散开，从而降低答案准确性。(2) 不可验证的中间步骤会放大策略梯度方差和计算开销。为了解决这些问题，本文提出了 SATORI (Spatially Anchored Task Optimization with ReInforcement Learning，空间锚定任务优化强化学习)，该方法将 VQA 分解为三个可验证的阶段，包括全局图像描述、区域定位和答案预测，每个阶段都提供明确的奖励信号。此外，我们还引入了 VQA-Verify 数据集，这是一个包含 12k 条标注了答案对齐的图像描述和边界框的数据集，用于促进训练。实验表明，该方法在七个 VQA 基准测试中实现了持续的性能提升，与类似 R1 的基线相比，准确率最高提高了 15.7%。我们对注意力图的分析证实了对关键区域增强的关注，从而带来了准确率的提高。我们的代码可在 https://github.com/justairr/SATORI-R1 获取。",
    "keywords": [
      "多模态推理",
      "视觉问答 (VQA)",
      "强化学习",
      "空间锚定",
      "可验证奖励"
    ],
    "area": [
      "多模态",
      "计算机视觉",
      "机器学习"
    ],
    "published_time": "2025-05-25T11:11:06.000Z",
    "download_time": "2025-05-29 07:12:19",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19094.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19094",
      "arxiv_url": "https://arxiv.org/abs/2505.19094"
    }
  },
  {
    "id": "2505.17908",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17908",
    "title": "ComfyMind：通过基于树的规划和反应式反馈实现通用生成",
    "summary": "随着生成模型的高速发展，通用生成作为一种有望在单一系统中统一跨模态多样化任务的方法，受到越来越多的关注。尽管取得了这些进展，但现有开源框架由于缺乏结构化工作流规划和执行层反馈，往往仍然脆弱且难以支持复杂的现实世界应用。为了解决这些局限性，我们提出了 ComfyMind，这是一个协作式AI系统，旨在实现鲁棒且可扩展的通用生成，该系统构建于 ComfyUI 平台之上。ComfyMind 引入了两项核心创新：语义工作流接口 (SWI)，它将低层次节点图抽象为自然语言描述的可调用功能模块，实现高层次组合并减少结构错误；具有局部反馈执行的搜索树规划机制，它将生成建模为分层决策过程，并允许在每个阶段进行自适应修正。这些组件共同提升了复杂生成工作流的稳定性和灵活性。我们在三个公开基准测试集上评估了 ComfyMind：ComfyBench、GenEval 和 Reason-Edit，这些基准涵盖生成、编辑和推理任务。结果表明，ComfyMind 持续优于现有开源基线，并达到了与 GPT-Image-1 相当的性能。ComfyMind 为开源通用生成式AI系统的发展开辟了一条有前景的道路。项目页面：https://github.com/LitaoGuo/ComfyMind",
    "keywords": [
      "通用生成",
      "树形规划",
      "反应式反馈",
      "工作流",
      "ComfyUI"
    ],
    "area": [
      "人工智能",
      "生成式AI",
      "智能体"
    ],
    "published_time": "2025-05-23T13:53:03.000Z",
    "download_time": "2025-05-29 07:12:35",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17908.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17908",
      "arxiv_url": "https://arxiv.org/abs/2505.17908"
    }
  },
  {
    "id": "2505.16673",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16673",
    "title": "R1-ShareVL：通过 Share-GRPO 激励多模态大语言模型的推理能力",
    "summary": "在这项工作中，我们旨在通过强化学习 (RL) 激励多模态大语言模型 (MLLMs) 的推理能力，并开发一种有效方法来缓解 강화学习过程中稀疏奖励和优势值消失的问题。为此，我们提出了 Share-GRPO，一种新颖的强化学习方法，它通过在扩展的问题空间上探索和共享多样化的推理轨迹来解决这些问题。具体来说，Share-GRPO 首先通过数据转换技术扩展给定问题的问题空间，然后在强化学习过程中鼓励 MLLM 有效地探索扩展问题空间上的多样化推理轨迹，并跨扩展问题共享发现的推理轨迹。此外，Share-GRPO 在优势计算期间也共享奖励信息，它在问题变体之间和内部层级地估计解决方案优势，从而实现更准确的相对优势估计并提高策略训练的稳定性。在六个广泛使用的推理基准上进行的广泛评估展示了我们方法的优越性能。代码将发布在 https://github.com/HJYao00/R1-ShareVL。",
    "keywords": [
      "多模态大语言模型",
      "强化学习",
      "推理能力",
      "Share-GRPO",
      "推理轨迹"
    ],
    "area": [
      "人工智能",
      "多模态",
      "大模型"
    ],
    "published_time": "2025-05-22T13:39:32.000Z",
    "download_time": "2025-05-29 07:13:00",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16673.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16673",
      "arxiv_url": "https://arxiv.org/abs/2505.16673"
    }
  },
  {
    "id": "2505.22096",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.22096",
    "title": "用于知识增强的Text-to-SQL的知识库构建",
    "summary": "Text-to-SQL旨在将自然语言查询转换为SQL语句，这非常实用，因为它使任何人都能轻松地从数据库中检索所需信息。近年来，许多现有方法利用大型语言模型（LLMs）强大的理解用户查询和生成相应SQL代码的能力来解决这个问题。然而，LLMs中的参数知识可能不足以覆盖需要基于各种数据库模式（schema）进行推理的各种多样化和领域特定查询，这常常导致生成的SQL不够准确。为了解决这个问题，我们提出构建用于Text-to-SQL的知识库，它是一个基础知识来源，我们可以从中检索和生成给定查询所需的必要知识。特别地，与现有方法不同（它们或手动标注知识，或仅为每个查询生成少量知识），我们的知识库是全面的，它是结合所有可用问题及其相关的数据库模式及其相关知识构建的，并且可以重复用于不同数据集和领域中未曾见过的数据库。我们在多个Text-to-SQL数据集上验证了我们的方法，同时考虑了数据库重叠和非重叠的场景，结果显示其表现显著优于相关基线方法。",
    "keywords": [
      "Text-to-SQL",
      "知识库构建",
      "知识增强",
      "LLMs",
      "数据库模式"
    ],
    "area": [
      "自然语言处理",
      "机器学习",
      "大模型"
    ],
    "published_time": "2025-05-28T08:17:58.000Z",
    "download_time": "2025-05-29 07:13:16",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22096.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.22096",
      "arxiv_url": "https://arxiv.org/abs/2505.22096"
    }
  },
  {
    "id": "2505.21062",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21062",
    "title": "逆向虚拟试穿：从穿着衣服的个体图像中生成多类别产品样式图像",
    "summary": "虚拟试穿（VTON）系统旨在将一件服装渲染到目标人物图像上，与此不同，本文处理一项新颖的任务——虚拟脱衣（VTOFF），它解决的是一个逆向问题：从穿着衣服的个体的真实世界照片中生成标准化的服装产品图像。与VTON必须解决多样的姿态和样式变化不同，VTOFF受益于一致且明确定义的输出格式——通常是服装的平铺式表示——这使其成为数据生成和数据集增强的有前途的工具。然而，现有的VTOFF方法面临两个主要限制：(i) 难以将服装特征与遮挡和复杂姿态分离，经常导致视觉伪影，以及 (ii) 仅限于单类别服装（例如，仅限上半身衣物），限制了泛化能力。为解决这些挑战，我们提出了一种文本增强的多类别虚拟脱衣（TEMU-VTOFF）方法，这是一种新颖的架构，其核心是一个基于双DiT的骨干网络，并带有改进的多模态注意力机制，用于鲁棒的服装特征提取。我们的架构被设计为接收来自图像、文本和掩码等多种模态的服装信息，以适应多类别设置。最后，我们提出了一个额外的对齐模块，以进一步细化生成的视觉细节。在VITON-HD和Dress Code数据集上的实验表明，TEMU-VTOFF在VTOFF任务上达到了新的最先进水平，显著提高了视觉质量以及与目标服装的逼真度。",
    "keywords": [
      "虚拟脱衣 (VTOFF)",
      "多类别",
      "图像生成",
      "逆向问题",
      "多模态"
    ],
    "area": [
      "计算机视觉",
      "深度学习",
      "生成式AI"
    ],
    "published_time": "2025-05-27T11:47:51.000Z",
    "download_time": "2025-05-29 07:13:31",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21062.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21062",
      "arxiv_url": "https://arxiv.org/abs/2505.21062"
    }
  },
  {
    "id": "2505.19377",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19377",
    "title": "绝对坐标使运动生成更容易",
    "summary": "最先进的文本到运动生成模型依赖于 HumanML3D 推广的、具有运动学感知能力的局部相对运动表示，该表示将运动编码为相对于骨盆和前一帧，并具有内置冗余。虽然这种设计简化了早期生成模型的训练，但它给扩散模型带来了关键限制，并阻碍了其在下游任务中的应用。在这项工作中，我们重新审视了运动表示，并提出了一种针对文本到运动生成的、被长期放弃但被彻底简化的替代方案：全局空间中的绝对关节坐标。通过对设计选择进行系统分析，我们表明，即使采用简单的 Transformer 主干网络且不使用辅助的运动学感知损失，这种表达方式也能显著提高运动保真度、改善文本对齐，并具有强大的可扩展性。此外，我们的表达方式自然地支持文本驱动的运动控制以及时间/空间编辑等下游任务，无需额外的针对特定任务的重新设计以及昂贵的来自控制信号的分类器指导生成。最后，我们展示了直接从文本生成运动中的 SMPL-H 网格顶点的良好泛化能力，为未来的研究和运动相关应用奠定了坚实基础。",
    "keywords": [
      "文本到运动生成",
      "绝对关节坐标",
      "运动表示",
      "局部相对运动",
      "下游任务"
    ],
    "area": [
      "生成式AI",
      "多模态",
      "深度学习"
    ],
    "published_time": "2025-05-26T00:36:00.000Z",
    "download_time": "2025-05-29 07:13:48",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19377.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19377",
      "arxiv_url": "https://arxiv.org/abs/2505.19377"
    }
  },
  {
    "id": "2505.19235",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19235",
    "title": "CoreMatching: 一种结合Token与神经元剪枝的协同自适应稀疏推理框架，用于视觉-语言模型的全面加速",
    "summary": "视觉-语言模型 (VLM) 在各种任务中表现出色，但面临高昂的时空推理成本。Token 稀疏性缓解了 Token 使用中的低效问题，而神经元稀疏性则减少了高维计算，两者都为提高效率提供了有前景的解决方案。近年来，这两种稀疏范式在很大程度上是并行发展的，由此产生了它们独立运作的主流假设。然而，一个基本却未得到充分探索的问题依然存在：它们真的各自独立运作吗，还是存在尚未被揭示的更深层潜在相互作用？在本文中，我们首次对这一问题进行了全面探究。通过引入和分析核心神经元（Core Neurons）与核心 Token（Core Tokens）之间的匹配机制，我们发现对于推理而言，关键神经元和 Token 之间存在相互影响和强化的关系。基于这一洞察，我们提出了 CoreMatching，一个协同自适应稀疏推理框架，它利用 Token 和神经元稀疏性之间的协同作用来提高推理效率。通过理论分析和效率评估，我们证明了所提出的方法在十个图像理解任务和三种硬件设备上超越了现有最佳基线。值得注意的是，在 NVIDIA Titan Xp 上，它实现了 5 倍的 FLOPs 降低和 10 倍的整体加速。代码已在 https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main 发布。",
    "keywords": [
      "VLMs",
      "Sparse Inference",
      "Token Pruning",
      "Neuron Pruning",
      "CoreMatching"
    ],
    "area": [
      "多模态",
      "深度学习",
      "计算机视觉"
    ],
    "published_time": "2025-05-25T17:16:34.000Z",
    "download_time": "2025-05-29 07:14:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19235.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19235",
      "arxiv_url": "https://arxiv.org/abs/2505.19235"
    }
  },
  {
    "id": "2505.17855",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17855",
    "title": "解释自动化事实核查中不确定性的来源",
    "summary": "理解模型对其预测的不确定性来源对于有效的人机协作至关重要。先前工作提出了使用数值不确定性或模糊表达（如“我不确定，但……”），但这些方法未能解释源于证据冲突的不确定性，导致用户无法解决分歧或依赖输出。我们引入了 CLUE（冲突与一致性感知语言模型不确定性解释），这是首个通过以下方式生成模型不确定性自然语言解释的框架：(i) 以无监督方式识别文本片段之间揭示主张-证据或证据间冲突与一致性关系的联系，这些关系驱动着模型的预测不确定性；(ii) 通过提示和注意力导向生成解释，以语言形式阐述这些关键的交互。在三种语言模型和两个事实核查数据集上，我们表明 CLUE 生成的解释比没有文本片段交互指导的直接提示方式，更能忠实于模型的不确定性，也与事实核查决策更一致。人类评估者认为我们的解释比该基线方法更有帮助、信息量更大、更少冗余且与输入更具逻辑一致性。CLUE 无需微调或改变模型架构，使其对于任何白盒语言模型都即插即用。通过将不确定性明确关联到证据冲突，它为事实核查提供了实用的支持，并易于推广到其他需要对复杂信息进行推理的任务。",
    "keywords": [
      "自动化事实核查",
      "不确定性解释",
      "语言模型",
      "证据冲突",
      "自然语言解释"
    ],
    "area": [
      "自然语言处理",
      "大模型",
      "人工智能"
    ],
    "published_time": "2025-05-23T13:06:43.000Z",
    "download_time": "2025-05-29 07:14:18",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17855.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17855",
      "arxiv_url": "https://arxiv.org/abs/2505.17855"
    }
  },
  {
    "id": "2505.15561",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.15561",
    "title": "RAG 系统存在位置偏差问题吗？",
    "summary": "检索增强生成（Retrieval Augmented Generation, RAG）通过将从外部语料库检索到的文本段落添加到大型语言模型（LLM）的提示中，从而提高LLM的准确性。本文研究了位置偏差——即LLM根据信息在提示中的位置赋予其不同权重的倾向——如何不仅影响LLM利用相关段落的能力，还影响其对干扰性段落的敏感性。通过在三个基准数据集上的广泛实验，我们表明，最先进的检索流程在试图检索相关段落的同时，系统性地将高度干扰性的段落置于靠前的位置，超过60%的查询在其检索到的前10个段落中至少包含一个高度干扰性段落。因此，在真实场景中，LLM位置偏差的影响实际上是微不足道的，这与相关工作在受控设置下报告的非常显著的结果不同，因为相关段落和干扰性段落在这类位置上的影响都会受到削弱。事实上，我们的研究发现，试图根据LLM的位置偏好重新排列段落的复杂策略，其表现并不优于随机打乱。",
    "keywords": [
      "RAG",
      "LLM",
      "位置偏差",
      "检索增强生成",
      "干扰信息"
    ],
    "area": [
      "大模型",
      "生成式AI",
      "自然语言处理"
    ],
    "published_time": "2025-05-21T14:18:01.000Z",
    "download_time": "2025-05-29 07:14:36",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15561.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.15561",
      "arxiv_url": "https://arxiv.org/abs/2505.15561"
    }
  },
  {
    "id": "2505.21501",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.21501",
    "title": "带自蒸馏寄存器的视觉Transformer",
    "summary": "视觉Transformer (ViT) 已成为视觉处理任务的主流架构，随着训练数据和模型规模的增加，展现出出色的可扩展性。然而，最近的研究发现 ViT 中会出现与局部语义不符的伪影（artifact）标记。这些异常标记会降低 ViT 在需要细粒度定位或结构一致性的任务中的性能。解决此问题的一种有效方法是向 ViT 添加寄存器（register）标记，这些标记在训练期间隐式地“吸收”伪影项。鉴于存在各种大规模预训练的 ViT 模型，本文旨在为其配备此类寄存器标记，而无需从头开始对其进行重新训练，考虑到模型的规模，这通常是不可行的。具体来说，我们提出了 后验寄存器 (Post Hoc Registers, PH-Reg) 方法，这是一种高效的自蒸馏方法，可以将寄存器整合到现有的 ViT 中，而无需额外的标注数据和全面再训练。PH-Reg 从相同的预训练 ViT 初始化教师网络和学生网络。教师网络保持冻结和不变，而学生网络则新增了随机初始化的寄存器标记。通过对教师网络的输入应用测试时增强（test-time augmentation），我们生成了无伪影的降噪密集嵌入（dense embeddings），然后仅使用这些嵌入来优化学生网络中一小部分未冻结的权重。我们证明了我们的方法可以有效减少伪影标记数量，并在零样本（zero-shot）和线性探测（linear probing）设置下改进学生 ViT 的分割和深度预测性能。",
    "keywords": [
      "Vision Transformers",
      "Registers",
      "Self-Distillation",
      "Artifact Tokens",
      "Pre-trained Models"
    ],
    "area": [
      "计算机视觉",
      "深度学习",
      "人工智能"
    ],
    "published_time": "2025-05-27T17:59:41.000Z",
    "download_time": "2025-05-29 07:14:49",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21501.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.21501",
      "arxiv_url": "https://arxiv.org/abs/2505.21501"
    }
  },
  {
    "id": "2505.20052",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20052",
    "title": "Ankh3：通过序列去噪和补全的多任务预训练增强蛋白质表征",
    "summary": "蛋白质语言模型 (PLMs) 已成为检测蛋白质序列复杂模式的强大工具。然而，PLMs 充分捕捉蛋白质序列信息的能力可能因专注于单一预训练任务而受到限制。尽管增加数据模态或监督目标可以提高 PLMs 的性能，但 pre-training 通常仍专注于对受损序列进行去噪。为了拓展 PLMs 的边界，我们的研究探索了一种多任务预训练策略。我们开发了 Ankh3 模型，该模型针对两个目标联合优化：具有多种掩码概率的掩码语言建模，以及仅依赖蛋白质序列作为输入的蛋白质序列补全。这种多任务 pre-training 表明，PLMs 仅凭蛋白质序列就能学习到更丰富、更具泛化性的表征。结果表明，在二级结构预测、荧光、GB1 适应度和接触预测等下游任务中性能有所提升。多任务的整合使得模型对蛋白质特性有了更全面的理解，从而带来了更稳健和准确的预测。",
    "keywords": [
      "蛋白质语言模型",
      "多任务预训练",
      "蛋白质表征",
      "序列学习",
      "下游任务"
    ],
    "area": [
      "深度学习",
      "机器学习",
      "人工智能"
    ],
    "published_time": "2025-05-26T14:41:10.000Z",
    "download_time": "2025-05-29 07:15:06",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20052.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20052",
      "arxiv_url": "https://arxiv.org/abs/2505.20052"
    }
  },
  {
    "id": "2505.20036",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.20036",
    "title": "超越简单拼接：公平评估 PLM 架构用于多链蛋白-蛋白相互作用预测",
    "summary": "蛋白-蛋白相互作用（PPIs）是众多细胞过程的基础，其特征描述对于理解疾病机制和指导药物发现至关重要。虽然蛋白质语言模型（PLMs）在预测蛋白质结构和功能方面已取得显著成功，但它们在基于序列的 PPI 结合亲和力预测方面的应用仍相对有待深入探索。这一差距通常归因于高质量、严格精炼数据集的稀缺性以及对简单拼接蛋白质表示策略的依赖。在本工作中，我们解决了这些局限性。首先，我们通过解决多链蛋白质相互作用的注释不一致和重复条目问题，构建了一个精心整理的 PPB-Affinity 数据集版本，共包含 8,207 个独特的蛋白-蛋白相互作用条目。该数据集采用了严格的、小于等于 30% 的序列同一性阈值，以确保数据集能够稳健地划分为训练集、验证集和测试集，从而最大程度地减少数据泄露。其次，我们提出并系统评估了四种用于将 PLMs 适配到 PPI 结合亲和力预测的架构：嵌入层拼接（EC）、序列拼接（SC）、分层池化（HP）和池化注意力相加（PAD）。这些架构使用两种训练方法进行了评估：全微调和一种在冻结的 PLM 特征上使用 ConvBERT 头部的轻量级方法。我们对多种领先的 PLMs（ProtT5、ESM2、Ankh、Ankh2 和 ESM3）进行的综合实验表明，HP 和 PAD 架构始终优于传统的拼接方法，在 Spearman 相关系数方面实现了高达 12% 的提升。这些结果突显了需要复杂的架构设计来充分利用 PLMs 的能力进行细致的 PPI 结合亲和力预测。",
    "keywords": [
      "Protein-Protein Interaction",
      "Protein Language Models",
      "Deep Learning Architectures",
      "Binding Affinity Prediction",
      "Datasets"
    ],
    "area": [
      "深度学习",
      "机器学习",
      "自然语言处理"
    ],
    "published_time": "2025-05-26T14:23:08.000Z",
    "download_time": "2025-05-29 07:15:24",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20036.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.20036",
      "arxiv_url": "https://arxiv.org/abs/2505.20036"
    }
  },
  {
    "id": "2505.19954",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.19954",
    "title": "一种基于强化优化大语言模型推理的神经退行性痴呆可解释诊断框架",
    "summary": "神经退行性痴呆的鉴别诊断是一项具有挑战性的临床任务，主要原因是症状表现的重叠以及结构性神经影像中观察到的相似模式。为了提高诊断效率和准确性，已经提出了基于深度学习的方法，例如卷积神经网络和视觉Transformer，用于脑部MRI的自动分类。然而，尽管这些模型具有很强的预测性能，但由于其不透明的决策过程，临床应用受到限制。在本文中，我们提出一种框架，整合两个核心组件以增强诊断透明性。首先，我们引入了一个模块化流程，用于将三维T1加权脑部MRI转换为文本形式的放射学报告。其次，我们探索了现代大语言模型（LLMs）基于生成的报告辅助临床医生对额颞叶痴呆亚型、阿尔茨海默病和正常衰老进行鉴别诊断的潜力。为了弥合预测准确性和可解释性之间的差距，我们采用强化学习来激励LLMs进行诊断推理。我们的方法无需监督的推理轨迹或从大型模型中蒸馏，即可使基于神经影像学发现的结构化诊断理由得以产生。与追溯解释模型决策的后验可解释性方法不同，我们的框架在推理过程中生成诊断理由，从而产生因果基础的解释，这些解释告知并指导模型的决策过程。通过这种方式，我们的框架在提供支持其诊断结论的理由的同时，也达到了现有深度学习方法的诊断性能。",
    "keywords": [
      "神经退行性痴呆",
      "可解释AI",
      "大语言模型",
      "强化学习",
      "神经影像"
    ],
    "area": [
      "大模型",
      "强化学习",
      "计算机视觉"
    ],
    "published_time": "2025-05-26T13:18:32.000Z",
    "download_time": "2025-05-29 07:15:41",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19954.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.19954",
      "arxiv_url": "https://arxiv.org/abs/2505.19954"
    }
  },
  {
    "id": "2505.17190",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17190",
    "title": "热带注意力：用于组合算法的神经算法推理",
    "summary": "组合优化问题的动态规划（DP）算法在其递归算法中涉及取最大值、最小值和经典加法运算。相关的价值函数对应于最大加半环中的凸多面体。然而，现有的神经算法推理模型依赖于softmax归一化的点积注意力，这种平滑的指数加权会模糊这些尖锐的多面体结构，并在分布外（OOD）设置下评估时出现性能崩溃。我们引入了热带注意力，这是一种新颖的注意力函数，它在热带几何的最大加半环中原生运行。我们证明了热带注意力可以近似DP型组合算法的热带电路。接着我们提出，在算法推理任务中，使用热带Transformer可以增强在长度泛化和值泛化方面的经验性OOD性能，超越softmax基线，同时在对抗性攻击下保持稳定。我们还将对抗性攻击泛化作为神经算法推理基准测试的第三个维度。我们的结果表明，热带注意力恢复了softmax所缺乏的尖锐、尺度不变的推理能力。",
    "keywords": [
      "热带注意力",
      "神经算法推理",
      "组合优化",
      "分布外泛化",
      "最大加半环"
    ],
    "area": [
      "深度学习",
      "机器学习",
      "人工智能"
    ],
    "published_time": "2025-05-22T18:01:25.000Z",
    "download_time": "2025-05-29 07:15:52",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17190.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17190",
      "arxiv_url": "https://arxiv.org/abs/2505.17190"
    }
  }
]