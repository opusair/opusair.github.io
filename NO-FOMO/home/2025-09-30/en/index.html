<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-09-30</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-09-30</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Sora 2</h2>
                <span class="published-time">Published: 2025-09-30 16:55:01</span>
                
                <p class="summary">OpenAI has introduced Sora 2, the latest iteration of its text-to-video generation model, building upon the capabilities of its predecessor. Sora 2 is designed to create highly realistic and imaginative video scenes purely from text descriptions, demonstrating significant advancements in understanding complex prompts, character consistency, and dynamic scene generation. The release is accompanied by a 'system card,' outlining the model's technical specifications, potential applications, and, crucially, safety considerations and ethical implications associated with advanced generative AI. This development underscores OpenAI's ongoing commitment to pushing the boundaries of multimodal AI, enabling users to translate abstract textual ideas into visually compelling moving images with unprecedented fidelity and control, while also addressing responsible deployment.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Sora 2</span><span>Text-to-Video</span><span>Generative AI</span><span>AI Video Generation</span><span>OpenAI</span><span>Deep Learning</span><span>Multimodal AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://openai.com/index/sora-2/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Claude Agent SDK for Python</h2>
                <span class="published-time">Published: 2025-09-30 18:55:22</span>
                
                <p class="summary">Anthropic has officially launched the Claude Agent SDK for Python, a new software development kit aimed at empowering developers to build and deploy sophisticated AI agents utilizing Anthropic's Claude large language models. This SDK provides a comprehensive suite of tools and functionalities designed to simplify the complex process of integrating advanced conversational and reasoning capabilities into agent-based applications. By offering a Pythonic interface, the SDK enables developers to programmatically interact with Claude, facilitating the creation of intelligent agents that can understand natural language, engage in complex reasoning, make informed decisions, and execute actions within various environments. This release is expected to accelerate the development of practical AI agent applications across diverse sectors, allowing for more efficient automation, enhanced problem-solving, and novel interactive experiences. It represents a significant step towards making powerful AI agent technology more accessible and easier to implement for a broad range of development needs, fostering innovation in the AI ecosystem.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Claude</span><span>AI Agent</span><span>Python SDK</span><span>Large Language Model</span><span>Anthropic</span><span>Software Development</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/anthropics/claude-agent-sdk-python" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Comprehension debt: A ticking time bomb of LLM-generated code</h2>
                <span class="published-time">Published: 2025-09-30 10:37:39</span>
                
                <p class="summary">The growing dependency on Large Language Models (LLMs) for generating code introduces a critical, yet often overlooked, challenge termed "comprehension debt." This concept, akin to technical debt, refers to the accumulated difficulty and time investment required for human developers to fully understand, maintain, and debug software components predominantly authored by AI. While LLMs offer significant advantages in accelerating initial development phases and boosting productivity, the article warns that a reduced cognitive load during generation can lead to a substantial increase in cognitive burden during later stages of the software lifecycle. This "ticking time bomb" threatens future software projects with potential slowdowns in maintenance, higher risks of introducing new bugs due to incomplete understanding, and a general decline in the long-term sustainability and quality of codebases. The implication is that without strategic interventions, organizations risk accumulating an unsustainable level of comprehension debt, which could severely impede innovation and operational efficiency as these AI-generated systems mature and require complex modifications or extensive troubleshooting. Addressing this requires a re-evaluation of developer workflows, potentially integrating new tools or methodologies to enhance human understanding and oversight of LLM-generated code from its inception.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Code Generation</span><span>Software Development</span><span>Technical Debt</span><span>AI Ethics</span><span>Code Comprehension</span><span>Software Maintenance</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Extract-0: A specialized language model for document information extraction</h2>
                <span class="published-time">Published: 2025-09-30 16:31:27</span>
                
                <p class="summary">Extract-0 introduces a novel specialized language model designed specifically for document information extraction tasks. This model addresses the common challenges associated with accurately and efficiently extracting structured data from unstructured or semi-structured documents. By leveraging advanced natural language processing and deep learning techniques, Extract-0 aims to significantly improve the precision and recall of information retrieval, making it particularly useful for applications requiring automated data capture from various document types, such as invoices, contracts, and research papers. Its specialized architecture allows for fine-tuned performance on complex extraction scenarios, moving beyond general-purpose models to offer enhanced capabilities in targeted information extraction. The development signifies a crucial step forward in creating more adaptable and precise AI tools for business and research workflows that heavily rely on advanced document understanding and data digitization.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Document Information Extraction</span><span>Specialized Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span><span>Information Retrieval</span><span>Data Extraction</span><span>AI for Documents</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://arxiv.org/abs/2509.22906" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>What is 'compute-in-memory' and why is it important for AI?</h2>
                <span class="published-time">Published: 2025-09-30 18:20:42</span>
                
                <p class="summary">Compute-in-memory (CIM) represents a transformative architectural paradigm designed to address the inherent inefficiencies of traditional von Neumann architectures, where data must constantly shuttle between a central processing unit and discrete memory modules. This incessant data movement, known as the 'von Neumann bottleneck,' consumes significant energy and introduces latency, severely limiting the performance and power efficiency of modern computing systems. CIM tackles this challenge by integrating computational capabilities directly into or very close to memory units, allowing data processing to occur where the data resides. This novel approach is particularly vital for Artificial Intelligence applications, which are inherently data-intensive and computationally demanding. By minimizing the energy-intensive and time-consuming transfer of large datasets, CIM can dramatically accelerate neural network inference and training, enhancing overall AI system performance. It promises substantial improvements in energy efficiency and computational speed, making it an attractive solution for deploying complex AI models on resource-constrained edge devices, mobile platforms, and for enabling more powerful high-performance AI accelerators. As AI workloads continue to grow in complexity and scale, CIM offers a critical pathway toward achieving the necessary computational throughput and power efficiency for the next generation of intelligent systems, facilitating broader and more efficient AI adoption across various domains.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Compute-in-memory</span><span>Artificial Intelligence</span><span>AI hardware</span><span>Neural networks</span><span>Energy efficiency</span><span>Von Neumann bottleneck</span><span>Edge AI</span><span>In-memory computing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.microcontrollertips.com/what-is-compute-in-memory-and-why-is-it-important-for-ai/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AI will happily design the wrong thing for you</h2>
                <span class="published-time">Published: 2025-09-30 15:20:37</span>
                
                <p class="summary">The article, "AI will happily design the wrong thing for you," critically examines the inherent limitations of artificial intelligence when applied to design processes. It posits that while AI tools can efficiently generate numerous design solutions based on given parameters, they frequently lack the deeper contextual understanding, nuanced human empathy, and comprehensive grasp of underlying user intent required for truly effective and appropriate outcomes. The core argument is that AI, without significant human oversight and ethical considerations, may optimize for technical correctness or superficial metrics rather than actual suitability, potentially leading to designs that are functionally flawed, ethically problematic, or simply irrelevant to human needs. The author stresses the importance of human designers maintaining a pivotal role in defining objectives, evaluating AI outputs for relevance and impact, and integrating the critical ethical and social dimensions that AI currently struggles to comprehend. This perspective underscores the necessity of a collaborative human-AI approach, advocating for AI to serve as a powerful assistant rather than an autonomous decision-maker in complex creative and problem-solving tasks, particularly in fields like user experience and product design. This ensures that innovation remains aligned with human values and practical efficacy.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Generative AI</span><span>AI Limitations</span><span>Design Automation</span><span>Ethical AI</span><span>Human-AI Collaboration</span><span>AI in Product Design</span><span>Contextual AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.antonsten.com/articles/ai-will-happily-design-the-wrong-thing-for-you/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>MoneyPrinterTurbo</h2>
                <span class="published-time">Published: 2025-05-16T03:03:36Z</span>
                
                <p class="summary">MoneyPrinterTurbo is an open-source project for fully automated short video generation. Users provide a video theme or keywords, and the system automatically generates video scripts, selects visual materials, creates subtitles, and adds background music, culminating in a high-definition short video. Key features include a robust MVC architecture with both API and web interfaces, supporting AI-driven or custom script creation, and multiple HD video formats (9:16 and 16:9). It offers batch video generation, adjustable segment durations, and multi-language support (Chinese and English). The platform integrates various large language models such as OpenAI, Moonshot, Azure, DeepSeek, and more for advanced content generation, along with customizable voice synthesis and subtitle options. It utilizes high-quality, copyright-free video materials and allows for local asset integration, providing a comprehensive tool for streamlined video content creation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video Generation</span><span>AI Video Creation</span><span>Short Video</span><span>Large Language Models</span><span>Content Automation</span><span>Open Source</span><span>Multimodal AI</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/harry0703/MoneyPrinterTurbo" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>openpilot</h2>
                <span class="published-time">Published: 2025-09-30T10:11:42Z</span>
                
                <p class="summary">openpilot is an open-source operating system designed for robotics, specifically enhancing driver assistance systems in over 300 supported vehicles. Developed by comma.ai, it transforms compatible cars into semi-autonomous vehicles by providing advanced driving features. The system operates on dedicated hardware like the comma 3X device, requiring specific software installation and a car harness for integration. Beyond its core functionality, openpilot emphasizes robust safety and testing protocols, adhering to ISO26262 guidelines and incorporating extensive software-in-the-loop and hardware-in-the-loop testing, including a custom C-based safety model within its panda hardware. The project fosters an active development community, welcoming contributions via GitHub, and provides detailed documentation and development tools. It also offers various branches, from stable releases to bleeding-edge nightly builds, catering to different user needs. openpilot leverages user-consented driving data uploads to continuously train and improve its models, enhancing its performance across the fleet. While offering advanced capabilities, it is explicitly stated as alpha-quality software for research purposes, stressing user responsibility for compliance with local regulations and the absence of warranty.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Robotics</span><span>Driver Assistance System</span><span>Autonomous Driving</span><span>Open Source</span><span>Vehicle Control</span><span>ADAS</span><span>Safety Standards</span><span>Software-in-the-Loop</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>Artificial Intelligence</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/commaai/openpilot" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by Composing Old Ones</h2>
                <span class="published-time">Published: 2025-09-29T17:44:27.000Z</span>
                
                <p class="summary">We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLMs</span><span>Reinforcement Learning</span><span>Compositional Learning</span><span>Skill Transfer</span><span>New Skills</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Machine Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.25123" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer</h2>
                <span class="published-time">Published: 2025-09-29T12:28:09.000Z</span>
                
                <p class="summary">We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>SANA-Video</span><span>Video Generation</span><span>Diffusion Models</span><span>Linear Attention</span><span>Block Linear Diffusion</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Deep Learning</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.24695" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Scaling Generalist Data-Analytic Agents</h2>
                <span class="published-time">Published: 2025-09-29T17:23:08.000Z</span>
                
                <p class="summary">Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Data-analytic agents</span><span>Generalist agents</span><span>Data synthesis</span><span>Agent training</span><span>Open-source models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.25084" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation</h2>
                <span class="published-time">Published: 2025-09-29T12:08:33.000Z</span>
                
                <p class="summary">Long-sequence processing is a critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer a promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional pretrain-on-short, finetune-on-long workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4times faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.1 (https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model, providing a reproducible implementation for the research community.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>dense-sparse attention</span><span>large language models</span><span>long-sequence processing</span><span>Transformer architecture</span><span>computational efficiency</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.24663" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>HunyuanImage 3.0 Technical Report</h2>
                <span class="published-time">Published: 2025-09-28T16:14:10.000Z</span>
                
                <p class="summary">We present HunyuanImage 3.0, a native multimodal model that unifies multimodal understanding and generation within an autoregressive framework, with its image generation module publicly available. The achievement of HunyuanImage 3.0 relies on several key components, including meticulous data curation, advanced architecture design, a native Chain-of-Thoughts schema, progressive model pre-training, aggressive model post-training, and an efficient infrastructure that enables large-scale training and inference. With these advancements, we successfully trained a Mixture-of-Experts (MoE) model comprising over 80 billion parameters in total, with 13 billion parameters activated per token during inference, making it the largest and most powerful open-source image generative model to date. We conducted extensive experiments and the results of automatic and human evaluation of text-image alignment and visual quality demonstrate that HunyuanImage 3.0 rivals previous state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0, we aim to enable the community to explore new ideas with a state-of-the-art foundation model, fostering a dynamic and vibrant multimodal ecosystem. All open source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanImage-3.0</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>HunyuanImage 3.0</span><span>multimodal model</span><span>image generation</span><span>Mixture-of-Experts</span><span>autoregressive framework</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Generative AI</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.23951" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>The Era of Real-World Human Interaction: RL from User Conversations</h2>
                <span class="published-time">Published: 2025-09-29T17:50:31.000Z</span>
                
                <p class="summary">We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reinforcement Learning</span><span>Human Interaction</span><span>User Conversations</span><span>Model Alignment</span><span>Personalization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.25137" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>