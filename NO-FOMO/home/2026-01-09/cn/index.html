<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2026-01-09</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2026-01-09</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Show HN: EuConform – Offline-first EU AI Act compliance tool (open source)</h2>
                <span class="published-time">Published: 2026-01-09 19:11:57</span>
                
                <p class="summary">EuConform is an open-source project developed to offer an offline-first solution for complying with the EU AI Act. Initiated as a personal endeavor, its core purpose is to transform the EU AI Act's intricate requirements into tangible, inspectable technical checks for AI systems. The tool emphasizes a local-first compliance methodology, ensuring heightened data privacy and operational autonomy. Its functionalities encompass comprehensive risk classification, aligning with Articles 5–15 of the Act, which includes identifying prohibited use cases. EuConform also integrates bias evaluation mechanisms, specifically leveraging the CrowS-Pairs method to assess potential biases in AI models. Additionally, it automates the generation of Annex IV-oriented PDF reports, streamlining documentation for regulatory adherence. A key technical differentiator is its complete independence from cloud services or external APIs, operating entirely browser-based and integrating with local AI models via Ollama. The developer is seeking feedback on the practical relevance and effectiveness of this technical interpretation of AI regulation in real-world scenarios.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>EU AI Act</span><span>AI compliance</span><span>open source</span><span>offline-first</span><span>risk classification</span><span>bias evaluation</span><span>Ollama</span><span>technical checks</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/Hiepler/EuConform" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Scroll Wikipedia like TikTok</h2>
                <span class="published-time">Published: 2026-01-09 18:15:16</span>
                
                <p class="summary">This Hacker News submission introduces a novel application demonstrating fully generative user interfaces (UIs), where HTML and Canvas elements are generated just-in-time, powered by Large Language Models like Gemini 3 Flash. The platform simulates a TikTok-like scrolling feed, with each post being dynamically streamed and rendered. A key technical aspect involves the use of Cloudflare Workers Durable Objects to facilitate fast, bidirectional communication for features such as comments and direct messages. Additionally, generated content is stored in a Durable Object SQLite database, optimizing subsequent delivery to user feeds. The developer highlights inspiration from previous projects, including a VSCode extension called Wikitok and other generative UI experiments, aiming to create an immersive, dynamic content consumption experience akin to popular short-form video platforms but applied to informational content like Wikipedia.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Generative UI</span><span>Large Language Model</span><span>Cloudflare Workers</span><span>Durable Objects</span><span>Real-time UI Generation</span><span>Dynamic Content</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Generative AI</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://quack.sdan.io" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Executable Markdown files with Unix pipes</h2>
                <span class="published-time">Published: 2026-01-09 02:29:12</span>
                
                <p class="summary">An innovative open-source tool has been unveiled, designed to transform Markdown files into executable scripts using a shebang line. This functionality allows users to pipe Markdown content through "Claude Code," an AI-powered execution environment that supports full stdin/stdout capabilities, akin to traditional Unix programs. This enables Markdown files to not only contain descriptive text but also to execute complex tasks, run shell commands, generate scripts, read files, and make API calls, effectively turning them into dynamic command orchestrators. The tool facilitates powerful workflows, such as automating test execution and summarizing results directly within a Markdown document. Its seamless integration with Unix pipes allows for chaining multiple operations, enhancing its utility for data processing and complex automation scenarios. This development represents a significant step towards bridging the gap between documentation and execution, offering a flexible and powerful way to manage and run code-related tasks within a familiar Markdown format, thereby streamlining development and operational processes.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Executable Markdown</span><span>Unix Pipes</span><span>Shebang</span><span>Claude Code</span><span>AI Agent</span><span>Scripting</span><span>Automation</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://news.ycombinator.com/item?id=46549444" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Anthropic blocks third-party use of Claude Code subscriptions</h2>
                <span class="published-time">Published: 2026-01-09 03:44:35</span>
                
                <p class="summary">Anthropic, a prominent artificial intelligence research company known for its Claude family of large language models, has reportedly implemented stringent measures to block the third-party use of its specialized Claude Code subscriptions. This significant development, initially brought to light through discussions on GitHub, specifically within the anomalyco/opencode repository, indicates a strategic shift in how AI service providers manage access and licensing for their advanced generative AI tools dedicated to code generation. The action suggests Anthropic is asserting tighter control over the distribution and utilization of its powerful code-generating capabilities, potentially aiming to safeguard intellectual property, ensure strict compliance with its terms of service, or to optimize resource allocation specifically for its direct subscribers. This decision could carry substantial implications for developers, AI agents, and integrated platforms that have historically relied on Claude Code through indirect or aggregated third-party channels, potentially necessitating a swift re-evaluation of existing integration strategies and impacting the broader ecosystem of third-party AI tool access and availability. The incident highlights the complex and rapidly evolving landscape of commercial AI deployment and the inherent challenges in managing widespread AI model access within a dynamic and expanding global developer community.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Anthropic</span><span>Claude Code</span><span>AI Subscriptions</span><span>Third-Party Access</span><span>API Restrictions</span><span>Generative AI</span><span>Code Generation</span><span>Developer Tools</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/anomalyco/opencode/issues/7410" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AI Zealotry</h2>
                <span class="published-time">Published: 2026-01-09 18:17:24</span>
                
                <p class="summary">The concept of 'AI Zealotry' highlights a growing concern within the technology community regarding an overly enthusiastic and often uncritical approach to artificial intelligence development and adoption. This phenomenon, characterized by an unquestioning belief in AI's transformative power and a tendency to overlook its limitations or potential pitfalls, warrants careful examination. Critics argue that such zealotry can lead to unrealistic expectations, misallocation of resources, and a neglect of ethical considerations and societal impacts. Rather than fostering genuine innovation, an unbridled enthusiasm might inadvertently stifle critical discourse and hinder the development of robust, equitable, and responsible AI systems. The discourse around AI zealotry calls for a more balanced and pragmatic perspective, encouraging developers, policymakers, and the public alike to temper excitement with a healthy dose of skepticism, rigorous evaluation, and a commitment to addressing the complex challenges inherent in advanced AI technologies. It emphasizes the importance of understanding AI as a tool, with inherent strengths and weaknesses, rather than an infallible solution or an object of blind faith, advocating for cautious optimism.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Ethics</span><span>Responsible AI</span><span>Technology Hype</span><span>AI Development</span><span>Critical Assessment</span><span>Technological Impact</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://matthewrocklin.com/ai-zealotry/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>How to store a chess position in 26 bytes (2022)</h2>
                <span class="published-time">Published: 2026-01-09 15:07:17</span>
                
                <p class="summary">This article presents an ingenious method for compactly storing a complete chess board position using only 26 bytes, a remarkable feat of data compression. The technique relies on sophisticated bit-level manipulation and optimized encoding strategies to represent all critical game elements. These include the precise placement of each piece, the availability of castling rights for both players, the potential en passant target square, and the halfmove clock, which tracks moves since the last capture or pawn push. By drastically minimizing the memory footprint for each unique position, this approach offers substantial advantages for the development of high-performance chess engines, extensive chess databases, and advanced analysis software. Such efficiency enables significantly faster state lookups, reduces overall memory overhead, and improves cache utilization. The methodology vividly illustrates how deep understanding of game rules combined with clever bit packing can lead to profound data compression, providing invaluable practical insights for engineers developing resource-constrained applications or systems demanding rapid processing and storage of complex game states. This low-level optimization showcases the profound impact on storage and retrieval performance.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Chess</span><span>Data Compression</span><span>Bit Manipulation</span><span>Memory Optimization</span><span>Game State</span><span>Encoding</span><span>Computer Science</span><span>Game Development</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://ezzeriesa.notion.site/How-to-store-a-chess-position-in-26-bytes-using-bit-level-magic-df1fdb5364eb42fdac11eb23b25e9605" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Agent-as-a-Judge</h2>
                <span class="published-time">Published: 2026-01-08T16:58:10.000Z</span>
                
                <p class="summary">LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Agent-as-a-Judge</span><span>AI evaluation</span><span>Large Language Models</span><span>Agentic systems</span><span>Multi-agent collaboration</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.05111" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Plenoptic Video Generation</h2>
                <span class="published-time">Published: 2026-01-08T18:58:32.000Z</span>
                
                <p class="summary">Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Plenoptic Video Generation</span><span>Generative Video Re-rendering</span><span>Multi-view Consistency</span><span>Spatio-temporal Coherence</span><span>Autoregressive Models</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Computer Vision</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.05239" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering</h2>
                <span class="published-time">Published: 2026-01-08T05:49:01.000Z</span>
                
                <p class="summary">Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AgentDevel</span><span>LLM Agents</span><span>Release Engineering</span><span>Self-Evolving Agents</span><span>Software Development</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>AI Agent</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.04620" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AT^2PO: Agentic Turn-based Policy Optimization via Tree Search</h2>
                <span class="published-time">Published: 2026-01-08T09:35:49.000Z</span>
                
                <p class="summary">LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>LLM Agents</span><span>Reinforcement Learning</span><span>Tree Search</span><span>Policy Optimization</span><span>Multi-turn Tasks</span></div>
                    <div class="area"><span class="label">Areas：</span><span>AI Agent</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.04767" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling</h2>
                <span class="published-time">Published: 2026-01-06T15:41:35.000Z</span>
                
                <p class="summary">The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Reinforcement Learning</span><span>Large Language Models</span><span>One-shot Learning</span><span>Polymath Learning</span><span>Sample Engineering</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Machine Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.03111" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control</h2>
                <span class="published-time">Published: 2026-01-08T17:28:52.000Z</span>
                
                <p class="summary">Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Video World Models</span><span>4D Geometric Control</span><span>3D Gaussian Trajectories</span><span>Video Diffusion</span><span>Automatic Data Engine</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Computer Vision</span><span>Generative AI</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.05138" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>