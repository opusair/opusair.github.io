<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2026-01-01</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2026-01-01</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Building an internal agent: Code-driven vs. LLM-driven workflows</h2>
                <span class="published-time">Published: 2026-01-01 18:34:25</span>
                
                <p class="summary">This analysis delves into the contrasting methodologies for developing internal agents: code-driven versus LLM-driven workflows. The code-driven approach, rooted in traditional software engineering principles, offers high determinism, precise control, and easier debugging, making it suitable for tasks requiring strict logic and predictability. However, it can be rigid and resource-intensive for evolving requirements. Conversely, LLM-driven workflows harness the flexibility and emergent reasoning capabilities of large language models, allowing for more adaptive and context-aware agents, particularly beneficial for handling unstructured data or ambiguous instructions. While potentially accelerating development for certain applications, this method introduces challenges related to reliability, interpretability, and the potential for 'hallucinations.' The discussion likely explores the trade-offs between these paradigms, considering factors such as development speed, maintenance overhead, operational costs, and the level of autonomy required for internal automation tasks, potentially advocating for hybrid models that combine the strengths of both approaches to optimize agent performance and robustness.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Agent</span><span>LLM Workflows</span><span>Code-driven Development</span><span>Workflow Automation</span><span>Internal Agents</span><span>System Design</span><span>Agentic Systems</span></div>
                    <div class="area"><span class="label">Areas：</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://lethain.com/agents-coordinators/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Ask HN: When do we expose "Humans as Tools" so LLM agents can call us on demand?</h2>
                <span class="published-time">Published: 2026-01-01 18:09:31</span>
                
                <p class="summary">The discussion explores the integration of human labor directly into the toolchains of advanced LLM agents. As agentic AI systems evolve to plan, reason, and utilize APIs as tools, the concept proposes formalizing human involvement for tasks requiring judgment, creativity, or physical actions. The idea suggests an interface similar to platforms like TaskRabbit or Fiverr, where LLM agents could programmatically </p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>LLM Agents</span><span>AI Tooling</span><span>Human-in-the-Loop AI</span><span>Agentic Systems</span><span>Human-AI Collaboration</span><span>Future of Work</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://news.ycombinator.com/item?id=46456400" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Children and Helical Time</h2>
                <span class="published-time">Published: 2026-01-01 09:53:48</span>
                
                <p class="summary">The article, 'Children and Helical Time,' introduces a conceptual framework that posits time as a helical or spiraling progression, moving beyond a purely linear understanding, particularly when examining childhood development. While detailed content is not available, the title suggests an exploration into how children perceive and interact with temporal cycles, potentially contrasting this with adult perceptions or conventional linear models. This philosophical inquiry could shed light on various aspects of cognitive development, the formation of memory, and the inherently iterative nature of human learning. From a technological perspective, these insights bear relevance for advanced AI systems, particularly in designing agents capable of more nuanced temporal reasoning and understanding. The principles of helical time might inform novel approaches to modeling developmental stages in artificial intelligence, creating AI agents with more adaptive learning trajectories, or processing sequential data in non-linear, context-rich ways. The concept challenges traditional chronological views, advocating for a cyclical yet progressively evolving model for comprehending growth, experience, and knowledge acquisition.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Temporal Reasoning</span><span>Cognitive Modeling</span><span>Developmental AI</span><span>Non-linear Systems</span><span>Learning Trajectories</span><span>Time Perception</span><span>Philosophical Foundations of AI</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://moultano.wordpress.com/2025/12/30/children-and-helical-time/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Implementing HNSW (Hierarchical Navigable Small World) Vector Search in PHP</h2>
                <span class="published-time">Published: 2026-01-01 15:48:31</span>
                
                <p class="summary">The article details the implementation of Hierarchical Navigable Small World (HNSW) vector search within the PHP programming language. HNSW stands as a highly efficient approximate nearest neighbor (ANN) algorithm, critical for rapidly performing similarity searches across high-dimensional data vectors, a foundational component in numerous artificial intelligence and machine learning applications. This integration into PHP provides developers with the capability to embed performant vector search functionalities directly into their web applications or backend services, thereby potentially mitigating the need for external specialized databases or microservices dedicated to vector indexing. Such a native PHP solution facilitates the development of features like real-time recommendation engines, semantic search capabilities, and efficient data retrieval systems. The initiative underscores a growing trend to equip widely adopted web development languages with robust AI-related computational tools, offering a practical and performant approach to managing vector embeddings and enhancing the responsiveness of data-intensive applications. It represents a significant step towards broadening PHP's utility in advanced data indexing and retrieval scenarios.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>HNSW</span><span>Vector Search</span><span>PHP</span><span>Approximate Nearest Neighbor</span><span>Similarity Search</span><span>High-dimensional Data</span><span>Data Structures</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://centamori.com/index.php?slug=hierarchical-navigable-small-world-hnsw-php&lang=en" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Build a Deep Learning Library</h2>
                <span class="published-time">Published: 2026-01-01 14:53:50</span>
                
                <p class="summary">This Hacker News story highlights a resource titled "Build a Deep Learning Library," which offers a comprehensive guide for developers and enthusiasts interested in constructing a deep learning framework from the ground up. The initiative aims to demystify the complex internal workings of modern artificial intelligence by walking users through the practical implementation of core deep learning components. Participants are expected to gain invaluable hands-on experience with foundational elements such as various neural network layer types, activation functions, loss computations, optimizers like SGD or Adam, and the intricate backpropagation algorithm. This practical approach not only solidifies theoretical understanding but also equips learners with the skills to debug, customize, and optimize their own AI models more effectively. The resource likely addresses crucial aspects of software design, numerical computation, and performance considerations inherent in developing an efficient and scalable deep learning tool. It caters specifically to those aspiring to move beyond black-box usage of existing frameworks towards a profound comprehension and mastery of deep learning architecture and implementation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Deep Learning</span><span>Machine Learning</span><span>Neural Networks</span><span>Library Development</span><span>AI Frameworks</span><span>Programming</span><span>Backpropagation</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Deep Learning</span><span>Machine Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://zekcrates.quarto.pub/deep-learning-library/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models</h2>
                <span class="published-time">Published: 2025-12-31T04:25:11.000Z</span>
                
                <p class="summary">We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled "Commonsense-STEM-Agent" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Youtu-LLM</span><span>Lightweight LLMs</span><span>Agentic Intelligence</span><span>Long-Context Support</span><span>Pre-training</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>AI Agent</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24618" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem</h2>
                <span class="published-time">Published: 2025-12-31T14:03:39.000Z</span>
                
                <p class="summary">Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Agentic LLMs</span><span>Agentic Learning Ecosystem</span><span>ROME Model</span><span>Policy Optimization</span><span>Benchmarking</span></div>
                    <div class="area"><span class="label">Areas：</span><span>AI Agent</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24873" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents</h2>
                <span class="published-time">Published: 2025-12-29T10:01:32.000Z</span>
                
                <p class="summary">Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Memory Systems</span><span>Cognitive Neuroscience</span><span>Autonomous Agents</span><span>LLM-driven Agents</span><span>Memory Security</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.23343" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation</h2>
                <span class="published-time">Published: 2025-12-31T01:19:14.000Z</span>
                
                <p class="summary">Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Text-to-Video Generation</span><span>Physics-Aware</span><span>Direct Preference Optimization</span><span>Vision-Language Model</span><span>Physical Consistency</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Computer Vision</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24551" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</h2>
                <span class="published-time">Published: 2025-12-31T18:59:57.000Z</span>
                
                <p class="summary">We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>SpaceTimePilot</span><span>Video Diffusion Model</span><span>Generative Rendering</span><span>Dynamic Scenes</span><span>Space-time disentanglement</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.25075" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation</h2>
                <span class="published-time">Published: 2025-12-28T12:25:43.000Z</span>
                
                <p class="summary">This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>JavisGPT</span><span>Multimodal LLM</span><span>Audio-Video Comprehension</span><span>Audio-Video Generation</span><span>Instruction-tuning</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Multimodal</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.22905" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>