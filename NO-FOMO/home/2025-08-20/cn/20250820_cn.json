[
  {
    "id": "twitter_madebygoogle_1958216279300403670",
    "source": "Twitter",
    "url": "https://x.com/madebygoogle/status/1958216279300403670",
    "title": "madebygoogle_Gemini Live实时视觉对话与建议",
    "summary": "谷歌Made by Google宣布Gemini Live新增摄像头共享功能，用户可在对话中实时分享所见，并获得即时建议。Gemini应用能够识别并指出具体事物，例如根据脸型推荐合适的眼镜款式，极大地增强了AI助手的实用性和交互性，为用户提供更直观、个性化的视觉辅助体验。",
    "keywords": [
      "Gemini Live",
      "实时建议",
      "视觉对话",
      "摄像头共享",
      "AI助手",
      "多模态"
    ],
    "area": [
      "人工智能",
      "多模态",
      "产品发布"
    ],
    "published_time": "2025-08-20T17:15:17.000Z",
    "download_time": "2025-08-21 01:32:34",
    "visual_resource": [
      "screenshot/twitter/madebygoogle_1958216279300403670.png"
    ],
    "extra_info": "{\"username\": \"madebygoogle\", \"tweet_id\": \"1958216279300403670\"}"
  },
  {
    "id": "twitter_madebygoogle_1958215989352440270",
    "source": "Twitter",
    "url": "https://x.com/madebygoogle/status/1958215989352440270",
    "title": "madebygoogle_谷歌Gemini应用推出视频生成功能，购Pixel 10 Pro享AI Pro服务",
    "summary": "谷歌Made by Google宣布，其Gemini应用现已支持视频生成功能，用户可通过文字或图片快速创建带声音的视频。此功能旨在将创意迅速转化为现实。此外，购买Pixel 10 Pro或Pixel 10 Pro Fold的用户将免费获得一年的Google AI Pro服务，进一步推广其AI能力。",
    "keywords": [
      "Gemini",
      "视频生成",
      "Google AI Pro",
      "Pixel 10 Pro",
      "人工智能"
    ],
    "area": [
      "生成式AI",
      "产品发布",
      "人工智能"
    ],
    "published_time": "2025-08-20T17:14:08.000Z",
    "download_time": "2025-08-21 01:32:33",
    "visual_resource": [
      "screenshot/twitter/madebygoogle_1958215989352440270.png"
    ],
    "extra_info": "{\"username\": \"madebygoogle\", \"tweet_id\": \"1958215989352440270\"}"
  },
  {
    "id": "twitter_CerebrasSystems_1957957962514960567",
    "source": "Twitter",
    "url": "https://twitter.com/CerebrasSystems/status/1957957962514960567",
    "title": "CerebrasSystems_成为Hugging Face推理服务提供商",
    "summary": "Cerebras Systems宣布已成为Hugging Face的推理服务提供商，每月处理500万次请求。Hugging Face的推理服务提供商网络每月请求量已突破2000万次，其中Cerebras、Novita Labs和Fireworks AI增长最快。该服务目前正为OpenAI的官方开放平台提供支持，并已集成到多个应用中。",
    "keywords": [
      "Cerebras",
      "Hugging Face",
      "推理服务",
      "AI模型",
      "OpenAI",
      "云计算"
    ],
    "area": [
      "人工智能",
      "大模型",
      "技术动态"
    ],
    "published_time": "2025-08-20T00:08:50.000Z",
    "download_time": "2025-08-21 05:51:10",
    "visual_resource": [
      "screenshot/twitter/CerebrasSystems_1957957962514960567.png"
    ],
    "extra_info": "{\"username\": \"CerebrasSystems\", \"tweet_id\": \"1957957962514960567\"}"
  },
  {
    "id": "twitter_AndrewYNg_1958165941369634825",
    "source": "Twitter",
    "url": "https://x.com/AndrewYNg/status/1958165941369634825",
    "title": "AndrewYNg_AI Dev 25纽约峰会即将举行",
    "summary": "Andrew Ng宣布AI Dev 25大会将于11月14日在纽约举行，预计吸引逾1200名开发者。大会将深入探讨前沿AI技术，涵盖智能体AI（如多智能体编排、工具使用）、AI辅助编程、上下文工程（如高级RAG、记忆系统）、多模态AI（如视觉语言模型）及金融科技应用等多个领域。此次活动因上次Pi Day大会迅速售罄而预订了更大场地。",
    "keywords": [
      "AI Dev 25",
      "智能体AI",
      "多模态AI",
      "金融科技",
      "技术峰会",
      "纽约"
    ],
    "area": [
      "技术动态",
      "智能体",
      "多模态"
    ],
    "published_time": "2025-08-20T13:55:16.000Z",
    "download_time": "2025-08-21 01:26:25",
    "visual_resource": [
      "screenshot/twitter/AndrewYNg_1958165941369634825.png"
    ],
    "extra_info": "{\"username\": \"AndrewYNg\", \"tweet_id\": \"1958165941369634825\"}"
  },
  {
    "id": "twitter_Google_1958284725526643090",
    "source": "Twitter",
    "url": "https://x.com/Google/status/1958284725526643090",
    "title": "Google_Pixel 10相机十大更新",
    "summary": "谷歌宣布为Pixel 10手机带来十大相机更新，旨在显著提升用户摄影体验。新功能包括高分辨率人像与自拍、更流畅的防抖、100倍Pro Res变焦、AI辅助的构图指导（Camera Coach）、自动优选（Auto Best Take）以及与Google Photos的深度集成。此外，还引入了基于Gemini模型的引导构图和C2PA内容凭证，确保图像真实性，并提供实时预览等功能，全面优化拍照效果。",
    "keywords": [
      "谷歌",
      "Pixel 10",
      "相机更新",
      "摄影",
      "AI功能",
      "图像处理"
    ],
    "area": [
      "产品发布",
      "技术动态",
      "人工智能"
    ],
    "published_time": "2025-08-20T21:47:16.000Z",
    "download_time": "2025-08-21 01:31:49",
    "visual_resource": [
      "screenshot/twitter/Google_1958284725526643090.png"
    ],
    "extra_info": "{\"username\": \"Google\", \"tweet_id\": \"1958284725526643090\"}"
  },
  {
    "id": "twitter_Google_1958269277003256044",
    "source": "Twitter",
    "url": "https://x.com/Google/status/1958269277003256044",
    "title": "Google_Gemini视觉与交互能力升级",
    "summary": "谷歌宣布Gemini助手迎来重大升级，使其成为更实用、自然和视觉化的AI助手。新功能包括视觉引导，允许Gemini在用户分享摄像头时直接在屏幕上高亮显示内容；语音交互更加自然，提升了语调、节奏和音高；同时，Gemini可连接更多谷歌应用如信息、电话和时钟，提供更全面的服务。",
    "keywords": [
      "Gemini",
      "AI助手",
      "视觉引导",
      "语音交互",
      "谷歌应用",
      "多模态"
    ],
    "area": [
      "人工智能",
      "多模态",
      "产品发布"
    ],
    "published_time": "2025-08-20T20:45:53.000Z",
    "download_time": "2025-08-21 01:31:49",
    "visual_resource": [
      "screenshot/twitter/Google_1958269277003256044.png"
    ],
    "extra_info": "{\"username\": \"Google\", \"tweet_id\": \"1958269277003256044\"}"
  },
  {
    "id": "7uk3f4fFlEtx06pjr3oiuA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/7uk3f4fFlEtx06pjr3oiuA",
    "title": "OpenAI高管自爆：Scaling不死，GPT-5「双轴训练」撕开智能天花板",
    "summary": "OpenAI首席运营官Brad Lightcap揭示GPT-5核心突破，强调其能自主判断是否进行深度推理再回答，大幅提升用户体验。GPT-5的智能飞跃并非单纯指数增长，而是通过“双轴训练”（预训练与后训练）实现，尤其在后训练维度取得显著收益。模型在准确性、响应速度、工具使用及结构化思考能力上全面升级，并在健康、编码、法律等领域展现强大潜力。Lightcap指出，Scaling Law依然成立，后训练是推动模型智能水平的新范式。尽管GPT-5并非AGI，但其代表了通用化学习系统的雏形，OpenAI将持续在算法、规模、计算和数据等多维度发力，致力于让更多用户和企业受益于GPT-5的强大能力，并推动AI生态发展。",
    "keywords": [
      "GPT-5",
      "双轴训练",
      "后训练",
      "Scaling Law",
      "人工智能",
      "OpenAI"
    ],
    "area": [
      "人工智能",
      "大模型",
      "生成式AI"
    ],
    "published_time": "2025-08-20T01:46:22.000Z",
    "download_time": "2025-08-21T13:52:18.484827",
    "visual_resource": [
      "screenshot/wechat/wechat_image_7uk3f4fFlEtx06pjr3oiuA.png"
    ],
    "extra_info": null
  },
  {
    "id": "yq5mvap6ldR5hwZWGJSEOw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/yq5mvap6ldR5hwZWGJSEOw",
    "title": "DeepSeek V3.1 Base突袭上线！击败Claude 4编程爆表，全网在蹲R2和V4",
    "summary": "DeepSeek官方突袭上线V3.1版本，该685B参数模型将上下文长度拓展至128K，并在编程能力上取得显著突破。实测显示，V3.1在Aider编程基准测试中以71.6%的高分超越Claude Opus 4，且推理和响应速度更快，成本仅为专有系统的六分之一。新版本还新增了原生“search token”支持，并暗示未来可能采用混合架构。尽管模型卡尚未公布，V3.1已在Hugging Face趋势榜位居前列，引发社区对R2和V4的强烈期待。此次更新进一步巩固了DeepSeek在开源大模型领域的领先地位，尤其在编程和成本效益方面展现出强大竞争力。",
    "keywords": [
      "DeepSeek V3.1",
      "编程能力",
      "上下文长度",
      "大模型",
      "成本效益",
      "AI性能"
    ],
    "area": [
      "大模型",
      "人工智能",
      "生成式AI"
    ],
    "published_time": "2025-08-20T14:01:05.000Z",
    "download_time": "2025-08-21T13:51:50.895456",
    "visual_resource": [
      "screenshot/wechat/wechat_image_yq5mvap6ldR5hwZWGJSEOw.png"
    ],
    "extra_info": null
  },
  {
    "id": "8WROvfZYtob_JSk0XTSNeQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/8WROvfZYtob_JSk0XTSNeQ",
    "title": "当VLM学会了“回头看” ！Qwen-2.5-VL突破性发现，7B模型自主激活'视觉反思'，感知任务性能提升6.3%",
    "summary": "极市导读指出，多模态大语言模型（MLLMs）在推理后期常过度依赖文本，忽视视觉信息。针对此问题，Qwen-2.5-VL 7B模型提出并实现了“Look-Back”机制，使其能在推理过程中自主决定何时、何地重新关注视觉输入，无需额外图像输入或结构修改。通过两阶段训练框架（监督微调与强化学习），该机制显著提升了模型在数学任务上约7%和感知任务上6.3%的平均性能，刷新了多模态推理范式，并增强了模型泛化能力，展现出强大的竞争力。",
    "keywords": [
      "多模态大语言模型",
      "视觉反思",
      "Look-Back",
      "推理能力",
      "强化学习"
    ],
    "area": [
      "多模态",
      "大模型",
      "计算机视觉"
    ],
    "published_time": "2025-08-20T14:01:05.000Z",
    "download_time": "2025-08-21T13:51:57.269393",
    "visual_resource": [
      "screenshot/wechat/wechat_image_8WROvfZYtob_JSk0XTSNeQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "wCqh9BIPoXjiK5yTGOPrqA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/wCqh9BIPoXjiK5yTGOPrqA",
    "title": "DiT在数学和形式上是错的？谢赛宁回应：不要在脑子里做科学",
    "summary": "近期，一篇帖子质疑DiT模型存在架构缺陷，指出其FID过早稳定、使用不稳定的后层归一化及限制表达能力的adaLN-zero，甚至发现替换部分计算单元为恒等函数反而提升性能。DiT作者谢赛宁对此回应，承认部分“硬伤”如sd-vae的低效，但强调DiT作为Sora和Stable Diffusion 3基础架构的学术地位，并指出科学研究需基于实验验证而非臆测。他认为TREAD等方法更接近正则化效应，而非揭示DiT根本性错误，并提供了DiT的稳健升级建议，重申了实验验证在科学探索中的核心作用。",
    "keywords": [
      "DiT",
      "扩散模型",
      "Transformer",
      "架构缺陷",
      "谢赛宁",
      "TREAD"
    ],
    "area": [
      "深度学习",
      "生成式AI",
      "计算机视觉"
    ],
    "published_time": "2025-08-20T04:23:48.000Z",
    "download_time": "2025-08-21T13:52:13.997474",
    "visual_resource": [
      "screenshot/wechat/wechat_image_wCqh9BIPoXjiK5yTGOPrqA.png"
    ],
    "extra_info": null
  },
  {
    "id": "2C5hi2o0RzAO_y6MTh2JMQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/2C5hi2o0RzAO_y6MTh2JMQ",
    "title": "厉害了，智谱造了全球首个手机通用Agent！人人免费，APP甚至直接操控云电脑",
    "summary": "智谱AI发布全球首个手机通用Agent AutoGLM，其核心创新在于云端执行，使用户能通过语音指令让AI跨应用完成复杂任务，如点外卖、比价购物、生成报告和PPT，且不占用本地设备资源。AutoGLM支持手机及云电脑操作，解决了传统Agent算力限制和资源占用问题，提升了用户体验。该Agent整合了推理、编码等多模态能力，是智谱AI迈向通用人工智能（AGI）的关键一步，并验证了云端Agent的可行性。智谱AI已开放API，旨在推动Agent技术普及与生态共建，预示着“问一句，剩下全交给Agent”的时代即将到来。",
    "keywords": [
      "智谱AI",
      "通用Agent",
      "云端执行",
      "AutoGLM",
      "通用人工智能"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-08-20T04:31:04.000Z",
    "download_time": "2025-08-21T13:52:12.151887",
    "visual_resource": [
      "screenshot/wechat/wechat_image_2C5hi2o0RzAO_y6MTh2JMQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "C1h6QveDrX_-yDxwI1CNUA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/C1h6QveDrX_-yDxwI1CNUA",
    "title": "Claude Code与Gemini放弃代码索引，是一步烂棋",
    "summary": "文章批评Claude Code和Gemini等AI IDE放弃代码索引（RAG）而采用传统grep文本搜索，指出其导致高Token消耗、低效率和缺乏语义理解。作者通过实测证明，引入向量检索能显著提升效率并节省Token。为解决此痛点，作者开源了“claude-context”项目，一个集成向量数据库和Embedding模型的代码检索工具，详细阐述了其基于MCP架构、AST代码切分、Merkle Tree同步机制等技术细节，并展示了其在实际应用中节省40% Token的显著效果，强调了代码索引对于AI IDE提供优质上下文的重要性。",
    "keywords": [
      "代码检索",
      "向量数据库",
      "AI IDE",
      "RAG",
      "claude-context"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "大模型"
    ],
    "published_time": "2025-08-20T10:05:26.000Z",
    "download_time": "2025-08-21T13:51:57.224743",
    "visual_resource": [
      "screenshot/wechat/wechat_image_C1h6QveDrX_-yDxwI1CNUA.png"
    ],
    "extra_info": null
  },
  {
    "id": "sim",
    "source": "GitHub",
    "url": "https://github.com/simstudioai/sim",
    "title": "Build and deploy AI agent workflows in minutes.",
    "summary": "Sim是一个用于快速构建和部署AI智能体工作流的平台。它提供云托管和多种自托管方案（NPM包、Docker Compose、开发容器、手动安装），支持通过Ollama集成本地AI模型，并利用PostgreSQL（含pgvector扩展）实现向量嵌入，支持知识库和语义搜索等AI功能。其技术栈包括Next.js、Bun、Drizzle ORM和Socket.io，旨在简化AI应用开发。",
    "keywords": [
      "AI智能体",
      "工作流",
      "本地部署",
      "大模型",
      "向量数据库",
      "PostgreSQL",
      "Docker",
      "Next.js"
    ],
    "area": [
      "智能体",
      "人工智能",
      "大模型"
    ],
    "published_time": "2025-08-20T16:56:09Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/public/static/demo.gif",
      "https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/public/logo/reverse/text/large.png"
    ],
    "extra_info": null
  },
  {
    "id": "airi",
    "source": "GitHub",
    "url": "https://github.com/moeru-ai/airi",
    "title": "Project AIRI",
    "summary": "Project AIRI旨在重现Neuro-sama，构建一个AI虚拟角色/数字伴侣容器，使其能够与用户互动、玩游戏、聊天。该项目是Neuro-sama的开源替代方案，利用WebGPU、WebAssembly等Web技术，支持在浏览器、桌面和移动设备上运行。它集成了多种大型语言模型API，并具备记忆、语音识别、语音合成、VRM/Live2D模型控制等核心功能，致力于让用户轻松拥有个性化的数字生命。",
    "keywords": [
      "AI虚拟角色",
      "数字伴侣",
      "大语言模型",
      "Web技术",
      "VRM",
      "Live2D",
      "语音交互",
      "智能体"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-08-20T20:19:11Z",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "https://github.com/moeru-ai/airi/raw/main/docs/content/public/banner-dark-1280x640.avif",
      "https://github.com/moeru-ai/airi/raw/main/docs/content/public/readme-image-pc-preview.avif"
    ],
    "extra_info": null
  },
  {
    "id": "self-hosted-ai-starter-kit",
    "source": "GitHub",
    "url": "https://github.com/n8n-io/self-hosted-ai-starter-kit",
    "title": "Self-hosted AI starter kit",
    "summary": "Self-hosted AI Starter Kit是一个开源的Docker Compose模板，旨在快速搭建全面的本地AI和低代码开发环境。它整合了n8n低代码平台、Ollama本地大模型、Qdrant向量数据库和PostgreSQL，支持多种GPU配置。该套件使用户能够安全地构建AI代理、文档摘要、智能聊天机器人等应用，特别适用于概念验证项目，提供了一个便捷的自托管AI工作流解决方案。",
    "keywords": [
      "自托管AI",
      "低代码",
      "Docker Compose",
      "大语言模型",
      "向量数据库",
      "AI工作流",
      "n8n",
      "Ollama"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-07-17T08:11:19Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/assets/n8n-demo.gif"
    ],
    "extra_info": null
  },
  {
    "id": "LLMs-from-scratch",
    "source": "GitHub",
    "url": "https://github.com/rasbt/LLMs-from-scratch",
    "title": "Build a Large Language Model (From Scratch)",
    "summary": "该GitHub仓库是《从零构建大型语言模型》一书的官方代码库，旨在指导读者从头开发、预训练和微调类GPT的大型语言模型。项目提供了详细的代码示例，涵盖了从文本数据处理、注意力机制实现到GPT模型构建、无监督数据预训练及分类与指令微调的全过程。它强调通过实践编码深入理解LLM内部工作原理，并支持加载大型预训练模型进行微调，适合希望系统学习LLM开发的读者。",
    "keywords": [
      "大型语言模型",
      "从零开始",
      "预训练",
      "微调",
      "GPT",
      "PyTorch",
      "自然语言处理",
      "深度学习"
    ],
    "area": [
      "深度学习",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2025-08-20T02:08:29Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/LLMs-from-scratch.png"
    ],
    "extra_info": null
  },
  {
    "id": "terminal-bench",
    "source": "GitHub",
    "url": "https://github.com/laude-institute/terminal-bench",
    "title": "terminal-bench",
    "summary": "Terminal-Bench是一个专为评估AI智能体在真实终端环境中表现而设计的基准测试平台。它通过提供可复现的任务集和执行框架，使智能体能够自主处理从代码编译到模型训练、服务器设置等端到端的实际任务。该平台包含任务数据集和连接语言模型至沙盒终端环境的执行器两大部分，旨在为构建LLM智能体、基准测试框架及压力测试系统级推理能力提供实用、真实的评估手段。目前处于测试阶段，拥有约100个任务，并计划持续扩展。",
    "keywords": [
      "AI智能体",
      "终端环境",
      "基准测试",
      "大模型",
      "执行框架",
      "沙盒环境",
      "任务数据集",
      "系统级推理"
    ],
    "area": [
      "人工智能",
      "智能体",
      "大模型"
    ],
    "published_time": "2025-08-20T22:24:50Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "screenshot/github/terminal-bench.png"
    ],
    "extra_info": null
  },
  {
    "id": "BitNet",
    "source": "GitHub",
    "url": "https://github.com/microsoft/BitNet",
    "title": "bitnet.cpp",
    "summary": "bitnet.cpp是微软官方推出的1比特大语言模型（如BitNet b1.58）推理框架。它提供了一套优化的内核，支持在CPU和GPU上对1.58比特模型进行快速、无损推理。该框架在ARM CPU上实现1.37x至5.07x的加速，x86 CPU上实现2.37x至6.17x的加速，并显著降低能耗。bitnet.cpp能够在一台CPU上运行100B的BitNet b1.58模型，达到接近人类阅读的速度，极大地提升了LLM在本地设备上运行的潜力，是边缘AI推理的重要进展。",
    "keywords": [
      "1比特LLM",
      "推理框架",
      "模型优化",
      "CPU推理",
      "GPU推理",
      "边缘计算",
      "能效"
    ],
    "area": [
      "人工智能",
      "大模型",
      "深度学习"
    ],
    "published_time": "2025-06-03T06:14:20Z",
    "download_time": "2024-05-20 10:00:00",
    "visual_resource": [
      "https://github.com/microsoft/BitNet/raw/main/assets/m2_performance.jpg",
      "https://github.com/microsoft/BitNet/raw/main/assets/intel_performance.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "2508.13167",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.13167",
    "title": "智能体链：通过多智能体蒸馏和智能体强化学习实现端到端智能体基础模型",
    "summary": "大型语言模型（LLMs）和多智能体系统在深度研究、情感编码和数学推理等复杂问题解决任务中展现出卓越的能力。然而，现有的大多数多智能体系统都依赖于手动提示/工作流工程和复杂的智能体框架构建，这使得它们计算效率低下、能力有限，并且无法从数据驱动的学习中受益。在这项工作中，我们引入了智能体链（Chain-of-Agents, CoA），这是一种新颖的LLM推理范式，它能够在单个模型内以与多智能体系统相同的方式（即，通过多个工具和多个智能体的多轮问题解决）实现原生的端到端复杂问题解决。在智能体链问题解决中，模型动态激活不同的工具智能体和角色扮演智能体，以端到端的方式模拟多智能体协作。为了激发LLM中端到端智能体链问题解决能力，我们引入了一个多智能体蒸馏框架，将最先进的多智能体系统蒸馏成智能体链轨迹，用于智能体监督微调。随后，我们利用智能体强化学习在可验证的智能体任务上进一步提升模型在智能体链问题解决方面的能力。我们将由此产生的模型称为智能体基础模型（Agent Foundation Models, AFMs）。我们的实证研究表明，AFM在网络智能体和代码智能体设置的各种基准测试中均取得了新的最先进性能。我们将包括模型权重、训练和评估代码以及训练数据在内的全部研究成果完全开源，这为未来智能体模型和智能体强化学习的研究提供了坚实的起点。",
    "keywords": [
      "智能体链",
      "智能体基础模型",
      "多智能体系统",
      "智能体强化学习",
      "大型语言模型"
    ],
    "area": [
      "智能体",
      "大模型",
      "人工智能"
    ],
    "published_time": "2025-08-06T17:01:02.000Z",
    "download_time": "2025-08-20 18:38:41",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13167.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.13167\", \"arxiv_url\": \"https://arxiv.org/abs/2508.13167\"}"
  },
  {
    "id": "2508.13948",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.13948",
    "title": "提示编排标记语言",
    "summary": "大语言模型（LLMs）需要复杂的提示，然而当前的实践在结构、数据集成、格式敏感性和工具方面面临挑战。现有方法缺乏全面的解决方案来组织涉及不同数据类型（文档、表格、图像）的复杂提示，或系统地管理呈现变体。为了解决这些问题，我们引入了POML（提示编排标记语言）。POML采用基于组件的标记来实现逻辑结构（角色、任务、示例），使用专用标签实现无缝数据集成，并引入类似CSS的样式系统以将内容与呈现分离，从而降低格式敏感性。它还包括用于动态提示的模板化功能，以及一个全面的开发者工具包（IDE支持、SDK），以改进版本控制和协作。我们通过两个案例研究（展示其在复杂应用集成PomLink和准确性性能TableQA上的影响）以及一项用户研究（评估其在实际开发场景中的有效性）验证了POML。",
    "keywords": [
      "大语言模型",
      "提示编排",
      "标记语言",
      "数据集成",
      "开发工具包"
    ],
    "area": [
      "大语言模型",
      "自然语言处理",
      "生成式AI"
    ],
    "published_time": "2025-08-19T15:37:29.000Z",
    "download_time": "2025-08-20 18:38:36",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13948.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.13948\", \"arxiv_url\": \"https://arxiv.org/abs/2508.13948\"}"
  },
  {
    "id": "2508.13998",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.13998",
    "title": "Embodied-R1：用于通用机器人操作的强化具身推理",
    "summary": "具身人工智能中的泛化能力受到“看到即做到”鸿沟的阻碍，这源于数据稀缺和具身异构性。为解决此问题，我们开创性地将“指向”作为一种统一的、与具身无关的中间表示，并定义了四种核心具身指向能力，以弥合高级视觉-语言理解与低级动作原语之间的差距。我们引入了Embodied-R1，这是一个专门为具身推理和指向设计的3B视觉-语言模型（VLM）。我们利用广泛的具身和通用视觉推理数据集作为来源，构建了一个大规模数据集Embodied-Points-200K，该数据集支持关键的具身指向能力。随后，我们采用两阶段强化微调（RFT）课程和专门的多任务奖励设计来训练Embodied-R1。Embodied-R1在11个具身空间和指向基准测试中取得了最先进的性能。至关重要的是，它通过在SIMPLEREnv中实现56.2%的成功率以及在8个真实世界XArm任务中实现87.5%的成功率（无需任何特定任务微调）展示了强大的零样本泛化能力，这比强基线提高了62%。此外，该模型对各种视觉干扰表现出高度的鲁棒性。我们的工作表明，以指向为中心的表示与RFT训练范式相结合，为弥合机器人技术中的感知-动作鸿沟提供了一条有效且可泛化的途径。",
    "keywords": [
      "具身推理",
      "机器人操作",
      "视觉-语言模型",
      "强化微调",
      "指向"
    ],
    "area": [
      "机器人",
      "智能体",
      "多模态"
    ],
    "published_time": "2025-08-19T16:50:01.000Z",
    "download_time": "2025-08-20 18:38:43",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13998.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.13998\", \"arxiv_url\": \"https://arxiv.org/abs/2508.13998\"}"
  },
  {
    "id": "2508.12903",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.12903",
    "title": "未雨绸缪：语言模型的主动式自我优化",
    "summary": "近期自我优化方面的进展已显示出通过迭代优化显著提升大型语言模型（LLMs）输出的巨大潜力。然而，大多数现有自我优化方法依赖于固定迭代次数的被动过程，这使得根据不断演变的生成上下文来确定最佳优化时机和内容变得困难。受人类在执行过程中动态优化思维方式的启发，我们提出了一种新颖的方法——主动式自我优化（PASR），它使LLMs能够在生成过程中优化其输出。与那些重新生成整个响应的方法不同，PASR根据模型的内部状态和不断演变的上下文，主动决定是否、何时以及如何进行优化。我们对10项不同任务进行了广泛实验，以评估PASR的有效性。实验结果表明，PASR显著提升了问题解决性能。特别是，在Qwen3-8B模型上，与标准生成相比，PASR将平均令牌消耗降低了41.6%，同时准确率提高了8.2%。我们的代码和论文中使用的所有基线都已在GitHub上提供。",
    "keywords": [
      "语言模型",
      "自我优化",
      "主动式自我优化",
      "大语言模型",
      "迭代优化"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "published_time": "2025-08-18T13:05:48.000Z",
    "download_time": "2025-08-20 18:38:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12903.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.12903\", \"arxiv_url\": \"https://arxiv.org/abs/2508.12903\"}"
  },
  {
    "id": "2508.09131",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.09131",
    "title": "基于多模态扩散Transformer的免训练文本引导色彩编辑",
    "summary": "图像和视频中的文本引导色彩编辑是一个基础但尚未解决的问题，它需要对色彩属性进行细粒度操作，包括反照率、光源颜色和环境光照，同时保持几何、材料属性和光物质相互作用的物理一致性。现有的免训练方法在各种编辑任务中具有广泛适用性，但在精确色彩控制方面表现不佳，并且经常在编辑和非编辑区域引入视觉不一致性。在这项工作中，我们提出了ColorCtrl，一种免训练的色彩编辑方法，它利用了现代多模态扩散Transformer（MM-DiT）的注意力机制。通过有针对性地操纵注意力图和值令牌来解耦结构和颜色，我们的方法实现了准确且一致的色彩编辑，以及属性强度的词级控制。我们的方法仅修改提示指定的预期区域，而未触及不相关的区域。在SD3和FLUX.1-dev上进行的广泛实验表明，ColorCtrl优于现有的免训练方法，并在编辑质量和一致性方面取得了最先进的性能。此外，我们的方法在一致性方面超越了强大的商业模型，如FLUX.1 Kontext Max和GPT-4o图像生成。当扩展到CogVideoX等视频模型时，我们的方法表现出更大的优势，特别是在保持时间连贯性和编辑稳定性方面。最后，我们的方法也推广到基于指令的编辑扩散模型，如Step1X-Edit和FLUX.1 Kontext dev，进一步展示了其多功能性。",
    "keywords": [
      "文本引导色彩编辑",
      "多模态扩散Transformer",
      "免训练",
      "注意力机制",
      "图像视频编辑"
    ],
    "area": [
      "计算机视觉",
      "多模态",
      "生成式AI"
    ],
    "published_time": "2025-08-12T17:57:04.000Z",
    "download_time": "2025-08-20 18:38:41",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09131.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.09131\", \"arxiv_url\": \"https://arxiv.org/abs/2508.09131\"}"
  },
  {
    "id": "2508.04326",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.04326",
    "title": "XR 中的辐射场：辐射场在扩展现实研究中的构想与应用综述",
    "summary": "辐射场（RF）技术的发展，例如3D高斯泼溅（3DGS）和神经辐射场（NeRF），彻底改变了交互式真实感视图合成，并为XR（扩展现实）研究和应用带来了巨大的机遇。然而，尽管辐射场研究呈指数级增长，但与XR社区相关的辐射场贡献仍然稀少。为了更好地理解这一研究空白，我们对当前的辐射场文献进行了系统性综述，以分析（i）辐射场如何被构想用于XR应用，（ii）它们如何已被实现，以及（iii）剩余的研究空白。我们从计算机视觉、计算机图形学、机器人学、多媒体、人机交互和XR社区收集了365篇与XR相关的辐射场贡献，旨在回答上述研究问题。在这365篇论文中，我们对其中66篇已详细探讨XR辐射场研究的论文进行了分析。通过这项综述，我们扩展并定位了XR特有的辐射场研究主题在更广泛的辐射场研究领域中的位置，并为XR社区在辐射场研究的快速发展中提供了一个有益的资源。",
    "keywords": [
      "辐射场",
      "扩展现实",
      "视图合成",
      "3D高斯泼溅",
      "综述"
    ],
    "area": [
      "计算机视觉",
      "生成式AI",
      "机器人"
    ],
    "published_time": "2025-08-06T11:14:06.000Z",
    "download_time": "2025-08-20 18:38:38",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04326.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.04326\", \"arxiv_url\": \"https://arxiv.org/abs/2508.04326\"}"
  }
]