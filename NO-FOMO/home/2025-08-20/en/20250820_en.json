[
  {
    "id": "twitter_madebygoogle_1958216279300403670",
    "source": "Twitter",
    "url": "https://x.com/madebygoogle/status/1958216279300403670",
    "title_en": "madebygoogle_Gemini Live Real-time Visual Conversation and Advice",
    "summary_en": "Made by Google announced a new camera sharing feature in Gemini Live, allowing users to share their camera feed in conversations for real-time advice. The Gemini App can identify and point out specific objects, such as recommending the best glasses shape for a user's face, significantly enhancing the AI assistant's practicality and interactivity by providing a more intuitive and personalized visual assistance experience.",
    "keywords_en": [
      "Gemini Live",
      "Real-time Advice",
      "Visual Conversation",
      "Camera Sharing",
      "AI Assistant",
      "Multimodal"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "Product Launch"
    ],
    "published_time": "2025-08-20T17:15:17.000Z",
    "download_time": "2025-08-21 01:32:34",
    "visual_resource": [
      "screenshot/twitter/madebygoogle_1958216279300403670.png"
    ],
    "extra_info": "{\"username\": \"madebygoogle\", \"tweet_id\": \"1958216279300403670\"}"
  },
  {
    "id": "twitter_madebygoogle_1958215989352440270",
    "source": "Twitter",
    "url": "https://x.com/madebygoogle/status/1958215989352440270",
    "title_en": "madebygoogle_Google Gemini App Launches Video Generation, Pixel 10 Pro Buyers Get AI Pro",
    "summary_en": "Google's Made by Google announced that its Gemini app now features video generation, allowing users to quickly create videos with sound from text or photos. This new capability aims to bring ideas to life in minutes. Furthermore, customers who purchase the Pixel 10 Pro or Pixel 10 Pro Fold will receive a complimentary one-year subscription to Google AI Pro, promoting the broader adoption of Google's AI services.",
    "keywords_en": [
      "Gemini",
      "Video Generation",
      "Google AI Pro",
      "Pixel 10 Pro",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Generative AI",
      "Product Launch",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-20T17:14:08.000Z",
    "download_time": "2025-08-21 01:32:33",
    "visual_resource": [
      "screenshot/twitter/madebygoogle_1958215989352440270.png"
    ],
    "extra_info": "{\"username\": \"madebygoogle\", \"tweet_id\": \"1958215989352440270\"}"
  },
  {
    "id": "twitter_CerebrasSystems_1957957962514960567",
    "source": "Twitter",
    "url": "https://twitter.com/CerebrasSystems/status/1957957962514960567",
    "title_en": "CerebrasSystems_Becomes Hugging Face Inference Provider",
    "summary_en": "Cerebras Systems announced it has become an inference provider for Hugging Face, handling 5 million monthly requests. Hugging Face's inference provider network has surpassed 20 million monthly requests, with Cerebras, Novita Labs, and Fireworks AI showing the fastest growth. This service currently powers OpenAI's official open playground and is integrated into various applications.",
    "keywords_en": [
      "Cerebras",
      "Hugging Face",
      "Inference Service",
      "AI Models",
      "OpenAI",
      "Cloud Computing"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-08-20T00:08:50.000Z",
    "download_time": "2025-08-21 05:51:10",
    "visual_resource": [
      "screenshot/twitter/CerebrasSystems_1957957962514960567.png"
    ],
    "extra_info": "{\"username\": \"CerebrasSystems\", \"tweet_id\": \"1957957962514960567\"}"
  },
  {
    "id": "twitter_AndrewYNg_1958165941369634825",
    "source": "Twitter",
    "url": "https://x.com/AndrewYNg/status/1958165941369634825",
    "title_en": "AndrewYNg_AI Dev 25 NYC Summit Announced",
    "summary_en": "Andrew Ng announced the AI Dev 25 conference will take place on November 14 in NYC, expecting over 1,200 developers. The event will delve into cutting-edge AI topics, including Agentic AI (e.g., multi-agent orchestration, tool use), AI-assisted coding, Context Engineering (e.g., advanced RAG, memory systems), Multimodal AI (e.g., vision-language models), and Fintech applications. A larger venue was secured due to the rapid sell-out of the previous Pi Day AI Dev event.",
    "keywords_en": [
      "AI Dev 25",
      "Agentic AI",
      "Multimodal AI",
      "Fintech",
      "Tech Conference",
      "NYC"
    ],
    "area_en": [
      "Tech News",
      "AI Agent",
      "Multimodal"
    ],
    "published_time": "2025-08-20T13:55:16.000Z",
    "download_time": "2025-08-21 01:26:25",
    "visual_resource": [
      "screenshot/twitter/AndrewYNg_1958165941369634825.png"
    ],
    "extra_info": "{\"username\": \"AndrewYNg\", \"tweet_id\": \"1958165941369634825\"}"
  },
  {
    "id": "twitter_Google_1958284725526643090",
    "source": "Twitter",
    "url": "https://x.com/Google/status/1958284725526643090",
    "title_en": "Google_Pixel 10 Ten Camera Updates",
    "summary_en": "Google announced ten significant camera updates for its Pixel 10 smartphone, aiming to greatly enhance the user photography experience. New features include high-resolution portraits and selfies, smoother stabilization, 100x Pro Res Zoom, AI-assisted composition guidance (Camera Coach), Auto Best Take for group shots, and deep integration with Google Photos. Additionally, Guided Frame powered by Gemini models and C2PA Content Credentials for image authenticity are introduced, along with real-time preview, comprehensively optimizing photo capture.",
    "keywords_en": [
      "Google",
      "Pixel 10",
      "Camera Updates",
      "Photography",
      "AI Features",
      "Image Processing"
    ],
    "area_en": [
      "Product Launch",
      "Tech News",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-20T21:47:16.000Z",
    "download_time": "2025-08-21 01:31:49",
    "visual_resource": [
      "screenshot/twitter/Google_1958284725526643090.png"
    ],
    "extra_info": "{\"username\": \"Google\", \"tweet_id\": \"1958284725526643090\"}"
  },
  {
    "id": "twitter_Google_1958269277003256044",
    "source": "Twitter",
    "url": "https://x.com/Google/status/1958269277003256044",
    "title_en": "Google_Gemini Visual and Interaction Capabilities Upgrade",
    "summary_en": "Google announced significant upgrades to its Gemini assistant, transforming it into a more helpful, natural, and visual AI. Key enhancements include new visual guidance, enabling Gemini to see what users see and highlight objects directly on screen when the camera is shared. Speech interaction has become more natural and expressive, with improvements in intonation, rhythm, and pitch. Furthermore, Gemini can now connect to a wider range of Google applications, such as Messages, Phone, and Clock, with user permission, offering more comprehensive assistance.",
    "keywords_en": [
      "Gemini",
      "AI Assistant",
      "Visual Guidance",
      "Speech Interaction",
      "Google Apps",
      "Multimodal"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "Product Launch"
    ],
    "published_time": "2025-08-20T20:45:53.000Z",
    "download_time": "2025-08-21 01:31:49",
    "visual_resource": [
      "screenshot/twitter/Google_1958269277003256044.png"
    ],
    "extra_info": "{\"username\": \"Google\", \"tweet_id\": \"1958269277003256044\"}"
  },
  {
    "id": "7uk3f4fFlEtx06pjr3oiuA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/7uk3f4fFlEtx06pjr3oiuA",
    "title_en": "OpenAI Executive Reveals: Scaling Endures, GPT-5's \"Dual-Axis Training\" Breaks Through Intelligence Ceiling",
    "summary_en": "OpenAI COO Brad Lightcap revealed key breakthroughs in GPT-5, emphasizing its ability to autonomously decide whether to perform deep reasoning before answering, significantly enhancing user experience. GPT-5's intelligence leap is not merely exponential growth but achieved through \"dual-axis training,\" combining pre-training (where scaling laws still hold) and post-training, yielding substantial gains in the latter. The model demonstrates comprehensive upgrades in accuracy, response speed, tool utilization, and structured thinking, showcasing strong potential in health, coding, and legal domains. Lightcap noted that while scaling laws remain valid, post-training represents a new paradigm for advancing model intelligence. Although GPT-5 is not AGI, it embodies the nascent form of a generalized learning system. OpenAI will continue to advance across multiple dimensions—algorithms, scale, compute, and data—to ensure more users and enterprises benefit from GPT-5's powerful capabilities and to foster the AI ecosystem.",
    "keywords_en": [
      "GPT-5",
      "Dual-Axis Training",
      "Post-training",
      "Scaling Law",
      "Artificial Intelligence",
      "OpenAI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-08-20T01:46:22.000Z",
    "download_time": "2025-08-21T13:52:18.484827",
    "visual_resource": [
      "screenshot/wechat/wechat_image_7uk3f4fFlEtx06pjr3oiuA.png"
    ],
    "extra_info": null
  },
  {
    "id": "yq5mvap6ldR5hwZWGJSEOw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/yq5mvap6ldR5hwZWGJSEOw",
    "title_en": "DeepSeek V3.1 Base Stealthily Launched! Outperforms Claude 4 in Programming, Community Awaits R2 and V4",
    "summary_en": "DeepSeek has stealthily launched its V3.1 model, a 685B-parameter large language model that extends its context length to 128K and achieves significant breakthroughs in programming capabilities. Benchmarking reveals V3.1 scored 71.6% on the Aider programming benchmark, surpassing Claude Opus 4, while offering faster inference and response times at a cost merely one-sixtieth of proprietary systems. The new version also introduces native “search token” support and hints at a potential future mixed architecture. Despite the model card not yet being released, V3.1 has already climbed to the top of Hugging Face's trending list, fueling strong community anticipation for R2 and V4. This update further solidifies DeepSeek's leading position in the open-source large model domain, demonstrating robust competitiveness, particularly in programming performance and cost-efficiency.",
    "keywords_en": [
      "DeepSeek V3.1",
      "Programming Capability",
      "Context Length",
      "Large Language Model",
      "Cost-Efficiency",
      "AI Performance"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Generative AI"
    ],
    "published_time": "2025-08-20T14:01:05.000Z",
    "download_time": "2025-08-21T13:51:50.895456",
    "visual_resource": [
      "screenshot/wechat/wechat_image_yq5mvap6ldR5hwZWGJSEOw.png"
    ],
    "extra_info": null
  },
  {
    "id": "8WROvfZYtob_JSk0XTSNeQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/8WROvfZYtob_JSk0XTSNeQ",
    "title_en": "Qwen-2.5-VL 7B Model Achieves Autonomous 'Visual Reflection' with 'Look-Back' Mechanism, Boosting Perception Task Performance by 6.3%",
    "summary_en": "Multimodal Large Language Models (MLLMs) frequently exhibit a tendency to over-rely on textual information during the latter stages of complex reasoning, often neglecting the critical integration of visual input. To address this limitation, the Qwen-2.5-VL 7B model introduces a groundbreaking \"Look-Back\" mechanism. This novel approach empowers the model to autonomously determine when and where to re-focus its attention on visual information throughout the inference process, crucially without requiring explicit image re-injection or any modifications to its core architecture. Developed through a sophisticated two-stage training framework, which combines supervised fine-tuning with reinforcement learning, the \"Look-Back\" mechanism has yielded substantial performance improvements. Specifically, it has boosted the model's average performance by approximately 7% in mathematical reasoning tasks and 6.3% in perception-based tasks. This significant advancement not only redefines the multimodal reasoning paradigm but also substantially enhances the model's generalization capabilities, positioning it as a highly competitive solution in the field.",
    "keywords_en": [
      "Multimodal Large Language Models",
      "Visual Reflection",
      "Look-Back",
      "Reasoning Capability",
      "Reinforcement Learning"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Computer Vision"
    ],
    "published_time": "2025-08-20T14:01:05.000Z",
    "download_time": "2025-08-21T13:51:57.269393",
    "visual_resource": [
      "screenshot/wechat/wechat_image_8WROvfZYtob_JSk0XTSNeQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "wCqh9BIPoXjiK5yTGOPrqA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/wCqh9BIPoXjiK5yTGOPrqA",
    "title_en": "Is DiT Mathematically and Formally Wrong? Xie Saining Responds: Don't Do Science in Your Head",
    "summary_en": "Recently, a post questioned the DiT model's architectural flaws, citing its premature FID stabilization, the use of unstable post-LayerNorm, and the expression-limiting adaLN-zero, even claiming that replacing some computational units with identity functions improved performance. Xie Saining, the author of DiT, responded by acknowledging some “hard flaws” like the inefficient sd-vae, but emphasized DiT's academic standing as the foundational architecture for Sora and Stable Diffusion 3. He stressed that scientific research requires empirical validation, not mere speculation. Xie argued that methods like TREAD are more akin to regularization effects rather than revealing fundamental errors in DiT, and offered suggestions for robust DiT upgrades, reiterating the pivotal role of experimental verification in scientific exploration.",
    "keywords_en": [
      "DiT",
      "Diffusion Models",
      "Transformer",
      "Architectural Flaws",
      "Xie Saining",
      "TREAD"
    ],
    "area_en": [
      "Deep Learning",
      "Generative AI",
      "Computer Vision"
    ],
    "published_time": "2025-08-20T04:23:48.000Z",
    "download_time": "2025-08-21T13:52:13.997474",
    "visual_resource": [
      "screenshot/wechat/wechat_image_wCqh9BIPoXjiK5yTGOPrqA.png"
    ],
    "extra_info": null
  },
  {
    "id": "2C5hi2o0RzAO_y6MTh2JMQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/2C5hi2o0RzAO_y6MTh2JMQ",
    "title_en": "Zhipu AI Unveils World's First Universal Mobile Agent, Free for All, Capable of Directly Controlling Cloud PCs",
    "summary_en": "Zhipu AI has launched AutoGLM, the world's first universal mobile agent, featuring a groundbreaking cloud-based execution architecture. This innovation allows users to command the AI via voice to perform complex, cross-application tasks such as ordering food, comparing product prices, generating reports, and creating presentations, all without consuming local device resources. AutoGLM supports operations on both mobile phones and cloud PCs, effectively addressing the computational limitations and resource occupation issues prevalent in traditional agents, thereby significantly enhancing user experience. This agent integrates multimodal capabilities including reasoning and coding, representing a pivotal stride for Zhipu AI towards Artificial General Intelligence (AGI) and validating the viability of cloud-based agents. Zhipu AI has also released an API to foster the widespread adoption of Agent technology and build a collaborative ecosystem, signaling the imminent arrival of an era where users can simply \"ask once and let the agent handle the rest.\"",
    "keywords_en": [
      "Zhipu AI",
      "Universal Agent",
      "Cloud Execution",
      "AutoGLM",
      "Artificial General Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-20T04:31:04.000Z",
    "download_time": "2025-08-21T13:52:12.151887",
    "visual_resource": [
      "screenshot/wechat/wechat_image_2C5hi2o0RzAO_y6MTh2JMQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "C1h6QveDrX_-yDxwI1CNUA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/C1h6QveDrX_-yDxwI1CNUA",
    "title_en": "Claude Code and Gemini Abandon Code Indexing: A Misstep",
    "summary_en": "The article criticizes AI IDEs like Claude Code and Gemini for abandoning code indexing (RAG) in favor of traditional grep-like text search, highlighting issues such as high token consumption, inefficiency, and a lack of semantic understanding. The author demonstrates through practical testing that integrating vector retrieval significantly improves efficiency and saves tokens. To address this pain point, the author open-sourced the \"claude-context\" project, a code retrieval tool that integrates vector databases and embedding models. The article elaborates on its technical details, including its MCP architecture, AST-based code splitting, and Merkle Tree synchronization mechanism. It showcases the project's remarkable effectiveness in real-world applications, achieving over 40% token savings, and emphasizes the critical role of code indexing in providing high-quality context for AI IDEs.",
    "keywords_en": [
      "Code Retrieval",
      "Vector Database",
      "AI IDE",
      "RAG",
      "claude-context"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-08-20T10:05:26.000Z",
    "download_time": "2025-08-21T13:51:57.224743",
    "visual_resource": [
      "screenshot/wechat/wechat_image_C1h6QveDrX_-yDxwI1CNUA.png"
    ],
    "extra_info": null
  },
  {
    "id": "sim",
    "source": "GitHub",
    "url": "https://github.com/simstudioai/sim",
    "title_en": "Build and deploy AI agent workflows in minutes.",
    "summary_en": "Sim is a platform designed for rapidly building and deploying AI agent workflows. It offers flexible deployment options, including cloud-hosted services and various self-hosting methods such as NPM packages, Docker Compose, development containers, and manual setup. The platform supports integration with local AI models via Ollama and leverages PostgreSQL with the pgvector extension for efficient vector embeddings, enabling advanced AI functionalities like knowledge bases and semantic search. Built on a modern tech stack including Next.js, Bun, Drizzle ORM, and Socket.io, Sim aims to streamline the development of sophisticated AI applications.",
    "keywords_en": [
      "AI Agent",
      "Workflow",
      "Self-hosting",
      "Large Language Model",
      "Vector Database",
      "PostgreSQL",
      "Docker",
      "Next.js"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-08-20T16:56:09Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/public/static/demo.gif",
      "https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/public/logo/reverse/text/large.png"
    ],
    "extra_info": null
  },
  {
    "id": "airi",
    "source": "GitHub",
    "url": "https://github.com/moeru-ai/airi",
    "title_en": "Project AIRI",
    "summary_en": "Project AIRI aims to recreate Neuro-sama, developing a comprehensive container for AI waifu and virtual characters, enabling them to interact, play games, and engage in natural conversations with users. Positioned as a robust open-source alternative to existing proprietary solutions like Neuro-sama, AIRI distinguishes itself by leveraging cutting-edge Web technologies such as WebGPU, WebAssembly, and Web Workers. This architectural choice ensures broad compatibility, allowing the application to run seamlessly across modern browsers, desktop environments, and even mobile devices with PWA support. The platform integrates with a wide array of large language model APIs, providing flexibility and power. Its core functionalities include sophisticated memory systems, advanced client-side speech recognition, realistic speech synthesis, and precise control over VRM and Live2D models. Project AIRI is dedicated to empowering individuals to easily own, customize, and manage their personalized digital companions, ushering in a new era of interactive and accessible virtual life experiences.",
    "keywords_en": [
      "AI Virtual Character",
      "Digital Companion",
      "Large Language Model",
      "Web Technologies",
      "VRM",
      "Live2D",
      "Speech Interaction",
      "AI Agent"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-20T20:19:11Z",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "https://github.com/moeru-ai/airi/raw/main/docs/content/public/banner-dark-1280x640.avif",
      "https://github.com/moeru-ai/airi/raw/main/docs/content/public/readme-image-pc-preview.avif"
    ],
    "extra_info": null
  },
  {
    "id": "self-hosted-ai-starter-kit",
    "source": "GitHub",
    "url": "https://github.com/n8n-io/self-hosted-ai-starter-kit",
    "title_en": "Self-hosted AI starter kit",
    "summary_en": "The Self-hosted AI Starter Kit is an open-source Docker Compose template meticulously crafted to swiftly initialize a comprehensive local AI and low-code development environment. This robust solution seamlessly combines the self-hosted n8n platform, renowned for its extensive integrations and advanced AI components, with essential AI products like Ollama for running local large language models, Qdrant as a high-performance vector store, and PostgreSQL for reliable data management. It supports diverse hardware configurations, including Nvidia and AMD GPUs, as well as CPU-only setups. The kit empowers users to securely develop a wide array of AI-powered applications, such as intelligent AI agents for task automation, confidential document summarization without data leaks, and enhanced communication tools like smarter Slack bots. Ideal for rapid prototyping and proof-of-concept projects, it provides a complete, self-contained ecosystem for building and experimenting with AI workflows locally, ensuring data privacy and cost efficiency.",
    "keywords_en": [
      "Self-hosted AI",
      "Low-code",
      "Docker Compose",
      "Large Language Model",
      "Vector Database",
      "AI Workflow",
      "n8n",
      "Ollama"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-07-17T08:11:19Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/assets/n8n-demo.gif"
    ],
    "extra_info": null
  },
  {
    "id": "LLMs-from-scratch",
    "source": "GitHub",
    "url": "https://github.com/rasbt/LLMs-from-scratch",
    "title_en": "Build a Large Language Model (From Scratch)",
    "summary_en": "This GitHub repository serves as the official code companion for the book \"Build a Large Language Model (From Scratch),\" guiding readers through the process of developing, pretraining, and finetuning GPT-like large language models from the ground up. The project provides comprehensive code examples, covering text data handling, attention mechanism implementation, GPT model construction, unsupervised pretraining, and both classification and instruction finetuning. It emphasizes a hands-on coding approach to deeply understand the internal workings of LLMs, also supporting the loading of larger pretrained models for finetuning. This resource is ideal for individuals seeking a systematic and practical learning experience in LLM development.",
    "keywords_en": [
      "Large Language Model",
      "From Scratch",
      "Pretraining",
      "Finetuning",
      "GPT",
      "PyTorch",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "area_en": [
      "Deep Learning",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-20T02:08:29Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/LLMs-from-scratch.png"
    ],
    "extra_info": null
  },
  {
    "id": "terminal-bench",
    "source": "GitHub",
    "url": "https://github.com/laude-institute/terminal-bench",
    "title_en": "terminal-bench",
    "summary_en": "Terminal-Bench is a robust benchmark platform specifically designed to rigorously evaluate the performance of AI agents within authentic terminal environments. It offers a comprehensive, reproducible task suite and an advanced execution harness, empowering agents to autonomously tackle complex, end-to-end real-world challenges, ranging from intricate code compilation and model training to sophisticated server setup. The platform fundamentally consists of two core components: a diverse dataset of tasks and an innovative execution harness that seamlessly connects large language models to a secure, sandboxed terminal environment. This design provides a practical and realistic evaluation framework for developers building LLM agents, benchmarking AI frameworks, and stress-testing advanced system-level reasoning capabilities. Currently in its beta phase with approximately 100 diverse tasks, Terminal-Bench is actively expanding to become a more comprehensive testbed for AI agents in text-based environments.",
    "keywords_en": [
      "AI Agent",
      "Terminal Environment",
      "Benchmark",
      "Large Language Model",
      "Execution Harness",
      "Sandbox Environment",
      "Task Dataset",
      "System-level Reasoning"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-20T22:24:50Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "screenshot/github/terminal-bench.png"
    ],
    "extra_info": null
  },
  {
    "id": "BitNet",
    "source": "GitHub",
    "url": "https://github.com/microsoft/BitNet",
    "title_en": "bitnet.cpp",
    "summary_en": "bitnet.cpp is the official inference framework from Microsoft for 1-bit Large Language Models (LLMs) like BitNet b1.58. It provides a suite of optimized kernels that enable fast and lossless inference of 1.58-bit models on both CPUs and GPUs. The framework achieves significant speedups, ranging from 1.37x to 5.07x on ARM CPUs and 2.37x to 6.17x on x86 CPUs, while also substantially reducing energy consumption. Notably, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU at speeds comparable to human reading, greatly enhancing the potential for deploying LLMs on local devices and marking a significant advancement in edge AI inference.",
    "keywords_en": [
      "1-bit LLM",
      "Inference Framework",
      "Model Optimization",
      "CPU Inference",
      "GPU Inference",
      "Edge Computing",
      "Energy Efficiency"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Deep Learning"
    ],
    "published_time": "2025-06-03T06:14:20Z",
    "download_time": "2024-05-20 10:00:00",
    "visual_resource": [
      "https://github.com/microsoft/BitNet/raw/main/assets/m2_performance.jpg",
      "https://github.com/microsoft/BitNet/raw/main/assets/intel_performance.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "2508.13167",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.13167",
    "title_en": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent\n  Distillation and Agentic RL",
    "summary_en": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.",
    "keywords_en": [
      "Chain-of-Agents",
      "Agent Foundation Models",
      "Multi-agent systems",
      "Agentic Reinforcement Learning",
      "Large Language Models"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-06T17:01:02.000Z",
    "download_time": "2025-08-20 18:38:41",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13167.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.13167\", \"arxiv_url\": \"https://arxiv.org/abs/2508.13167\"}"
  },
  {
    "id": "2508.13948",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.13948",
    "title_en": "Prompt Orchestration Markup Language",
    "summary_en": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
    "keywords_en": [
      "Large Language Models",
      "Prompt Orchestration",
      "Markup Language",
      "Data Integration",
      "Developer Toolkit"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-08-19T15:37:29.000Z",
    "download_time": "2025-08-20 18:38:36",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13948.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.13948\", \"arxiv_url\": \"https://arxiv.org/abs/2508.13948\"}"
  },
  {
    "id": "2508.13998",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.13998",
    "title_en": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic\n  Manipulation",
    "summary_en": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which\nstems from data scarcity and embodiment heterogeneity. To address this, we\npioneer \"pointing\" as a unified, embodiment-agnostic intermediate\nrepresentation, defining four core embodied pointing abilities that bridge\nhigh-level vision-language comprehension with low-level action primitives. We\nintroduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed\nfor embodied reasoning and pointing. We use a wide range of embodied and\ngeneral visual reasoning datasets as sources to construct a large-scale\ndataset, Embodied-Points-200K, which supports key embodied pointing\ncapabilities. We then train Embodied-R1 using a two-stage Reinforced\nFine-tuning (RFT) curriculum with a specialized multi-task reward design.\nEmbodied-R1 achieves state-of-the-art performance on 11 embodied spatial and\npointing benchmarks. Critically, it demonstrates robust zero-shot\ngeneralization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%\nacross 8 real-world XArm tasks without any task-specific fine-tuning,\nrepresenting a 62% improvement over strong baselines. Furthermore, the model\nexhibits high robustness against diverse visual disturbances. Our work shows\nthat a pointing-centric representation, combined with an RFT training paradigm,\noffers an effective and generalizable pathway to closing the perception-action\ngap in robotics.",
    "keywords_en": [
      "Embodied Reasoning",
      "Robotic Manipulation",
      "Vision-Language Model",
      "Reinforced Fine-tuning",
      "Pointing"
    ],
    "area_en": [
      "Robotics",
      "AI Agent",
      "Multimodal"
    ],
    "published_time": "2025-08-19T16:50:01.000Z",
    "download_time": "2025-08-20 18:38:43",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13998.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.13998\", \"arxiv_url\": \"https://arxiv.org/abs/2508.13998\"}"
  },
  {
    "id": "2508.12903",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.12903",
    "title_en": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language\n  Models",
    "summary_en": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.",
    "keywords_en": [
      "Language Models",
      "Self-Refinement",
      "Proactive Self-Refinement",
      "Large Language Models",
      "Iterative Refinement"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-18T13:07:21.000Z",
    "download_time": "2025-08-20 18:38:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12903.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.12903\", \"arxiv_url\": \"https://arxiv.org/abs/2508.12903\"}"
  },
  {
    "id": "2508.09131",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.09131",
    "title_en": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion\n  Transformer",
    "summary_en": "Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.",
    "keywords_en": [
      "Text-guided color editing",
      "Multi-Modal Diffusion Transformer",
      "Training-free",
      "Attention mechanisms",
      "Image and video editing"
    ],
    "area_en": [
      "Computer Vision",
      "Multimodal",
      "Generative AI"
    ],
    "published_time": "2025-08-12T17:57:04.000Z",
    "download_time": "2025-08-20 18:38:41",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09131.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.09131\", \"arxiv_url\": \"https://arxiv.org/abs/2508.09131\"}"
  },
  {
    "id": "2508.04326",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.04326",
    "title_en": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned\n  and Addressed for XR Research",
    "summary_en": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)\nand Neural Radiance Fields (NeRF), has revolutionized interactive\nphotorealistic view synthesis and presents enormous opportunities for XR\nresearch and applications. However, despite the exponential growth of RF\nresearch, RF-related contributions to the XR community remain sparse. To better\nunderstand this research gap, we performed a systematic survey of current RF\nliterature to analyze (i) how RF is envisioned for XR applications, (ii) how\nthey have already been implemented, and (iii) the remaining research gaps. We\ncollected 365 RF contributions related to XR from computer vision, computer\ngraphics, robotics, multimedia, human-computer interaction, and XR communities,\nseeking to answer the above research questions. Among the 365 papers, we\nperformed an analysis of 66 papers that already addressed a detailed aspect of\nRF research for XR. With this survey, we extended and positioned XR-specific RF\nresearch topics in the broader RF research field and provide a helpful resource\nfor the XR community to navigate within the rapid development of RF research.",
    "keywords_en": [
      "Radiance Fields",
      "XR",
      "View Synthesis",
      "3D Gaussian Splatting",
      "Survey"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Robotics"
    ],
    "published_time": "2025-08-06T11:14:06.000Z",
    "download_time": "2025-08-20 18:38:38",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04326.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.04326\", \"arxiv_url\": \"https://arxiv.org/abs/2508.04326\"}"
  }
]