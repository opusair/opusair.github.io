[
  {
    "id": "twitter_rasbt_1956130338431713307",
    "source": "Twitter",
    "url": "https://x.com/rasbt/status/1956130338431713307",
    "title_en": "rasbt_Gemma 3 270M Release: Powerful Performance from a Small Open-Weight LLM",
    "summary_en": "Sebastian Raschka highlights the release of Gemma 3 270M, a new small open-weight large language model from Google, featuring only 270 million parameters. This compact model demonstrates surprisingly strong instruction following capabilities and can be fine-tuned rapidly. It boasts a large vocabulary, serving as a high-quality foundation for various applications. A notable technical detail is its efficient performance despite having only four attention heads, making it ideal for local experimentation and development.",
    "keywords_en": [
      "Gemma 3 270M",
      "Large Language Model",
      "Open-Weight Model",
      "Small Model",
      "Attention Mechanism",
      "Model Release"
    ],
    "area_en": [
      "Large Language Model",
      "Product Launch",
      "Tech News"
    ],
    "published_time": "2025-08-14T23:06:30.000Z",
    "download_time": "2025-08-16 03:51:43",
    "visual_resource": [
      "screenshot/twitter/rasbt_1956130338431713307.png"
    ],
    "extra_info": "{\"username\": \"rasbt\", \"tweet_id\": \"1956130338431713307\"}"
  },
  {
    "id": "twitter_francoisfleuret_1956122489853239329",
    "source": "Twitter",
    "url": "https://x.com/francoisfleuret/status/1956122489853239329",
    "title_en": "francoisfleuret_Questioning LLM Universality and Necessity of New Architectures",
    "summary_en": "François Fleuret presents a \"hot take,\" suggesting that the universality of language and the capabilities of Large Language Models (LLMs) might be misleading us. He argues that since chains-of-thought can implement virtually any cognitive process, new model architectures might seem unnecessary. He draws an analogy to the universal representation theorem not rendering convolutional neural networks (convnets) unnecessary, implying that current LLM capabilities should not deter the exploration of novel architectures.",
    "keywords_en": [
      "Large Language Models",
      "Chains-of-Thought",
      "Model Architectures",
      "Artificial Intelligence",
      "Cognitive Processes"
    ],
    "area_en": [
      "Large Language Model",
      "Research Progress",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-14T22:35:19.000Z",
    "download_time": "2025-08-16 03:41:38",
    "visual_resource": [
      "screenshot/twitter/francoisfleuret_1956122489853239329.png"
    ],
    "extra_info": "{\"username\": \"francoisfleuret\", \"tweet_id\": \"1956122489853239329\"}"
  },
  {
    "id": "twitter_AIatMeta_1956027795051831584",
    "source": "Twitter",
    "url": "https://x.com/AIatMeta/status/1956027795051831584",
    "title_en": "AIatMeta_Introducing DINOv3: A Breakthrough in Self-Supervised Vision Models",
    "summary_en": "Meta AI introduces DINOv3, a state-of-the-art computer vision model trained with self-supervised learning (SSL). It generates powerful, high-resolution image features. For the first time, a single frozen vision backbone in DINOv3 outperforms specialized solutions on multiple long-standing dense prediction tasks, marking a significant advancement in computer vision. This model enhances image feature extraction, bringing substantial performance improvements to related applications.",
    "keywords_en": [
      "DINOv3",
      "Computer Vision",
      "Self-supervised Learning",
      "Meta AI",
      "Image Features",
      "Dense Prediction"
    ],
    "area_en": [
      "Computer Vision",
      "Deep Learning",
      "Research Progress"
    ],
    "published_time": "2025-08-14T16:19:02.000Z",
    "download_time": "2025-08-16 03:41:38",
    "visual_resource": [
      "screenshot/twitter/AIatMeta_1956027795051831584.png"
    ],
    "extra_info": "{\"username\": \"AIatMeta\", \"tweet_id\": \"1956027795051831584\"}"
  },
  {
    "id": "twitter_shai_s_shwartz_1955968602978320727",
    "source": "Twitter",
    "url": "https://x.com/shai_s_shwartz/status/1955968602978320727",
    "title_en": "shai_s_shwartz_Launches FormulaOne Benchmark to Evaluate Frontier AI Model Reasoning",
    "summary_en": "Shai Shalev-Shwartz introduced FormulaOne, a new benchmark designed to evaluate the “PhD-level” reasoning capabilities of frontier AI models on Dynamic Programming problems. The benchmark features three tiers: 'shallow', 'deeper', and 'deepest'. Results indicate that while top models perform adequately on the 'shallow' tier, they struggle significantly with 'deeper' problems, including Grok 4, Gemini-Pro, and Opus-4. Even GPT-5 Pro solves only a few problems, and all models fail completely on the 'deepest' tier, highlighting current AI models' substantial limitations in complex reasoning.",
    "keywords_en": [
      "AI Models",
      "Reasoning",
      "Benchmark",
      "Dynamic Programming",
      "FormulaOne",
      "Large Language Models"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-08-14T12:23:49.000Z",
    "download_time": "2025-08-16 03:41:39",
    "visual_resource": [
      "screenshot/twitter/shai_s_shwartz_1955968602978320727.png"
    ],
    "extra_info": "{\"username\": \"shai_s_shwartz\", \"tweet_id\": \"1955968602978320727\"}"
  },
  {
    "id": "twitter_awnihannun_1956053493216895406",
    "source": "Twitter",
    "url": "https://twitter.com/awnihannun/status/1956053493216895406",
    "title_en": "awnihannun_Gemma 3 270M Model Achieves Ultra-Fast Text Generation on M4 Max",
    "summary_en": "Awni Hannun shared the impressive performance of the Gemma 3 270M 4-bit quantized model on the M4 Max chip. Utilizing the mlx-lm library, the model achieved text generation speeds exceeding 650 tokens per second while consuming less than 200MB of memory. This demonstrates the high efficiency and low resource consumption of compact large language models running on Apple hardware, holding significant implications for AI applications on edge devices and personal computers.",
    "keywords_en": [
      "Gemma 3",
      "Large Language Model",
      "Text Generation",
      "M4 Max",
      "mlx-lm",
      "Quantization"
    ],
    "area_en": [
      "Large Language Model",
      "Generative AI",
      "Tech News"
    ],
    "published_time": "2025-08-14T18:01:09.000Z",
    "download_time": "2025-08-16 09:35:16",
    "visual_resource": [
      "screenshot/twitter/awnihannun_1956053493216895406.png"
    ],
    "extra_info": "{\"username\": \"awnihannun\", \"tweet_id\": \"1956053493216895406\"}"
  },
  {
    "id": "twitter_hwchase17_1956036358709108979",
    "source": "Twitter",
    "url": "https://twitter.com/hwchase17/status/1956036358709108979",
    "title_en": "hwchase17_LangChain Academy Launches Deep Research Course",
    "summary_en": "Harrison Chase announced that LangChain Academy has launched a new hour-long course titled \"Deep Research with LangGraph\". This new offering focuses on how to effectively build and utilize AI agents for comprehensive and in-depth information retrieval and analysis, identifying it as one of the most popular and powerful use cases for agents. The course aims to equip learners with practical skills to navigate complex research challenges and apply advanced AI techniques in real-world scenarios, further expanding the capabilities of LangChain's ecosystem.",
    "keywords_en": [
      "LangChain",
      "AI Agent",
      "Deep Research",
      "AI Course",
      "Product Launch"
    ],
    "area_en": [
      "AI Agent",
      "Tech News",
      "Product Launch"
    ],
    "published_time": "2025-08-14T16:53:03.000Z",
    "download_time": "2025-08-16 09:36:43",
    "visual_resource": [
      "screenshot/twitter/hwchase17_1956036358709108979.png"
    ],
    "extra_info": "{\"username\": \"hwchase17\", \"tweet_id\": \"1956036358709108979\"}"
  },
  {
    "id": "S0JuK0nOHV0iekdGUg0f7A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/S0JuK0nOHV0iekdGUg0f7A",
    "title_en": "Is Chain-of-Thought Reasoning a Mirage? Re-examining Large Language Model Inference from a Data Distribution Perspective, Elon Musk Responds, Grok Breaks Down",
    "summary_en": "A study from Arizona State University reveals that Chain-of-Thought (CoT) reasoning in large language models (LLMs) may not represent true logical deduction but rather a reproduction of patterns within their training data distribution. The research indicates that CoT's generalization capabilities rapidly collapse when tasks deviate from the training data distribution (Out-of-Distribution, OOD), demonstrating fragility across task, length, and format generalization. Using a controllable experimental platform called DataAlchemy, the study confirmed this \"mirage\" nature, highlighting that CoT's vulnerability is a universal phenomenon, unaffected by model scale or temperature. This finding cautions against blindly relying on CoT in high-stakes domains, suggests that current evaluation methods might overestimate model robustness, and clarifies that supervised fine-tuning only locally extends distribution rather than enhancing abstract reasoning. Future development must acknowledge CoT's generalization limitations and maintain caution in evaluation and deployment.",
    "keywords_en": [
      "Chain-of-Thought",
      "Large Language Models",
      "Data Distribution",
      "Generalization",
      "Reasoning",
      "Out-of-Distribution"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "published_time": "2025-08-14T16:08:29.000Z",
    "download_time": "2025-08-16T17:37:42.853028",
    "visual_resource": [
      "screenshot/wechat/wechat_image_S0JuK0nOHV0iekdGUg0f7A.png"
    ],
    "extra_info": null
  },
  {
    "id": "MYxLWj5hRLIj2xTQ2X_OWg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/MYxLWj5hRLIj2xTQ2X_OWg",
    "title_en": "Omni-Effects: The First Unified Multi-Effect Generation Framework",
    "summary_en": "Omni-Effects introduces an innovative unified framework for diverse customized visual effects (VFX) generation, supporting single, multi, and spatially controllable multi-effect synthesis. It integrates pure prompt-driven generation with Spatial-Aware Prompt (SAP) technology, enabling precise spatial control of effects within videos and complex visual outcomes that adapt to environmental changes. Validated through the comprehensive Omni-VFX dataset and a novel data production pipeline, the framework demonstrates high robustness, capable of generating high-fidelity, diverse VFX composite videos. Its core technical innovations, including LoRA-MoE and SAP, effectively address the challenges of multi-conditional VFX generation. Omni-Effects holds significant application potential in film production, game development, and advertising, representing the first comprehensive solution for controllable multi-effect generation.",
    "keywords_en": [
      "Visual Effects",
      "Multi-Effect Generation",
      "Unified Framework",
      "Spatial-Aware Prompt",
      "Video Generation"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-08-14T13:22:35.000Z",
    "download_time": "2025-08-16T17:37:38.984371",
    "visual_resource": [
      "screenshot/wechat/wechat_image_MYxLWj5hRLIj2xTQ2X_OWg.png"
    ],
    "extra_info": null
  },
  {
    "id": "gXegvKs4BZxkeUa7CPnMqw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/gXegvKs4BZxkeUa7CPnMqw",
    "title_en": "Microsoft Introduces GFPO, a Groundbreaking Improvement to DeepSeek GRPO, Reducing Lengthy Responses by 80%",
    "summary_en": "Microsoft researcher Dimitris Papailiopoulos has introduced a groundbreaking reinforcement learning algorithm, Group Filtered Policy Optimization (GFPO), designed to address the issue of lengthy responses generated by models like DeepSeek GRPO. GFPO expands the candidate response pool and explicitly filters for desired properties, enabling it to reduce redundant tokens in inference by up to 80% while simultaneously improving accuracy. This algorithm eliminates the need for complex reward engineering, allowing for the joint optimization of multiple response attributes such as conciseness and accuracy. Experimental results demonstrate that GFPO significantly shortens responses across various difficulty levels. Notably, its adaptive difficulty variant efficiently balances computational cost and accuracy when tackling complex problems, and it effectively mitigates length inflation in out-of-distribution tasks, establishing a new paradigm for efficient reinforcement learning.",
    "keywords_en": [
      "GFPO",
      "GRPO",
      "Reinforcement Learning",
      "Response Length",
      "Large Model Optimization",
      "Token Efficiency"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-14T04:55:37.000Z",
    "download_time": "2025-08-16T17:37:59.471964",
    "visual_resource": [
      "screenshot/wechat/wechat_image_gXegvKs4BZxkeUa7CPnMqw.png"
    ],
    "extra_info": null
  },
  {
    "id": "vBaauj88Jd-Xdi3RU_F9IA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/vBaauj88Jd-Xdi3RU_F9IA",
    "title_en": "HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation",
    "summary_en": "HERMES introduces the first unified self-driving world model, designed to bridge the critical gap between comprehensive 3D scene understanding and accurate future scene generation. This innovative model leverages a shared Large Language Model (LLM), a unified Bird's-Eye View (BEV) feature space, and a novel World Queries mechanism. This architecture enables HERMES to achieve deep comprehension of complex urban environments, such as identifying specific landmarks and traffic conditions, alongside precise prediction of future dynamics, like vehicle and pedestrian movements. Extensive experiments on datasets including nuScenes demonstrate HERMES's superior performance; its generation accuracy significantly surpasses existing separate models, while its understanding capabilities remain robust. The model's multi-task comparative results underscore its powerful integrated abilities, offering a new, efficient paradigm for developing more intelligent and reliable autonomous driving systems and paving the way for general-purpose driving foundation models.",
    "keywords_en": [
      "Autonomous Driving",
      "World Model",
      "Scene Understanding",
      "Scene Generation",
      "Large Language Model",
      "Unified Framework"
    ],
    "area_en": [
      "Computer Vision",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-08-14T04:55:37.000Z",
    "download_time": "2025-08-16T17:37:55.102819",
    "visual_resource": [
      "screenshot/wechat/wechat_image_vBaauj88Jd-Xdi3RU_F9IA.png"
    ],
    "extra_info": null
  },
  {
    "id": "mfokWcYudJPWhY69h_kBkA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/mfokWcYudJPWhY69h_kBkA",
    "title_en": "Kunlun Wanwei Launches Skywork Deep Research Agent V2, Revolutionizing Multimodal Research and Browser Automation",
    "summary_en": "Kunlun Wanwei has officially launched Skywork Deep Research Agent V2, a groundbreaking AI agent model that revolutionizes multimodal deep research and browser automation. This new iteration excels in processing both textual and visual information, setting new SOTA records on authoritative benchmarks like BrowseComp and GAIA, surpassing leading domestic and international competitors. The V2 introduces advanced multimodal information retrieval, an asynchronous parallel multi-agent architecture, and integrated result presentation capabilities, effectively addressing the limitations of traditional text-only research. Furthermore, its multimodal deep browser agent overcomes common efficiency and stability bottlenecks of conventional browser agents, enabling in-depth social media content analysis and visual report generation. This launch signifies Kunlun Wanwei's significant progress in AI application deployment and full-stack development, indicating a strategic shift in the AI industry's focus from singular general large models to building comprehensive toolchains and application ecosystems.",
    "keywords_en": [
      "Kunlun Wanwei",
      "AI Agent",
      "Multimodal",
      "Deep Research",
      "Browser Agent",
      "Skywork"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Multimodal"
    ],
    "published_time": "2025-08-14T04:55:37.000Z",
    "download_time": "2025-08-16T17:38:01.646703",
    "visual_resource": [
      "screenshot/wechat/wechat_image_mfokWcYudJPWhY69h_kBkA.png"
    ],
    "extra_info": null
  },
  {
    "id": "pXKpvJ-PWV2DXMywyJO4jA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/pXKpvJ-PWV2DXMywyJO4jA",
    "title_en": "Boosting AI Multi-Domain Reinforcement Learning Capabilities by Integrating Math, Programming, and Logic Data | Shanghai AI Lab",
    "summary_en": "The OpenDataLab team at Shanghai AI Lab has developed a multi-domain evaluation framework encompassing mathematics, programming, and logic puzzles, thoroughly analyzing the complex mechanisms of Reinforcement Learning with Verifiable Rewards (RLVR) in multi-domain reasoning. Their research, based on the Qwen2.5-7B series models, reveals that joint training across these three domains significantly boosts overall average performance to 56.57, outperforming any dual-domain combination. Key findings include: mutual support between logic and mathematical abilities, cross-domain effects of code reasoning, enhanced robustness with diversified data, improved RL effectiveness through SFT, critical importance of template consistency, benefits of Policy Refresh, necessity of task-adaptive reward design, and RLVR's sensitivity to language. This study emphasizes that multi-domain joint training effectively prevents performance \"collapse\" in specific tasks, ensuring balanced model development and offering new perspectives for optimizing large model reasoning capabilities.",
    "keywords_en": [
      "Reinforcement Learning",
      "Multi-domain Reasoning",
      "Large Language Models",
      "Math Programming Logic",
      "RLVR"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-08-14T04:00:26.000Z",
    "download_time": "2025-08-16T17:38:07.530475",
    "visual_resource": [
      "screenshot/wechat/wechat_image_pXKpvJ-PWV2DXMywyJO4jA.png"
    ],
    "extra_info": null
  },
  {
    "id": "fastapi_mcp",
    "source": "GitHub",
    "url": "https://github.com/tadata-org/fastapi_mcp",
    "title_en": "FastAPI-MCP",
    "summary_en": "FastAPI-MCP is an innovative Python library designed to seamlessly expose FastAPI API endpoints as Model Context Protocol (MCP) tools, complete with built-in authentication. This project adopts a FastAPI-native approach, distinguishing itself from mere OpenAPI converters, and supports zero or minimal configuration. It automatically preserves the schemas of request and response models, as well as Swagger documentation. Key advantages include efficient communication via FastAPI's ASGI interface and flexible deployment options, allowing it to function either as an extension to an existing FastAPI application or as a standalone service. By offering native dependency management and a unified infrastructure, FastAPI-MCP significantly streamlines the integration of existing FastAPI services into the MCP ecosystem, making it particularly suitable for developing and managing AI-powered tools.",
    "keywords_en": [
      "FastAPI",
      "Model Context Protocol",
      "MCP",
      "API Tools",
      "Authentication",
      "Python",
      "ASGI",
      "AI Tools"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-10T09:07:00Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/fastapi_mcp.png"
    ],
    "extra_info": null
  },
  {
    "id": "SpatialLM",
    "source": "GitHub",
    "url": "https://github.com/manycore-research/SpatialLM",
    "title_en": "SpatialLM",
    "summary_en": "SpatialLM is an innovative 3D large language model specifically engineered to interpret and process complex 3D point cloud data. Its primary function is to generate highly structured 3D scene understanding outputs, which encompass detailed architectural elements like walls, doors, and windows, as well as precisely oriented object bounding boxes complete with their semantic categories. A key advantage of SpatialLM is its versatility in handling point clouds derived from a wide array of sources, including monocular video sequences, RGBD images, and LiDAR sensors, thereby overcoming limitations of previous methods requiring specialized equipment. This multimodal architecture effectively bridges the critical gap between raw, unstructured 3D geometric data and refined, structured 3D representations, offering a high-level semantic understanding of environments. Furthermore, SpatialLM 1.1 introduces advanced capabilities such as doubled point cloud resolution, a more powerful point cloud encoder (Sonata), and the ability to perform detection based on user-specified categories, leveraging the flexibility of LLMs. These features collectively enhance spatial reasoning, making SpatialLM highly valuable for cutting-edge applications in embodied robotics, autonomous navigation, and sophisticated 3D scene analysis tasks.",
    "keywords_en": [
      "3D Large Language Model",
      "Point Cloud Processing",
      "Scene Understanding",
      "Spatial Reasoning",
      "Embodied Robotics",
      "Object Detection"
    ],
    "area_en": [
      "Large Language Model",
      "Computer Vision",
      "Robotics"
    ],
    "published_time": "2025-06-10T02:58:45Z",
    "download_time": "2024-05-15 12:00:00",
    "visual_resource": [
      "https://github.com/manycore-research/SpatialLM/raw/main/figures/stru3d.jpg",
      "https://github.com/manycore-research/SpatialLM/raw/main/figures/scannet.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "magentic-ui",
    "source": "GitHub",
    "url": "https://github.com/microsoft/magentic-ui",
    "title_en": "Magentic-UI",
    "summary_en": "Magentic-UI is a cutting-edge research prototype of a human-centered interface, leveraging a sophisticated multi-agent system to automate complex web tasks while ensuring users retain full control. It excels at browsing and performing actions on the web, generating and executing code, and analyzing various file types, addressing scenarios from form filling to deep website navigation and data-driven code execution. Built on the AutoGen framework, Magentic-UI provides a transparent and highly controllable interaction paradigm, fostering efficient human-in-the-loop involvement. Its core functionalities include collaborative planning (Co-Planning), guided task execution (Co-Tasking), robust security measures (Action Guards), intelligent plan learning and retrieval from past runs, and efficient parallel task execution. This innovative approach significantly boosts human-agent interaction efficiency, making it an ideal solution for intricate web navigation, data extraction, and automated data processing challenges, as demonstrated by its performance on benchmarks like GAIA and AssistantBench.",
    "keywords_en": [
      "Multi-agent System",
      "Human-in-the-loop",
      "Web Automation",
      "Code Execution",
      "File Analysis",
      "LLM Applications",
      "AutoGen",
      "Docker"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-14T17:46:34Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://github.com/microsoft/magentic-ui/blob/main/docs/img/magenticui_running.png?raw=true",
      "https://github.com/microsoft/magentic-ui/blob/main/docs/img/magui-coplanning.png?raw=true"
    ],
    "extra_info": null
  },
  {
    "id": "marker",
    "source": "GitHub",
    "url": "https://github.com/datalab-to/marker",
    "title_en": "Marker",
    "summary_en": "Marker is an efficient and accurate document conversion tool, supporting various file formats such as PDF, images, Office documents, HTML, and EPUB, converting them into Markdown, JSON, chunks, or HTML. Its core features include intelligent recognition and formatting of complex elements like tables, equations, and code blocks, with support for all languages. Marker outperforms existing cloud services and open-source solutions in performance. Furthermore, it can significantly enhance conversion accuracy and structured data extraction capabilities by integrating Large Language Models (LLMs), particularly excelling in table recognition. The tool supports GPU/CPU/MPS operation, offering flexible API and CLI interfaces, making it widely applicable in document digitization, content management, and RAG data preparation.",
    "keywords_en": [
      "Document Conversion",
      "PDF Processing",
      "Structured Extraction",
      "Large Language Model",
      "OCR",
      "Multimodal"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Large Language Model"
    ],
    "published_time": "2025-08-15T23:20:30Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "screenshot/github/marker.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.09736",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.09736",
    "title_en": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory",
    "summary_en": "We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent",
    "keywords_en": [
      "Multimodal Agent",
      "Long-Term Memory",
      "Memory-based Reasoning",
      "Video Question Answering",
      "Reinforcement Learning"
    ],
    "area_en": [
      "Multimodal",
      "AI Agent",
      "Video Understanding"
    ],
    "published_time": "2025-08-13T12:03:03.000Z",
    "download_time": "2025-08-16 02:38:27",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09736.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.09736\", \"arxiv_url\": \"https://arxiv.org/abs/2508.09736\"}"
  },
  {
    "id": "2508.07901",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.07901",
    "title_en": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video\n  Generation",
    "summary_en": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just sim1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.",
    "keywords_en": [
      "Video Generation",
      "Identity Control",
      "Lightweight Framework",
      "Plug-and-Play",
      "Identity Preservation"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-08-11T12:17:38.000Z",
    "download_time": "2025-08-16 02:38:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07901.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.07901\", \"arxiv_url\": \"https://arxiv.org/abs/2508.07901\"}"
  },
  {
    "id": "2508.06944",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.06944",
    "title_en": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal\n  Imitation-Exploration Balance",
    "summary_en": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks\nthrough a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by\nReinforcement Learning (RL), a process fraught with catastrophic forgetting and\nsuboptimal trade-offs between imitation and exploration. Recent single-stage\nmethods attempt to unify SFT and RL using heuristics, but lack a principled\nmechanism for dynamically balancing the two paradigms. In this paper, we\nreframe this challenge through the theoretical lens of implicit\nrewards, viewing SFT and RL not as distinct methods but as complementary\nreward signals. We introduce Adaptive Meta Fine-Tuning (AMFT), a novel\nsingle-stage algorithm that learns the optimal balance between SFT's implicit,\npath-level reward and RL's explicit, outcome-based reward. The core of AMFT is\na meta-gradient adaptive weight controller that treats the SFT-RL\nbalance as a learnable parameter, dynamically optimizing it to maximize\nlong-term task performance. This forward-looking approach, regularized by\npolicy entropy for stability, autonomously discovers an effective training\ncurriculum. We conduct a comprehensive evaluation on challenging benchmarks\nspanning mathematical reasoning, abstract visual reasoning (General Points),\nand vision-language navigation (V-IRL). AMFT consistently establishes a new\nstate-of-the-art and demonstrats superior generalization on out-of-distribution\n(OOD) tasks. Ablation studies and training dynamic analysis confirm that the\nmeta-learning controller is crucial for AMFT's stability, sample efficiency,\nand performance, offering a more principled and effective paradigm for LLM\nalignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.",
    "keywords_en": [
      "Large Language Models",
      "Meta-Learning",
      "Reinforcement Learning",
      "LLM Alignment",
      "Imitation-Exploration Balance"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Multimodal"
    ],
    "published_time": "2025-08-09T11:40:54.000Z",
    "download_time": "2025-08-16 02:38:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06944.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.06944\", \"arxiv_url\": \"https://arxiv.org/abs/2508.06944\"}"
  },
  {
    "id": "2508.09945",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.09945",
    "title_en": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models",
    "summary_en": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.",
    "keywords_en": [
      "Multimodal Code Generation",
      "Model Merging",
      "Multimodal Large Language Models",
      "Vision-Language Models",
      "Coding Dataset"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-08-13T17:00:44.000Z",
    "download_time": "2025-08-16 02:38:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09945.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.09945\", \"arxiv_url\": \"https://arxiv.org/abs/2508.09945\"}"
  },
  {
    "id": "2508.09889",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.09889",
    "title_en": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving",
    "summary_en": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems.",
    "keywords_en": [
      "Multi-Agent System",
      "Intelligent Agents",
      "Robustness",
      "Dynamic Maneuvering",
      "GAIA"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-13T15:46:25.000Z",
    "download_time": "2025-08-16 02:38:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09889.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.09889\", \"arxiv_url\": \"https://arxiv.org/abs/2508.09889\"}"
  },
  {
    "id": "2508.09987",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.09987",
    "title_en": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved\n  Image Generation",
    "summary_en": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.",
    "keywords_en": [
      "Image Generation",
      "GPT-4o",
      "Synthetic Data",
      "Multimodal",
      "Evaluation Benchmarks"
    ],
    "area_en": [
      "Generative AI",
      "Multimodal",
      "Computer Vision"
    ],
    "published_time": "2025-08-13T17:59:28.000Z",
    "download_time": "2025-08-16 02:38:24",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09987.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.09987\", \"arxiv_url\": \"https://arxiv.org/abs/2508.09987\"}"
  }
]