<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-11-26</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-11-26</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Gemini CLI Tips and Tricks for Agentic Coding</h2>
                <span class="published-time">Published: 2025-11-26 18:08:02</span>
                
                <p class="summary">This resource offers practical tips and tricks for developers looking to integrate Google's Gemini AI model into their coding workflows through a Command Line Interface (CLI). Focusing on 'agentic coding,' the guide provides methods to leverage Gemini's advanced capabilities for automating various software development tasks. It details how to use the Gemini CLI to enhance productivity by generating code snippets, assisting with debugging, refactoring existing code, and even generating documentation. The content aims to empower developers to adopt an AI-assisted approach, transforming traditional coding practices into a more efficient and intelligent process where AI acts as a proactive assistant or 'agent.' By mastering the Gemini CLI, users can streamline their development cycles, reduce manual effort, and tap into the power of large language models for creative problem-solving and rapid prototyping within their terminal environments. The tips cover setup, common commands, and advanced use cases to maximize the utility of AI in everyday programming challenges.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini</span><span>Command Line Interface</span><span>Agentic AI</span><span>Code Generation</span><span>Developer Tools</span><span>AI Programming</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/addyosmani/gemini-cli-tips" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>API that auto-routes to the cheapest AI provider (OpenAI/Anthropic/Gemini)</h2>
                <span class="published-time">Published: 2025-11-26 19:12:26</span>
                
                <p class="summary">This Hacker News story introduces Tokensaver.org, an innovative API designed to automatically route user requests to the most cost-effective large language model (LLM) provider among major players like OpenAI, Anthropic, and Gemini. The service acts as an intelligent intermediary, abstracting away the complexities of managing multiple AI API integrations and their fluctuating pricing structures. Its primary objective is to optimize expenditure for businesses and developers by dynamically identifying and utilizing the cheapest available LLM service for each query. This dynamic routing mechanism ensures users benefit from real-time price arbitrage, significantly reducing operational costs associated with AI inference. By offering a unified endpoint, Tokensaver.org simplifies the development process, allowing applications to seamlessly access diverse AI capabilities while ensuring financial efficiency and maintaining high performance standards across various leading AI platforms. This solution is particularly valuable for developers aiming to build cost-efficient AI-powered applications that leverage the best market prices for their AI workloads.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>API Management</span><span>AI Cost Optimization</span><span>Large Language Models</span><span>Dynamic Routing</span><span>AI Providers</span><span>Cloud AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://tokensaver.org/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAI needs to raise at least $207B by 2030</h2>
                <span class="published-time">Published: 2025-11-26 15:06:37</span>
                
                <p class="summary">OpenAI, a leading artificial intelligence research organization, is projected to require a staggering $207 billion in funding by 2030. This substantial financial requirement highlights the immense capital expenditure necessary for developing and deploying advanced AI systems, including large language models and future artificial general intelligence initiatives. The funding is critical for investing in high-performance computing infrastructure, such as state-of-the-art GPUs, and covering significant operational costs associated with large-scale AI research and development. This aggressive fundraising target underscores the escalating costs of technological innovation in the AI sector and reflects OpenAI's strategic ambitions to maintain its competitive edge and achieve its long-term objectives in shaping the future of AI.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>OpenAI</span><span>AI Funding</span><span>Investment</span><span>Large Language Models</span><span>Artificial General Intelligence</span><span>AI Development</span><span>Compute Infrastructure</span><span>AI Industry</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://ft.com/content/23e54a28-6f63-4533-ab96-3756d9c88bad" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos</h2>
                <span class="published-time">Published: 2025-11-26 07:55:49</span>
                
                <p class="summary">Image diffusion models, primarily developed for generating high-quality static images, have demonstrated a surprising emergent capability: temporal propagation within video sequences. This research investigates how these models, despite being trained solely on individual images, can exhibit a form of temporal coherence and consistency when applied to video generation or manipulation tasks. The study highlights that the internal representations learned by these models implicitly capture motion and temporal dynamics, allowing them to propagate information across frames. This emergent property suggests that diffusion models possess a more profound understanding of visual reality than previously assumed, potentially opening new avenues for video synthesis, interpolation, and other spatiotemporal tasks without explicit temporal training. The findings challenge conventional wisdom regarding the necessity of explicit temporal architectures for video understanding and generation, indicating a significant step towards more versatile and efficient generative models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Image Diffusion Models</span><span>Temporal Propagation</span><span>Video Generation</span><span>Emergent Properties</span><span>Deep Learning</span><span>Computer Vision</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://arxiv.org/abs/2511.19936" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Slop Detective ‚Äì Fight the Slop Syndicate</h2>
                <span class="published-time">Published: 2025-11-26 16:24:29</span>
                
                <p class="summary">The "Slop Detective" initiative, spearheaded by Kagi, is introduced as a crucial tool aimed at combating the increasing prevalence of low-quality and unoriginal content, widely termed "slop," across the digital landscape. This platform is designed to equip users and content curators with advanced mechanisms for identifying and mitigating the impact of mass-produced, potentially AI-generated, or spammy material. Slop Detective's primary objective is to uphold stringent standards of information quality and authenticity, addressing a growing concern regarding content integrity in an oversaturated online environment. The project positions itself as a proactive defense against the "Slop Syndicate," a term used to describe entities or trends contributing to the dilution of genuine content with filler and misinformation. This development underscores Kagi's commitment to fostering a more reliable and curated online experience through dedicated solutions for content quality assessment.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Content Quality</span><span>AI Content Detection</span><span>Information Integrity</span><span>Digital Hygiene</span><span>Kagi</span><span>Web Search Enhancement</span><span>Content Moderation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://slopdetective.kagi.com/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>CS234: Reinforcement Learning Winter 2025</h2>
                <span class="published-time">Published: 2025-11-26 00:33:29</span>
                
                <p class="summary">Stanford University has officially announced the upcoming offering of its highly anticipated course, CS234: Reinforcement Learning, scheduled for Winter 2025. This postgraduate-level course is designed to provide a comprehensive and rigorous introduction to the fundamental theory, algorithms, and practical applications of reinforcement learning (RL). Students will delve into core concepts, including Markov Decision Processes (MDPs), dynamic programming, Monte Carlo methods, and temporal-difference learning. The curriculum also typically covers advanced topics such as policy gradient methods, actor-critic architectures, and an essential introduction to deep reinforcement learning, exploring how neural networks enhance RL capabilities. Through a blend of theoretical lectures and hands-on programming assignments, students will gain proficiency in designing and implementing RL solutions for complex sequential decision-making problems, preparing them for research and industry roles in artificial intelligence and machine learning. This offering underscores Stanford's ongoing leadership in advancing AI education.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reinforcement Learning</span><span>Machine Learning</span><span>Artificial Intelligence</span><span>Markov Decision Processes</span><span>Deep Reinforcement Learning</span><span>Algorithms</span><span>Policy Gradient</span><span>Dynamic Programming</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://web.stanford.edu/class/cs234/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>TrendRadar</h2>
                <span class="published-time">Published: 2025-11-26T13:10:11Z</span>
                
                <p class="summary">TrendRadar is a lightweight, easily deployable hotspot assistant designed to provide personalized news and information. It aggregates trending topics from over 11 mainstream platforms including Zhihu, Douyin, Weibo, and financial news sites. The project offers smart push strategies
‚Äîdaily summaries, currentÊ¶úÂçï updates, and incremental monitoring
‚Äîto prevent information overload, allowing users to focus on truly relevant news. Key features include precise content filtering using customizable keywords with advanced sorting and quantity controls, real-time hotspot trend analysis, and a personalized hot-spot algorithm for re-ranking news. It supports multi-channel real-time notifications via WeChat Work, Feishu, DingTalk, Telegram, Email, ntfy, Bark, and Slack, alongside multi-device adaptation with GitHub Pages and Docker deployment. A significant enhancement is the AI Smart Analysis, leveraging the MCP (Model Context Protocol) for conversational data querying, trend analysis, sentiment analysis, and smart summarization, making it ideal for investors, content creators, and PR professionals to efficiently track market, brand, or industry dynamics.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>News Aggregation</span><span>Hotspot Monitoring</span><span>AI Analysis</span><span>Content Filtering</span><span>Real-time Notifications</span><span>GitHub Actions</span><span>Docker</span><span>MCP Protocol</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/sansan0/TrendRadar" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent Development Kit (ADK) for Go</h2>
                <span class="published-time">Published: 2025-11-25T13:00:27Z</span>
                
                <p class="summary">The Agent Development Kit (ADK) for Go is an open-source, code-first toolkit designed to simplify the building, evaluating, and deploying of sophisticated AI agents. This flexible and modular framework applies software development principles to agent creation, enabling orchestration of workflows from simple tasks to complex multi-agent systems. While optimized for Google's Gemini, ADK remains model-agnostic and deployment-agnostic, ensuring compatibility with various other frameworks. The Go version is particularly well-suited for developers creating cloud-native agent applications, leveraging Go's strengths in concurrency and performance. Key features include idiomatic Go design, a rich tool ecosystem for integrating pre-built or custom functions, code-first development for maximum flexibility and testability, modular multi-agent system design, and strong support for containerization and deployment in cloud environments like Google Cloud Run.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agent Development</span><span>Go Programming</span><span>Cloud-Native</span><span>Modular AI Systems</span><span>Tool Orchestration</span><span>Code-First Development</span><span>Gemini Integration</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/google/adk-go" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms</h2>
                <span class="published-time">Published: 2025-11-17T14:44:47.000Z</span>
                
                <p class="summary">Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLMs</span><span>Evolutionary Algorithms</span><span>Optimization Framework</span><span>GigaEvo</span><span>AlphaEvolve</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.17592" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning</h2>
                <span class="published-time">Published: 2025-11-25T04:15:14.000Z</span>
                
                <p class="summary">Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Vision-Language Agents</span><span>Tool-Integrated Reasoning</span><span>Self-Evolving Systems</span><span>Multimodal Reasoning</span><span>Reinforcement Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Multimodal</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.19900" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow</h2>
                <span class="published-time">Published: 2025-11-25T16:27:58.000Z</span>
                
                <p class="summary">Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Normalizing Flows</span><span>Video Generation</span><span>Generative Models</span><span>Autoregressive Models</span><span>Spatiotemporal Modeling</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.20462" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Soft Adaptive Policy Optimization</h2>
                <span class="published-time">Published: 2025-11-25T14:25:19.000Z</span>
                
                <p class="summary">Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reinforcement Learning</span><span>Large Language Models</span><span>Policy Optimization</span><span>Soft Adaptive Policy Optimization</span><span>Training Stability</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.20347" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MedSAM3: Delving into Segment Anything with Medical Concepts</h2>
                <span class="published-time">Published: 2025-11-24T12:34:38.000Z</span>
                
                <p class="summary">Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Medical Image Segmentation</span><span>Segment Anything Model</span><span>Text Promptable</span><span>Multimodal Large Language Models</span><span>AI Agent</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Deep Learning</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.19046" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>HunyuanOCR Technical Report</h2>
                <span class="published-time">Published: 2025-11-24T17:59:59.000Z</span>
                
                <p class="summary">This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters. HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow "OCR expert models" and inefficient "General VLMs". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks. HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>HunyuanOCR</span><span>OCR</span><span>Vision-Language Model</span><span>End-to-End Architecture</span><span>Reinforcement Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Multimodal</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.19575" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>