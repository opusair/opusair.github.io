[
  {
    "id": "hackernews_46060508",
    "source": "Hacker News",
    "url": "https://github.com/addyosmani/gemini-cli-tips",
    "title": "Gemini CLI Tips and Tricks for Agentic Coding",
    "summary": "This resource offers practical tips and tricks for developers looking to integrate Google's Gemini AI model into their coding workflows through a Command Line Interface (CLI). Focusing on 'agentic coding,' the guide provides methods to leverage Gemini's advanced capabilities for automating various software development tasks. It details how to use the Gemini CLI to enhance productivity by generating code snippets, assisting with debugging, refactoring existing code, and even generating documentation. The content aims to empower developers to adopt an AI-assisted approach, transforming traditional coding practices into a more efficient and intelligent process where AI acts as a proactive assistant or 'agent.' By mastering the Gemini CLI, users can streamline their development cycles, reduce manual effort, and tap into the power of large language models for creative problem-solving and rapid prototyping within their terminal environments. The tips cover setup, common commands, and advanced use cases to maximize the utility of AI in everyday programming challenges.",
    "keywords": [
      "Gemini",
      "Command Line Interface",
      "Agentic AI",
      "Code Generation",
      "Developer Tools",
      "AI Programming"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-11-26 18:08:02",
    "download_time": "2025-11-26 20:02:22",
    "extra_info": "{\"score\": 50, \"by\": \"ayoisaiah\", \"descendants\": 16, \"story_id\": 46060508}"
  },
  {
    "id": "hackernews_46061232",
    "source": "Hacker News",
    "url": "https://tokensaver.org/",
    "title": "API that auto-routes to the cheapest AI provider (OpenAI/Anthropic/Gemini)",
    "summary": "This Hacker News story introduces Tokensaver.org, an innovative API designed to automatically route user requests to the most cost-effective large language model (LLM) provider among major players like OpenAI, Anthropic, and Gemini. The service acts as an intelligent intermediary, abstracting away the complexities of managing multiple AI API integrations and their fluctuating pricing structures. Its primary objective is to optimize expenditure for businesses and developers by dynamically identifying and utilizing the cheapest available LLM service for each query. This dynamic routing mechanism ensures users benefit from real-time price arbitrage, significantly reducing operational costs associated with AI inference. By offering a unified endpoint, Tokensaver.org simplifies the development process, allowing applications to seamlessly access diverse AI capabilities while ensuring financial efficiency and maintaining high performance standards across various leading AI platforms. This solution is particularly valuable for developers aiming to build cost-efficient AI-powered applications that leverage the best market prices for their AI workloads.",
    "keywords": [
      "API Management",
      "AI Cost Optimization",
      "Large Language Models",
      "Dynamic Routing",
      "AI Providers",
      "Cloud AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-11-26 19:12:26",
    "download_time": "2025-11-26 20:02:21",
    "extra_info": "{\"score\": 8, \"by\": \"h2o_wine\", \"descendants\": 4, \"story_id\": 46061232}"
  },
  {
    "id": "hackernews_46058065",
    "source": "Hacker News",
    "url": "https://ft.com/content/23e54a28-6f63-4533-ab96-3756d9c88bad",
    "title": "OpenAI needs to raise at least $207B by 2030",
    "summary": "OpenAI, a leading artificial intelligence research organization, is projected to require a staggering $207 billion in funding by 2030. This substantial financial requirement highlights the immense capital expenditure necessary for developing and deploying advanced AI systems, including large language models and future artificial general intelligence initiatives. The funding is critical for investing in high-performance computing infrastructure, such as state-of-the-art GPUs, and covering significant operational costs associated with large-scale AI research and development. This aggressive fundraising target underscores the escalating costs of technological innovation in the AI sector and reflects OpenAI's strategic ambitions to maintain its competitive edge and achieve its long-term objectives in shaping the future of AI.",
    "keywords": [
      "OpenAI",
      "AI Funding",
      "Investment",
      "Large Language Models",
      "Artificial General Intelligence",
      "AI Development",
      "Compute Infrastructure",
      "AI Industry"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-11-26 15:06:37",
    "download_time": "2025-11-26 20:02:16",
    "extra_info": "{\"score\": 451, \"by\": \"akira_067\", \"descendants\": 395, \"story_id\": 46058065}"
  },
  {
    "id": "hackernews_46055177",
    "source": "Hacker News",
    "url": "https://arxiv.org/abs/2511.19936",
    "title": "Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos",
    "summary": "Image diffusion models, primarily developed for generating high-quality static images, have demonstrated a surprising emergent capability: temporal propagation within video sequences. This research investigates how these models, despite being trained solely on individual images, can exhibit a form of temporal coherence and consistency when applied to video generation or manipulation tasks. The study highlights that the internal representations learned by these models implicitly capture motion and temporal dynamics, allowing them to propagate information across frames. This emergent property suggests that diffusion models possess a more profound understanding of visual reality than previously assumed, potentially opening new avenues for video synthesis, interpolation, and other spatiotemporal tasks without explicit temporal training. The findings challenge conventional wisdom regarding the necessity of explicit temporal architectures for video understanding and generation, indicating a significant step towards more versatile and efficient generative models.",
    "keywords": [
      "Image Diffusion Models",
      "Temporal Propagation",
      "Video Generation",
      "Emergent Properties",
      "Deep Learning",
      "Computer Vision",
      "Generative AI"
    ],
    "area": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-11-26 07:55:49",
    "download_time": "2025-11-26 20:02:16",
    "extra_info": "{\"score\": 91, \"by\": \"50kIters\", \"descendants\": 12, \"story_id\": 46055177}"
  },
  {
    "id": "hackernews_46059069",
    "source": "Hacker News",
    "url": "https://slopdetective.kagi.com/",
    "title": "Slop Detective – Fight the Slop Syndicate",
    "summary": "The \"Slop Detective\" initiative, spearheaded by Kagi, is introduced as a crucial tool aimed at combating the increasing prevalence of low-quality and unoriginal content, widely termed \"slop,\" across the digital landscape. This platform is designed to equip users and content curators with advanced mechanisms for identifying and mitigating the impact of mass-produced, potentially AI-generated, or spammy material. Slop Detective's primary objective is to uphold stringent standards of information quality and authenticity, addressing a growing concern regarding content integrity in an oversaturated online environment. The project positions itself as a proactive defense against the \"Slop Syndicate,\" a term used to describe entities or trends contributing to the dilution of genuine content with filler and misinformation. This development underscores Kagi's commitment to fostering a more reliable and curated online experience through dedicated solutions for content quality assessment.",
    "keywords": [
      "Content Quality",
      "AI Content Detection",
      "Information Integrity",
      "Digital Hygiene",
      "Kagi",
      "Web Search Enhancement",
      "Content Moderation"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-11-26 16:24:29",
    "download_time": "2025-11-26 20:02:27",
    "extra_info": "{\"score\": 39, \"by\": \"speckx\", \"descendants\": 18, \"story_id\": 46059069}"
  },
  {
    "id": "hackernews_46052685",
    "source": "Hacker News",
    "url": "https://web.stanford.edu/class/cs234/",
    "title": "CS234: Reinforcement Learning Winter 2025",
    "summary": "Stanford University has officially announced the upcoming offering of its highly anticipated course, CS234: Reinforcement Learning, scheduled for Winter 2025. This postgraduate-level course is designed to provide a comprehensive and rigorous introduction to the fundamental theory, algorithms, and practical applications of reinforcement learning (RL). Students will delve into core concepts, including Markov Decision Processes (MDPs), dynamic programming, Monte Carlo methods, and temporal-difference learning. The curriculum also typically covers advanced topics such as policy gradient methods, actor-critic architectures, and an essential introduction to deep reinforcement learning, exploring how neural networks enhance RL capabilities. Through a blend of theoretical lectures and hands-on programming assignments, students will gain proficiency in designing and implementing RL solutions for complex sequential decision-making problems, preparing them for research and industry roles in artificial intelligence and machine learning. This offering underscores Stanford's ongoing leadership in advancing AI education.",
    "keywords": [
      "Reinforcement Learning",
      "Machine Learning",
      "Artificial Intelligence",
      "Markov Decision Processes",
      "Deep Reinforcement Learning",
      "Algorithms",
      "Policy Gradient",
      "Dynamic Programming"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-11-26 00:33:29",
    "download_time": "2025-11-26 20:02:24",
    "extra_info": "{\"score\": 177, \"by\": \"jonbaer\", \"descendants\": 45, \"story_id\": 46052685}"
  },
  {
    "id": "TrendRadar",
    "source": "GitHub",
    "url": "https://github.com/sansan0/TrendRadar",
    "title": "TrendRadar",
    "summary": "TrendRadar is a lightweight, easily deployable hotspot assistant designed to provide personalized news and information. It aggregates trending topics from over 11 mainstream platforms including Zhihu, Douyin, Weibo, and financial news sites. The project offers smart push strategies\n—daily summaries, current榜单 updates, and incremental monitoring\n—to prevent information overload, allowing users to focus on truly relevant news. Key features include precise content filtering using customizable keywords with advanced sorting and quantity controls, real-time hotspot trend analysis, and a personalized hot-spot algorithm for re-ranking news. It supports multi-channel real-time notifications via WeChat Work, Feishu, DingTalk, Telegram, Email, ntfy, Bark, and Slack, alongside multi-device adaptation with GitHub Pages and Docker deployment. A significant enhancement is the AI Smart Analysis, leveraging the MCP (Model Context Protocol) for conversational data querying, trend analysis, sentiment analysis, and smart summarization, making it ideal for investors, content creators, and PR professionals to efficiently track market, brand, or industry dynamics.",
    "keywords": [
      "News Aggregation",
      "Hotspot Monitoring",
      "AI Analysis",
      "Content Filtering",
      "Real-time Notifications",
      "GitHub Actions",
      "Docker",
      "MCP Protocol"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-11-26T13:10:11Z",
    "download_time": "2024-05-15 12:00:00",
    "extra_info": null
  },
  {
    "id": "adk-go",
    "source": "GitHub",
    "url": "https://github.com/google/adk-go",
    "title": "Agent Development Kit (ADK) for Go",
    "summary": "The Agent Development Kit (ADK) for Go is an open-source, code-first toolkit designed to simplify the building, evaluating, and deploying of sophisticated AI agents. This flexible and modular framework applies software development principles to agent creation, enabling orchestration of workflows from simple tasks to complex multi-agent systems. While optimized for Google's Gemini, ADK remains model-agnostic and deployment-agnostic, ensuring compatibility with various other frameworks. The Go version is particularly well-suited for developers creating cloud-native agent applications, leveraging Go's strengths in concurrency and performance. Key features include idiomatic Go design, a rich tool ecosystem for integrating pre-built or custom functions, code-first development for maximum flexibility and testability, modular multi-agent system design, and strong support for containerization and deployment in cloud environments like Google Cloud Run.",
    "keywords": [
      "AI Agent Development",
      "Go Programming",
      "Cloud-Native",
      "Modular AI Systems",
      "Tool Orchestration",
      "Code-First Development",
      "Gemini Integration"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-11-25T13:00:27Z",
    "download_time": "2024-05-15 10:30:00",
    "extra_info": null
  },
  {
    "id": "2511.17592",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2511.17592",
    "title": "GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms",
    "summary": "Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.",
    "keywords": [
      "LLMs",
      "Evolutionary Algorithms",
      "Optimization Framework",
      "GigaEvo",
      "AlphaEvolve"
    ],
    "area": [
      "Large Language Model",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "published_time": "2025-11-17T14:44:47.000Z",
    "download_time": "2025-11-26 12:03:01",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2511.17592\", \"arxiv_url\": \"https://arxiv.org/abs/2511.17592\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17592.png\", \"original_title\": \"GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms\"}"
  },
  {
    "id": "2511.19900",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2511.19900",
    "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning",
    "summary": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.",
    "keywords": [
      "Vision-Language Agents",
      "Tool-Integrated Reasoning",
      "Self-Evolving Systems",
      "Multimodal Reasoning",
      "Reinforcement Learning"
    ],
    "area": [
      "AI Agent",
      "Multimodal",
      "Machine Learning"
    ],
    "published_time": "2025-11-25T04:15:14.000Z",
    "download_time": "2025-11-26 12:03:03",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2511.19900\", \"arxiv_url\": \"https://arxiv.org/abs/2511.19900\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19900.png\", \"original_title\": \"Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning\"}"
  },
  {
    "id": "2511.20462",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2511.20462",
    "title": "STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow",
    "summary": "Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.",
    "keywords": [
      "Normalizing Flows",
      "Video Generation",
      "Generative Models",
      "Autoregressive Models",
      "Spatiotemporal Modeling"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-11-25T16:27:58.000Z",
    "download_time": "2025-11-26 12:03:02",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2511.20462\", \"arxiv_url\": \"https://arxiv.org/abs/2511.20462\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20462.png\", \"original_title\": \"STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow\"}"
  },
  {
    "id": "2511.20347",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2511.20347",
    "title": "Soft Adaptive Policy Optimization",
    "summary": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.",
    "keywords": [
      "Reinforcement Learning",
      "Large Language Models",
      "Policy Optimization",
      "Soft Adaptive Policy Optimization",
      "Training Stability"
    ],
    "area": [
      "Large Language Model",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-11-25T14:25:19.000Z",
    "download_time": "2025-11-26 12:03:02",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2511.20347\", \"arxiv_url\": \"https://arxiv.org/abs/2511.20347\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20347.png\", \"original_title\": \"Soft Adaptive Policy Optimization\"}"
  },
  {
    "id": "2511.19046",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2511.19046",
    "title": "MedSAM3: Delving into Segment Anything with Medical Concepts",
    "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.",
    "keywords": [
      "Medical Image Segmentation",
      "Segment Anything Model",
      "Text Promptable",
      "Multimodal Large Language Models",
      "AI Agent"
    ],
    "area": [
      "Computer Vision",
      "Deep Learning",
      "Multimodal"
    ],
    "published_time": "2025-11-24T12:34:38.000Z",
    "download_time": "2025-11-26 12:03:02",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2511.19046\", \"arxiv_url\": \"https://arxiv.org/abs/2511.19046\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19046.png\", \"original_title\": \"MedSAM3: Delving into Segment Anything with Medical Concepts\"}"
  },
  {
    "id": "2511.19575",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2511.19575",
    "title": "HunyuanOCR Technical Report",
    "summary": "This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters. HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow \"OCR expert models\" and inefficient \"General VLMs\". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks. HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.",
    "keywords": [
      "HunyuanOCR",
      "OCR",
      "Vision-Language Model",
      "End-to-End Architecture",
      "Reinforcement Learning"
    ],
    "area": [
      "Computer Vision",
      "Multimodal",
      "Deep Learning"
    ],
    "published_time": "2025-11-24T17:59:59.000Z",
    "download_time": "2025-11-26 12:03:02",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2511.19575\", \"arxiv_url\": \"https://arxiv.org/abs/2511.19575\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19575.png\", \"original_title\": \"HunyuanOCR Technical Report\"}"
  }
]