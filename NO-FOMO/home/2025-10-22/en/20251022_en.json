[
  {
    "id": "hackernews_45668990",
    "source": "Hacker News",
    "url": "https://www.bbc.co.uk/mediacentre/2025/new-ebu-research-ai-assistants-news-content",
    "title": "AI assistants misrepresent news content 45% of the time",
    "summary": "New research from the European Broadcasting Union (EBU) reveals a significant challenge in the accuracy of AI assistants when handling news content. The study indicates that these AI systems misrepresent news information in 45% of instances, highlighting critical concerns regarding the reliability of AI-generated summaries and responses in journalistic contexts. This finding underscores the necessity for more robust verification mechanisms and enhanced fact-checking capabilities within AI models designed for information dissemination. The EBU's report suggests that despite rapid advancements in artificial intelligence, current AI assistants struggle with consistently delivering accurate representations of complex journalistic narratives, posing potential risks for misinformation and eroding public trust in both AI technology and news sources. Developers and media organizations are urged to prioritize addressing these inaccuracies to ensure responsible AI integration.",
    "keywords": [
      "AI",
      "Misinformation",
      "Natural Language Processing",
      "Content Accuracy",
      "Generative AI",
      "AI Ethics",
      "News Media"
    ],
    "area": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Large Language Model"
    ],
    "published_time": "2025-10-22 13:39:26",
    "download_time": "2025-10-22 20:01:34",
    "extra_info": "{\"score\": 347, \"by\": \"sohkamyung\", \"descendants\": 244, \"story_id\": 45668990}"
  },
  {
    "id": "hackernews_45668264",
    "source": "Hacker News",
    "url": "https://quesma.com/blog/local-llms-security-paradox/",
    "title": "The security paradox of local LLMs",
    "summary": "The adoption of local Large Language Models (LLMs) presents a significant security paradox, where the perceived privacy benefits of on-device processing are offset by new and complex security challenges. While local LLMs can reduce reliance on third-party cloud services, enhancing data privacy by keeping sensitive information within an organization's control, this paradigm shift necessitates robust internal security infrastructures. Deploying LLMs locally introduces risks such as securing model weights, managing access controls, ensuring data integrity during inference, and safeguarding against supply chain vulnerabilities. Organizations must invest heavily in securing the local environment, including regular updates and sophisticated threat detection, to prevent potential data breaches or model tampering. This inherent trade-off emphasizes that increased data control often translates into a greater, more intricate security responsibility for the end-user or enterprise.",
    "keywords": [
      "Local LLMs",
      "AI Security",
      "Data Privacy",
      "On-device AI",
      "Large Language Models",
      "Information Security",
      "Edge AI"
    ],
    "area": [
      "Large Language Model",
      "Artificial Intelligence",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-22 12:48:01",
    "download_time": "2025-10-22 20:01:54",
    "extra_info": "{\"score\": 103, \"by\": \"jakozaur\", \"descendants\": 70, \"story_id\": 45668264}"
  },
  {
    "id": "hackernews_45674032",
    "source": "Hacker News",
    "url": "https://www.aha.io/engineering/articles/streaming-ai-responses-incomplete-json",
    "title": "Show HN: Incremental JSON parser for streaming LLM tool calls in Ruby",
    "summary": "A new incremental JSON parser has been introduced, specifically designed for efficiently handling streaming Large Language Model (LLM) tool calls in Ruby. Traditional JSON parsers often exhibit O(n\b\b) performance by reprocessing the entire JSON string with each new character, leading to noticeable UI lag in real-time AI applications. This innovative parser addresses this limitation by maintaining parsing state and processing only newly received characters, thereby achieving true O(n) performance. This optimization ensures a seamless and imperceptible user experience throughout the LLM's streaming response, preventing delays that would otherwise occur when function arguments are streamed as character-by-character JSON. Released as an MIT-licensed Ruby gem, this tool aims to enhance the responsiveness and efficiency of AI agent interactions by providing a performant solution for parsing incomplete and progressively updated JSON data streams. The developer is actively soliciting feedback from the community.",
    "keywords": [
      "Incremental JSON Parser",
      "Streaming LLM",
      "Ruby",
      "AI Tool Calls",
      "JSON Parsing",
      "Performance Optimization"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-10-22 19:32:01",
    "download_time": "2025-10-22 20:02:15",
    "extra_info": "{\"score\": 8, \"by\": \"hotk\", \"descendants\": 0, \"story_id\": 45674032}"
  },
  {
    "id": "hackernews_45668118",
    "source": "Hacker News",
    "url": "https://simonwillison.net/2025/Oct/22/living-dangerously-with-claude/",
    "title": "Living Dangerously with Claude",
    "summary": "This Hacker News story, \"Living Dangerously with Claude,\" is anticipated to provide an experiential account of a user's in-depth interactions with Anthropic's Claude large language model. The provocative title suggests an exploration beyond typical use cases, likely involving a series of experiments designed to probe the AI's boundaries, uncover its hidden behaviors, or test its resilience under unusual conditions. It is probable that the author recounts specific scenarios where they engaged with Claude in unconventional ways, potentially revealing unexpected responses, critical limitations, or even discovering novel, perhaps risky, applications. The article is expected to address the inherent challenges and ethical considerations when pushing an advanced AI system to its limits, offering valuable insights into the practical implications and fascinating possibilities that arise from 'living dangerously' with sophisticated artificial intelligence. This narrative aims to inform readers about the nuances of interacting with powerful LLMs, moving beyond basic functionalities to a more profound understanding of their operational characteristics and robustness in unconventional environments, thereby contributing to a deeper technical appreciation of advanced AI model behavior.",
    "keywords": [
      "Claude AI",
      "Large Language Models",
      "AI Interaction",
      "Prompt Engineering",
      "AI Ethics",
      "AI Safety",
      "Experimental AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-22 12:36:19",
    "download_time": "2025-10-22 20:02:18",
    "extra_info": "{\"score\": 16, \"by\": \"FromTheArchives\", \"descendants\": 1, \"story_id\": 45668118}"
  },
  {
    "id": "hackernews_45667458",
    "source": "Hacker News",
    "url": "https://blogs.nvidia.com/blog/starcloud/",
    "title": "Starcloud",
    "summary": "NVIDIA has announced Starcloud, an innovative platform poised to redefine the landscape of cloud-based artificial intelligence infrastructure and services. While comprehensive technical specifications are eagerly awaited, Starcloud is anticipated to harness NVIDIA's cutting-edge GPU technology and integrated software ecosystems to deliver unparalleled performance for demanding AI workloads. The initiative is expected to facilitate the development, training, and deployment of sophisticated machine learning and deep learning models at scale, offering enterprises and researchers a robust and highly scalable environment. This new offering aims to simplify the complexities associated with high-performance computing for AI, providing optimized solutions for data processing, model orchestration, and collaborative development. Analysts suggest that Starcloud represents a strategic move by NVIDIA to strengthen its position in the rapidly expanding cloud AI market, promising to accelerate AI innovation and democratize access to advanced computational resources for a broad range of industries.",
    "keywords": [
      "Cloud Computing",
      "Artificial Intelligence",
      "GPU Acceleration",
      "Machine Learning",
      "Deep Learning",
      "NVIDIA",
      "AI Infrastructure"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-10-22 11:23:33",
    "download_time": "2025-10-22 20:02:17",
    "extra_info": "{\"score\": 151, \"by\": \"jonbaer\", \"descendants\": 206, \"story_id\": 45667458}"
  },
  {
    "id": "hackernews_45671778",
    "source": "Hacker News",
    "url": "https://www.theverge.com/news/804253/meta-ai-research-layoffs-fair-superintelligence",
    "title": "Meta is axing 600 roles across its AI division",
    "summary": "Meta is implementing a significant workforce reduction, eliminating approximately 600 roles across its Artificial Intelligence division. This restructuring effort targets various teams involved in advanced AI research and development, including projects that might be linked to initiatives like 'Fair Superintelligence,' as indicated by the article's URL. The decision comes amidst a period of broader industry-wide tech layoffs and Meta's ongoing efforts to optimize its operational efficiency and strategic focus. While Meta remains committed to its long-term AI vision and continues to make substantial investments in critical AI technologies, these job cuts suggest a strategic recalibration in specific research areas or a consolidation of resources to prioritize key projects. This move aims to streamline development processes and ensure that the company's AI endeavors are aligned with its core business objectives and future growth strategies.",
    "keywords": [
      "Meta",
      "AI layoffs",
      "Artificial Intelligence",
      "Workforce reduction",
      "Tech industry",
      "AI Research",
      "Organizational restructuring"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Generative AI"
    ],
    "published_time": "2025-10-22 16:44:43",
    "download_time": "2025-10-22 20:01:27",
    "extra_info": "{\"score\": 290, \"by\": \"Lionga\", \"descendants\": 202, \"story_id\": 45671778}"
  },
  {
    "id": "parlant",
    "source": "GitHub",
    "url": "https://github.com/emcie-co/parlant",
    "title": "Finally, LLM agents that actually follow instructions",
    "summary": "Parlant is an innovative AI agent framework designed to ensure Large Language Model (LLM) agents consistently follow instructions, effectively addressing the unpredictability in production AI applications. Unlike traditional methods relying on complex system prompts, Parlant introduces a principle-based approach, enabling developers to define explicit behavioral guidelines, conversational journeys, and tool integrations using natural language. Key features include Journeys for structured customer interactions, Behavioral Guidelines for contextual rule application, Tool Use for external service integration, Domain Adaptation for specialized terminology, and Canned Responses for consistent style. The framework also offers Explainability, providing insights into agent decisions, and enterprise-grade features like built-in guardrails and conversation analytics. Parlant is suitable for critical applications in financial services, healthcare, e-commerce, and legal tech, aiming to deliver predictable and reliable AI agent behavior from day one.",
    "keywords": [
      "AI Agent",
      "LLM Framework",
      "Conversational AI",
      "Behavioral Guidelines",
      "Tool Use",
      "Agent Compliance",
      "Production AI",
      "Natural Language Processing"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-10-21T11:26:26Z",
    "download_time": "2024-05-15 12:00:00",
    "extra_info": null
  },
  {
    "id": "open-notebook",
    "source": "GitHub",
    "url": "https://github.com/lfnovo/open-notebook",
    "title": "Open Notebook",
    "summary": "Open Notebook is an open-source, privacy-focused, and 100% local alternative to Google's Notebook LM, designed for advanced knowledge acquisition and research management. It empowers users with complete control over their data and choice of AI models, supporting over 16 providers including OpenAI, Anthropic, and Ollama. The platform facilitates the organization of multi-modal content like PDFs, videos, and web pages, offering robust features such as intelligent full-text and vector search, context-aware AI conversations, and professional multi-speaker podcast generation. Technically, Open Notebook is built with Python, Next.js, React, and SurrealDB, supporting flexible Docker deployments. Its comprehensive REST API enables deep customization and integration, making it a powerful tool for researchers and developers seeking a private, flexible, and extensible AI-powered research environment.",
    "keywords": [
      "Privacy-preserving AI",
      "Knowledge Management",
      "Multi-model AI",
      "Local AI",
      "Podcast Generation",
      "Vector Search",
      "Docker Deployment",
      "REST API"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Multimodal"
    ],
    "published_time": "2025-10-21T19:54:59Z",
    "download_time": "2024-07-29 12:00:00",
    "extra_info": null
  },
  {
    "id": "2510.18855",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.18855",
    "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model",
    "summary": "We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance.",
    "keywords": [
      "Reinforcement Learning",
      "Trillion-Scale Parameters",
      "Thinking Model",
      "Mixture-of-Experts",
      "Reasoning Capabilities"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-10-21T17:46:14.000Z",
    "download_time": "2025-10-22 13:02:37",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.18855\", \"arxiv_url\": \"https://arxiv.org/abs/2510.18855\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18855.png\", \"original_title\": \"Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\\n  Thinking Model\"}"
  },
  {
    "id": "2510.18081",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.18081",
    "title": "Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth",
    "summary": "Large Language Models (LLMs) exhibit strong but shallow alignment: they directly refuse harmful queries when a refusal is expected at the very start of an assistant turn, yet this protection collapses once a harmful continuation is underway (either through the adversarial attacks or via harmful assistant-prefill attacks). This raises a fundamental question: Can the innate shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an effective inference-time defense with negligible overhead. ADA is built based on our observation that alignment is concentrated in the assistant header tokens through repeated use in shallow-refusal training, and these tokens possess the model's strong alignment priors. By reintroducing these tokens mid-stream, ADA induces the model to reassess harmfulness and recover refusals at any point in generation. Across diverse open-source model families (Llama, Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety performance without requiring any changes to the base model's parameters. It secures a near-100% refusal rate against challenging adversarial prefill attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces the average success rate of prominent adversarial prompt attacks (such as GCG, AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving utility on benign tasks with minimal over-refusal. ADA maintains this resilience even after the base model undergoes subsequent instruction tuning (benign or adversarial).",
    "keywords": [
      "Any-Depth Alignment",
      "Large Language Models",
      "Safety Alignment",
      "Adversarial Attacks",
      "Inference-time Defense"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-10-20T20:18:59.000Z",
    "download_time": "2025-10-22 13:02:38",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.18081\", \"arxiv_url\": \"https://arxiv.org/abs/2510.18081\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18081.png\", \"original_title\": \"Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to\\n  Any-Depth\"}"
  },
  {
    "id": "2510.18775",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.18775",
    "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
    "summary": "Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.",
    "keywords": [
      "Video Generation",
      "High-Resolution Video",
      "Hierarchical Attention",
      "Diffusion Transformers",
      "Computational Efficiency"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-10-21T16:23:21.000Z",
    "download_time": "2025-10-22 13:02:34",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.18775\", \"arxiv_url\": \"https://arxiv.org/abs/2510.18775\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18775.png\", \"original_title\": \"UltraGen: High-Resolution Video Generation with Hierarchical Attention\"}"
  },
  {
    "id": "2510.18135",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.18135",
    "title": "World-in-World: World Models in a Closed-Loop World",
    "summary": "Generative world models (WMs) can now simulate worlds with striking visual realism, which naturally raises the question of whether they can endow embodied agents with predictive perception for decision making. Progress on this question has been limited by fragmented evaluation: most existing benchmarks adopt open-loop protocols that emphasize visual quality in isolation, leaving the core issue of embodied utility unresolved, i.e., do WMs actually help agents succeed at embodied tasks? To address this gap, we introduce World-in-World, the first open platform that benchmarks WMs in a closed-loop world that mirrors real agent-environment interactions. World-in-World provides a unified online planning strategy and a standardized action API, enabling heterogeneous WMs for decision making. We curate four closed-loop environments that rigorously evaluate diverse WMs, prioritize task success as the primary metric, and move beyond the common focus on visual quality; we also present the first data scaling law for world models in embodied settings. Our study uncovers three surprises: (1) visual quality alone does not guarantee task success, controllability matters more; (2) scaling post-training with action-observation data is more effective than upgrading the pretrained video generators; and (3) allocating more inference-time compute allows WMs to substantially improve closed-loop performance.",
    "keywords": [
      "World Models",
      "Embodied Agents",
      "Closed-Loop Systems",
      "Predictive Perception",
      "Decision Making"
    ],
    "area": [
      "Generative AI",
      "AI Agent",
      "Deep Learning"
    ],
    "published_time": "2025-10-20T22:09:15.000Z",
    "download_time": "2025-10-22 13:02:38",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.18135\", \"arxiv_url\": \"https://arxiv.org/abs/2510.18135\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18135.png\", \"original_title\": \"World-in-World: World Models in a Closed-Loop World\"}"
  },
  {
    "id": "2510.18866",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.18866",
    "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
    "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggleto effectively leverage historical interaction information in dynamic andcomplex environments. Memory systems enable LLMs to move beyond statelessinteractions by introducing persistent information storage, retrieval, andutilization mechanisms. However, existing memory systems often introducesubstantial time and computational overhead. To this end, we introduce a newmemory system called LightMem, which strikes a balance between the performanceand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model ofhuman memory, LightMem organizes memory into three complementary stages. First,cognition-inspired sensory memory rapidly filters irrelevant informationthrough lightweight compression and groups information according to theirtopics. Next, topic-aware short-term memory consolidates these topic-basedgroups, organizing and summarizing content for more structured access. Finally,long-term memory with sleep-time update employs an offline procedure thatdecouples consolidation from online inference. Experiments on LongMemEval withGPT and Qwen backbones show that LightMem outperforms strong baselines inaccuracy (up to 10.9% gains) while reducing token usage by up to 117x, APIcalls by up to 159x, and runtime by over 12x. The code is available athttps://github.com/zjunlp/LightMem.",
    "keywords": [
      "Large Language Models",
      "Memory-Augmented Generation",
      "Memory systems",
      "LightMem",
      "Efficiency"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "AI Agent"
    ],
    "published_time": "2025-10-21T17:58:17.000Z",
    "download_time": "2025-10-22 13:02:34",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.18866\", \"arxiv_url\": \"https://arxiv.org/abs/2510.18866\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18866.png\", \"original_title\": \"LightMem: Lightweight and Efficient Memory-Augmented Generation\"}"
  },
  {
    "id": "2510.15862",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.15862",
    "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold",
    "summary": "Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at https://github.com/Pokee-AI/PokeeResearchOSS.",
    "keywords": [
      "Deep Research Agents",
      "Reinforcement Learning",
      "AI Feedback",
      "Large Language Models",
      "Reasoning Scaffold"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-10-17T17:53:06.000Z",
    "download_time": "2025-10-22 13:02:41",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.15862\", \"arxiv_url\": \"https://arxiv.org/abs/2510.15862\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15862.png\", \"original_title\": \"PokeeResearch: Effective Deep Research via Reinforcement Learning from\\n  AI Feedback and Robust Reasoning Scaffold\"}"
  }
]