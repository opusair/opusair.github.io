[
  {
    "id": "hackernews_46675271",
    "source": "Hacker News",
    "url": "https://teendifferent.substack.com/p/apply_chat_template-is-the-safety",
    "title": "Bypassing Gemma and Qwen safety with raw strings",
    "summary": "This report details a consistent vulnerability found in several small-scale open-weight large language models, including Qwen2.5-1.5B, Qwen3-1.7B, Gemma-3-1b-it, and SmolLM2-1.7B. The researcher discovered that the safety alignment mechanisms of these models are heavily reliant on the presence of specific chat templates and instruction tokens (e.g., &lt;|im_start|&gt;). By stripping these template elements and providing raw string inputs, the models' refusal rates for generating potentially harmful content significantly decreased. For instance, Gemma-3's refusal rate dropped from 100% to 60%, and Qwen3's from 80% to 40%, while SmolLM2 exhibited 0% refusal. Qualitatively, models that previously rejected requests for explosive tutorials or explicit fiction readily complied without the template. The findings suggest that current AI safety implementations may be over-relying on client-side string formatting as a primary safety barrier, highlighting a critical area for improvement in model robustness and alignment.",
    "keywords": [
      "Large Language Models",
      "AI Safety",
      "Vulnerability",
      "Red Teaming",
      "Chat Templates",
      "Model Alignment",
      "Gemma",
      "Qwen"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2026-01-19 05:11:24",
    "download_time": "2026-01-19 20:00:44",
    "extra_info": "{\"score\": 58, \"by\": \"teendifferent\", \"descendants\": 7, \"story_id\": 46675271}"
  },
  {
    "id": "hackernews_46679872",
    "source": "Hacker News",
    "url": "https://huggingface.co/zai-org/GLM-4.7-Flash",
    "title": "GLM-4.7-Flash",
    "summary": "GLM-4.7-Flash represents a significant advancement in the domain of large language models, specifically a new iteration within the General Language Model (GLM) series. Hosted on Hugging Face by zai-org, this model is positioned as an optimized or \"Flash\" version, indicating a focus on enhanced speed, efficiency, or reduced resource consumption compared to its predecessors. Such optimization is crucial for broader accessibility and deployment in real-world applications where computational resources and latency are critical considerations. The release of GLM-4.7-Flash on a prominent platform like Hugging Face suggests its availability for the broader AI research community and developers, enabling them to leverage its capabilities for various natural language processing tasks. It is expected to support applications ranging from text generation and summarization to complex conversational AI systems, potentially offering a more performant alternative for high-throughput scenarios. This development contributes to the ongoing efforts in making advanced AI models more practical and scalable for diverse use cases.",
    "keywords": [
      "GLM-4.7-Flash",
      "Large Language Model",
      "Generative AI",
      "Model Optimization",
      "Hugging Face",
      "Natural Language Processing"
    ],
    "area": [
      "Large Language Model",
      "Generative AI",
      "Natural Language Processing"
    ],
    "published_time": "2026-01-19 15:12:12",
    "download_time": "2026-01-19 20:00:31",
    "extra_info": "{\"score\": 265, \"by\": \"scrlk\", \"descendants\": 78, \"story_id\": 46679872}"
  },
  {
    "id": "hackernews_46677918",
    "source": "Hacker News",
    "url": "https://facebookresearch.github.io/ShapeR/",
    "title": "Robust Conditional 3D Shape Generation from Casual Captures",
    "summary": "Researchers have introduced a novel method for robust conditional 3D shape generation, enabling the creation of high-quality three-dimensional models even from casual, unconstrained captures. This advancement addresses a notable challenge in computer vision and graphics, where conventional methods often falter with imperfect or noisy input data. The proposed system employs sophisticated deep learning techniques to infer and construct intricate 3D geometries, conditioned on various input modalities sourced from everyday scenarios. By emphasizing robustness, the methodology aims to broaden the accessibility of 3D content creation, allowing users to transform informal photographs or sensor data into structured 3D representations. This research holds significant implications for applications spanning augmented reality, virtual reality content generation, rapid prototyping, and digital asset creation, pushing the boundaries of what generative AI can achieve in the 3D domain.",
    "keywords": [
      "3D Shape Generation",
      "Conditional Generative Models",
      "Computer Vision",
      "Deep Learning",
      "Robustness",
      "Generative AI",
      "Neural Networks"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2026-01-19 11:48:59",
    "download_time": "2026-01-19 20:00:45",
    "extra_info": "{\"score\": 40, \"by\": \"lastdong\", \"descendants\": 4, \"story_id\": 46677918}"
  },
  {
    "id": "hackernews_46674416",
    "source": "Hacker News",
    "url": "https://rijnard.com/blog/the-code-only-agent",
    "title": "The Code-Only Agent",
    "summary": "The concept of \"The Code-Only Agent\" introduces an AI agent paradigm where the primary mode of operation, reasoning, and planning is conducted through code generation and execution, rather than relying predominantly on natural language processing for internal thought processes. This approach posits that by leveraging code as the core representational and operational medium, AI agents can achieve greater precision, determinism, and reliability in executing complex tasks. Unlike traditional Large Language Model (LLM) agents that often \"think\" in natural language before translating to actions, code-only agents directly generate and interact with programming constructs, potentially reducing issues like hallucination and ambiguity inherent in natural language. This method is particularly relevant for tasks requiring rigorous logical steps, interacting with APIs, or manipulating data structures in software environments. While promising increased robustness and direct integration with computational tools, developing effective code-only agents presents challenges in generating sufficiently complex and correct code for diverse scenarios and ensuring robust error handling and debugging capabilities within the agent architecture. This research direction aims to enhance the capabilities and trustworthiness of autonomous AI systems.",
    "keywords": [
      "Code-Only Agent",
      "AI Agent",
      "Code Generation",
      "Large Language Model",
      "Autonomous Systems",
      "Agentic AI"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2026-01-19 02:27:07",
    "download_time": "2026-01-19 20:00:53",
    "extra_info": "{\"score\": 146, \"by\": \"emersonmacro\", \"descendants\": 63, \"story_id\": 46674416}"
  },
  {
    "id": "hackernews_46677628",
    "source": "Hacker News",
    "url": "https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/",
    "title": "Nvidia contacted Anna's Archive to access books",
    "summary": "Nvidia, a leading technology firm renowned for its graphics processing units and significant investments in artificial intelligence, reportedly made contact with Anna's Archive, a prominent and controversial digital library recognized for hosting millions of pirated books. The purpose of this outreach was to secure access to the archive's extensive collection. This development underscores the escalating and critical demand within the AI industry for vast and diverse datasets, particularly for the rigorous training of sophisticated large language models (LLMs). Nvidia's apparent interest suggests a proactive strategy to acquire substantial textual resources, which are indispensable for advancing and refining various AI capabilities, including natural language processing, comprehension, and generative AI applications. Although the exact details of the discussions and their ultimate resolution have not been publicly disclosed, this incident accentuates the substantial data acquisition challenges faced by top AI developers and illuminates the intricate, often legally ambiguous, terrain of content sourcing for AI training. It further highlights ongoing debates regarding intellectual property and ethical data procurement in the rapidly expanding AI landscape.",
    "keywords": [
      "Artificial Intelligence",
      "Large Language Models",
      "AI Training Data",
      "Data Acquisition",
      "Textual Data",
      "Intellectual Property",
      "Digital Content",
      "AI Development"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2026-01-19 11:11:10",
    "download_time": "2026-01-19 20:00:58",
    "extra_info": "{\"score\": 149, \"by\": \"antonmks\", \"descendants\": 99, \"story_id\": 46677628}"
  },
  {
    "id": "hackernews_46681587",
    "source": "Hacker News",
    "url": "https://theconversation.com/raccoons-break-into-liquor-stores-scale-skyscrapers-and-pick-locks-studying-their-clever-brains-can-clarify-human-intelligence-too-272487",
    "title": "Raccoons break into liquor stores, scale skyscrapers and pick locks â€“ studying their clever brains can clarify human intelligence too",
    "summary": "The story delves into the extraordinary cognitive capabilities of raccoons, showcased through their complex problem-solving behaviors, including breaching secure locations like liquor stores, adeptly scaling urban structures, and manipulating intricate locks. These advanced behaviors position raccoons as a prime subject for comprehensive research in cognitive science. Scientists are actively investigating the neural underpinnings and adaptive strategies that enable such sophisticated intelligence. By dissecting these mechanisms, researchers aim to uncover universal principles governing learning, memory, and reasoning. The insights gleaned from these comparative intelligence studies are profoundly significant, offering a window into the evolutionary trajectory of cognition and providing a valuable framework for understanding the intricacies of human intelligence. Furthermore, this research holds considerable promise for informing the design and development of next-generation artificial intelligence systems, particularly those focused on creating autonomous agents capable of adaptive problem-solving and navigating complex, dynamic environments. The study ultimately contributes to a holistic understanding of intelligence across the biological spectrum and its applications in synthetic intelligence.",
    "keywords": [
      "Cognitive Science",
      "Animal Cognition",
      "Comparative Intelligence",
      "Problem Solving",
      "Behavioral Adaptability",
      "Intelligence Research"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "AI Agent"
    ],
    "published_time": "2026-01-19 17:12:19",
    "download_time": "2026-01-19 20:01:13",
    "extra_info": "{\"score\": 22, \"by\": \"pseudolus\", \"descendants\": 7, \"story_id\": 46681587}"
  },
  {
    "id": "2601.11516",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.11516",
    "title": "Building Production-Ready Probes For Gemini",
    "summary": "Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift. We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes. These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.",
    "keywords": [
      "Language Models",
      "Misuse Mitigation",
      "AI Safety",
      "Probes",
      "Distribution Shifts"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2026-01-16T18:54:29.000Z",
    "download_time": "2026-01-19 12:01:25",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.11516\", \"arxiv_url\": \"https://arxiv.org/abs/2601.11516\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11516.png\", \"original_title\": \"Building Production-Ready Probes For Gemini\"}"
  },
  {
    "id": "2601.11514",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.11514",
    "title": "ShapeR: Robust Conditional 3D Shape Generation from Casual Captures",
    "summary": "Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.",
    "keywords": [
      "3D Shape Generation",
      "Conditional Generation",
      "Robustness",
      "SLAM",
      "Vision-Language Models"
    ],
    "area": [
      "Computer Vision",
      "Generative AI",
      "Multimodal"
    ],
    "published_time": "2026-01-16T18:51:24.000Z",
    "download_time": "2026-01-19 12:01:23",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.11514\", \"arxiv_url\": \"https://arxiv.org/abs/2601.11514\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11514.png\", \"original_title\": \"ShapeR: Robust Conditional 3D Shape Generation from Casual Captures\"}"
  },
  {
    "id": "2601.11496",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.11496",
    "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents",
    "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.",
    "keywords": [
      "AI Agents",
      "Strategic Manipulation",
      "Game Theory",
      "Market Design",
      "Regulatory Outcomes"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2026-01-16T18:18:03.000Z",
    "download_time": "2026-01-19 12:01:29",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.11496\", \"arxiv_url\": \"https://arxiv.org/abs/2601.11496\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11496.png\", \"original_title\": \"The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents\"}"
  },
  {
    "id": "2601.11404",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.11404",
    "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.",
    "keywords": [
      "Vision-Language-Action Models",
      "Action Chain-of-Thought",
      "Robot Policies",
      "Multimodal Inputs",
      "Action-level Reasoning"
    ],
    "area": [
      "Robotics",
      "Multimodal",
      "Deep Learning"
    ],
    "published_time": "2026-01-16T16:17:06.000Z",
    "download_time": "2026-01-19 12:01:24",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.11404\", \"arxiv_url\": \"https://arxiv.org/abs/2601.11404\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11404.png\", \"original_title\": \"ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models\"}"
  },
  {
    "id": "2601.11354",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.11354",
    "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
    "summary": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.",
    "keywords": [
      "Agentic Planning",
      "Space Planning Problems",
      "Large Language Models",
      "AI Agent",
      "Benchmark"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2026-01-16T15:02:41.000Z",
    "download_time": "2026-01-19 12:01:21",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.11354\", \"arxiv_url\": \"https://arxiv.org/abs/2601.11354\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11354.png\", \"original_title\": \"AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems\"}"
  },
  {
    "id": "2601.11227",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.11227",
    "title": "Language of Thought Shapes Output Diversity in Large Language Models",
    "summary": "Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.",
    "keywords": [
      "Language of Thought",
      "Output Diversity",
      "Large Language Models",
      "Multilingual Thinking",
      "Pluralistic Alignment"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2026-01-16T12:14:16.000Z",
    "download_time": "2026-01-19 12:01:23",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.11227\", \"arxiv_url\": \"https://arxiv.org/abs/2601.11227\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11227.png\", \"original_title\": \"Language of Thought Shapes Output Diversity in Large Language Models\"}"
  }
]