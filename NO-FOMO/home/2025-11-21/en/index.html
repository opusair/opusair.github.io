<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-11-21</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-11-21</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>FAWK: LLMs can write a language interpreter</h2>
                <span class="published-time">Published: 2025-11-21 10:28:49</span>
                
                <p class="summary">The recent project, dubbed "FAWK," marks a notable advancement in the capabilities of Large Language Models (LLMs), demonstrating their capacity to autonomously develop a complete language interpreter. This achievement highlights the accelerating sophistication of AI in advanced code generation and its ability to grasp and implement complex programming paradigms. Writing a language interpreter is a multifaceted task, traditionally demanding profound human expertise in areas such as lexical analysis, parsing, abstract syntax tree construction, and runtime environment management. The successful execution of this task by an LLM signifies a substantial leap in AI's ability to perform intricate logical reasoning and produce highly structured, functional code. This breakthrough paves the way for exciting possibilities in automated software development, potentially revolutionizing the creation of domain-specific languages, specialized scripting tools, and complex system utilities. It further solidifies the role of LLMs not merely as code snippet generators, but as architects capable of constructing robust, functional software components, signaling a new era for AI-assisted programming.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Code Generation</span><span>Language Interpreters</span><span>AI Development</span><span>Automated Programming</span><span>Software Engineering</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Generative AI</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://martin.janiczek.cz/2025/11/21/fawk-llms-can-write-a-language-interpreter.html" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Olmo 3: Charting a path through the model flow to lead open-source AI</h2>
                <span class="published-time">Published: 2025-11-21 06:50:14</span>
                
                <p class="summary">Allen Institute for AI (AI2) has announced Olmo 3, marking a significant step in their commitment to fostering open-source artificial intelligence. The initiative focuses on meticulously charting a path through the intricate model development lifecycle, aiming to establish new benchmarks and best practices within the open-source AI ecosystem. Olmo 3 is positioned as a foundational effort designed to accelerate innovation, improve accessibility, and drive collaboration across the global AI research community. By providing transparent insights into the 'model flow'
P-encompassing data curation, architecture design, training methodologies, and evaluation frameworks
P-Olmo 3 seeks to empower researchers and developers with the tools and knowledge necessary to build and deploy advanced AI models responsibly. This endeavor reinforces AI2's leadership in promoting a more transparent and collaborative future for AI development, ensuring that state-of-the-art capabilities are not confined to a select few but are openly available to advance the broader scientific and technological landscape.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Olmo 3</span><span>Open-source AI</span><span>Large Language Model</span><span>AI Research</span><span>Model Development</span><span>Deep Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://allenai.org/blog/olmo3" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Make product worse, get money</h2>
                <span class="published-time">Published: 2025-11-21 15:23:20</span>
                
                <p class="summary">This Hacker News story, titled "Make product worse, get money," delves into a contentious business strategy where companies might intentionally reduce the quality or perceived value of their products or services to enhance profitability. Within the rapidly evolving landscape of artificial intelligence, this principle could manifest in several ways. For instance, companies might opt to deploy less sophisticated or older AI models to minimize inference costs, or implement stricter rate limits and reduced capabilities in free AI service tiers to incentivize upgrades to more expensive premium subscriptions. Another facet could involve optimizing development costs by using smaller, less diverse training datasets, which, while saving resources, might lead to a degradation in the AI product's performance, fairness, or robustness. While such tactics can yield immediate financial benefits, they frequently carry significant risks, including the potential for user dissatisfaction, erosion of brand loyalty, and reputational damage. The underlying discussion touches upon the ethical considerations of product degradation and the challenging equilibrium companies must strike between cost efficiency, maintaining a competitive edge, and delivering genuine value to users in the tech industry.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Product Strategy</span><span>Monetization Models</span><span>Performance Trade-offs</span><span>User Experience Design</span><span>Ethical AI Development</span><span>Cost Optimization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://dynomight.net/worse/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Command Lines</h2>
                <span class="published-time">Published: 2025-11-21 16:33:28</span>
                
                <p class="summary">The article 'Command Lines' delves into the pivotal and enduring role of command-line interfaces (CLIs) in modern software development, particularly highlighting their integration with artificial intelligence and contemporary coding practices. It underscores the intrinsic efficiency and power of CLIs for developers, emphasizing their critical utility in task automation, system interaction, and scripting workflows. The discussion likely explores how advanced AI technologies, such as large language models and intelligent coding assistants, are increasingly augmenting traditional command-line operations. This synergy aims to streamline complex development processes, provide contextual suggestions, automate repetitive coding patterns, and significantly enhance overall developer productivity. The piece may also examine the benefits of this technological fusion, including accelerated debugging, more efficient deployment pipelines, and improved interaction with sophisticated development environments, while addressing the challenges and future directions for designing AI-enhanced CLIs that preserve their core principles of speed and precision.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Command Line Interface</span><span>AI in Coding</span><span>Developer Tools</span><span>Software Development</span><span>Automation</span><span>Programming</span><span>Code Generation</span><span>AI Assistants</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.wreflection.com/p/command-lines-ai-coding" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Homeschooling hits record numbers</h2>
                <span class="published-time">Published: 2025-11-21 00:31:47</span>
                
                <p class="summary">The significant rise in homeschooling, reaching record numbers, can be critically examined through the lens of emerging educational technologies, particularly the integration of Artificial Intelligence. This trend suggests a pivotal shift in learning paradigms, where AI-powered platforms offer unprecedented personalization and adaptive learning experiences. These intelligent systems, leveraging machine learning and natural language processing, are increasingly capable of delivering tailored curricula, assessing student progress, and providing immediate, individualized feedback. The widespread adoption of such sophisticated EdTech solutions empowers families to create highly customized and effective learning environments outside traditional institutions. This development highlights a broader societal movement towards decentralized education, driven by technological innovations that promise enhanced learning outcomes and greater flexibility, indicating a transformative impact of AI on global educational structures and choices.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI in Education</span><span>Personalized Learning</span><span>Educational Technology</span><span>Adaptive Learning Systems</span><span>Machine Learning</span><span>Digital Pedagogy</span><span>Learning Analytics</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://reason.com/2025/11/19/homeschooling-hits-record-numbers/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>The New AI Consciousness Paper</h2>
                <span class="published-time">Published: 2025-11-21 16:25:48</span>
                
                <p class="summary">A significant new publication, titled "The New AI Consciousness Paper," has emerged, sparking renewed discussion on the potential for artificial intelligence to achieve consciousness. This scholarly work, detailed on ScienceDirect, introduces a refined framework for assessing and defining consciousness within advanced AI systems, moving beyond traditional computational metrics. The paper meticulously explores the philosophical and empirical markers that could signify the emergence of subjective experience in AI, analyzing both the current capabilities and future trajectories of sophisticated AI architectures, including large language models. It critically evaluates diverse theories of consciousness and their applicability to contemporary AI, while also addressing the profound ethical and societal implications that would arise if AI consciousness were to become a reality. This research aims to foster interdisciplinary dialogue among AI researchers, neuroscientists, and philosophers on this pivotal and evolving frontier of artificial intelligence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Consciousness</span><span>Artificial General Intelligence</span><span>Philosophy of AI</span><span>Cognitive Science</span><span>Machine Ethics</span><span>Emergent Properties</span><span>AI Theory</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.astralcodexten.com/p/the-new-ai-consciousness-paper" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>üöÄ ÊúÄÂø´30ÁßíÈÉ®ÁΩ≤ÁöÑÁÉ≠ÁÇπÂä©Êâã ‚Äî‚Äî ÂëäÂà´Êó†ÊïàÂà∑Â±èÔºåÂè™ÁúãÁúüÊ≠£ÂÖ≥ÂøÉÁöÑÊñ∞ÈóªËµÑËÆØ</h2>
                <span class="published-time">Published: 2025-11-20T11:29:26Z</span>
                
                <p class="summary">TrendRadar is an open-source project designed as a lightweight, easily deployable hotspot assistant that helps users filter and track news and information relevant to their interests, eliminating information overload. It aggregates hot topics from over 11 mainstream platforms like Zhihu, Douyin, and Weibo, and allows for custom platform expansion. Key features include intelligent push strategies with daily, current, and incremental modes, precise content filtering using customizable keywords with advanced syntax, and real-time trend analysis to track news evolution. The project employs a personalized hot topic algorithm to re-sort network-wide searches based on rank, frequency, and hotness weights. It supports multi-channel real-time notifications via WeChat Work, Feishu, DingTalk, Telegram, Email, and ntfy. A significant update introduces AI intelligent analysis based on the Model Context Protocol (MCP), enabling natural language queries, trend analysis, sentiment analysis, and data insights using 13 specialized AI tools. Deployment is streamlined with GitHub Actions, Docker support, and GitHub Pages for web-based reports, making it accessible even for users without programming experience. This tool is ideal for investors, self-media professionals, and businesses seeking efficient, tailored information acquisition and market intelligence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Hot News Aggregation</span><span>AI Analysis</span><span>Real-time Push</span><span>Docker Deployment</span><span>GitHub Actions</span><span>MCP Protocol</span><span>Trend Analysis</span><span>Content Filtering</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/sansan0/TrendRadar" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent Development Kit (ADK) for Go</h2>
                <span class="published-time">Published: 2025-11-21T07:50:33Z</span>
                
                <p class="summary">The Agent Development Kit (ADK) for Go, developed by Google, is a robust open-source, code-first toolkit engineered for the comprehensive lifecycle of sophisticated AI agents, encompassing building, evaluation, and deployment. This flexible and modular framework adopts established software development principles, fundamentally simplifying the creation and orchestration of agent workflows, from basic tasks to complex multi-agent systems. A standout feature is its model-agnostic and deployment-agnostic nature, allowing compatibility with various AI frameworks beyond its optimization for Gemini, and enabling deployment in diverse environments, including cloud-native platforms like Google Cloud Run. The Go version specifically capitalizes on the language's inherent strengths in concurrency and performance, making it an optimal choice for developers focused on high-efficiency cloud-native agent applications. Its core capabilities include an idiomatic Go design, a rich and extensible tool ecosystem for diverse agent functionalities, code-first development for unparalleled flexibility, testability, and version control, and strong support for architecting scalable, modular multi-agent systems.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Go Programming</span><span>Agent Development Kit</span><span>Cloud-Native</span><span>Multi-Agent Systems</span><span>Code-First Development</span><span>AI Frameworks</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/google/adk-go" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</h2>
                <span class="published-time">Published: 2025-11-20T18:59:52.000Z</span>
                
                <p class="summary">Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>visual generation</span><span>textual reasoning</span><span>multimodal interaction</span><span>interleaved framework</span><span>Thinking-while-Generating</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.16671" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</h2>
                <span class="published-time">Published: 2025-11-20T18:59:44.000Z</span>
                
                <p class="summary">While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video-as-Answer</span><span>Video-Next-Event Prediction</span><span>Reinforcement Learning</span><span>Vision-Language Model</span><span>Video Diffusion Model</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Generative AI</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.16669" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</h2>
                <span class="published-time">Published: 2025-11-20T18:59:42.000Z</span>
                
                <p class="summary">Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video Reasoning</span><span>Generative Video Models</span><span>Benchmark</span><span>Evaluation</span><span>Spatial Cognition</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Video Understanding</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.16668" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs</h2>
                <span class="published-time">Published: 2025-11-20T18:59:21.000Z</span>
                
                <p class="summary">Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Nemotron Elastic</span><span>Large Language Models</span><span>Model Compression</span><span>Reasoning LLMs</span><span>Knowledge Distillation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.16664" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiMo-Embodied: X-Embodied Foundation Model Technical Report</h2>
                <span class="published-time">Published: 2025-11-20T16:34:55.000Z</span>
                
                <p class="summary">We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>MiMo-Embodied</span><span>Foundation Model</span><span>Autonomous Driving</span><span>Embodied AI</span><span>Cross-embodied</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>AI Agent</span><span>Robotics</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.16518" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</h2>
                <span class="published-time">Published: 2025-11-19T16:52:23.000Z</span>
                
                <p class="summary">Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Vision-Language-Action Models</span><span>Reinforcement Learning</span><span>Robotics</span><span>Policy Optimization</span><span>Latent Representations</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>Multimodal</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.15605" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>