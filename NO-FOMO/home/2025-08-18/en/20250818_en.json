[
  {
    "id": "twitter_ctnzr_1957504768156561413",
    "source": "Twitter",
    "url": "https://x.com/ctnzr/status/1957504768156561413",
    "title_en": "ctnzr_NVIDIA Releases Nemotron Nano v2 Model and Dataset",
    "summary_en": "Bryan Catanzaro of NVIDIA announced the release of Nemotron Nano v2, a 9B hybrid SSM that is 6 times faster and more accurate than similarly sized models. Alongside the model, NVIDIA is also releasing most of the data used for its creation, including the pretraining corpus. Links to the models, datasets, and the technical report are provided, fostering advancements in the AI community.",
    "keywords_en": [
      "NVIDIA",
      "Nemotron Nano v2",
      "Large Language Model",
      "SSM",
      "Open Source Data",
      "Pretraining"
    ],
    "area_en": [
      "Product Launch",
      "Large Language Model",
      "Open Source"
    ],
    "published_time": "2025-08-18T18:08:00.000Z",
    "download_time": "2025-08-19 03:36:02",
    "visual_resource": [
      "screenshot/twitter/ctnzr_1957504768156561413.png"
    ],
    "extra_info": "{\"username\": \"ctnzr\", \"tweet_id\": \"1957504768156561413\"}"
  },
  {
    "id": "twitter_Thom_Wolf_1957450682744606789",
    "source": "Twitter",
    "url": "https://x.com/Thom_Wolf/status/1957450682744606789",
    "title_en": "Thom_Wolf_Tencent Hunyuan Releases Open-Source Real-time Video Generation Model",
    "summary_en": "Prominent AI researcher Thomas Wolf highlighted that Tencent Hunyuan Lab has released an open-source alternative to \"Genie 3,\" a model capable of generating realistic, real-time controllable videos. Key features include long-term consistency and no need for expensive rendering, trained on over 1 million gameplay recordings, marking a significant advancement in the video generation field.",
    "keywords_en": [
      "Tencent Hunyuan",
      "Video Generation",
      "Open Source",
      "Real-time Control",
      "AI Model",
      "Genie 3"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Open Source"
    ],
    "published_time": "2025-08-18T14:33:05.000Z",
    "download_time": "2025-08-19 03:36:09",
    "visual_resource": [
      "screenshot/twitter/Thom_Wolf_1957450682744606789.png"
    ],
    "extra_info": "{\"username\": \"Thom_Wolf\", \"tweet_id\": \"1957450682744606789\"}"
  },
  {
    "id": "twitter_GoogleLabs_1957473402568225197",
    "source": "Twitter",
    "url": "https://x.com/GoogleLabs/status/1957473402568225197",
    "title_en": "GoogleLabs_Flow Reaches 100M Videos, AI Credits Doubled, New Account Launched",
    "summary_en": "Google Labs announced that its Flow platform has successfully generated over 100 million videos, demonstrating significant user engagement and adoption. As a token of appreciation for the continued enthusiasm and support from its community, Google Labs is rolling out two important updates: AI credits for all Ultra users will now be doubled, providing enhanced access to generative capabilities. Additionally, a new dedicated account, @FlowbyGoogle, has been launched to serve as a central source for Flow tips, news, and community engagement. This strategic move aims to further empower users and foster the growth of the Flow ecosystem.",
    "keywords_en": [
      "Google Labs",
      "Flow",
      "Video Generation",
      "AI Credits",
      "Product Update",
      "User Appreciation"
    ],
    "area_en": [
      "Product Launch",
      "Tech News",
      "Generative AI"
    ],
    "published_time": "2025-08-18T16:03:21.000Z",
    "download_time": "2025-08-19 03:20:58",
    "visual_resource": [
      "screenshot/twitter/GoogleLabs_1957473402568225197.png"
    ],
    "extra_info": "{\"username\": \"GoogleLabs\", \"tweet_id\": \"1957473402568225197\"}"
  },
  {
    "id": "twitter_allen_ai_1957473280706629968",
    "source": "Twitter",
    "url": "https://x.com/allen_ai/status/1957473280706629968",
    "title_en": "allen_ai_Introducing MoNaCo: A New Benchmark for LLM Cross-Source Reasoning",
    "summary_en": "Allen AI introduces MoNaCo, a novel evaluation benchmark designed to assess the cross-source reasoning capabilities of Large Language Models (LLMs). MoNaCo aims to test LLMs' ability to synthesize evidence from dozens or even hundreds of sources, addressing a critical gap in current benchmarks that often overlook complex information aggregation. This new evaluation will help advance LLMs' utility in research, decision-making, and exploration by providing a more comprehensive measure of their performance.",
    "keywords_en": [
      "Large Language Models",
      "Evaluation Benchmark",
      "Cross-Source Reasoning",
      "Natural Language Processing",
      "MoNaCo",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Large Language Model",
      "Research Progress",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-18T16:02:52.000Z",
    "download_time": "2025-08-19 03:34:22",
    "visual_resource": [
      "screenshot/twitter/allen_ai_1957473280706629968.png"
    ],
    "extra_info": "{\"username\": \"allen_ai\", \"tweet_id\": \"1957473280706629968\"}"
  },
  {
    "id": "twitter_arimorcos_1957460254276677987",
    "source": "Twitter",
    "url": "https://x.com/arimorcos/status/1957460254276677987",
    "title_en": "arimorcos_Datology Launches BeyondWeb Synthetic Data Approach, Enhancing LLM Training Efficiency",
    "summary_en": "Ari Morcos of Datology AI announced the launch of BeyondWeb, a synthetic data generation approach that significantly outperforms existing open synthetic data and is a key component of their data curation pipeline. BeyondWeb addresses the challenge of generating high-quality synthetic data at scale, optimized through thousands of experiments to produce trillions of synthetic tokens. This method helps overcome the \"data wall\" in large language model pretraining, enabling 3-billion-parameter LLMs to surpass 8-billion-parameter models, offering new insights into data science.",
    "keywords_en": [
      "Synthetic Data",
      "Large Language Models",
      "Data Generation",
      "Data Curation",
      "BeyondWeb",
      "Datology AI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-08-18T15:11:07.000Z",
    "download_time": "2025-08-19 03:16:39",
    "visual_resource": [
      "screenshot/twitter/arimorcos_1957460254276677987.png"
    ],
    "extra_info": "{\"username\": \"arimorcos\", \"tweet_id\": \"1957460254276677987\"}"
  },
  {
    "id": "twitter_gdb_1957464252689895477",
    "source": "Twitter",
    "url": "https://x.com/gdb/status/1957464252689895477",
    "title_en": "gdb_Showcasing GPT Series Model Evolution Comparison",
    "summary_en": "OpenAI co-founder Greg Brockman highlighted a significant development, allowing users to compare the outputs of GPT-1 through GPT-5 models using the same prompt. This initiative aims to visually demonstrate OpenAI's iterative and remarkable progress in large language model technology, revealing performance differences and evolutionary trajectories across different model generations, providing a clear perspective on AI development.",
    "keywords_en": [
      "GPT Series",
      "Model Comparison",
      "OpenAI",
      "Language Models",
      "Technological Evolution"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-08-18T15:27:00.000Z",
    "download_time": "2025-08-19 03:30:03",
    "visual_resource": [
      "screenshot/twitter/gdb_1957464252689895477.png"
    ],
    "extra_info": "{\"username\": \"gdb\", \"tweet_id\": \"1957464252689895477\"}"
  },
  {
    "id": "zkY7UyARPacQZqmJMXdVPA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/zkY7UyARPacQZqmJMXdVPA",
    "title_en": "Unbelievable! AI Agents Now Form '100-Strong' Teams, Completing 5 Complex Tasks in Parallel in 3 Minutes with Dynamic Modification",
    "summary_en": "Baidu Wenku and Baidu Netdisk have jointly launched GenFlow2.0, the world's first full-platform universal AI Agent. Its core innovation lies in orchestrating over a hundred specialized AI Agents to concurrently handle 5-6 complex multi-modal tasks, completing them in an average of three minutes, while allowing real-time intervention and demand modification throughout the process. GenFlow2.0 deeply integrates vast public and private domain data from Baidu Wenku, Netdisk, and Academic, ensuring seamless connection between production resources and tools, significantly enhancing content generation quality and efficiency. Technically, it is built upon the Cangzhou OS and MoE architecture, employing a Multi-Agent collaborative framework with intelligent scheduling. The platform aims to establish an \"omnipotent and ubiquitous\" AI productivity, redefining the boundaries of general AI Agents. Through an open ecosystem and collaborations with hardware manufacturers, such as Honor, GenFlow2.0 seeks to achieve deep integration of AI with diverse user scenarios, thereby building an unparalleled ecological moat.",
    "keywords_en": [
      "AI Agent",
      "GenFlow2.0",
      "Baidu Wenku",
      "Baidu Netdisk",
      "Parallel Task Execution",
      "Intelligent Scheduling"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-08-18T09:13:27.000Z",
    "download_time": "2025-08-19T11:54:40.050947",
    "visual_resource": [
      "screenshot/wechat/wechat_image_zkY7UyARPacQZqmJMXdVPA.png"
    ],
    "extra_info": null
  },
  {
    "id": "Rn5xkAionfct_YlNCIRpYQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Rn5xkAionfct_YlNCIRpYQ",
    "title_en": "ByteDance Seed Open-Sources M3-Agent: A Multimodal Agent with Human-like Long-Term Memory for Auditory and Visual Perception",
    "summary_en": "ByteDance Seed has unveiled and open-sourced M3-Agent, an innovative multimodal agent framework designed with human-like long-term memory capabilities. This agent can process real-time visual and auditory inputs, continuously building and updating its semantic knowledge base. M3-Agent operates through parallel memory and control processes, employing reinforcement learning for multi-round reasoning and iterative memory retrieval, demonstrating significant performance improvements over existing baseline models, including those based on commercial LLMs like Gemini-1.5-Pro and GPT-4o. To rigorously evaluate its memory effectiveness and reasoning abilities, the team also developed and open-sourced M3-Bench, a new long-video question-answering benchmark. Experimental results show M3-Agent's superior performance across various benchmarks, particularly excelling in human understanding and cross-modal reasoning, thus offering a novel paradigm for enhancing the long-term consistency and intelligence of multimodal agents.",
    "keywords_en": [
      "Multimodal Agent",
      "Long-Term Memory",
      "Video Understanding",
      "Reinforcement Learning",
      "Benchmark",
      "ByteDance"
    ],
    "area_en": [
      "Multimodal",
      "AI Agent",
      "Video Understanding"
    ],
    "published_time": "2025-08-18T16:19:17.000Z",
    "download_time": "2025-08-19T11:54:15.185522",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Rn5xkAionfct_YlNCIRpYQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "A0qFBiiohKxe2mT_hU46Ow",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/A0qFBiiohKxe2mT_hU46Ow",
    "title_en": "From GPT-2 to gpt-oss: Analyzing the Architectural Advances, And How They Stack Up Against Qwen3",
    "summary_en": "This article provides an in-depth analysis of OpenAI's gpt-oss open models, tracing their evolution from GPT-2. It meticulously details the architectural innovations and optimizations of gpt-oss-20b and 120b, including the removal of Dropout, adoption of RoPE, SwiGLU, Mixture-of-Experts (MoE), Grouped Query Attention (GQA), and sliding window attention. The piece also conducts a comparative analysis with Qwen3, discussing differences in model width vs. depth, expert configurations, and attention biases. Furthermore, it highlights MXFP4 quantization optimization enabling single-GPU operation, and examines gpt-oss's training profile, inference workload control, and benchmark performance. The analysis concludes that gpt-oss's performance is nearing that of top proprietary models, signaling a significant positive impact on the development of open-weight models.",
    "keywords_en": [
      "gpt-oss",
      "OpenAI",
      "Model Architecture",
      "Large Language Model",
      "Transformer",
      "Qwen3"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-18T05:11:20.000Z",
    "download_time": "2025-08-19T11:54:42.315607",
    "visual_resource": [
      "screenshot/wechat/wechat_image_A0qFBiiohKxe2mT_hU46Ow.png"
    ],
    "extra_info": null
  },
  {
    "id": "vB_nCuOJpg9FJ3vPo1kfiA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/vB_nCuOJpg9FJ3vPo1kfiA",
    "title_en": "Prompts: The Ultimate Weapon for Large Models, Driving 49% Performance Surge in Maryland-MIT Research",
    "summary_en": "A groundbreaking study by researchers from the University of Maryland, MIT, and Stanford reveals that prompt optimization is a critical, often underestimated, factor in boosting AI performance, contributing nearly as much as model upgrades themselves. Their research, using DALL-E 2 and DALL-E 3, demonstrated that while model advancements accounted for 51% of performance gains, an impressive 49% surge was attributed to user-driven \"prompt adaptation.\" This signifies that the ability of users to dynamically adjust and refine prompts in response to evolving model capabilities is paramount for fully unleashing the economic value and potential of large generative models. The findings underscore the enduring importance of human ingenuity in prompt engineering, suggesting that while automation has its place, human-centric prompt design remains indispensable for achieving optimal outcomes with advanced AI systems.",
    "keywords_en": [
      "Prompt",
      "Large Models",
      "Performance Improvement",
      "Prompt Adaptation",
      "Generative AI",
      "Human-AI Collaboration"
    ],
    "area_en": [
      "Large Language Model",
      "Generative AI",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-18T14:01:09.000Z",
    "download_time": "2025-08-19T11:54:26.772733",
    "visual_resource": [
      "screenshot/wechat/wechat_image_vB_nCuOJpg9FJ3vPo1kfiA.png"
    ],
    "extra_info": null
  },
  {
    "id": "Kl5AWp-AelHBrykowdVp2g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Kl5AWp-AelHBrykowdVp2g",
    "title_en": "August 18th arXiv: LogicCLIP and Other Cutting-Edge AI Research",
    "summary_en": "This article summarizes multiple cutting-edge artificial intelligence research findings published on arXiv on August 18th. Sun Yat-sen University and National University of Singapore proposed LogicCLIP, significantly enhancing the logical understanding capabilities of vision-language models. Several studies focused on multimodal large models, covering applications in video temporal grounding, 3D visual understanding, and code generation. In generative AI, achievements include audio-driven portrait animation, cinematic-style video generation, and specific-style image generation. New methods like Polaris, ImagiDrive, and VeteranAD emerged in the autonomous driving domain. In computer vision, advancements include efficient camera parameter estimation and robust object tracking. Progress was also made in medical image analysis. Collectively, these studies showcase the latest breakthroughs and advancements in current AI across multimodal understanding, generation, autonomous driving, and visual reasoning.",
    "keywords_en": [
      "Multimodal AI",
      "Large Models",
      "Generative AI",
      "Autonomous Driving",
      "Computer Vision",
      "Vision-Language Models"
    ],
    "area_en": [
      "Multimodal",
      "Generative AI",
      "Computer Vision"
    ],
    "published_time": "2025-08-18T10:41:27.000Z",
    "download_time": "2025-08-19T11:54:23.051352",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Kl5AWp-AelHBrykowdVp2g.png"
    ],
    "extra_info": null
  },
  {
    "id": "AAElOKWC_LnksgbNRI5h6Q",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/AAElOKWC_LnksgbNRI5h6Q",
    "title_en": "ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing",
    "summary_en": "ToonComposer introduces a novel generative post-keyframing model that streamlines cartoon production by automating tedious in-betweening and coloring tasks through a unified \"post-keyframing\" pipeline. Built upon the powerful DiT architecture, this model leverages innovative sparse sketch injection and regional control strategies. It requires only sparse keyframe sketches and a single colored reference image to generate high-quality, stylistically consistent cartoon video sequences. ToonComposer significantly reduces the need for laborious frame-by-frame sketches, thereby mitigating error accumulation across different production stages. Furthermore, it incorporates a novel Spatial Low-rank Adapter (SLRA) to efficiently preserve crucial temporal priors. Extensive experimental results demonstrate ToonComposer's superior performance over existing methods in terms of visual fidelity, motion coherence, and overall production efficiency, presenting an effective and flexible solution for modern cartoon creation.",
    "keywords_en": [
      "ToonComposer",
      "Animation Production",
      "In-betweening",
      "Coloring",
      "Generative AI",
      "Video Generation"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-08-18T16:19:17.000Z",
    "download_time": "2025-08-19T11:54:16.064229",
    "visual_resource": [
      "screenshot/wechat/wechat_image_AAElOKWC_LnksgbNRI5h6Q.png"
    ],
    "extra_info": null
  },
  {
    "id": "Archon",
    "source": "GitHub",
    "url": "https://github.com/coleam00/Archon",
    "title_en": "What is Archon?",
    "summary_en": "Archon functions as a central command center and Model Context Protocol (MCP) server specifically designed for AI coding assistants. It significantly boosts AI-driven coding efficiency by providing a custom knowledge base and robust task management capabilities. Key features include intelligent web crawling, comprehensive document processing, and advanced Retrieval-Augmented Generation (RAG) strategies. Archon supports multiple Large Language Models (LLMs) such as OpenAI, Ollama, and Google Gemini. Built on a scalable microservices architecture, it ensures real-time updates and seamless collaboration. This platform empowers AI agents to access relevant documentation, perform intelligent searches, and effectively manage project tasks, establishing itself as an indispensable tool for context management in AI-assisted development workflows.",
    "keywords_en": [
      "AI Coding Assistant",
      "Knowledge Base Management",
      "Task Management",
      "RAG",
      "Microservices Architecture",
      "LLM Integration",
      "Context Protocol"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-18T18:59:49Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://github.com/coleam00/Archon/raw/main/archon-ui-main/public/archon-main-graphic.png"
    ],
    "extra_info": null
  },
  {
    "id": "parlant",
    "source": "GitHub",
    "url": "https://github.com/emcie-co/parlant",
    "title_en": "Finally, LLM agents that actually follow instructions",
    "summary_en": "Parlant is an innovative AI agent framework designed to address the inconsistency of Large Language Models (LLMs) in following instructions and handling edge cases. It adopts a \"teach principles, not scripts\" approach, ensuring LLM agents reliably adhere to predefined rules and guidelines, leading to predictable and consistent behavior. The framework offers enterprise-grade features such as natural language rule definition, dynamic guideline matching, robust tool integration, conversation analytics, and built-in guardrails. It is well-suited for industries with high compliance and accuracy demands, including financial services, healthcare, e-commerce, and legal tech, empowering developers to build production-ready AI agents.",
    "keywords_en": [
      "AI Agent",
      "Large Language Model",
      "Instruction Following",
      "Framework",
      "Predictable Behavior",
      "Guardrails",
      "Conversational AI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-18T17:13:11Z",
    "download_time": "2024-05-16 08:00:00",
    "visual_resource": [
      "https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentDark.png?raw=true",
      "https://github.com/emcie-co/parlant/blob/develop/docs/demo.gif?raw=true"
    ],
    "extra_info": null
  },
  {
    "id": "LLMs-from-scratch",
    "source": "GitHub",
    "url": "https://github.com/rasbt/LLMs-from-scratch",
    "title_en": "Build a Large Language Model (From Scratch)",
    "summary_en": "This GitHub repository provides code for developing, pretraining, and finetuning GPT-like large language models from scratch, serving as the official code repository for the book \"Build a Large Language Model (From Scratch)\". The project aims to help readers deeply understand the internal workings of LLMs by coding them from the ground up, mirroring the approach used in building large-scale foundational models. The code is implemented in PyTorch, without reliance on external LLM libraries, and is designed to run on conventional laptops while leveraging GPUs if available. Furthermore, the repository includes functionalities for loading pretrained model weights for finetuning, along with comprehensive chapter code, exercise solutions, and bonus learning materials, making it a valuable resource for learning and practicing LLM development.",
    "keywords_en": [
      "Large Language Model",
      "GPT",
      "Pretraining",
      "Finetuning",
      "PyTorch",
      "From Scratch"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-18T23:58:46Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/LLMs-from-scratch.png"
    ],
    "extra_info": null
  },
  {
    "id": "awesome-llm-apps",
    "source": "GitHub",
    "url": "https://github.com/Shubhamsaboo/awesome-llm-apps",
    "title_en": "ðŸŒŸ Awesome LLM Apps",
    "summary_en": "The \"Awesome LLM Apps\" GitHub repository is a curated collection of Large Language Model (LLM) applications, showcasing diverse implementations built with advanced techniques such as Retrieval Augmented Generation (RAG), AI Agents, Multi-agent Teams, MCP (Multimodal Control Plane), and Voice Agents. It features applications utilizing prominent models from OpenAI, Anthropic, and Google, alongside open-source alternatives like DeepSeek, Qwen, and Llama that can be run locally. The project aims to provide practical examples of LLM applications across various domains, including code analysis, data processing, finance, healthcare, and entertainment, while fostering the growth of the open-source ecosystem.",
    "keywords_en": [
      "Large Language Model",
      "AI Agent",
      "Retrieval Augmented Generation",
      "Multi-Agent System",
      "LLM Applications",
      "Open Source"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-18T02:24:54Z",
    "download_time": "2024-07-30 12:34:56",
    "visual_resource": [
      "https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png",
      "https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&type=Date"
    ],
    "extra_info": null
  },
  {
    "id": "motia",
    "source": "GitHub",
    "url": "https://github.com/MotiaDev/motia",
    "title_en": "ðŸ”¥ The Unified Backend Framework That Eliminates Runtime Fragmentation ðŸ”¥",
    "summary_en": "Motia is a unified backend framework designed to eliminate runtime fragmentation in modern software engineering. It integrates APIs, background jobs, workflows, and AI agents into a single, coherent system, offering shared observability and a consistent developer experience. Motia supports multiple languages like JavaScript, TypeScript, and Python within the same codebase, simplifying backend development through its core \"Step\" concept. It provides an event-driven architecture, built-in fault tolerance, unified state management, and automated observability, significantly enhancing development efficiency and system scalability by consolidating functionalities that previously required multiple frameworks.",
    "keywords_en": [
      "Backend Framework",
      "Unified System",
      "Multi-language Development",
      "AI Agent",
      "Event-Driven",
      "Observability",
      "Workflow",
      "Runtime"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Others"
    ],
    "published_time": "2025-08-19T00:51:05Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://github.com/MotiaDev/motia/raw/main/assets/github-readme-banner.png",
      "https://github.com/MotiaDev/motia/raw/main/assets/Motia_Github_Repository_GIF.gif",
      "https://github.com/MotiaDev/motia/raw/main/assets/new-workbench.png"
    ],
    "extra_info": null
  },
  {
    "id": "bytebot",
    "source": "GitHub",
    "url": "https://github.com/bytebot-ai/bytebot",
    "title_en": "Bytebot: Open-Source AI Desktop Agent",
    "summary_en": "Bytebot is an open-source AI desktop agent that provides AI with a complete virtual desktop environment, enabling it to operate a computer like a human and execute complex, multi-step tasks across various applications. It supports using any desktop application, managing files, processing documents (PDFs, spreadsheets), and logging into websites. Its core components include a virtual desktop, an AI agent, a task interface, and APIs, supporting major AI models like Anthropic, OpenAI, and Google Gemini. Bytebot is applicable for business process automation, development and testing, and research and analysis, offering self-hosted deployment options for data privacy and full control.",
    "keywords_en": [
      "AI Desktop Agent",
      "Virtual Desktop",
      "Task Automation",
      "AI Agent",
      "Cross-application Operation",
      "Document Processing",
      "Open Source",
      "Self-hosting"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-08-13T20:08:35Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://github.com/bytebot-ai/bytebot/raw/main/docs/images/bytebot-logo.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.10874",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.10874",
    "title_en": "SSRL: Self-Search Reinforcement Learning",
    "summary_en": "We investigate the potential of large language models (LLMs) to serve as\nefficient simulators for agentic search tasks in reinforcement learning (RL),\nthereby reducing dependence on costly interactions with external search\nengines. To this end, we first quantify the intrinsic search capability of LLMs\nvia structured prompting and repeated sampling, which we term Self-Search. Our\nresults reveal that LLMs exhibit strong scaling behavior with respect to the\ninference budget, achieving high pass@k on question-answering benchmarks,\nincluding the challenging BrowseComp task. Building on these observations, we\nintroduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability\nthrough format-based and rule-based rewards. SSRL enables models to iteratively\nrefine their knowledge utilization internally, without requiring access to\nexternal tools. Empirical evaluations demonstrate that SSRL-trained policy\nmodels provide a cost-effective and stable environment for search-driven RL\ntraining, reducing reliance on external search engines and facilitating robust\nsim-to-real transfer. We draw the following conclusions: 1) LLMs possess world\nknowledge that can be effectively elicited to achieve high performance; 2) SSRL\ndemonstrates the potential of leveraging internal knowledge to reduce\nhallucination; 3) SSRL-trained models integrate seamlessly with external search\nengines without additional effort. Our findings highlight the potential of LLMs\nto support more scalable RL agent training.",
    "keywords_en": [
      "Large Language Models",
      "Reinforcement Learning",
      "Self-Search",
      "AI Agent",
      "Knowledge Utilization"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "AI Agent"
    ],
    "published_time": "2025-08-14T17:46:01.000Z",
    "download_time": "2025-08-18 20:38:45",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10874.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.10874\", \"arxiv_url\": \"https://arxiv.org/abs/2508.10874\"}"
  },
  {
    "id": "2508.11630",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.11630",
    "title_en": "Thyme: Think Beyond Images",
    "summary_en": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.",
    "keywords_en": [
      "Multimodal Large Language Models",
      "Image Processing",
      "Code Generation",
      "Logical Reasoning",
      "Reinforcement Learning"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-15T17:59:49.000Z",
    "download_time": "2025-08-18 20:38:47",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11630.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.11630\", \"arxiv_url\": \"https://arxiv.org/abs/2508.11630\"}"
  },
  {
    "id": "2508.10975",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.10975",
    "title_en": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale\n  Pretraining",
    "summary_en": "Recent advances in large language model (LLM) pretraining have shown that\nsimply scaling data quantity eventually leads to diminishing returns, hitting a\ndata wall. In response, the use of synthetic data for pretraining has emerged\nas a promising paradigm for pushing the frontier of performance. Despite this,\nthe factors affecting synthetic data quality remain poorly understood. In this\nwork, we introduce BeyondWeb, a synthetic data generation framework that\nproduces high-quality synthetic data for pretraining. BeyondWeb significantly\nextends the capabilities of traditional web-scale datasets, outperforming\nstate-of-the-art synthetic pretraining datasets such as Cosmopedia and\nNemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1\npercentage points (pp) and 2.6pp, respectively, when averaged across a suite of\n14 benchmark evaluations. It delivers up to 7.7x faster training than open web\ndata and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for\n180B tokens on BeyondWeb outperforms an 8B model trained for the same token\nbudget on Cosmopedia. We also present several insights from BeyondWeb on\nsynthetic data for pretraining: what drives its benefits, which data to\nrephrase and how, and the impact of model size and family on data quality.\nOverall, our work shows that there's no silver bullet for generating\nhigh-quality synthetic pretraining data. The best outcomes require jointly\noptimizing many factors, a challenging task that requires rigorous science and\npractical expertise. Naive approaches can yield modest improvements,\npotentially at great cost, while well-executed methods can yield transformative\nimprovements, as exemplified by BeyondWeb.",
    "keywords_en": [
      "Synthetic Data",
      "Large Language Models",
      "Pretraining",
      "Data Generation",
      "Data Quality"
    ],
    "area_en": [
      "Large Language Model",
      "Generative AI",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-14T17:55:47.000Z",
    "download_time": "2025-08-18 20:38:47",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10975.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.10975\", \"arxiv_url\": \"https://arxiv.org/abs/2508.10975\"}"
  },
  {
    "id": "2508.10395",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.10395",
    "title_en": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
    "summary_en": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2times\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\nsim 7.7times memory savings with <0.1 perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10times memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5times memory savings with only 0.1\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models.",
    "keywords_en": [
      "LLM Inference",
      "Memory Optimization",
      "KV Cache",
      "Quantization",
      "XQuant"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-08-14T06:52:38.000Z",
    "download_time": "2025-08-18 20:38:45",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10395.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.10395\", \"arxiv_url\": \"https://arxiv.org/abs/2508.10395\"}"
  },
  {
    "id": "2508.11255",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.11255",
    "title_en": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for\n  Audio-Driven Portrait Animation",
    "summary_en": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/",
    "keywords_en": [
      "Audio-driven portrait animation",
      "Preference optimization",
      "Multimodal reward model",
      "Diffusion models",
      "Human preference"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Multimodal"
    ],
    "published_time": "2025-08-15T06:43:46.000Z",
    "download_time": "2025-08-18 20:38:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11255.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.11255\", \"arxiv_url\": \"https://arxiv.org/abs/2508.11255\"}"
  },
  {
    "id": "2508.11203",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.11203",
    "title_en": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image\n  Translation",
    "summary_en": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).",
    "keywords_en": [
      "3D Morphable Model",
      "Stylization",
      "Text-Driven",
      "Image Translation",
      "Diffusion Model"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-08-15T04:29:46.000Z",
    "download_time": "2025-08-18 20:38:44",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11203.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.11203\", \"arxiv_url\": \"https://arxiv.org/abs/2508.11203\"}"
  }
]