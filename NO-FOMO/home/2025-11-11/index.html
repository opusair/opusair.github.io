<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-11-11</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="." class="active">‰∏≠Êñá</a>
                <a href="en/" class="">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-11-11</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† ËøîÂõû‰∏ªÈ°µ</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ ÊúÄÊñ∞Êó•Êä•</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ ÂÖ≥‰∫éÊàë‰ª¨</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Show HN: Data Formulator ‚Äì interactive AI agents for data analysis (Microsoft)</h2>
                <span class="published-time">Published: 2025-11-11 17:44:21</span>
                
                <p class="summary">Microsoft has unveiled Data Formulator, an innovative platform designed for interactive data analysis powered by AI agents. This new release enables users to explore datasets and generate insightful visualizations through a combination of intuitive user interfaces and natural language communication. Building on a previous version, the development focused on seamlessly blending autonomous agent capabilities with direct user interaction, offering a controlled yet flexible data exploration experience. Data Formulator aims to simplify the process of uncovering insights by allowing users to engage more dynamically with their data while maintaining oversight. This solution represents a forward-thinking approach to data analysis, integrating crucial human intervention and iterative refinement into AI-driven workflows, thereby enhancing the utility and accessibility of artificial intelligence in data science.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI agents</span><span>data analysis</span><span>data visualization</span><span>interactive AI</span><span>natural language processing</span><span>data exploration</span><span>Microsoft</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://data-formulator.ai/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAI may not use lyrics without license, German court rules</h2>
                <span class="published-time">Published: 2025-11-11 11:20:55</span>
                
                <p class="summary">A German court has delivered a landmark ruling, stipulating that OpenAI is prohibited from using copyrighted song lyrics without securing proper licensing. This judgment emerges from a copyright infringement lawsuit, underscoring the escalating legal complexities confronting artificial intelligence developers regarding intellectual property rights within their extensive training datasets. The decision unequivocally mandates that AI models, particularly large language models (LLMs) which assimilate immense quantities of textual content, must conform to existing copyright legislation. This outcome could profoundly influence the methodology by which AI companies acquire and process data for their advanced systems. Such a legal precedent is poised to reshape future data acquisition strategies and content licensing protocols across the AI industry, especially for generative AI applications designed to produce text, music, or other creative outputs potentially derived from copyrighted source materials. It intensifies the ongoing global discourse on harmonizing rapid AI innovation with the crucial protection of creators' rights, indicating a definitive move towards more stringent copyright enforcement in the era of sophisticated generative AI technologies.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Copyright</span><span>Intellectual Property</span><span>Large Language Models</span><span>Content Licensing</span><span>German Law</span><span>Generative AI</span><span>Data Governance</span><span>Legal Precedent</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.reuters.com/world/german-court-sides-with-plaintiff-copyright-case-against-openai-2025-11-11/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>We ran over 600 image generations to compare AI image models</h2>
                <span class="published-time">Published: 2025-11-11 17:26:54</span>
                
                <p class="summary">A recent study by LateniteSoft undertook an extensive evaluation of contemporary AI image generation models, performing over 600 distinct image generations to establish a comprehensive comparative analysis. The initiative aimed to meticulously assess the capabilities, performance metrics, and qualitative outputs of various leading-edge generative AI systems currently available. Researchers systematically generated images across diverse prompts and styles to identify strengths, weaknesses, and unique characteristics inherent to each model. This rigorous methodology sought to provide objective insights into the evolving landscape of AI-driven visual content creation, offering valuable data for developers, artists, and researchers keen on understanding the nuances of current generative AI technology. The findings are expected to delineate which models excel in specific use cases, offering a data-driven perspective on factors such as prompt adherence, image quality, stylistic versatility, and efficiency. Such detailed comparisons are crucial for advancing the field and guiding future development in multimodal AI.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Image Generation</span><span>Generative AI</span><span>Image Models</span><span>Model Comparison</span><span>Computer Vision</span><span>Performance Evaluation</span><span>Benchmarking</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Yann LeCun reportedly leaving Meta to launch new AI startup</h2>
                <span class="published-time">Published: 2025-11-11 16:03:22</span>
                
                <p class="summary">Reports indicate that Yann LeCun, a prominent figure in the field of artificial intelligence and Meta's Chief AI Scientist, is preparing to depart from the technology giant to establish his own AI startup. LeCun, widely recognized as one of the 'Godfathers of AI' for his foundational work in deep learning and convolutional neural networks, has been a driving force behind Meta's AI research initiatives, leading fundamental advancements and shaping the company's long-term vision in the domain. His potential departure marks a significant development within the AI landscape, suggesting a shift in strategic focus for one of the industry's most influential researchers. This move could signal an emerging trend of top-tier AI talent migrating from large corporations to entrepreneurial ventures, seeking to innovate more freely or explore niche areas within the rapidly evolving AI ecosystem. While specific details about the new startup's focus remain undisclosed, LeCun's track record suggests it would likely involve cutting-edge research in machine learning, potentially pushing boundaries in areas such as unsupervised learning, world models, or advanced neural architectures. This development underscores the dynamic nature of AI research and the continuous flow of talent shaping its future direction, with potential implications for both academic and industrial progress in the field.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Yann LeCun</span><span>Deep Learning</span><span>AI Startup</span><span>Meta AI</span><span>Machine Learning Research</span><span>Convolutional Neural Networks</span><span>AI Leadership</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Deep Learning</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://the-decoder.com/yann-lecun-reportedly-leaving-meta-to-launch-new-ai-startup/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Why effort scales superlinearly with the perceived quality of creative work</h2>
                <span class="published-time">Published: 2025-11-11 08:29:23</span>
                
                <p class="summary">The article investigates the intriguing principle that effort scales superlinearly with the perceived quality of creative work, meaning that achieving higher levels of excellence demands exponentially greater investment. It explores the intricate dynamics behind this disproportionate relationship, where initial quality improvements may come relatively easily, but subsequent, more subtle refinements‚Äîespecially in the final stages‚Äîrequire significantly amplified time, resources, and cognitive effort. This phenomenon has profound implications for understanding productivity models, optimizing resource allocation in creative projects, and managing expectations regarding the pursuit of perfection. The discussion provides valuable insights into the challenges faced by creators, whether human artists or sophisticated generative AI models, as they strive to elevate their outputs from merely good to truly exceptional. Furthermore, it highlights the subjective nature of quality assessment and the critical decisions involved in balancing effort against perceived value, offering guidance for strategic planning in both traditional and AI-driven creative endeavors.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Creative work</span><span>Effort scaling</span><span>Perceived quality</span><span>Superlinear growth</span><span>Creative process</span><span>Productivity</span><span>Quality standards</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Artificial Intelligence</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://markusstrasser.org/creative-work-landscapes.html" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>The 'Toy Story' You Remember</h2>
                <span class="published-time">Published: 2025-11-11 03:17:43</span>
                
                <p class="summary">The Hacker News story titled 'The 'Toy Story' You Remember' is anticipated to explore the profound cultural and technological impact of Pixar's groundbreaking film. It will likely delve into how 'Toy Story' revolutionized the animation industry through its pioneering use of computer-generated imagery (CGI), setting new standards for 3D animation and digital storytelling. The article might discuss the technical innovations prevalent during its production, such as early rendering algorithms and character animation workflows, which laid essential groundwork for subsequent advancements in computer graphics. Furthermore, the piece could examine the film's enduring legacy, its nostalgic appeal, and how its creative and technical achievements continue to influence modern visual effects and emerging fields like generative media, shaping contemporary approaches to animated content creation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>3D Animation</span><span>Computer Graphics</span><span>Digital Storytelling</span><span>Character Animation</span><span>Visual Effects</span><span>Rendering Technology</span><span>Procedural Animation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://animationobsessive.substack.com/p/the-toy-story-you-remember" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>Agent Development Kit (ADK) for Go</h2>
                <span class="published-time">Published: 2025-11-11T10:31:49Z</span>
                
                <p class="summary">The Agent Development Kit (ADK) for Go is an open-source, code-first toolkit designed for building, evaluating, and deploying sophisticated AI agents with flexibility and control. This modular and model-agnostic framework applies robust software development principles to AI agent creation, simplifying the process of building, deploying, and orchestrating complex agent workflows. While optimized for Gemini, ADK supports various models and deployment environments, making it highly adaptable. The Go version specifically leverages Go's strengths in concurrency and performance, making it ideal for cloud-native agent applications. Key features include an idiomatic Go design, a rich ecosystem for integrating pre-built or custom tools, code-first development for ultimate flexibility and testability, and support for modular multi-agent systems. Agents built with ADK Go can be easily containerized and deployed in environments like Google Cloud Run, empowering developers to create scalable and performant AI-driven solutions.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Go Programming</span><span>Agent Development Kit</span><span>Cloud-Native</span><span>Multi-Agent Systems</span><span>Software Development</span><span>Artificial Intelligence</span><span>Tool Ecosystem</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/google/adk-go" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Strix</h2>
                <span class="published-time">Published: 2025-11-10T10:19:18Z</span>
                
                <p class="summary">Strix presents an innovative open-source solution leveraging autonomous AI agents to secure applications by emulating real hackers. It dynamically executes code, detects vulnerabilities, and validates them through concrete proof-of-concepts, thereby bypassing the typical overhead of manual penetration testing and the high false-positive rates of static analysis tools. Strix offers a robust suite of agentic security tools, including HTTP proxies, browser automation, terminal environments, and Python runtimes for exploit development. Its "Graph of Agents" enables distributed, scalable testing with dynamic coordination. The platform is designed for developers and security teams, providing a CLI with actionable reports and capabilities for auto-fix. Key applications include swift detection of critical vulnerabilities, accelerating compliance-driven penetration tests, automating bug bounty research with PoC generation, and integrating seamlessly into CI/CD pipelines to enforce security pre-production. An enterprise cloud-hosted version further extends capabilities with custom fine-tuned models and large-scale scanning.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI agents</span><span>Application security</span><span>Vulnerability testing</span><span>Penetration testing</span><span>CI/CD security</span><span>Proof-of-concept</span><span>Dynamic analysis</span><span>Automated security</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/usestrix/strix" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>TrendRadar</h2>
                <span class="published-time">Published: 2025-11-11T01:42:19Z</span>
                
                <p class="summary">TrendRadar is a lightweight and easily deployable open-source news monitoring assistant designed to help users cut through information overload and focus on relevant news. It aggregates hot topics from over 11 mainstream platforms, including Weibo, Douyin, Zhihu, and Baidu Hot Search, with options for custom platform integration. The project features an intelligent push strategy offering daily summaries, currentÊ¶úÂçï, and incremental monitoring modes, adaptable to various user needs from corporate managers to investors. Users can precisely filter content using a keyword-based system that supports normal, mandatory, and exclusion terms, grouped for topic-specific tracking. A key innovation is its heat trend analysis, which tracks news evolution, identifies new topics, and provides cross-platform comparisons. Additionally, TrendRadar incorporates an AI-powered analysis system based on the Model Context Protocol (MCP), enabling natural language queries, trend prediction, sentiment analysis, and smart summarization across 13 analytical tools. The system supports multi-channel real-time notifications via platforms like WeChat Work, Feishu, Telegram, and email, alongside web reporting via GitHub Pages and Docker deployment for robust, multi-architecture operation. It empowers users to proactively acquire tailored information, making it ideal for market monitoring, brand reputation tracking, and general news consumption.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>News Monitoring</span><span>Hotspot Aggregation</span><span>AI Analysis</span><span>Model Context Protocol</span><span>Multi-channel Notification</span><span>Content Filtering</span><span>Docker Deployment</span><span>GitHub Actions</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/sansan0/TrendRadar" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>DIMO: Diverse 3D Motion Generation for Arbitrary Objects</h2>
                <span class="published-time">Published: 2025-11-10T18:56:49.000Z</span>
                
                <p class="summary">We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>3D Motion Generation</span><span>Generative AI</span><span>Object Animation</span><span>Latent Space</span><span>Video Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Video Understanding</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.07409" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS Modeling</h2>
                <span class="published-time">Published: 2025-11-09T02:45:12.000Z</span>
                
                <p class="summary">Generating editable 3D CAD models from natural language remains challenging, as existing text-to-CAD systems either produce meshes or rely on scarce design-history data. We present NURBGen, the first framework to generate high-fidelity 3D CAD models directly from text using Non-Uniform Rational B-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM) to translate free-form texts into JSON representations containing NURBS surface parameters (i.e, control points, knot vectors, degrees, and rational weights) which can be directly converted into BRep format using Python. We further propose a hybrid representation that combines untrimmed NURBS with analytic primitives to handle trimmed surfaces and degenerate regions more robustly, while reducing token complexity. Additionally, we introduce partABC, a curated subset of the ABC dataset consisting of individual CAD components, annotated with detailed captions using an automated annotation pipeline. NURBGen demonstrates strong performance on diverse prompts, surpassing prior methods in geometric fidelity and dimensional accuracy, as confirmed by expert evaluations. Code and dataset will be released publicly.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Text-to-CAD</span><span>NURBS Modeling</span><span>Large Language Models</span><span>3D CAD Generation</span><span>Geometric Fidelity</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.06194" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction</h2>
                <span class="published-time">Published: 2025-11-10T17:30:08.000Z</span>
                
                <p class="summary">Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5% to 42.5%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Deep-research agents</span><span>Long-horizon tasks</span><span>Markov Decision Process</span><span>Reinforcement learning</span><span>Prompting strategy</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.07327" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>HaluMem: Evaluating Hallucinations in Memory Systems of Agents</h2>
                <span class="published-time">Published: 2025-11-05T14:37:34.000Z</span>
                
                <p class="summary">Memory systems are key components that enable AI systems such as LLMs and AI agents to achieve long-term learning and sustained interaction. However, during memory storage and retrieval, these systems frequently exhibit memory hallucinations, including fabrication, errors, conflicts, and omissions. Existing evaluations of memory hallucinations are primarily end-to-end question answering, which makes it difficult to localize the operational stage within the memory system where hallucinations arise. To address this, we introduce the Hallucination in Memory Benchmark (HaluMem), the first operation level hallucination evaluation benchmark tailored to memory systems. HaluMem defines three evaluation tasks (memory extraction, memory updating, and memory question answering) to comprehensively reveal hallucination behaviors across different operational stages of interaction. To support evaluation, we construct user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and HaluMem-Long. Both include about 15k memory points and 3.5k multi-type questions. The average dialogue length per user reaches 1.5k and 2.6k turns, with context lengths exceeding 1M tokens, enabling evaluation of hallucinations across different context scales and task complexities. Empirical studies based on HaluMem show that existing memory systems tend to generate and accumulate hallucinations during the extraction and updating stages, which subsequently propagate errors to the question answering stage. Future research should focus on developing interpretable and constrained memory operation mechanisms that systematically suppress hallucinations and improve memory reliability.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Memory systems</span><span>AI agents</span><span>Memory hallucinations</span><span>HaluMem</span><span>Evaluation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.03506" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs</h2>
                <span class="published-time">Published: 2025-11-10T18:59:53.000Z</span>
                
                <p class="summary">Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs' generalization performance. Our method, "Routing Manifold Alignment (RoMA)", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Mixture-of-Experts</span><span>Large Language Models</span><span>Router Optimization</span><span>Generalization</span><span>Manifold Alignment</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.07419" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments</h2>
                <span class="published-time">Published: 2025-11-10T17:18:35.000Z</span>
                
                <p class="summary">We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reinforcement Learning</span><span>Language Models</span><span>Adaptive Environments</span><span>Verifiable Environments</span><span>Generalizable Reasoning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.07317" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>