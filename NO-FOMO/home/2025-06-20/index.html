<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI 日报 - 2025-06-20</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter, Noto Sans SC', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }

        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.3s ease, transform 0.2s ease;
            border: 2px solid transparent;
            font-size: 0.9em;
        }

        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }

        .language-switch a.active {
            background: var(--secondary-color);
            border-color: var(--border-color);
        }

        @media (max-width: 768px) {
            .language-switch {
                position: static;
                justify-content: center;
                margin-bottom: 20px;
            }
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="./" class="active">中文</a>
                <a href="en/">English</a>
            </div>

            <h1>AI 日报</h1>
            <p class="date">2025-06-20</p>
            <p class="theme-info">关于我们: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">🏠 返回主页</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">📅 最新日报</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">👤 关于我们</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Twitter</h2>

            <article class="item-card">
                <h2>gdb_OpenAI Codex日均合并万次PR，AI赋能软件工程</h2>
                <span class="published-time">发布时间: 2025-06-20T01:37:12.000Z</span>
                <img src="screenshot/twitter/gdb_1935874544931324325.png" alt="gdb_OpenAI Codex日均合并万次PR，AI赋能软件工程">
                <p class="summary">OpenAI联合创始人Greg Brockman引用Anjney Midha数据指出，OpenAI的Codex模型在过去35天内已在GitHub上合并了34.5万个拉取请求，平均每日高达1万个。这一惊人数字凸显了人工智能在软件工程领域日益增长的影响力，预示着AI正深刻改变软件开发模式，提升自动化水平和效率。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>OpenAI</span><span>Codex</span><span>GitHub</span><span>拉取请求</span><span>软件工程</span><span>人工智能</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>技术动态</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/gdb/status/1935874544931324325" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DeepLearningAI_苹果更新Foundation Models，提升设备端与服务器端AI性能</h2>
                <span class="published-time">发布时间: 2025-06-20T18:00:02.000Z</span>
                <img src="screenshot/twitter/DeepLearningAI_1936121879552537056.png" alt="DeepLearningAI_苹果更新Foundation Models，提升设备端与服务器端AI性能">
                <p class="summary">苹果公司更新了其Apple Foundation Models (AFM)，推出设备端和服务器端新版本，旨在提升图像理解和多语言推理性能。同时发布了Foundation Models API，供开发者集成。设备端AFM采用3B参数Transformer，在某些任务上表现优于同类竞品；服务器端AFM采用混合专家架构，但与GPT-4o等顶级模型相比结果不一。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>苹果</span><span>Foundation Models</span><span>大模型</span><span>设备端AI</span><span>服务器端AI</span><span>混合专家模型</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>产品发布</span><span>技术动态</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/DeepLearningAI/status/1936121879552537056" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>reach_vb_DeepMind发布Magenta实时音乐生成模型</h2>
                <span class="published-time">发布时间: 2025-06-20T21:14:25.000Z</span>
                <img src="screenshot/twitter/reach_vb_1936182860228034902.png" alt="reach_vb_DeepMind发布Magenta实时音乐生成模型">
                <p class="summary">DeepMind发布了Apache 2.0许可的Magenta实时音乐生成模型，该模型拥有超8亿参数，基于19万小时器乐训练，能通过2秒音频块实时生成音乐，并利用10秒上下文进行条件控制。它还引入了MusicCoCa联合音乐-文本嵌入模型，支持风格嵌入实现流派/乐器实时变形，且可在免费Colab TPU上实现1.25秒生成2秒音频，模型权重已在Hugging Face发布。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>DeepMind</span><span>Magenta Real-time</span><span>音乐生成</span><span>实时生成</span><span>变压器模型</span><span>MusicCoCa</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>产品发布</span><span>开源项目</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/reach_vb/status/1936182860228034902" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiniMax__AI_发布突破性语音生成技术</h2>
                <span class="published-time">发布时间: 2025-06-20T17:27:21.000Z</span>
                <img src="screenshot/twitter/MiniMax__AI_1936113656372379680.png" alt="MiniMax__AI_发布突破性语音生成技术">
                <p class="summary">MiniMax在“MiniMax周”活动中发布了名为“Audio Dessert”的语音生成技术，标志着语音设计领域的突破性进展。该技术允许用户通过任意提示词、任意音色和任意情感生成语音，并具备完全可定制和多语言支持能力，极大地提升了语音合成的灵活性和表现力，有望革新AI语音应用。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>语音生成</span><span>MiniMax</span><span>Audio Dessert</span><span>语音设计</span><span>AI语音</span><span>技术突破</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>自然语言处理</span><span>产品发布</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/MiniMax__AI/status/1936113656372379680" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>osanseviero_Google AI发布医疗AI模型MedGemma</h2>
                <span class="published-time">发布时间: 2025-06-20T16:00:01.000Z</span>
                <img src="screenshot/twitter/osanseviero_1936096973691539652.png" alt="osanseviero_Google AI发布医疗AI模型MedGemma">
                <p class="summary">Google AI开发者团队正式发布了MedGemma，这是一系列基于Gemma 3架构的变体模型，专为医疗文本和图像理解任务优化。MedGemma提供4B多模态模型和27B纯文本模型两种版本，旨在显著加速医疗健康领域的AI项目开发，为研究人员和开发者提供高效、专业的AI工具。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>MedGemma</span><span>Gemma</span><span>医疗AI</span><span>多模态</span><span>Google AI</span><span>医疗健康</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>多模态</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/osanseviero/status/1936096973691539652" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>meshcapade_在CVPR2025展示数字人建模未来</h2>
                <span class="published-time">发布时间: 2025-06-20T14:15:29.000Z</span>
                <img src="screenshot/twitter/meshcapade_1936065369287925838.png" alt="meshcapade_在CVPR2025展示数字人建模未来">
                <p class="summary">Meshcapade在CVPR 2025大会上展示了其生产级数字人技术，揭示了人体建模的未来。该技术支持从图像和视频实时生成虚拟形象，并基于SMPL进行运动和形态估计，提供本地部署且隐私合规的解决方案。应用场景涵盖机器人、自动驾驶、人工智能及时尚领域。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>数字人</span><span>计算机视觉</span><span>SMPL</span><span>虚拟形象生成</span><span>机器人</span><span>人工智能</span></div>
                    <div class="area"><span class="label">区域：</span><span>计算机视觉</span><span>机器人</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/meshcapade/status/1936065369287925838" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>Andrej Karpathy最新演讲刷屏：软件 3.0 时代已经到来！</h2>
                <span class="published-time">发布时间: 2025-06-20T16:02:17.000Z</span>
                <img src="screenshot/wechat/wechat_image_AEc0uKb9vUOpqoX_iPJ4MQ.png" alt="Andrej Karpathy最新演讲刷屏：软件 3.0 时代已经到来！">
                <p class="summary">前OpenAI初始成员、前特斯拉AI总监Andrej Karpathy近期在Y Combinator的AI Startup School上发表了题为《Software in the era of AI》的演讲，引发广泛关注。他指出，AI时代正推动软件开发经历深刻的范式变革，预示着“软件3.0”时代的到来。此次演讲再次印证了Karpathy作为深度学习黄金时代塑造者的影响力，其观点对理解未来AI驱动的软件发展方向具有重要指导意义，并持续引发技术社区的深入讨论与思考。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>Andrej Karpathy</span><span>软件3.0</span><span>AI时代</span><span>深度学习</span><span>软件开发</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>深度学习</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/AEc0uKb9vUOpqoX_iPJ4MQ" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agentic AI时刻！多智能体驱动，「一人公司」这就要来了</h2>
                <span class="published-time">发布时间: 2025-06-20T10:39:07.000Z</span>
                <img src="screenshot/wechat/wechat_image_8JfR11MUxZneJBDnLDCX_g.png" alt="Agentic AI时刻！多智能体驱动，「一人公司」这就要来了">
                <p class="summary">文章指出，Agentic AI正迎来爆发时刻，其能独立感知环境、使用工具完成复杂任务，将AI从“问答”推向“执行”。亚马逊云科技在此领域提供全栈技术支持，包括Amazon Bedrock、Q Developer和Transform等，助力企业快速开发应用、实现代码现代化并大幅提升生产力。复星医药、合合信息等案例表明，Agentic AI在医学撰写、文档处理等知识密集型领域显著降本增效。该技术预示着“一人公司”模式的兴起，并有望重塑IT系统，亚马逊云科技已将其视为未来核心业务，加速推动AI创新与落地。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>智能体</span><span>生成式AI</span><span>亚马逊云科技</span><span>大模型</span><span>生产力</span></div>
                    <div class="area"><span class="label">区域：</span><span>智能体</span><span>大模型</span><span>生成式AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/8JfR11MUxZneJBDnLDCX_g" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>坏了！R1的秘密被Deepmind发现了！「啊哈时刻」首次被披露，现已可量化！</h2>
                <span class="published-time">发布时间: 2025-06-20T13:24:45.000Z</span>
                <img src="screenshot/wechat/wechat_image_sS8ZyUWlo5mNfmu0b9PnGw.png" alt="坏了！R1的秘密被Deepmind发现了！「啊哈时刻」首次被披露，现已可量化！">
                <p class="summary">东京大学与Google DeepMind研究团队首次通过“推理图”可视化了DeepSeek-R1等推理模型的内部思考过程，揭示了其惊人智能背后的机制。研究发现，模型自我纠错的“啊哈时刻”在推理图中表现为清晰的环形结构，可量化为平均每个问题约5次“反悔”。推理图还显示，推理模型相比基础模型拥有更多环路、更大图直径及小世界特征，这些拓扑结构是其高效推理和错误修正的关键。此外，高质量训练数据能显著扩展推理图直径。这项研究为理解和改进AI推理能力提供了全新视角，预示着未来AI架构将基于智能的拓扑本质进行设计。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>推理模型</span><span>推理图</span><span>啊哈时刻</span><span>拓扑结构</span><span>小世界网络</span><span>自我纠错</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>深度学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/sS8ZyUWlo5mNfmu0b9PnGw" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SIGGRAPH 2025｜Large Avatar Model：单图秒级打造超写实3D交互数字人，跨平台超实时驱动渲染</h2>
                <span class="published-time">发布时间: 2025-06-20T10:39:07.000Z</span>
                <img src="screenshot/wechat/wechat_image_41mpwUeWFARsXRWuDrCBZA.png" alt="SIGGRAPH 2025｜Large Avatar Model：单图秒级打造超写实3D交互数字人，跨平台超实时驱动渲染">
                <p class="summary">阿里巴巴通义实验室提出大型头像模型LAM，旨在通过单张图像在秒级内生成可驱动的超写实3D高斯数字人。该模型突破传统方法对多视角数据和复杂后处理的依赖，采用规范化空间高斯球生成、多模态特征交互Transformer及网格细分等技术，实现轻量化、跨平台超实时驱动渲染，支持WebGL在移动端达到120FPS。LAM可直接兼容传统图形管线，无需神经后处理，并能结合大模型实现文本驱动生成和3D风格迁移。该技术已应用于构建低延迟、低成本的交互对话数字人解决方案，并已全面开源，为虚拟会议、游戏开发等领域提供新思路。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>数字人</span><span>3D高斯</span><span>单图生成</span><span>实时渲染</span><span>跨平台</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>计算机视觉</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/41mpwUeWFARsXRWuDrCBZA" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>只改2行代码，RAG效率暴涨30%！多种任务适用，可扩展至百亿级数据规模应用</h2>
                <span class="published-time">发布时间: 2025-06-20T16:02:17.000Z</span>
                <img src="screenshot/wechat/wechat_image_r7jt1BDGP3ycVuCnscf4IQ.png" alt="只改2行代码，RAG效率暴涨30%！多种任务适用，可扩展至百亿级数据规模应用">
                <p class="summary">浙江大学与傅聪团队开源新方法PSP，通过仅修改两行代码，将RAG向量检索效率提升30%。该方法突破了最大内积检索的“度量错配”难题，证明了基于欧式距离构建的图索引结构，通过贪心算法可找到全局最优最大内积解。PSP还引入了自适应早停策略，显著提升搜索速度并减少冗余计算。该技术泛化性强，适用于多种模态任务，并展现出优异的可扩展性，有望应用于十亿乃至百亿级数据规模，为AI产品核心组件带来显著性能提升。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>RAG</span><span>向量检索</span><span>PSP</span><span>内积检索</span><span>欧式距离</span><span>图索引</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>机器学习</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/r7jt1BDGP3ycVuCnscf4IQ" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>首个「万亿级时间点」预训练，清华发布生成式时序大模型日晷 | ICML Oral</h2>
                <span class="published-time">发布时间: 2025-06-20T04:05:05.000Z</span>
                <img src="screenshot/wechat/wechat_image_y3sc2e2lmW1sqfnoK-ZdDA.png" alt="首个「万亿级时间点」预训练，清华发布生成式时序大模型日晷 | ICML Oral">
                <p class="summary">清华大学发布生成式时序大模型“日晷”，其研究成果被ICML 2025接收为Oral论文。该模型首次实现万亿级时间点预训练，并构建了领域内最大的时序数据集TimeBench。日晷通过引入基于流匹配的预测损失函数，有效解决了时序预测的非确定性及模式坍塌问题，能够生成多条预测轨迹并提供概率预测能力。相较传统方法和现有深度模型，日晷在多项预测榜单上展现出卓越的零样本预测性能，且推理速度达到毫秒级，为多领域提供高效预测，有望扩展时序模型应用前景。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>时序大模型</span><span>生成式</span><span>流匹配</span><span>零样本预测</span><span>概率预测</span><span>模式坍塌</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>深度学习</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/y3sc2e2lmW1sqfnoK-ZdDA" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>Build a Large Language Model (From Scratch)</h2>
                <span class="published-time">发布时间: 2025-06-22T21:36:39Z</span>
                <img src="screenshot/github/LLMs-from-scratch.png" alt="Build a Large Language Model (From Scratch)">
                <p class="summary">该GitHub仓库是《从零构建大型语言模型》一书的官方代码库，旨在指导用户从头开发、预训练和微调类GPT大语言模型。项目通过逐步编码，深入剖析LLM工作原理，并提供加载预训练模型权重进行微调的功能。其方法论与大型基础模型的构建方式相仿，且代码设计可在普通笔记本电脑上运行，无需专业硬件，适合教育和深入理解LLM的开发者。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大语言模型</span><span>GPT</span><span>从零构建</span><span>预训练</span><span>微调</span><span>自然语言处理</span><span>深度学习</span><span>代码实现</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>深度学习</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/rasbt/LLMs-from-scratch" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AI Engineering Hub 🚀</h2>
                <span class="published-time">发布时间: 2025-06-22T06:10:52Z</span>
                <img src="https://github.com/patchy631/ai-engineering-hub/raw/main/assets/ai-eng-hub.gif" alt="AI Engineering Hub 🚀">
                <p class="summary">“AI Engineering Hub”是一个专注于AI工程领域的GitHub仓库，旨在为开发者提供深入的教程和丰富的实践经验。它涵盖了大语言模型（LLMs）、检索增强生成（RAGs）以及AI智能体等前沿技术，通过实际应用案例帮助用户掌握并扩展AI工程技能。该项目致力于赋能不同水平的AI从业者，加速其在AI领域的学习与创新，是探索和实践AI工程的宝贵资源。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>AI工程</span><span>大语言模型</span><span>检索增强生成</span><span>AI智能体</span><span>教程</span><span>实践</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/patchy631/ai-engineering-hub" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Claude Code</h2>
                <span class="published-time">发布时间: 2025-06-18T20:29:20Z</span>
                <img src="https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif" alt="Claude Code">
                <p class="summary">Claude Code是一款由Anthropic开发的智能编码工具，它作为终端智能体，能够理解整个代码库，并通过自然语言命令执行日常编码任务、解释复杂代码并管理Git工作流，从而显著提升开发效率。该工具支持在终端、IDE中使用，并可集成到GitHub平台，为开发者提供便捷、高效的AI辅助编程体验。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>智能编码工具</span><span>AI智能体</span><span>自然语言编程</span><span>代码理解</span><span>Git工作流</span><span>终端工具</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>智能体</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/anthropics/claude-code" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Gitingest</h2>
                <span class="published-time">发布时间: 2025-06-22T23:41:14Z</span>
                <img src="https://github.com/cyclotruc/gitingest/raw/main/docs/frontpage.png" alt="Gitingest">
                <p class="summary">Gitingest是一个创新的工具，旨在将任意Git仓库内容转化为对大型语言模型（LLM）友好的文本摘要。它能够从Git仓库URL或本地目录生成代码上下文的文本摘要，并提供智能格式化、文件结构、提取大小和Token计数等统计信息。Gitingest支持作为命令行工具、Python包和浏览器扩展使用，极大地简化了LLM处理代码库的流程，提升了代码理解和分析的效率。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>Git仓库</span><span>LLM</span><span>代码摘要</span><span>文本处理</span><span>开发者工具</span><span>提示工程</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/cyclotruc/gitingest" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Web Development for Beginners - A Curriculum</h2>
                <span class="published-time">发布时间: 2025-05-29T17:34:21Z</span>
                <img src="https://github.com/microsoft/Web-Dev-For-Beginners/raw/main/images/background.png" alt="Web Development for Beginners - A Curriculum">
                <p class="summary">微软云倡导者团队推出了一项为期12周的综合性Web开发课程，旨在教授JavaScript、CSS和HTML基础知识。该课程通过构建如虚拟生态缸、浏览器扩展和太空游戏等24个动手项目，结合测验、讨论和实践作业，采用项目式教学法，有效提升学习者的技能和知识留存。此外，该团队还发布了针对JavaScript的生成式AI新课程，进一步拓展学习领域。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>Web开发</span><span>前端开发</span><span>JavaScript</span><span>HTML</span><span>CSS</span><span>项目式学习</span><span>编程教育</span><span>生成式AI</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>生成式AI</span><span>其他</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/microsoft/Web-Dev-For-Beginners" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>n8n - Secure Workflow Automation for Technical Teams</h2>
                <span class="published-time">发布时间: 2025-06-23T06:59:40Z</span>
                <img src="https://user-images.githubusercontent.com/10284570/173569848-c624317f-42b1-45a6-ab09-f0ea3c247648.png" alt="n8n - Secure Workflow Automation for Technical Teams">
                <p class="summary">n8n是一个面向技术团队的工作流自动化平台，融合了代码的灵活性与无代码的便捷性。它提供400+集成、原生AI能力（支持基于LangChain构建AI智能体工作流），并采用公平代码许可，允许用户完全控制数据和部署。n8n具备企业级特性，如高级权限、SSO和气隙部署，并通过活跃社区提供丰富的模板和支持，是构建强大自动化解决方案的理想选择。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>工作流自动化</span><span>低代码</span><span>无代码</span><span>AI集成</span><span>数据控制</span><span>自托管</span><span>企业级</span><span>LangChain</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>智能体</span><span>其他</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/n8n-io/n8n" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>从跨领域视角再探LLM推理的强化学习</h2>
                <span class="published-time">发布时间: 2025-06-17T20:24:00.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14965.png" alt="从跨领域视角再探LLM推理的强化学习">
                <p class="summary">强化学习（RL）已成为改进大型语言模型（LLM）推理的一种有前景的方法，然而大多数公开研究工作狭隘地集中在数学和代码领域，这限制了我们对其在通用推理方面更广泛适用性的理解。一个关键挑战在于缺乏跨不同推理领域的可靠、可扩展的RL奖励信号。我们引入了Guru，这是一个精心策划的RL推理语料库，包含9.2万个可验证的示例，涵盖数学、代码、科学、逻辑、模拟和表格六个推理领域，每个领域都通过领域特定的奖励设计、去重和过滤构建，以确保RL训练的可靠性和有效性。基于Guru，我们系统地重新审视了LLM推理中RL的既有发现，并观察到跨领域的显著差异。例如，虽然先前的工作表明RL主要从预训练模型中激发现有知识，但我们的结果揭示了一种更细致的模式：在预训练期间经常出现的领域（数学、代码、科学）可以很容易地从跨领域RL训练中受益，而预训练暴露有限的领域（逻辑、模拟和表格）则需要领域内训练才能获得有意义的性能提升，这表明RL可能有助于真正的技能习得。最后，我们提出了Guru-7B和Guru-32B，这两个模型在公开可用数据RL训练的开放模型中取得了最先进的性能，在涵盖六个推理领域的17项任务评估套件中，分别比最佳基线高出7.9%和6.7%。我们还表明，我们的模型有效地提高了其基础模型的Pass@k性能，特别是在预训练数据中不太可能出现的复杂任务上。我们发布了数据、模型、训练和评估代码，以促进通用推理。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>强化学习</span><span>大型语言模型</span><span>推理</span><span>跨领域</span><span>通用推理</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>自然语言处理</span><span>机器学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.14965" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show-o2：改进的原生统一多模态模型</h2>
                <span class="published-time">发布时间: 2025-06-18T15:39:15.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15564.png" alt="Show-o2：改进的原生统一多模态模型">
                <p class="summary">本文介绍了改进的原生统一多模态模型Show-o2，该模型利用自回归建模和流匹配技术。Show-o2构建于一个3D因果变分自编码器空间之上，通过空间（-时间）融合的双路径构建统一的视觉表示，从而实现在图像和视频模态间的可扩展性，同时确保有效的多模态理解和生成。基于语言模型，自回归建模和流匹配分别原生应用于语言头和流头，以促进文本令牌预测和图像/视频生成。设计了两阶段训练方案，以有效学习并扩展到更大的模型。最终的Show-o2模型在处理包括文本、图像和视频在内的多种模态的广泛多模态理解和生成任务中展现出多功能性。代码和模型已在https://github.com/showlab/Show-o发布。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>多模态模型</span><span>自回归建模</span><span>流匹配</span><span>统一视觉表示</span><span>多模态理解与生成</span></div>
                    <div class="area"><span class="label">区域：</span><span>多模态</span><span>深度学习</span><span>生成式AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.15564" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>基于结构化指令的图表到代码生成迭代细化改进方法</h2>
                <span class="published-time">发布时间: 2025-06-15T14:10:16.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14837.png" alt="基于结构化指令的图表到代码生成迭代细化改进方法">
                <p class="summary">近期，多模态大语言模型（MLLMs）因其强大的视觉理解能力而受到越来越多的研究关注。尽管它们在各种视觉任务上取得了令人瞩目的成果，但其在图表到代码生成方面的表现仍不尽理想。这项任务要求MLLMs生成可重现给定图表的可执行代码，这不仅需要精确的视觉理解，还需要将视觉元素准确地转换为结构化代码。直接提示MLLMs执行这项复杂任务通常会产生不令人满意的结果。为了解决这一挑战，我们提出了{ChartIR}，一种基于结构化指令的迭代细化方法。首先，我们区分了两个任务：视觉理解和代码翻译。为了完成视觉理解部分，我们设计了两种类型的结构化指令：描述和差异。描述指令捕获参考图表的视觉元素，而差异指令则表征参考图表与生成图表之间的差异。这些指令有效地将视觉特征转换为语言表示，从而促进后续的代码翻译过程。其次，我们将整个图表生成流程分解为两个阶段：初始代码生成和迭代细化，从而实现最终输出的逐步增强。实验结果表明，与其他方法相比，我们的方法在开源模型Qwen2-VL和闭源模型GPT-4o上均取得了卓越的性能。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>图表到代码生成</span><span>迭代细化</span><span>结构化指令</span><span>多模态大语言模型</span><span>视觉理解</span></div>
                    <div class="area"><span class="label">区域：</span><span>多模态</span><span>大模型</span><span>生成式AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.14837" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RE-IMAGINE：用于推理评估的符号基准合成</h2>
                <span class="published-time">发布时间: 2025-06-18T13:35:47.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15455.png" alt="RE-IMAGINE：用于推理评估的符号基准合成">
                <p class="summary">近期大型语言模型（LLMs）在推理基准测试中展现出高准确性。然而，目前尚不清楚观察到的结果是源于真正的推理能力，还是仅仅对训练集的统计性记忆。受因果关系阶梯（Pearl, 2009）及其三个层次（关联、干预和反事实）的启发，本文提出了RE-IMAGINE框架，旨在刻画LLMs推理能力的层次结构，并提供一个自动化流程，用于生成不同层次的问题变体。通过在中间符号表示中修改问题，RE-IMAGINE能够生成任意数量的、无法仅凭记忆解决的问题。此外，该框架具有通用性，可应用于数学、代码和逻辑等多种推理领域。我们通过在四个广泛使用的基准测试上演示该框架，评估了多个LLM家族，并观察到当模型面对问题变体时，其性能有所下降。这些评估结果表明，LLMs在过去的表现中一定程度上依赖于统计性记忆，并为进一步研究推理层次结构中的各项技能打开了大门。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大型语言模型</span><span>推理能力</span><span>符号基准合成</span><span>问题变体</span><span>统计性记忆</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>人工智能</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.15455" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            由 AI 助手生成
        </footer>
    </div>
</body>
</html>