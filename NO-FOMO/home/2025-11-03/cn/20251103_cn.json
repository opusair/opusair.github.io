[
  {
    "id": "hackernews_45799211",
    "source": "Hacker News",
    "url": "https://www.nytimes.com/2025/11/03/technology/openai-amazon-cloud-computing.html",
    "title": "OpenAI signs $38B cloud computing deal with Amazon",
    "summary": "OpenAI, a leading artificial intelligence research and deployment company, has reportedly secured a substantial $38 billion cloud computing agreement with Amazon. This landmark deal underscores the escalating demand for high-performance computing infrastructure necessary to fuel advanced AI development, particularly for training and deploying large language models and other complex AI systems. The partnership with Amazon's dominant cloud division, Amazon Web Services (AWS), will provide OpenAI with access to vast computational resources, including specialized hardware like GPUs, which are critical for processing the immense datasets required for cutting-edge AI research. This strategic alliance is expected to significantly enhance OpenAI's capacity for innovation, enabling the company to accelerate the development of next-generation AI technologies and expand its services globally. For Amazon, the deal solidifies its position as a premier provider of AI infrastructure, leveraging its extensive cloud capabilities to support one of the most prominent players in the AI industry. The massive investment highlights the significant capital expenditure involved in scaling AI operations and signals a long-term commitment from both entities to drive the future of artificial intelligence.",
    "keywords": [
      "Cloud Computing",
      "Artificial Intelligence",
      "OpenAI",
      "Amazon Web Services",
      "Large Language Models",
      "AI Infrastructure",
      "Computational Resources",
      "Strategic Partnership"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Machine Learning"
    ],
    "published_time": "2025-11-03 14:20:05",
    "download_time": "2025-11-03 20:06:35",
    "extra_info": "{\"score\": 129, \"by\": \"donohoe\", \"descendants\": 119, \"story_id\": 45799211}"
  },
  {
    "id": "hackernews_45802029",
    "source": "Hacker News",
    "url": "https://www.newyorker.com/magazine/2025/11/10/the-case-that-ai-is-thinking",
    "title": "The Case That A.I. Is Thinking",
    "summary": "The article, \"The Case That A.I. Is Thinking,\" delves into the profound and increasingly contentious debate surrounding whether contemporary artificial intelligence systems, particularly advanced large language models, possess genuine cognitive capabilities akin to human thought. It meticulously examines the philosophical and scientific arguments, exploring various criteria for defining consciousness, cognition, and sentience when applied to non-biological entities. The discussion likely encompasses perspectives ranging from proponents who argue that AI's sophisticated problem-solving, reasoning, and creative generation are evidence of true thought, to skeptics who contend that these systems merely simulate understanding without true internal states, subjective experience, or intentionality. The piece probably analyzes the performance of state-of-the-art AI in tasks that traditionally required human intellect, prompting a re-evaluation of our understanding of intelligence itself. Furthermore, it might touch upon the ethical implications and societal impact if AI were indeed capable of thinking, potentially reshaping human-machine interactions and the future trajectory of AI development.",
    "keywords": [
      "AI Cognition",
      "Artificial General Intelligence",
      "Machine Consciousness",
      "Large Language Models",
      "Philosophy of AI",
      "Turing Test"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-11-03 17:55:10",
    "download_time": "2025-11-03 20:06:19",
    "extra_info": "{\"score\": 69, \"by\": \"ascertain\", \"descendants\": 145, \"story_id\": 45802029}"
  },
  {
    "id": "hackernews_45800688",
    "source": "Hacker News",
    "url": "https://www.theverge.com/news/812376/google-removes-gemma-senator-blackburn-hallucination",
    "title": "Google pulls AI model after senator says it fabricated assault allegation",
    "summary": "Google has temporarily withdrawn one of its artificial intelligence models following an accusation from a senator that the AI system fabricated an assault allegation. This incident highlights ongoing challenges in AI development, particularly concerning the reliability and ethical implications of generative AI systems. The withdrawal underscores the critical need for robust safety protocols and stringent content moderation mechanisms within AI applications. Concerns about AI hallucination, where models generate false or misleading information, are increasingly prominent as these technologies become more integrated into daily life. This event emphasizes the responsibility of AI developers to ensure the factual accuracy and mitigate potential harms from their products, prompting a re-evaluation of current deployment strategies and user interaction safeguards to prevent the spread of misinformation and protect individuals from AI-generated fabrications. The swift action by Google reflects the company's commitment to addressing serious concerns regarding its AI offerings, though the specific model and details of the fabricated allegation were not extensively detailed in the initial report.",
    "keywords": [
      "AI Hallucination",
      "AI Safety",
      "Generative AI",
      "Content Moderation",
      "AI Ethics",
      "Google AI",
      "Large Language Models"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Large Language Model"
    ],
    "published_time": "2025-11-03 16:17:22",
    "download_time": "2025-11-03 20:06:54",
    "extra_info": "{\"score\": 61, \"by\": \"croemer\", \"descendants\": 57, \"story_id\": 45800688}"
  },
  {
    "id": "hackernews_45798193",
    "source": "Hacker News",
    "url": "https://github.com/ServiceStack/llms",
    "title": "OSS Alternative to Open WebUI – ChatGPT-Like UI, API and CLI",
    "summary": "This Hacker News story introduces a new open-source project from ServiceStack, dubbed \"llms\", serving as a direct alternative to platforms like Open WebUI. It offers a multifaceted approach to interacting with Large Language Models (LLMs), encompassing a user-friendly interface reminiscent of ChatGPT, a comprehensive API for seamless programmatic integration, and a powerful command-line interface (CLI) designed for automation and scripting. This project is significant for developers and organizations seeking flexible, self-hostable solutions to manage their LLM workflows. By providing a customizable and community-driven platform, \"llms\" aims to democratize access to advanced AI capabilities, fostering innovation and reducing reliance on closed ecosystems. Its release underscores the increasing demand for robust, open-source tools that empower users with greater control over their AI deployments and interactions, thereby accelerating the adoption and application of LLM technologies across various domains.",
    "keywords": [
      "Open Source",
      "Large Language Models",
      "User Interface",
      "API",
      "CLI",
      "ChatGPT"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-11-03 12:05:28",
    "download_time": "2025-11-03 20:06:38",
    "extra_info": "{\"score\": 72, \"by\": \"mythz\", \"descendants\": 23, \"story_id\": 45798193}"
  },
  {
    "id": "hackernews_45803160",
    "source": "Hacker News",
    "url": "https://www.reuters.com/technology/waymo-expand-robotaxi-service-las-vegas-san-diego-detroit-next-year-2025-11-03/",
    "title": "Waymo to expand robotaxi service to Las Vegas, San Diego and Detroit next year",
    "summary": "Waymo, a pioneering company in autonomous driving technology, has announced ambitious plans to significantly expand its robotaxi service, targeting the major U.S. cities of Las Vegas, San Diego, and Detroit for launch next year. This strategic expansion represents a critical milestone in the commercialization and broader adoption of self-driving vehicles, showcasing Waymo's continued confidence in its fully autonomous ride-hailing platform. Operating in diverse urban environments like these will provide invaluable data and operational insights, further refining the sophisticated AI-powered systems that govern the vehicles' perception, prediction, and planning capabilities. The deployment into new geographical areas necessitates robust advancements in real-time mapping, sensor fusion, and complex decision-making algorithms to adapt to varied traffic patterns, infrastructure, and local regulations. This move underscores the ongoing progress in integrating advanced robotics and artificial intelligence into mainstream mobility solutions, aiming to enhance urban transportation's safety, efficiency, and accessibility.",
    "keywords": [
      "Robotaxi",
      "Autonomous Vehicles",
      "Waymo",
      "Self-driving Cars",
      "AI in Transportation",
      "Robotics",
      "Mobility Services"
    ],
    "area": [
      "Robotics",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "published_time": "2025-11-03 19:17:08",
    "download_time": "2025-11-03 20:06:43",
    "extra_info": "{\"score\": 9, \"by\": \"standardUser\", \"descendants\": 3, \"story_id\": 45803160}"
  },
  {
    "id": "hackernews_45798881",
    "source": "Hacker News",
    "url": "https://skyfall-gs.jayinnn.dev/",
    "title": "Skyfall-GS \n– Synthesizing Immersive 3D Urban Scenes from Satellite Imagery",
    "summary": "Skyfall-GS is an innovative project focused on the synthesis of immersive 3D urban scenes directly from satellite imagery. This research leverages advanced computational techniques, likely involving deep learning and computer vision, to reconstruct detailed three-dimensional models of cityscapes from two-dimensional aerial data. The primary objective is to generate highly realistic and interactive urban environments that can be explored. This approach offers significant advantages over traditional 3D modeling methods, providing a scalable and efficient way to create digital twins of urban areas. The resulting immersive scenes hold substantial potential for applications in urban planning, architectural visualization, environmental monitoring, simulation for autonomous systems, and the development of rich virtual reality and gaming experiences. The project highlights advancements in transforming geospatial data into high-fidelity, actionable 3D content, pushing the boundaries of spatial computing and generative AI in environmental reconstruction.",
    "keywords": [
      "3D Reconstruction",
      "Urban Modeling",
      "Satellite Imagery",
      "Generative AI",
      "Computer Vision",
      "Deep Learning",
      "Scene Synthesis",
      "Geospatial Data"
    ],
    "area": [
      "Computer Vision",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-11-03 13:46:19",
    "download_time": "2025-11-03 20:06:07",
    "extra_info": "{\"score\": 75, \"by\": \"ChrisArchitect\", \"descendants\": 22, \"story_id\": 45798881}"
  },
  {
    "id": "BettaFish",
    "source": "GitHub",
    "url": "https://github.com/666ghj/BettaFish",
    "title": "Weibo Public Opinion Analysis System",
    "summary": "The \"Weibo Public Opinion Analysis System,\" also known as \"Weiyu\" or \"BettaFish,\" is an innovative multi-agent system designed to analyze public sentiment, predict trends, create decision-making support. It enables users to submit analysis requests conversationally, prompting intelligent agents to autonomously process information from over 30 mainstream domestic and international social media platforms and millions of user comments. This system boasts six key advantages: AI-driven 24/7 all-domain monitoring by an AI crawler, a composite analysis engine that integrates five specialized agents with fine-tuned and statistical models, robust multi-modal capabilities for analyzing short videos and structured data, an Agent \"Forum\" for collaborative intelligence, seamless integration of public and private data, and a lightweight, extensible Python-based framework. Ultimately, \"Weiyu\" aims to be a versatile data analysis engine beyond mere public opinion analysis.",
    "keywords": [
      "Multi-agent system",
      "Public Opinion Analysis",
      "AI Crawler",
      "LLM",
      "Multimodal Analysis",
      "Sentiment Analysis",
      "Data Integration",
      "Flask"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-11-03T17:51:24Z",
    "download_time": "2024-07-28 12:47:33",
    "extra_info": null
  },
  {
    "id": "nano-vllm",
    "source": "GitHub",
    "url": "https://github.com/GeeeekExplorer/nano-vllm",
    "title": "Nano-vLLM",
    "summary": "Nano-vLLM is a lightweight, from-scratch implementation of the vLLM inference engine, engineered for fast offline inference of Large Language Models. It boasts comparable, and often superior, inference speeds to the original vLLM, as validated by benchmarks showcasing higher throughput. The project prioritizes code clarity with a highly readable Python implementation spanning approximately 1,200 lines, making it accessible for developers. Its robust optimization suite includes critical features such as prefix caching, Tensor Parallelism, Torch compilation, and CUDA graph integration, all designed to maximize performance and efficiency. Nano-vLLM is an ideal solution for scenarios demanding optimized LLM deployment, particularly where a lean, high-performance, and understandable inference framework is paramount. Its API mirrors vLLM's interface, simplifying adoption for existing vLLM users seeking an enhanced alternative. This tool is valuable for researchers and engineers focused on advancing LLM inference capabilities.",
    "keywords": [
      "vLLM",
      "Large Language Models",
      "Inference Optimization",
      "Tensor Parallelism",
      "CUDA Graph",
      "Deep Learning",
      "Python"
    ],
    "area": [
      "Artificial Intelligence",
      "Deep Learning",
      "Large Language Model"
    ],
    "published_time": "2025-11-03T17:44:42Z",
    "download_time": "2024-05-15 10:30:00",
    "extra_info": null
  },
  {
    "id": "opencode",
    "source": "GitHub",
    "url": "https://github.com/sst/opencode",
    "title": "OpenCode: The AI coding agent built for the terminal.",
    "summary": "OpenCode is an open-source AI coding agent specifically engineered for terminal environments, offering developers a versatile and powerful tool. Its core strength lies in its provider-agnostic design, supporting major AI models from Anthropic, OpenAI, and Google, alongside local models, which ensures future-proofing and cost efficiency. The agent features out-of-the-box Language Server Protocol (LSP) support, enhancing code understanding and interaction within the terminal. With a strong focus on Terminal User Interface (TUI), OpenCode is built to maximize productivity for command-line enthusiasts. Furthermore, its innovative client/server architecture enables flexible operation, allowing remote control of the agent from various frontends, including mobile applications. OpenCode aims to redefine terminal-based development by offering a highly adaptable and open alternative to proprietary coding assistants, with straightforward installation across multiple operating systems.",
    "keywords": [
      "AI Agent",
      "Terminal UI",
      "Code Generation",
      "Open Source",
      "LSP Support",
      "Provider-Agnostic",
      "Client-Server Architecture",
      "Developer Tools"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-11-03T17:08:32Z",
    "download_time": "2024-07-29 08:31:00",
    "extra_info": null
  },
  {
    "id": "2510.24795",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.24795",
    "title": "A Survey on Efficient Vision-Language-Action Models",
    "summary": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/",
    "keywords": [
      "Vision-Language-Action models",
      "Embodied intelligence",
      "Efficient VLAs",
      "Model compression",
      "Robotics"
    ],
    "area": [
      "Multimodal",
      "Robotics",
      "AI Agent"
    ],
    "published_time": "2025-10-27T17:57:33.000Z",
    "download_time": "2025-11-03 12:07:31",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.24795\", \"arxiv_url\": \"https://arxiv.org/abs/2510.24795\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24795.png\", \"original_title\": \"A Survey on Efficient Vision-Language-Action Models\"}"
  },
  {
    "id": "2510.26887",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.26887",
    "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
    "summary": "We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.",
    "keywords": [
      "AI agents",
      "Scientific discovery",
      "Multi-agent systems",
      "Automated research",
      "Interdisciplinary AI"
    ],
    "area": [
      "AI Agent",
      "Artificial Intelligence",
      "Generative AI"
    ],
    "published_time": "2025-10-30T18:00:12.000Z",
    "download_time": "2025-11-03 12:07:38",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.26887\", \"arxiv_url\": \"https://arxiv.org/abs/2510.26887\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26887.png\", \"original_title\": \"The Denario project: Deep knowledge AI agents for scientific discovery\"}"
  },
  {
    "id": "2510.27688",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.27688",
    "title": "Continuous Autoregressive Language Models",
    "summary": "The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM.",
    "keywords": [
      "Continuous Autoregressive Language Models",
      "Large Language Models",
      "next-vector prediction",
      "autoencoder",
      "likelihood-free framework"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-10-31T17:58:11.000Z",
    "download_time": "2025-11-03 12:07:33",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.27688\", \"arxiv_url\": \"https://arxiv.org/abs/2510.27688\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27688.png\", \"original_title\": \"Continuous Autoregressive Language Models\"}"
  },
  {
    "id": "2510.26707",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.26707",
    "title": "Value Drifts: Tracing Value Alignment During LLM Post-Training",
    "summary": "As LLMs occupy an increasingly important role in society, they are more and more confronted with questions that require them not only to draw on their general knowledge but also to align with certain human value systems. Therefore, studying the alignment of LLMs with human values has become a crucial field of inquiry. Prior work, however, mostly focuses on evaluating the alignment of fully trained models, overlooking the training dynamics by which models learn to express human values. In this work, we investigate how and at which stage value alignment arises during the course of a model's post-training. Our analysis disentangles the effects of post-training algorithms and datasets, measuring both the magnitude and time of value drifts during training. Experimenting with Llama-3 and Qwen-3 models of different sizes and popular supervised fine-tuning (SFT) and preference optimization datasets and algorithms, we find that the SFT phase generally establishes a model's values, and subsequent preference optimization rarely re-aligns these values. Furthermore, using a synthetic preference dataset that enables controlled manipulation of values, we find that different preference optimization algorithms lead to different value alignment outcomes, even when preference data is held constant. Our findings provide actionable insights into how values are learned during post-training and help to inform data curation, as well as the selection of models and algorithms for preference optimization to improve model alignment to human values.",
    "keywords": [
      "LLM post-training",
      "value alignment",
      "supervised fine-tuning",
      "preference optimization",
      "training dynamics"
    ],
    "area": [
      "Large Language Model",
      "Deep Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-30T17:09:09.000Z",
    "download_time": "2025-11-03 12:07:32",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.26707\", \"arxiv_url\": \"https://arxiv.org/abs/2510.26707\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26707.png\", \"original_title\": \"Value Drifts: Tracing Value Alignment During LLM Post-Training\"}"
  },
  {
    "id": "2510.27684",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.27684",
    "title": "Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals",
    "summary": "Distribution Matching Distillation (DMD) distills score-based generative models into efficient one-step generators, without requiring a one-to-one correspondence with the sampling trajectories of their teachers. However, limited model capacity causes one-step distilled models underperform on complex generative tasks, e.g., synthesizing intricate object motions in text-to-video generation. Directly extending DMD to multi-step distillation increases memory usage and computational depth, leading to instability and reduced efficiency. While prior works propose stochastic gradient truncation as a potential solution, we observe that it substantially reduces the generation diversity of multi-step distilled models, bringing it down to the level of their one-step counterparts. To address these limitations, we propose Phased DMD, a multi-step distillation framework that bridges the idea of phase-wise distillation with Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model capacity. Phased DMD is built upon two key ideas: progressive distribution matching and score matching within subintervals. First, our model divides the SNR range into subintervals, progressively refining the model to higher SNR levels, to better capture complex distributions. Next, to ensure the training objective within each subinterval is accurate, we have conducted rigorous mathematical derivations. We validate Phased DMD by distilling state-of-the-art image and video generation models, including Qwen-Image (20B parameters) and Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD preserves output diversity better than DMD while retaining key generative capabilities. We will release our code and models.",
    "keywords": [
      "Distribution Matching Distillation",
      "Score Matching",
      "Generative Models",
      "Multi-step Distillation",
      "Mixture-of-Experts"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-10-31T17:55:10.000Z",
    "download_time": "2025-11-03 12:07:35",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.27684\", \"arxiv_url\": \"https://arxiv.org/abs/2510.27684\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27684.png\", \"original_title\": \"Phased DMD: Few-step Distribution Matching Distillation via Score\\n  Matching within Subintervals\"}"
  },
  {
    "id": "2510.27623",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.27623",
    "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning",
    "summary": "Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.",
    "keywords": [
      "Visual Backdoor Attacks",
      "MLLM Embodied Agents",
      "Contrastive Trigger Learning",
      "Multimodal Large Language Models",
      "Security Risk"
    ],
    "area": [
      "Multimodal",
      "AI Agent",
      "Computer Vision"
    ],
    "published_time": "2025-10-31T16:50:49.000Z",
    "download_time": "2025-11-03 12:07:36",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.27623\", \"arxiv_url\": \"https://arxiv.org/abs/2510.27623\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27623.png\", \"original_title\": \"Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\\n  Trigger Learning\"}"
  }
]