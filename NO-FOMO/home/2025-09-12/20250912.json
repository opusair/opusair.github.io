[
  {
    "id": "twitter_maxjaderberg_1966545934935789675",
    "source": "Twitter",
    "url": "https://x.com/maxjaderberg/status/1966545934935789675",
    "title": "maxjaderberg_Drug Design Engine",
    "summary": "A new multi-modal drug design engine has been developed, with the potential to revolutionize the future of health. This engine is capable of generating novel molecule designs tailored for various disease areas and indications. The ultimate goal is to enable the application of this technology to any target across any disease, facilitating continuous innovation in pharmaceutical research and development. The tweet also includes a link to watch an episode, suggesting a broader discussion or presentation of this groundbreaking work. This advancement signifies a significant step forward in leveraging AI for drug discovery and personalized medicine.",
    "keywords": [
      "Drug Design",
      "Multi-modal",
      "Molecule Design",
      "Health",
      "AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Industry News"
    ],
    "published_time": "2025-09-12 16:54:22",
    "download_time": "2025-09-12 18:34:17",
    "extra_info": "{\"username\": \"maxjaderberg\", \"tweet_id\": \"1966545934935789675\", \"retweet_count\": 0, \"reply_count\": 0, \"like_count\": 1, \"quote_count\": 0, \"view_count\": 359, \"is_reply\": true, \"language\": \"en\", \"bookmark_count\": 1}"
  },
  {
    "id": "twitter_natolambert_1966330861068165566",
    "source": "Twitter",
    "url": "https://x.com/natolambert/status/1966330861068165566",
    "title": "natolambert_RL to Prod Pipeline",
    "summary": "The tweet highlights the showcasing of a Reinforcement Learning (RL) to production pipeline as a significant example of continual learning. The author expresses surprise at this development, noting that such a progression would have been unexpected just twelve months prior. This suggests a rapid advancement in the practical application and integration of RL methodologies into live production environments, marking a notable shift in the field's capabilities and deployment strategies. The emphasis on a 'pipeline' implies a structured and potentially automated process for moving RL models from development to operational use, underscoring a move towards more dynamic and adaptive AI systems in real-world scenarios. This advancement could have broad implications for various industries relying on AI for continuous improvement and adaptation.",
    "keywords": [
      "Reinforcement Learning",
      "RL",
      "Production Pipeline",
      "Continual Learning",
      "AI Advancement"
    ],
    "area": [
      "Machine Learning",
      "Artificial Intelligence",
      "Research Progress"
    ],
    "published_time": "2025-09-12 02:39:44",
    "download_time": "2025-09-12 18:33:04",
    "extra_info": "{\"username\": \"natolambert\", \"tweet_id\": \"1966330861068165566\", \"retweet_count\": 27, \"reply_count\": 5, \"like_count\": 531, \"quote_count\": 0, \"view_count\": 74302, \"is_reply\": false, \"language\": \"en\", \"bookmark_count\": 237}"
  },
  {
    "id": "twitter_GaryMarcus_1966375813651272055",
    "source": "Twitter",
    "url": "https://x.com/GaryMarcus/status/1966375813651272055",
    "title": "GaryMarcus_Sora Physics Critique",
    "summary": "Gary Marcus, a prominent AI researcher, has shared an essay critiquing Sora, OpenAI's text-to-video model. Marcus suggests that Sora does not fundamentally grasp the principles of physics, a viewpoint he articulated in an essay written shortly after Sora's release. He notes that this perspective was not widely accepted or understood at the time of his writing. The tweet also mentions Rohan Paul, indicating a discussion or engagement with the AI community regarding Sora's capabilities and limitations. The shared link points to a more detailed analysis of his arguments concerning Sora's understanding of physical laws and its implications for generative AI development.",
    "keywords": [
      "Sora",
      "OpenAI",
      "Physics",
      "Generative AI",
      "Video Understanding",
      "AI Critique"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Video Understanding"
    ],
    "published_time": "2025-09-12 05:38:22",
    "download_time": "2025-09-12 18:32:17",
    "extra_info": "{\"username\": \"GaryMarcus\", \"tweet_id\": \"1966375813651272055\", \"retweet_count\": 0, \"reply_count\": 0, \"like_count\": 5, \"quote_count\": 0, \"view_count\": 571, \"is_reply\": true, \"language\": \"en\", \"bookmark_count\": 1}"
  },
  {
    "id": "twitter_ylecun_1966303762814795868",
    "source": "Twitter",
    "url": "https://x.com/ylecun/status/1966303762814795868",
    "title": "ylecun_AI Study Context",
    "summary": "This tweet from Yann LeCun highlights a study by AI at Meta, providing additional context to Fei-Fei Li's discussions on the limitations of Large Language Models (LLMs). The retweeted content suggests that the paper offers empirical evidence or analysis that supports or elaborates on the challenges and boundaries inherent in current LLM technology. This research likely delves into specific aspects of LLM performance, potential biases, or areas where their capabilities fall short, contributing to a deeper understanding of the field's current state and future research directions. The study aims to offer concrete data and insights to complement theoretical explanations of LLM constraints.",
    "keywords": [
      "AI at Meta",
      "LLM limitations",
      "Fei-Fei Li",
      "AI study",
      "LLMs"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-09-12 00:52:03",
    "download_time": "2025-09-12 18:30:39",
    "extra_info": "{\"username\": \"ylecun\", \"tweet_id\": \"1966303762814795868\", \"retweet_count\": 101, \"reply_count\": 16, \"like_count\": 542, \"quote_count\": 10, \"view_count\": 94987, \"is_reply\": false, \"language\": \"en\", \"bookmark_count\": 374}"
  },
  {
    "id": "twitter_polynoamial_1966527147469598794",
    "source": "Twitter",
    "url": "https://x.com/polynoamial/status/1966527147469598794",
    "title": "OpenAI Reasoning Models Advance",
    "summary": "The tweet highlights significant advancements in OpenAI's reasoning models, contrasting the capabilities of the o1-preview released a year ago with current models. The latest models can now process information for hours, browse the web, and write code, demonstrating a substantial leap in performance. The author expresses excitement about the potential for further improvements in reasoning capabilities over the next year, indicating a strong focus on pushing the boundaries of AI reasoning and functionality. This progress suggests a rapid development cycle within OpenAI, particularly in areas requiring complex cognitive tasks and extended operational capacity.",
    "keywords": [
      "OpenAI",
      "Reasoning Models",
      "AI Advancement",
      "Web Browsing",
      "Code Generation"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-09-12 15:39:42",
    "download_time": "2025-09-12 18:33:43",
    "extra_info": "{\"username\": \"polynoamial\", \"tweet_id\": \"1966527147469598794\", \"retweet_count\": 34, \"reply_count\": 30, \"like_count\": 707, \"quote_count\": 11, \"view_count\": 59367, \"is_reply\": false, \"language\": \"en\", \"bookmark_count\": 63}"
  },
  {
    "id": "twitter_rasbt_1966292818495991897",
    "source": "Twitter",
    "url": "https://x.com/rasbt/status/1966292818495991897",
    "title": "Qwen3 Next Expert Count",
    "summary": "The tweet highlights the recent release of Qwen3 Next, noting its exceptionally large number of experts and the implementation of a shared expert. The author, rasbt, expresses a positive sentiment, suggesting that their previously shared suggestions have been incorporated into this new version. This development points to significant advancements in the architecture and capabilities of the Qwen series, likely focusing on efficiency and performance through expert specialization. The mention of a shared expert could indicate a novel approach to parameter sharing or routing mechanisms within the model. Further details are available via the provided URL.",
    "keywords": [
      "Qwen3 Next",
      "Large Number of Experts",
      "Shared Expert",
      "AI Model",
      "Model Architecture"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-09-12 00:08:34",
    "download_time": "2025-09-12 18:32:43",
    "extra_info": "{\"username\": \"rasbt\", \"tweet_id\": \"1966292818495991897\", \"retweet_count\": 3, \"reply_count\": 2, \"like_count\": 41, \"quote_count\": 0, \"view_count\": 8424, \"is_reply\": true, \"language\": \"en\", \"bookmark_count\": 21}"
  },
  {
    "id": "hackernews_45219228",
    "source": "Hacker News",
    "url": "https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list",
    "title": "Qwen3-Next",
    "summary": "Qwen3-Next represents the forthcoming generation in the acclaimed Qwen series of large language models, spearheaded by Alibaba Cloud. Although comprehensive technical specifications and performance metrics are currently under wraps, the designation \"Next\" strongly indicates a significant leap forward from its predecessors. Industry expectations for such an advancement typically include substantial enhancements in areas like complex reasoning, multimodal integration, and computational efficiency. Furthermore, improvements in model robustness, ethical alignment, and reduced hallucination rates are often key objectives for new foundational models. This strategic development underscores the ongoing global race to innovate in artificial intelligence, promising to deliver more sophisticated tools for natural language understanding, content generation, and diverse AI applications. The introduction of Qwen3-Next is poised to influence the trajectory of generative AI research and practical deployment, offering new capabilities to the broader AI community.",
    "keywords": [
      "Large Language Model",
      "AI Research",
      "Generative AI",
      "Qwen",
      "Model Architecture",
      "Alibaba Cloud",
      "Natural Language Processing",
      "Foundational Models"
    ],
    "area": [
      "Large Language Model",
      "Artificial Intelligence",
      "Generative AI"
    ],
    "published_time": "2025-09-12 06:32:04",
    "download_time": "2025-09-12 19:11:49",
    "extra_info": "{\"score\": 473, \"by\": \"tosh\", \"descendants\": 184, \"story_id\": 45219228}"
  },
  {
    "id": "hackernews_45223726",
    "source": "Hacker News",
    "url": "https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/",
    "title": "VaultGemma: The most capable differentially private LLM",
    "summary": "Google Research has introduced VaultGemma, touted as the most capable differentially private Large Language Model to date. This development marks a significant milestone in the field of AI, addressing the critical challenge of balancing powerful language processing capabilities with robust data privacy protections. Differential privacy is a rigorous mathematical framework that ensures individual data points cannot be inferred from the model's outputs, thereby safeguarding sensitive user information during both training and inference. The emergence of VaultGemma suggests that it overcomes previous limitations where applying differential privacy often led to a substantial degradation in model performance. By achieving high capability alongside strong privacy guarantees, VaultGemma opens new avenues for deploying advanced LLMs in sensitive applications, such as healthcare, finance, and government, where data confidentiality is paramount. This innovation is expected to accelerate the adoption of privacy-preserving AI technologies and foster greater trust in large-scale AI systems.",
    "keywords": [
      "VaultGemma",
      "Differentially Private LLM",
      "Differential Privacy",
      "Large Language Model",
      "AI Privacy",
      "Google Research",
      "Machine Learning"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-09-12 16:14:50",
    "download_time": "2025-09-12 19:11:52",
    "extra_info": "{\"score\": 36, \"by\": \"meetpateltech\", \"descendants\": 7, \"story_id\": 45223726}"
  },
  {
    "id": "hackernews_45223102",
    "source": "Hacker News",
    "url": "https://github.com/Edison-Watch/open-edison",
    "title": "Show HN: An MCP Gateway to block the lethal trifecta",
    "summary": "Inspired by Simon Willison's 'lethal trifecta' concept, a new project introduces an MCP Gateway designed to enhance the security of Large Language Models (LLMs) interacting with multiple Multi-Capability Platform (MCP) servers. This gateway acts as an intermediary, inspecting the tools and requirements of each connected MCP server. It classifies these tools along three critical axes: private data access, untrusted content handling, and external communications. The primary function of the gateway is to identify and block potentially dangerous operations where all three 'trifecta' conditions are about to align within a single session. By intercepting such actions, the gateway prevents hazardous outcomes and prompts the LLM to issue a warning, thereby nudging the user to review the situation before any irreversible or harmful steps are taken. This proactive security measure aims to safeguard against unintended consequences arising from LLM interactions with diverse and potentially risky external services.",
    "keywords": [
      "LLM Security",
      "AI Safety",
      "Gateway Architecture",
      "Multi-Capability Platform (MCP)",
      "Tool Classification",
      "Data Privacy",
      "External Communication Control"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-12 15:22:00",
    "download_time": "2025-09-12 19:11:59",
    "extra_info": "{\"score\": 31, \"by\": \"76SlashDolphin\", \"descendants\": 14, \"story_id\": 45223102}"
  },
  {
    "id": "hackernews_45224219",
    "source": "Hacker News",
    "url": "https://arxiv.org/abs/2509.07604",
    "title": "K2-Think: A Parameter-Efficient Reasoning System",
    "summary": "The recently introduced K2-Think system represents a significant advancement in the field of artificial intelligence, specifically targeting the development of parameter-efficient reasoning capabilities. This novel approach aims to overcome the inherent computational and memory limitations often associated with large-scale AI models, which typically require vast numbers of parameters to achieve sophisticated reasoning. K2-Think proposes a methodology or architecture designed to enable complex logical inference and problem-solving with a substantially reduced parameter count. This efficiency is crucial for deploying advanced AI systems in environments with constrained resources, such as edge devices or mobile platforms, and for making high-performance AI more accessible. The system's focus on efficiency without compromising reasoning quality could lead to breakthroughs in areas requiring deep understanding and inference, such as scientific discovery, complex decision-making, and advanced AI agents. By optimizing parameter usage, K2-Think contributes to the ongoing effort to make AI models more sustainable, scalable, and practical for real-world applications, potentially setting a new benchmark for efficient AI reasoning.",
    "keywords": [
      "Parameter Efficiency",
      "Reasoning Systems",
      "AI Models",
      "Computational Efficiency",
      "Machine Learning"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-09-12 17:04:03",
    "download_time": "2025-09-12 19:12:05",
    "extra_info": "{\"score\": 4, \"by\": \"mgl\", \"descendants\": 2, \"story_id\": 45224219}"
  },
  {
    "id": "hackernews_45221103",
    "source": "Hacker News",
    "url": "https://synbol.github.io/Lumina-DiMOO/",
    "title": "Lumina-DiMOO: An open-source discrete multimodal diffusion model",
    "summary": "Lumina-DiMOO introduces an innovative open-source discrete multimodal diffusion model, representing a significant advancement in the field of generative artificial intelligence. This novel framework is engineered to effectively process and generate content across various data modalities, including but not limited to text, images, and potentially audio, all within a unified discrete latent space. By employing a discrete diffusion process, Lumina-DiMOO offers a distinct methodological approach to generative modeling, which could potentially address certain challenges inherent in continuous diffusion models, such as computational demands or the precise generation of inherently discrete data types. The open-source availability of Lumina-DiMOO is a pivotal aspect, designed to foster widespread community collaboration, accelerate further research, and facilitate the broader adoption and application of its advanced multimodal generative capabilities. This development is expected to have a substantial impact on domains requiring sophisticated content creation, synthesis, and understanding across diverse data formats, thereby pushing the boundaries of current AI capabilities in generation and interpretation.",
    "keywords": [
      "Diffusion Models",
      "Multimodal AI",
      "Generative AI",
      "Open Source",
      "Discrete Models",
      "AI Research"
    ],
    "area": [
      "Multimodal",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-09-12 11:45:58",
    "download_time": "2025-09-12 19:12:09",
    "extra_info": "{\"score\": 34, \"by\": \"SweetSoftPillow\", \"descendants\": 2, \"story_id\": 45221103}"
  },
  {
    "id": "hackernews_45223660",
    "source": "Hacker News",
    "url": "https://openai.com/index/openai-grove/",
    "title": "OpenAI Grove",
    "summary": "OpenAI Grove has been unveiled as a new strategic initiative or platform by OpenAI, marking a potential new direction in the company's ongoing efforts to advance artificial intelligence. While specific details regarding its precise functionalities and overarching goals are yet to be fully disclosed, the chosen name \"Grove\" strongly suggests an environment designed for growth, collaboration, and the cultivation of ideas or resources. It is widely anticipated that OpenAI Grove will serve as a dedicated ecosystem aimed at fostering significant advancements across various domains of AI. This could manifest as a collaborative framework for researchers and developers, offering enhanced access to cutting-edge AI tools, novel datasets, or establishing a community-driven platform for sharing insights and accelerating the development of next-generation AI models. The initiative is expected to reinforce OpenAI's commitment to pushing the boundaries of AI research and application, potentially facilitating more structured experimentation and the responsible deployment of their advanced technologies. Further official communications are awaited to fully elucidate the scope and transformative impact of OpenAI Grove on the broader artificial intelligence landscape and its community.",
    "keywords": [
      "Artificial Intelligence",
      "AI Development",
      "Innovation",
      "OpenAI",
      "Research Platform",
      "Collaboration",
      "AI Ecosystem"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Generative AI"
    ],
    "published_time": "2025-09-12 16:05:58",
    "download_time": "2025-09-12 19:12:24",
    "extra_info": "{\"score\": 45, \"by\": \"manveerc\", \"descendants\": 46, \"story_id\": 45223660}"
  },
  {
    "id": "github_deepseek-ai_DeepSeek-V3",
    "source": "GitHub",
    "url": "https://github.com/deepseek-ai/DeepSeek-V3",
    "title": "DeepSeek-V3",
    "summary": "DeepSeek-V3, developed by DeepSeek AI, signifies a significant advancement in the realm of artificial intelligence, particularly in large language models. Although the repository's README is concise, featuring primarily a brand logo and essential navigation links to its official homepage, a dedicated chat interface, and its Hugging Face profile, these elements collectively point towards a sophisticated AI offering. The prominent 'Chat' badge strongly suggests DeepSeek-V3 is engineered as a powerful conversational AI, capable of engaging in nuanced dialogue, understanding complex queries, and generating coherent and contextually relevant responses. Its availability on Hugging Face underscores its potential as a resource for researchers and developers, facilitating broader adoption and experimentation. This model is positioned to enhance capabilities in natural language processing, reasoning, and various generative AI applications, contributing to the ongoing evolution of intelligent systems and human-computer interaction.",
    "keywords": [
      "Large Language Model",
      "Generative AI",
      "Natural Language Processing",
      "Conversational AI",
      "Deep Learning",
      "AI Model"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-28T03:24:26Z",
    "download_time": "2025-09-12 18:34:44",
    "extra_info": "{\"stars\": 99231, \"forks\": 16194, \"language\": \"Python\", \"description\": null, \"topics\": []}"
  },
  {
    "id": "github_google-gemini_gemini-cli",
    "source": "GitHub",
    "url": "https://github.com/google-gemini/gemini-cli",
    "title": "Gemini CLI",
    "summary": "The Gemini CLI is an open-source artificial intelligence agent that seamlessly integrates Google's Gemini model directly into the terminal environment. This command-line interface tool provides a lightweight and highly efficient pathway for users to interact with the Gemini AI, offering the most direct access from their prompt to the underlying model. It is designed to empower developers and users by bringing advanced AI capabilities into their daily terminal workflows. Notable features include a generous free tier, allowing for up to 60 requests per minute and a total of 1,000 requests per day, which significantly lowers the barrier to entry for experimenting with and utilizing powerful AI. The Gemini CLI simplifies the process of leveraging sophisticated AI models, making it an invaluable resource for rapid prototyping, scripting, and general AI-powered tasks directly from the command line. Its open-source nature also encourages community contributions and further development.",
    "keywords": [
      "Gemini CLI",
      "AI Agent",
      "Command-line Interface",
      "Google Gemini",
      "Open Source",
      "Generative AI",
      "AI Model Access"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-09-12T17:49:37Z",
    "download_time": "2025-09-12 18:34:42",
    "extra_info": "{\"stars\": 74896, \"forks\": 7910, \"language\": \"TypeScript\", \"description\": \"An open-source AI agent that brings the power of Gemini directly into your terminal.\", \"topics\": [\"gemini\", \"gemini-api\"]}"
  },
  {
    "id": "github_browser-use_browser-use",
    "source": "GitHub",
    "url": "https://github.com/browser-use/browser-use",
    "title": "Enable AI to control your browser ğŸ¤–",
    "summary": "Browser Use is an innovative open-source project dedicated to enabling artificial intelligence agents to control and interact with web browsers. This platform provides the necessary tools and infrastructure for AI models to perform complex actions such as navigation, data input, and information extraction on websites. By bridging the gap between AI's analytical capabilities and the dynamic nature of the internet, Browser Use facilitates advanced web automation, intelligent data collection, and the execution of sophisticated AI-driven tasks. It is designed for developers and researchers looking to integrate autonomous AI control into their applications, paving the way for a new generation of intelligent web agents. The project aims to streamline workflows, enhance productivity, and unlock novel applications by allowing AI to operate seamlessly within the digital landscape, making it a crucial component for future AI-powered web interactions.",
    "keywords": [
      "AI Agent",
      "Browser Automation",
      "Web Control",
      "Artificial Intelligence",
      "Web Automation",
      "AI Tools"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Others"
    ],
    "published_time": "2025-09-12T03:11:03Z",
    "download_time": "2025-09-12 18:34:42",
    "extra_info": "{\"stars\": 69745, \"forks\": 8132, \"language\": \"Python\", \"description\": \"ğŸŒ Make websites accessible for AI agents. Automate tasks online with ease.\", \"topics\": [\"ai-agents\", \"ai-tools\", \"browser-automation\", \"browser-use\", \"llm\", \"playwright\", \"python\"]}"
  },
  {
    "id": "github_modelcontextprotocol_servers",
    "source": "GitHub",
    "url": "https://github.com/modelcontextprotocol/servers",
    "title": "Model Context Protocol servers",
    "summary": "This repository presents a comprehensive collection of reference implementations for the Model Context Protocol (MCP), a pivotal framework designed to empower Large Language Models (LLMs) with secure and controlled access to external tools and diverse data sources. It effectively showcases the inherent versatility and extensibility of MCP through practical server examples. Each implementation typically utilizes a dedicated MCP Software Development Kit (SDK), with robust support demonstrated across a wide array of programming languages, including C#, Go, Java, Kotlin, Python, Ruby, Rust, and Swift. The project underscores how MCP facilitates seamless integration of LLMs with various operational environments and functionalities, thereby enabling sophisticated AI agent capabilities. By providing these concrete, working examples, the repository serves as an invaluable resource for developers aiming to construct secure, reliable, and extensible interfaces for LLMs, significantly broadening their practical application beyond their intrinsic knowledge base. This initiative is fundamental for advancing the deployment of LLMs in complex, real-world scenarios demanding dynamic interaction with external systems and data.",
    "keywords": [
      "Model Context Protocol",
      "Large Language Models",
      "AI Agents",
      "SDK",
      "Reference Implementations",
      "Tool Access",
      "Data Sources",
      "Protocol Servers"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-12T01:17:43Z",
    "download_time": "2025-09-12 18:34:37",
    "extra_info": "{\"stars\": 67564, \"forks\": 7935, \"language\": \"TypeScript\", \"description\": \"Model Context Protocol Servers\", \"topics\": []}"
  },
  {
    "id": "github_xai-org_grok-1",
    "source": "GitHub",
    "url": "https://github.com/xai-org/grok-1",
    "title": "Grok-1",
    "summary": "This repository offers JAX example code designed for loading and executing the Grok-1 open-weights model, a formidable large language model boasting 314 billion parameters. It outlines the necessary steps for users to download the model checkpoint (`ckpt-0`) and run a Python script to perform inference on a test input. Key technical specifications of Grok-1 are detailed, including its Mixture of 8 Experts (MoE) architecture, which employs 2 experts per token, distributed across 64 layers with 48 attention heads. The documentation emphasizes the significant GPU memory required to run such a large model. While the current MoE layer implementation prioritizes correctness validation over efficiency, avoiding custom kernels, this project serves as a crucial resource for developers and researchers aiming to explore and deploy state-of-the-art, large-scale MoE language models within the JAX ecosystem.",
    "keywords": [
      "Grok-1",
      "JAX",
      "Large Language Model",
      "Mixture of Experts",
      "MoE",
      "Deep Learning",
      "Model Inference",
      "Open-weights model"
    ],
    "area": [
      "Large Language Model",
      "Deep Learning",
      "Machine Learning"
    ],
    "published_time": "2024-03-19T15:48:22Z",
    "download_time": "2025-09-12 18:34:49",
    "extra_info": "{\"stars\": 50495, \"forks\": 8365, \"language\": \"Python\", \"description\": \"Grok open release\", \"topics\": []}"
  },
  {
    "id": "github_anthropics_claude-code",
    "source": "GitHub",
    "url": "https://github.com/anthropics/claude-code",
    "title": "Claude Code",
    "summary": "Claude Code is an innovative agentic coding tool designed to significantly enhance developer productivity by integrating AI capabilities directly within the terminal environment. This intelligent assistant possesses a deep understanding of a user's codebase, enabling it to streamline development workflows through natural language commands. Its core functionalities include executing routine coding tasks, providing clear and concise explanations for complex code segments, and efficiently managing Git operations. Claude Code offers versatile application scenarios, functioning seamlessly within the terminal, popular Integrated Development Environments (IDEs), and even on GitHub, where users can tag `@claude` for direct assistance. By automating repetitive actions and offering on-demand code insights, Claude Code aims to accelerate coding processes, reduce manual effort, and improve overall code comprehension. This makes it a valuable asset for modern software development teams looking to optimize their practices and integrate advanced AI-powered assistance into their daily routines, ultimately fostering faster and more efficient development cycles.",
    "keywords": [
      "AI Agent",
      "Coding Tool",
      "Natural Language Processing",
      "Code Explanation",
      "Git Workflow Automation",
      "Developer Productivity",
      "Terminal Application"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-09-12T01:19:27Z",
    "download_time": "2025-09-12 18:35:01",
    "extra_info": "{\"stars\": 33230, \"forks\": 2039, \"language\": \"TypeScript\", \"description\": \"Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.\", \"topics\": []}"
  },
  {
    "id": "2509.09614",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.09614",
    "title": "LoCoBenchï¼šå¤æ‚è½¯ä»¶å·¥ç¨‹ä¸­é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†æµ‹è¯•",
    "summary": "ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ°æ•°ç™¾ä¸‡ä¸ªä»¤ç‰Œçš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œä¸ºå¤æ‚çš„ä»£ç ç†è§£å’Œè½¯ä»¶å¼€å‘è¯„ä¼°åˆ›é€ äº†æ–°çš„æœºä¼šã€‚æˆ‘ä»¬æå‡ºäº† LoCoBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºåœ¨çœŸå®ã€å¤æ‚çš„è½¯ä»¶å¼€å‘åœºæ™¯ä¸­è¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»¼åˆåŸºå‡†ã€‚ä¸ä¸“æ³¨äºå•å‡½æ•°å®Œæˆæˆ–çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡çš„ç°æœ‰ä»£ç è¯„ä¼°åŸºå‡†ä¸åŒï¼ŒLoCoBench è§£å†³äº†é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›çš„å…³é”®è¯„ä¼°ç©ºç™½ï¼Œè¿™äº›èƒ½åŠ›éœ€è¦ç†è§£æ•´ä¸ªä»£ç åº“ã€è·¨å¤šä¸ªæ–‡ä»¶è¿›è¡Œæ¨ç†ä»¥åŠåœ¨å¤§è§„æ¨¡è½¯ä»¶ç³»ç»Ÿä¸­ä¿æŒæ¶æ„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„åŸºå‡†æä¾›äº† 8,000 ä¸ªè¯„ä¼°åœºæ™¯ï¼Œè¿™äº›åœºæ™¯ç³»ç»Ÿåœ°ç”Ÿæˆè‡ª 10 ç§ç¼–ç¨‹è¯­è¨€ï¼Œä¸Šä¸‹æ–‡é•¿åº¦è·¨è¶Š 10K åˆ° 1M ä»¤ç‰Œï¼Œè¿™æ˜¯ä¸€ä¸ª 100 å€çš„å˜åŒ–ï¼Œèƒ½å¤Ÿç²¾ç¡®è¯„ä¼°çœŸå®è½¯ä»¶å¼€å‘ç¯å¢ƒä¸­é•¿ä¸Šä¸‹æ–‡æ€§èƒ½çš„ä¸‹é™ã€‚LoCoBench å¼•å…¥äº† 8 ä¸ªä»»åŠ¡ç±»åˆ«ï¼Œæ•æ‰äº†åŸºæœ¬é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼šæ¶æ„ç†è§£ã€è·¨æ–‡ä»¶é‡æ„ã€å¤šä¼šè¯å¼€å‘ã€é”™è¯¯è°ƒæŸ¥ã€åŠŸèƒ½å®ç°ã€ä»£ç ç†è§£ã€é›†æˆæµ‹è¯•å’Œå®‰å…¨åˆ†æã€‚é€šè¿‡ä¸€ä¸ª 5 é˜¶æ®µçš„ç®¡é“ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„åœºæ™¯ï¼ŒæŒ‘æˆ˜å¤§å‹è¯­è¨€æ¨¡å‹ä»¥å‰æ‰€æœªæœ‰çš„è§„æ¨¡å¯¹å¤æ‚ä»£ç åº“è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å« 4 ä¸ªç»´åº¦ä¸Šçš„ 17 ä¸ªæŒ‡æ ‡ï¼Œå…¶ä¸­åŒ…æ‹¬ 8 ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶ç»“åˆæˆä¸€ä¸ª LoCoBench åˆ†æ•°ï¼ˆLCBSï¼‰ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„è¯„ä¼°æ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œè¡¨æ˜å¤æ‚è½¯ä»¶å¼€å‘ä¸­çš„é•¿ä¸Šä¸‹æ–‡ç†è§£æ˜¯ä¸€ä¸ªå°šæœªè§£å†³çš„é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦æ›´å¤šå…³æ³¨ã€‚LoCoBench å·²å‘å¸ƒäºï¼šhttps://github.com/SalesforceAIResearch/LoCoBenchã€‚",
    "keywords": [
      "é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹",
      "è½¯ä»¶å·¥ç¨‹",
      "åŸºå‡†æµ‹è¯•",
      "ä»£ç ç†è§£",
      "æ€§èƒ½è¯„ä¼°"
    ],
    "area": [
      "äººå·¥æ™ºèƒ½",
      "æœºå™¨å­¦ä¹ ",
      "å¤§æ¨¡å‹"
    ],
    "published_time": "2025-09-11T16:55:04.000Z",
    "download_time": "2025-09-12 11:36:21",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.09614\", \"arxiv_url\": \"https://arxiv.org/abs/2509.09614\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09614.png\", \"original_title\": \"LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\\n  Software Engineering\"}"
  },
  {
    "id": "2509.05739",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.05739",
    "title": "æ¨ç†å¼•å…¥äº†æ–°å‹æŠ•æ¯’æ”»å‡»ï¼Œä½†åŒæ—¶ä¹Ÿå¢åŠ äº†å…¶å¤æ‚æ€§",
    "summary": "æ—©æœŸé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°æ®æŠ•æ¯’æ”»å‡»ç ”ç©¶è¡¨æ˜ï¼Œæ³¨å…¥åé—¨æ˜¯ç›¸å¯¹å®¹æ˜“çš„ã€‚ç„¶è€Œï¼Œè¿‘æœŸLLMså¢åŠ äº†é€æ­¥æ¨ç†èƒ½åŠ›ï¼Œè¿™æ‰©å±•äº†æ”»å‡»é¢ï¼Œä½¿å…¶åŒ…å«äº†ä¸­é—´çš„æ€ç»´é“¾ï¼ˆCoTï¼‰åŠå…¶å°†é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜çš„å›ºæœ‰ç‰¹æ€§ã€‚åˆ©ç”¨è¿™äº›å‘é‡è¿›è¡Œæ›´éšè”½çš„æŠ•æ¯’ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œåˆ†è§£å¼æ¨ç†æŠ•æ¯’â€ï¼Œå…¶ä¸­æ”»å‡»è€…ä»…ä¿®æ”¹æ¨ç†è·¯å¾„ï¼Œè€Œä¿æŒæç¤ºå’Œæœ€ç»ˆç­”æ¡ˆçš„æ¸…æ´ï¼Œå¹¶å°†è§¦å‘å™¨åˆ†æ•£åˆ°å¤šä¸ªå•ç‹¬æ— å®³çš„ç»„ä»¶ä¸­ã€‚ä»¤äººç€è¿·çš„æ˜¯ï¼Œå°½ç®¡æ³¨å…¥è¿™äº›åˆ†è§£å¼æŠ•æ¯’ä»ç„¶å¯è¡Œï¼Œä½†è¦å¯é åœ°æ¿€æ´»å®ƒä»¬ä»¥æ”¹å˜æœ€ç»ˆç­”æ¡ˆï¼ˆè€Œä¸ä»…ä»…æ˜¯CoTï¼‰å´å‡ºå¥‡åœ°å›°éš¾ã€‚è¿™ç§å›°éš¾æºäºæ¨¡å‹é€šå¸¸èƒ½å¤Ÿä»åœ¨å…¶æ€ç»´è¿‡ç¨‹ä¸­è¢«æ¿€æ´»çš„åé—¨ä¸­æ¢å¤ã€‚æœ€ç»ˆï¼Œä¼¼ä¹ä¸€ç§æ–°å…´çš„åé—¨é²æ£’æ€§æ­£æºäºè¿™äº›å…ˆè¿›LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä»¥åŠæ¨ç†ä¸æœ€ç»ˆç­”æ¡ˆç”Ÿæˆä¹‹é—´çš„æ¶æ„åˆ†ç¦»ã€‚",
    "keywords": [
      "å¤§å‹è¯­è¨€æ¨¡å‹",
      "æ•°æ®æŠ•æ¯’æ”»å‡»",
      "æ¨ç†èƒ½åŠ›",
      "æ€ç»´é“¾",
      "åé—¨é²æ£’æ€§"
    ],
    "area": [
      "å¤§æ¨¡å‹",
      "è‡ªç„¶è¯­è¨€å¤„ç†",
      "æœºå™¨å­¦ä¹ "
    ],
    "published_time": "2025-09-06T15:06:18.000Z",
    "download_time": "2025-09-12 11:36:27",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.05739\", \"arxiv_url\": \"https://arxiv.org/abs/2509.05739\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05739.png\", \"original_title\": \"Reasoning Introduces New Poisoning Attacks Yet Makes Them More\\n  Complicated\"}"
  },
  {
    "id": "2509.09332",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.09332",
    "title": "OmniEVAï¼šé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”ä¸‰ç»´æ¥åœ°å’Œå…·èº«æ„ŸçŸ¥æ¨ç†å®ç°çš„å…·èº«å¤šåŠŸèƒ½è§„åˆ’å™¨",
    "summary": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå…·èº«æ™ºèƒ½å¼€è¾Ÿäº†æ–°æœºé‡ï¼Œå®ç°äº†å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œäº¤äº’ï¼Œä»¥åŠè¿ç»­çš„ç©ºé—´å†³ç­–ã€‚ç„¶è€Œï¼Œå½“å‰åŸºäºMLLMçš„å…·èº«ç³»ç»Ÿé¢ä¸´ä¸¤ä¸ªå…³é”®é™åˆ¶ã€‚é¦–å…ˆæ˜¯å‡ ä½•é€‚åº”æ€§å·®è·ï¼šä»…åœ¨2Dè¾“å…¥ä¸Šè®­ç»ƒæˆ–é€šè¿‡ç¡¬ç¼–ç 3Då‡ ä½•æ³¨å…¥çš„æ¨¡å‹ï¼Œè¦ä¹ˆç©ºé—´ä¿¡æ¯ä¸è¶³ï¼Œè¦ä¹ˆ2Dæ³›åŒ–å—é™ï¼Œå¯¼è‡´åœ¨å…·æœ‰ä¸åŒç©ºé—´éœ€æ±‚çš„ä»»åŠ¡ä¸­é€‚åº”æ€§å·®ã€‚å…¶æ¬¡æ˜¯å…·èº«çº¦æŸå·®è·ï¼šä»¥å¾€çš„å·¥ä½œå¸¸å¸¸å¿½ç•¥çœŸå®æœºå™¨äººçš„ç‰©ç†çº¦æŸå’Œèƒ½åŠ›ï¼Œå¯¼è‡´ä»»åŠ¡è§„åˆ’åœ¨ç†è®ºä¸Šæœ‰æ•ˆä½†åœ¨å®è·µä¸­ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniEVAâ€”â€”ä¸€ä¸ªå…·èº«å¤šåŠŸèƒ½è§„åˆ’å™¨ï¼Œå®ƒé€šè¿‡ä¸¤é¡¹å…³é”®åˆ›æ–°å®ç°äº†å…ˆè¿›çš„å…·èº«æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼š(1) ä»»åŠ¡è‡ªé€‚åº”3Dæ¥åœ°æœºåˆ¶ï¼Œå¼•å…¥é—¨æ§è·¯ç”±å™¨æ ¹æ®ä¸Šä¸‹æ–‡éœ€æ±‚æ‰§è¡Œ3Dèåˆçš„æ˜¾å¼é€‰æ‹©æ€§è°ƒèŠ‚ï¼Œä»è€Œä¸ºå¤šæ ·åŒ–çš„å…·èº«ä»»åŠ¡å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„3Dæ¥åœ°ã€‚(2) å…·èº«æ„ŸçŸ¥æ¨ç†æ¡†æ¶ï¼Œå°†ä»»åŠ¡ç›®æ ‡å’Œå…·èº«çº¦æŸå…±åŒçº³å…¥æ¨ç†å¾ªç¯ï¼Œä»è€Œäº§ç”Ÿæ—¢é¢å‘ç›®æ ‡åˆå¯æ‰§è¡Œçš„è§„åˆ’å†³ç­–ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniEVAä¸ä»…å®ç°äº†æœ€å…ˆè¿›çš„é€šç”¨å…·èº«æ¨ç†æ€§èƒ½ï¼Œè€Œä¸”åœ¨å¹¿æ³›çš„ä¸‹æ¸¸åœºæ™¯ä¸­ä¹Ÿå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚å¯¹ä¸€ç³»åˆ—æå‡ºçš„å…·èº«åŸºå‡†ï¼ˆåŒ…æ‹¬åŸºæœ¬ä»»åŠ¡å’Œå¤åˆä»»åŠ¡ï¼‰çš„è¯„ä¼°è¯å®äº†å…¶é²æ£’å’Œå¤šåŠŸèƒ½çš„è§„åˆ’èƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://omnieva.github.io",
    "keywords": [
      "å…·èº«æ™ºèƒ½",
      "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹",
      "ä»»åŠ¡è§„åˆ’",
      "3Dæ¥åœ°",
      "å…·èº«æ„ŸçŸ¥æ¨ç†"
    ],
    "area": [
      "äººå·¥æ™ºèƒ½",
      "å¤šæ¨¡æ€",
      "å¤§æ¨¡å‹"
    ],
    "published_time": "2025-09-11T10:32:22.000Z",
    "download_time": "2025-09-12 11:36:30",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.09332\", \"arxiv_url\": \"https://arxiv.org/abs/2509.09332\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09332.png\", \"original_title\": \"OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\\n  Embodiment-aware Reasoning\"}"
  },
  {
    "id": "2509.09265",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.09265",
    "title": "é©¾é©­ä¸ç¡®å®šæ€§ï¼šç”¨äºé•¿å‘¨æœŸLLMæ™ºèƒ½ä½“çš„ç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦",
    "summary": "åœ¨é•¿å‘¨æœŸä»»åŠ¡ä¸­ï¼Œè¿‘æœŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“é¢ä¸´ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼šç¨€ç–çš„ã€åŸºäºç»“æœçš„å¥–åŠ±ä½¿å¾—éš¾ä»¥å¯¹ä¸­é—´æ­¥éª¤è¿›è¡Œå½’å› ã€‚ä»¥å¾€çš„æ–¹æ³•ä¸»è¦é€šè¿‡åˆ›å»ºå¯†é›†çš„å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼å­¦ä¹ ï¼Œè¿™åŒ…æ‹¬é€†å¼ºåŒ–å­¦ä¹ ç­‰ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œæˆ–ä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æä¾›é€æ­¥åé¦ˆã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºLLMå­¦ä¹ åŠ¨æ€ä¸­çš„ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼šç­–ç•¥æ¢¯åº¦çš„å¹…åº¦ä¸ç†µå›ºæœ‰åœ°è€¦åˆï¼Œè¿™å¯¼è‡´å¯¹ç¡®ä¿¡çš„æ­£ç¡®åŠ¨ä½œè¿›è¡Œä½æ•ˆçš„å°å¹…æ›´æ–°ï¼Œå¹¶å¯èƒ½ä½¿ä¸ç¡®å®šåŠ¨ä½œçš„å¤§å¹…æ›´æ–°å˜å¾—ä¸ç¨³å®šã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦ï¼ˆEMPGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ ¹æ®åˆ†æ­¥ä¸ç¡®å®šæ€§å’Œæœ€ç»ˆä»»åŠ¡ç»“æœé‡æ–°æ ¡å‡†å­¦ä¹ ä¿¡å·çš„æ¡†æ¶ã€‚EMPGæ”¾å¤§å¯¹ç¡®ä¿¡æ­£ç¡®åŠ¨ä½œçš„æ›´æ–°ï¼Œæƒ©ç½šç¡®ä¿¡çš„é”™è¯¯ï¼Œå¹¶å‡å¼±æ¥è‡ªä¸ç¡®å®šæ­¥éª¤çš„æ›´æ–°ä»¥ç¨³å®šæ¢ç´¢ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªæœªæ¥æ¸…æ™°åº¦å¥–åŠ±é¡¹ï¼Œé¼“åŠ±æ™ºèƒ½ä½“å¯»æ‰¾æ›´å¯é¢„æµ‹çš„è§£å†³æ–¹æ¡ˆè·¯å¾„ã€‚é€šè¿‡åœ¨WebShopã€ALFWorldå’ŒDeep Searchè¿™ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ™ºèƒ½ä½“ä»»åŠ¡ä¸Šçš„å…¨é¢å®éªŒï¼Œæˆ‘ä»¬è¯æ˜EMPGå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„ç­–ç•¥æ¢¯åº¦åŸºçº¿æ–¹æ³•ã€‚",
    "keywords": [
      "å¤§å‹è¯­è¨€æ¨¡å‹",
      "æ™ºèƒ½ä½“",
      "ç­–ç•¥æ¢¯åº¦",
      "ç†µè°ƒåˆ¶",
      "é•¿å‘¨æœŸä»»åŠ¡"
    ],
    "area": [
      "å¤§æ¨¡å‹",
      "æ™ºèƒ½ä½“",
      "æœºå™¨å­¦ä¹ "
    ],
    "published_time": "2025-09-11T08:50:01.000Z",
    "download_time": "2025-09-12 11:36:28",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.09265\", \"arxiv_url\": \"https://arxiv.org/abs/2509.09265\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09265.png\", \"original_title\": \"Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\\n  Long-Horizon LLM Agents\"}"
  },
  {
    "id": "2509.09595",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.09595",
    "title": "Kling-Avatarï¼šåŸºäºå¤šæ¨¡æ€æŒ‡ä»¤çš„çº§è”é•¿æ—¶ç¨‹è™šæ‹Ÿå½¢è±¡åŠ¨ç”»åˆæˆ",
    "summary": "è¿‘æœŸéŸ³é¢‘é©±åŠ¨çš„è™šæ‹Ÿå½¢è±¡è§†é¢‘ç”ŸæˆæŠ€æœ¯åœ¨è§†å¬çœŸå®æ„Ÿæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å°†æŒ‡ä»¤æ¡ä»¶å¤„ç†ä¸ºä»…ç”±å£°å­¦æˆ–è§†è§‰çº¿ç´¢é©±åŠ¨çš„ä½çº§è·Ÿè¸ªï¼Œæœªèƒ½å¯¹æŒ‡ä»¤æ‰€ä¼ è¾¾çš„äº¤æµç›®çš„è¿›è¡Œå»ºæ¨¡ã€‚è¿™ä¸€å±€é™æ€§æŸå®³äº†å…¶å™äº‹è¿è´¯æ€§å’Œè§’è‰²è¡¨ç°åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬å¼•å…¥äº†Kling-Avatarï¼Œä¸€ä¸ªæ–°é¢–çš„çº§è”æ¡†æ¶ï¼Œå®ƒå°†å¤šæ¨¡æ€æŒ‡ä»¤ç†è§£ä¸ç…§ç‰‡çº§çœŸå®æ„Ÿè‚–åƒç”Ÿæˆç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæµæ°´çº¿ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯¼æ¼”ï¼Œæ ¹æ®å¤šæ ·åŒ–çš„æŒ‡ä»¤ä¿¡å·ç”Ÿæˆè“å›¾è§†é¢‘ï¼Œä»è€Œæ§åˆ¶è§’è‰²åŠ¨ä½œå’Œæƒ…æ„Ÿç­‰é«˜çº§è¯­ä¹‰ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåœ¨è“å›¾å…³é”®å¸§çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨é¦–å°¾å¸§ç­–ç•¥å¹¶è¡Œç”Ÿæˆå¤šä¸ªå­ç‰‡æ®µã€‚è¿™ç§ä»å…¨å±€åˆ°å±€éƒ¨çš„æ¡†æ¶åœ¨å¿ å®ç¼–ç å¤šæ¨¡æ€æŒ‡ä»¤èƒŒåé«˜çº§æ„å›¾çš„åŒæ—¶ï¼Œä¿ç•™äº†ç»†ç²’åº¦ç»†èŠ‚ã€‚æˆ‘ä»¬çš„å¹¶è¡Œæ¶æ„è¿˜æ”¯æŒå¿«é€Ÿç¨³å®šåœ°ç”Ÿæˆé•¿æ—¶ç¨‹è§†é¢‘ï¼Œä½¿å…¶é€‚ç”¨äºæ•°å­—äººç›´æ’­å’Œè§†é¢‘åšå®¢ç­‰å®é™…åº”ç”¨ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«375ä¸ªç²¾é€‰æ ·æœ¬çš„åŸºå‡†ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„æŒ‡ä»¤å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒKling-Avatarèƒ½å¤Ÿç”Ÿæˆé«˜è¾¾1080på’Œ48 fpsçš„ç”ŸåŠ¨ã€æµç•…ã€é•¿æ—¶ç¨‹è§†é¢‘ï¼Œåœ¨å”‡å½¢åŒæ­¥å‡†ç¡®æ€§ã€æƒ…æ„Ÿå’ŒåŠ¨æ€è¡¨ç°åŠ›ã€æŒ‡ä»¤å¯æ§æ€§ã€èº«ä»½ä¿æŒå’Œè·¨é¢†åŸŸæ³›åŒ–æ–¹é¢å–å¾—äº†å“è¶Šæ€§èƒ½ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†Kling-Avatarä½œä¸ºè¯­ä¹‰æ¥åœ°ã€é«˜ä¿çœŸéŸ³é¢‘é©±åŠ¨è™šæ‹Ÿå½¢è±¡åˆæˆçš„æ–°åŸºå‡†ã€‚",
    "keywords": [
      "Kling-Avatar",
      "å¤šæ¨¡æ€æŒ‡ä»¤",
      "è™šæ‹Ÿå½¢è±¡åŠ¨ç”»",
      "é•¿æ—¶ç¨‹è§†é¢‘ç”Ÿæˆ",
      "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹"
    ],
    "area": [
      "å¤šæ¨¡æ€",
      "ç”Ÿæˆå¼AI",
      "å¤§æ¨¡å‹"
    ],
    "published_time": "2025-09-11T16:34:57.000Z",
    "download_time": "2025-09-12 11:36:30",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.09595\", \"arxiv_url\": \"https://arxiv.org/abs/2509.09595\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09595.png\", \"original_title\": \"Kling-Avatar: Grounding Multimodal Instructions for Cascaded\\n  Long-Duration Avatar Animation Synthesis\"}"
  },
  {
    "id": "2509.09680",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.09680",
    "title": "FLUX-Reason-6M & PRISM-Benchï¼šç™¾ä¸‡çº§æ–‡æœ¬åˆ°å›¾åƒæ¨ç†æ•°æ®é›†ä¸ç»¼åˆåŸºå‡†",
    "summary": "å¼€æºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„è¿›æ­¥ä¸€ç›´å—é™äºç¼ºä¹å¤§è§„æ¨¡ã€ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ•°æ®é›†å’Œå…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼Œå¯¼è‡´ä¸é¢†å…ˆçš„é—­æºç³»ç»Ÿç›¸æ¯”å­˜åœ¨æ€§èƒ½å·®è·ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FLUX-Reason-6Må’ŒPRISM-Benchï¼ˆç²¾ç¡®é²æ£’å›¾åƒåˆæˆæµ‹é‡åŸºå‡†ï¼‰ã€‚FLUX-Reason-6Mæ˜¯ä¸€ä¸ªåºå¤§çš„æ•°æ®é›†ï¼ŒåŒ…å«600ä¸‡å¼ é«˜è´¨é‡çš„FLUXç”Ÿæˆå›¾åƒå’Œ2000ä¸‡æ¡åŒè¯­ï¼ˆè‹±è¯­å’Œä¸­æ–‡ï¼‰æè¿°ï¼Œä¸“é—¨ç”¨äºæ•™æˆå¤æ‚æ¨ç†ã€‚å›¾åƒæ ¹æ®å…­ä¸ªå…³é”®ç‰¹å¾è¿›è¡Œç»„ç»‡ï¼šæƒ³è±¡åŠ›ã€å®ä½“ã€æ–‡æœ¬æ¸²æŸ“ã€é£æ ¼ã€æƒ…æ„Ÿå’Œæ„å›¾ï¼Œå¹¶è®¾è®¡äº†æ˜ç¡®çš„ç”Ÿæˆæ€ç»´é“¾ï¼ˆGCoTï¼‰æ¥æä¾›å›¾åƒç”Ÿæˆæ­¥éª¤çš„è¯¦ç»†åˆ†è§£ã€‚æ•´ä¸ªæ•°æ®æ•´ç†è€—è´¹äº†15,000ä¸ªA100 GPUå¤©ï¼Œä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªå¤§å‹å·¥ä¸šå®éªŒå®¤ä¹‹å¤–å‰æ‰€æœªæœ‰çš„èµ„æºã€‚PRISM-Benchæä¾›äº†ä¸€ä¸ªæ–°é¢–çš„è¯„ä¼°æ ‡å‡†ï¼ŒåŒ…å«ä¸ƒä¸ªä¸åŒçš„èµ›é“ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªä½¿ç”¨GCoTçš„è‰°å·¨é•¿æ–‡æœ¬æŒ‘æˆ˜ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œå®ƒåˆ©ç”¨å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æç¤º-å›¾åƒå¯¹é½å’Œå›¾åƒç¾å­¦è¿›è¡Œç»†è‡´çš„ã€ä¸äººç±»å¯¹é½çš„è¯„ä¼°ã€‚æˆ‘ä»¬å¯¹PRISM-Benchä¸Š19ä¸ªé¢†å…ˆæ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ­ç¤ºäº†å…³é”®çš„æ€§èƒ½å·®è·ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ”¹è¿›çš„å…·ä½“é¢†åŸŸã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†å’Œè¯„ä¼°ä»£ç å·²å‘å¸ƒï¼Œä»¥å‚¬åŒ–ä¸‹ä¸€æ³¢é¢å‘æ¨ç†çš„T2Iç”Ÿæˆã€‚é¡¹ç›®é¡µé¢ï¼šhttps://flux-reason-6m.github.io/ã€‚",
    "keywords": [
      "æ–‡æœ¬åˆ°å›¾åƒ",
      "æ¨ç†æ•°æ®é›†",
      "è¯„ä¼°åŸºå‡†",
      "ç”Ÿæˆæ€ç»´é“¾",
      "å¤šæ¨¡æ€"
    ],
    "area": [
      "ç”Ÿæˆå¼AI",
      "å¤šæ¨¡æ€",
      "äººå·¥æ™ºèƒ½"
    ],
    "published_time": "2025-09-11T17:59:59.000Z",
    "download_time": "2025-09-12 11:36:32",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.09680\", \"arxiv_url\": \"https://arxiv.org/abs/2509.09680\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09680.png\", \"original_title\": \"FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\\n  Dataset and Comprehensive Benchmark\"}"
  }
]