<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-16</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-12-16</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>GPT Image 1.5</h2>
                <span class="published-time">Published: 2025-12-16 18:07:07</span>
                
                <p class="summary">OpenAI has unveiled "GPT Image 1.5," signaling a notable progression in its multimodal artificial intelligence offerings. This new release, though detailed specifications are anticipated, is strongly indicative of an enhanced model designed to improve capabilities in image generation, comprehensive image understanding, or a more deeply integrated vision-language processing framework within the overarching GPT architecture. The introduction of "GPT Image 1.5" suggests a strategic evolution from prior models like DALL-E, aiming to deliver more sophisticated visual processing directly within a unified generative pre-trained transformer. This advancement is poised to enable more nuanced and contextually aware interactions across both textual and visual data modalities. The initiative underscores OpenAI's ongoing commitment to pushing the frontiers of generative AI, developing more versatile and robust tools for a wide array of creative, analytical, and practical applications that demand high-fidelity understanding and generation across different data types. This iteration holds the potential to significantly streamline and enrich user experiences in various domains, from advanced content creation to complex data interpretation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>GPT</span><span>Image Generation</span><span>Multimodal AI</span><span>OpenAI</span><span>Generative AI</span><span>Computer Vision</span><span>AI Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Multimodal</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://openai.com/index/new-chatgpt-images-is-here/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SHARP, an approach to photorealistic view synthesis from a single image</h2>
                <span class="published-time">Published: 2025-12-16 04:06:51</span>
                
                <p class="summary">SHARP introduces a novel approach to photorealistic view synthesis, enabling the generation of high-fidelity, novel views from just a single input image. This method significantly advances the field of computer vision by tackling the complex challenge of reconstructing detailed 3D scene information and rendering it from arbitrary viewpoints with remarkable realism. The technique likely leverages advanced neural rendering or implicit neural representation architectures to infer geometric and appearance properties that allow for consistent and convincing viewpoint changes. Its capability to produce photorealistic results from minimal input has broad implications for applications in virtual reality, augmented reality, content creation, and 3D modeling, reducing the need for extensive multi-view datasets or intricate 3D scans. This breakthrough demonstrates progress in synthesizing complex visual data, pushing the boundaries of what's achievable with single-image inference.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>View Synthesis</span><span>Photorealism</span><span>Single Image Reconstruction</span><span>Neural Rendering</span><span>Computer Graphics</span><span>3D Reconstruction</span><span>Deep Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Deep Learning</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://apple.github.io/ml-sharp/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Solving the ~95% legislative coverage gap using LLM's</h2>
                <span class="published-time">Published: 2025-12-16 14:39:59</span>
                
                <p class="summary">Jacek, the solo founder of Lustra, has developed a digital public infrastructure to address the significant legislative coverage gap, where approximately 95% of legislation goes unnoticed due to the complexity of raw legal texts. The platform aims to provide insightful, unbiased information by utilizing Large Language Models (LLMs), specifically Vertex AI, to ingest and sterilize raw legislative bills (PDF/XML) from US and Polish APIs, stripping away political spin and presenting data in a strict JSON format. Lustra incorporates a "Civic Algorithm" that sorts content based on user votes, fostering community-driven engagement. Additionally, it features "Civic Projects," an incubator for citizen legislation where user-submitted drafts are vetted using AI scoring and displayed alongside government bills. The technical foundation of Lustra includes Flutter for a unified web and mobile frontend, backed by Firebase and Google Cloud Run for its backend infrastructure.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLMs</span><span>Vertex AI</span><span>Legislative Analysis</span><span>Civic Tech</span><span>Firebase</span><span>Google Cloud Run</span><span>Flutter</span><span>API Parsing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://lustra.news/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AI is wiping out entry-level tech jobs, leaving graduates stranded</h2>
                <span class="published-time">Published: 2025-12-16 17:37:41</span>
                
                <p class="summary">The proliferation of artificial intelligence technologies is profoundly transforming the entry-level job market across the tech sector, creating significant hurdles for recent university graduates. A growing body of evidence suggests a marked decline in positions traditionally open to individuals new to the industry, as AI-driven automation solutions increasingly take over tasks that previously required human input. This paradigm shift is fostering a highly competitive environment, leaving many qualified graduates struggling to secure their initial professional roles despite their academic achievements and specialized training. The situation has prompted widespread concern regarding AI's enduring impact on employment dynamics, particularly for those entering the workforce. Industry analysts and educators are increasingly calling for a fundamental reassessment of current educational frameworks and professional development programs. The objective is to foster skills that empower future graduates to effectively collaborate with, rather than be displaced by, advanced AI systems, thereby adapting to the rapidly evolving demands of an AI-centric economy.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Artificial Intelligence</span><span>Job Displacement</span><span>Entry-Level Tech Jobs</span><span>Workforce Automation</span><span>Graduate Employment</span><span>Tech Industry Impact</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://restofworld.org/2025/engineering-graduates-ai-job-losses/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>A2UI: A Protocol for Agent-Driven Interfaces</h2>
                <span class="published-time">Published: 2025-12-16 09:16:31</span>
                
                <p class="summary">A2UI introduces a novel protocol designed to facilitate the development and deployment of agent-driven interfaces, aiming to standardize how autonomous agents interact with and control user interfaces. This initiative addresses the growing need for robust and interoperable communication mechanisms between AI agents and diverse front-end systems. The proposed protocol seeks to define a structured framework for agents to perceive, interpret, and manipulate interface elements, enabling more sophisticated and dynamic user experiences. By establishing common data formats and interaction patterns, A2UI intends to reduce integration complexities, foster innovation in agent-based applications, and promote a more unified ecosystem for AI-powered interactions. The core objective is to move beyond simple command-response systems to enable agents to actively manage and adapt interfaces, leading to more intuitive and context-aware human-computer interaction paradigms. This standardization effort is crucial for scaling the capabilities of AI agents across various platforms and applications, from smart assistants to complex operational dashboards, ensuring consistency and reliability in agent-orchestrated environments.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>A2UI</span><span>Agent-Driven Interfaces</span><span>AI Protocol</span><span>Autonomous Agents</span><span>Human-Computer Interaction</span><span>User Interface Design</span><span>Standardization</span><span>AI Systems</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Artificial Intelligence</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://a2ui.org/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>CC, a new AI productivity agent that connects your Gmail, Calendar and Drive</h2>
                <span class="published-time">Published: 2025-12-16 18:43:21</span>
                
                <p class="summary">Google has unveiled CC, an innovative AI productivity agent designed to deeply integrate with and enhance user experiences across key Google Workspace applications: Gmail, Calendar, and Drive. This new agent aims to optimize daily workflows by intelligently managing communications, scheduling events, and organizing digital assets. Leveraging advanced artificial intelligence capabilities, CC is poised to automate routine administrative tasks, offer intelligent recommendations, and empower users to maintain greater command over their digital environments. The introduction of CC signifies a notable advancement in the development of more intuitive and proactive personal AI assistants within the Google ecosystem, prioritizing enhanced efficiency and reduced cognitive burden for individuals navigating extensive information and task loads. The agent's fundamental purpose centers on interpreting context and user intent to proactively facilitate various productivity-centric activities, representing an evolution in personalized AI support for both professional and personal spheres.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agent</span><span>Productivity Tool</span><span>Google Workspace</span><span>Email Management</span><span>Calendar Management</span><span>Cloud Integration</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Artificial Intelligence</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://labs.google/cc/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Memory in the Age of AI Agents</h2>
                <span class="published-time">Published: 2025-12-15T17:22:34.000Z</span>
                
                <p class="summary">Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Agent Memory</span><span>Foundation Models</span><span>Large Language Models</span><span>Memory Systems</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.13564" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management</h2>
                <span class="published-time">Published: 2025-12-15T04:11:11.000Z</span>
                
                <p class="summary">We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Long-Context Reasoning</span><span>Memory Management</span><span>Reinforcement Learning</span><span>Large Language Model</span><span>Post-Training</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.12967" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LongVie 2: Multimodal Controllable Ultra-Long Video World Model</h2>
                <span class="published-time">Published: 2025-12-15T17:59:58.000Z</span>
                
                <p class="summary">Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video World Models</span><span>Multimodal</span><span>Video Generation</span><span>Controllability</span><span>Temporal Consistency</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Generative AI</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.13604" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</h2>
                <span class="published-time">Published: 2025-12-14T13:56:54.000Z</span>
                
                <p class="summary">LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>WebOperator</span><span>Autonomous Agents</span><span>Tree Search</span><span>LLM Agents</span><span>Web Environments</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.12692" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</h2>
                <span class="published-time">Published: 2025-12-09T00:24:29.000Z</span>
                
                <p class="summary">Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when "anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Image Generation</span><span>Reward Models</span><span>Aesthetic Bias</span><span>Generative Models</span><span>Systemic Bias</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.11883" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Rethinking Expert Trajectory Utilization in LLM Post-training</h2>
                <span class="published-time">Published: 2025-12-12T11:13:00.000Z</span>
                
                <p class="summary">While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLM Post-training</span><span>Expert Trajectory Utilization</span><span>Supervised Fine-Tuning</span><span>Reinforcement Learning</span><span>Plasticity-Ceiling Framework</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Machine Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.11470" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>