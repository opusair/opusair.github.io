[
  {
    "id": "hackernews_46241849",
    "source": "Hacker News",
    "url": "https://vulpinecitrus.info/blog/guarding-git-forge-ai-scrapers/",
    "title": "Guarding My Git Forge Against AI Scrapers",
    "summary": "This article addresses the emerging challenge faced by repository maintainers in protecting their codebases from automated data extraction by artificial intelligence systems. It delves into various defensive strategies designed to prevent AI models, particularly those intended for large language model (LLM) training, from indiscriminately scraping intellectual property from Git forges, whether public or private. The discussion likely covers practical implementations such as leveraging robots.txt directives to signal exclusion to compliant bots, employing sophisticated rate-limiting mechanisms to detect and block excessive access patterns, and implementing CAPTCHAs or other bot detection techniques. Furthermore, it may explore the necessity of robust authentication requirements for programmatic access, the importance of monitoring access logs for suspicious activity, and potentially legal or policy frameworks to assert data ownership. The overarching aim is to maintain control over data dissemination, mitigate unauthorized usage of proprietary code for AI development, and ensure that human developers and legitimate integrations can still access the forge unimpeded.",
    "keywords": [
      "AI scrapers",
      "Git forge",
      "data protection",
      "web scraping prevention",
      "bot detection",
      "access control",
      "repository security"
    ],
    "area": [
      "Artificial Intelligence",
      "Others",
      "Large Language Model"
    ],
    "published_time": "2025-12-12 07:51:04",
    "download_time": "2025-12-12 20:00:50",
    "extra_info": "{\"score\": 129, \"by\": \"todsacerdoti\", \"descendants\": 85, \"story_id\": 46241849}"
  },
  {
    "id": "hackernews_46246117",
    "source": "Hacker News",
    "url": "https://github.com/Ami3466/tomcp",
    "title": "Show HN: tomcp.org â€“ Turn any URL into an MCP server",
    "summary": "Tomcp.org presents an innovative service designed to convert any web URL into an MCP (Markdown Content Protocol) server, streamlining the process for artificial intelligence applications like Cursor and Claude to ingest clean, structured web content. By simply adding \"tomcp.org/\" prefix to any URL, users can transform a webpage into a standardized MCP Resource. This system functions as an intelligent proxy, fetching the specified URL, effectively stripping away extraneous elements such as advertisements and navigation bars, and subsequently converting the purified content into a Markdown format. The core benefits of this approach, when compared to conventional web scraping or manual copy-pasting, include a significant enhancement in AI's ability to understand the content's structure, thereby improving processing accuracy. Furthermore, it leads to a notable reduction in token consumption for large language models, making AI interactions with web information more efficient and cost-effective. The project, open-sourced on GitHub and inspired by GitMCP, extends this capability to the general web, facilitating better integration of diverse online information into AI's operational context.",
    "keywords": [
      "MCP (Markdown Content Protocol)",
      "Web Content Extraction",
      "AI Integration",
      "Token Optimization",
      "Markdown Conversion",
      "Proxy Server"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-12-12 17:10:16",
    "download_time": "2025-12-12 20:00:48",
    "extra_info": "{\"score\": 31, \"by\": \"ami3466\", \"descendants\": 9, \"story_id\": 46246117}"
  },
  {
    "id": "hackernews_46242795",
    "source": "Hacker News",
    "url": "https://arxiv.org/abs/2512.08093",
    "title": "Training LLMs for Honesty via Confessions",
    "summary": "This research explores a novel methodology for enhancing the honesty of Large Language Models (LLMs) through a process termed \"confessions.\" The core idea involves training LLMs to explicitly acknowledge limitations, uncertainties, or potential biases in their generated responses, effectively \"confessing\" when their knowledge base is incomplete or when a statement might be speculative. This approach aims to cultivate greater transparency and truthfulness in AI outputs, moving beyond mere factual accuracy to encompass the model's self-assessment of its reliability. By integrating such confessional mechanisms into the training pipeline, the study proposes a pathway to build more trustworthy and aligned AI systems, capable of critical self-evaluation and candid communication about the veracity of their information. This innovative method could significantly mitigate issues like hallucination and overconfidence in current LLM applications, thereby paving the way for the development of more robust, ethically sound, and dependable AI interactions across various domains.",
    "keywords": [
      "Large Language Models",
      "AI Alignment",
      "LLM Training",
      "AI Honesty",
      "Truthfulness",
      "Transparency in AI"
    ],
    "area": [
      "Large Language Model",
      "Artificial Intelligence",
      "Natural Language Processing"
    ],
    "published_time": "2025-12-12 10:37:51",
    "download_time": "2025-12-12 20:01:00",
    "extra_info": "{\"score\": 52, \"by\": \"arabello\", \"descendants\": 34, \"story_id\": 46242795}"
  },
  {
    "id": "hackernews_46246921",
    "source": "Hacker News",
    "url": "https://www.ign.com/articles/everyone-disliked-that-amazon-pulls-ai-powered-fallout-recap-after-getting-key-story-details-wrong",
    "title": "Amazon pulls AI-powered Fallout recap after getting key story details wrong",
    "summary": "Amazon recently launched an AI-powered recap for its popular Fallout television series, a move intended to provide viewers with quick summaries of the show's storyline. However, the initiative quickly faced significant backlash after the AI-generated content was found to contain critical inaccuracies regarding key story details from the Fallout universe. Following widespread criticism from fans and media, Amazon promptly removed the erroneous recap. This incident highlights the ongoing challenges and limitations associated with deploying artificial intelligence for content generation, particularly when accuracy in specific domain knowledge, such as established lore, is paramount. The failure underscores the necessity for rigorous fact-checking and human editorial oversight even in AI-driven applications, especially within sensitive intellectual properties where factual integrity directly impacts audience reception and brand credibility. It serves as a stark reminder that while AI offers powerful tools for content creation, its outputs still require careful validation to prevent factual errors and maintain quality standards.",
    "keywords": [
      "AI content generation",
      "Factual accuracy",
      "AI limitations",
      "Generative AI",
      "Content errors",
      "Entertainment AI",
      "Large Language Model"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Large Language Model"
    ],
    "published_time": "2025-12-12 18:16:52",
    "download_time": "2025-12-12 20:00:56",
    "extra_info": "{\"score\": 20, \"by\": \"jsheard\", \"descendants\": 2, \"story_id\": 46246921}"
  },
  {
    "id": "hackernews_46246031",
    "source": "Hacker News",
    "url": "https://finance.yahoo.com/news/oracle-made-a-300-billion-bet-on-openai-its-paying-the-price-205441863.html",
    "title": "Oracle made a $300B bet on OpenAI. It's paying the price",
    "summary": "Oracle's substantial $300 billion strategic investment in OpenAI, primarily aimed at providing critical cloud infrastructure and computational resources for advanced AI development, is reportedly facing significant hurdles. This immense financial commitment was intended to bolster Oracle's standing in the burgeoning artificial intelligence sector by catering to the intense demands of large language models and generative AI projects. However, recent observations suggest that the anticipated returns or operational efficiencies from this partnership are not materializing as quickly or favorably as initially projected, sparking concerns regarding the company's financial performance and market valuation. The phrase 'paying the price' implies potential challenges such as elevated operational costs associated with scaling AI infrastructure, intense competition from rival cloud providers, or a slower-than-expected monetization of AI services enabled by Oracle's support. This scenario underscores the inherent risks and long-term implications of multi-billion dollar strategic gambles within the highly volatile and capital-intensive AI industry, prompting stakeholders to scrutinize Oracle's long-term AI strategy and its execution in a fiercely competitive technological landscape.",
    "keywords": [
      "Oracle",
      "OpenAI",
      "Cloud Computing",
      "AI Infrastructure",
      "Strategic Investment",
      "Financial Performance",
      "Generative AI",
      "Large Language Model"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Others"
    ],
    "published_time": "2025-12-12 17:01:07",
    "download_time": "2025-12-12 20:01:00",
    "extra_info": "{\"score\": 105, \"by\": \"pera\", \"descendants\": 84, \"story_id\": 46246031}"
  },
  {
    "id": "hackernews_46246683",
    "source": "Hacker News",
    "url": "https://www.ajc.com/news/2025/12/waymo-cars-ignored-stopped-school-buses-in-atlanta-what-happens-now/",
    "title": "Waymo cars ignored stopped school buses in Atlanta. What happens now?",
    "summary": "A recent incident in Atlanta reportedly involved Waymo's autonomous vehicles failing to recognize and adhere to the legal requirement of stopping for school buses with flashing lights. This event brings to the forefront critical questions regarding the operational safety protocols and regulatory compliance of self-driving car technology. Such an oversight, particularly in sensitive scenarios involving children's safety, could significantly impact public perception and trust in autonomous vehicle capabilities. The incident prompts an immediate examination of Waymo's sensor suite, decision-making algorithms, and their ability to interpret dynamic real-world traffic situations accurately. Regulators and local authorities are expected to scrutinize the event, potentially leading to revised operational guidelines or temporary restrictions on Waymo's expansion in certain areas. Furthermore, the incident highlights the ongoing challenges in perfecting autonomous driving systems to handle complex, context-dependent traffic laws, emphasizing the need for robust testing, continuous software updates, and transparent incident reporting to ensure the safe integration of these technologies into urban infrastructure. The outcome could set precedents for how future autonomous vehicle deployments are managed and regulated.",
    "keywords": [
      "Autonomous Vehicles",
      "Waymo",
      "Traffic Safety",
      "Regulatory Compliance",
      "Self-Driving Cars",
      "AI Ethics",
      "Sensor Technology",
      "Urban Mobility"
    ],
    "area": [
      "Robotics",
      "Artificial Intelligence",
      "Computer Vision"
    ],
    "published_time": "2025-12-12 17:58:33",
    "download_time": "2025-12-12 20:01:07",
    "extra_info": "{\"score\": 13, \"by\": \"themaninthedark\", \"descendants\": 12, \"story_id\": 46246683}"
  },
  {
    "id": "2512.10739",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.10739",
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
    "keywords": [
      "Large Language Models",
      "AI Agent",
      "Reinforcement Learning",
      "Mathematical Reasoning",
      "Verifiers"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-12-11T15:26:28.000Z",
    "download_time": "2025-12-12 12:01:28",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.10739\", \"arxiv_url\": \"https://arxiv.org/abs/2512.10739\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10739.png\", \"original_title\": \"Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving\"}"
  },
  {
    "id": "2512.10949",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.10949",
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
    "keywords": [
      "Reinforcement Learning",
      "Text-to-3D Generation",
      "3D Generation",
      "Generative AI",
      "Reward Design"
    ],
    "area": [
      "Generative AI",
      "Multimodal",
      "Machine Learning"
    ],
    "published_time": "2025-12-11T18:59:52.000Z",
    "download_time": "2025-12-12 12:01:29",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.10949\", \"arxiv_url\": \"https://arxiv.org/abs/2512.10949\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10949.png\", \"original_title\": \"Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation\"}"
  },
  {
    "id": "2512.10430",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.10430",
    "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.",
    "keywords": [
      "Russian LLM",
      "Hybrid Reasoning",
      "Efficient Inference",
      "Speculative Decoding",
      "Open-Weight Model"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-12-11T08:40:10.000Z",
    "download_time": "2025-12-12 12:01:28",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.10430\", \"arxiv_url\": \"https://arxiv.org/abs/2512.10430\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10430.png\", \"original_title\": \"T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground\"}"
  },
  {
    "id": "2512.10791",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.10791",
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "summary": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
    "keywords": [
      "FACTS Leaderboard",
      "Large Language Models",
      "Factuality",
      "Benchmarks",
      "Multimodal"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Multimodal"
    ],
    "published_time": "2025-12-11T16:35:14.000Z",
    "download_time": "2025-12-12 12:01:26",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.10791\", \"arxiv_url\": \"https://arxiv.org/abs/2512.10791\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10791.png\", \"original_title\": \"The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality\"}"
  },
  {
    "id": "2512.10398",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.10398",
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "summary": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
    "keywords": [
      "AI Software Engineer",
      "Coding Agents",
      "Industrial Scale",
      "Confucius SDK",
      "SWE-Bench-Pro"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-12-11T08:05:58.000Z",
    "download_time": "2025-12-12 12:01:30",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.10398\", \"arxiv_url\": \"https://arxiv.org/abs/2512.10398\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10398.png\", \"original_title\": \"Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale\"}"
  },
  {
    "id": "2512.04537",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.04537",
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "summary": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
    "keywords": [
      "X-Humanoid",
      "Humanoid Robots",
      "Video Generation",
      "Generative Video Editing",
      "Large-scale Dataset"
    ],
    "area": [
      "Computer Vision",
      "Generative AI",
      "Robotics"
    ],
    "published_time": "2025-12-04T07:34:08.000Z",
    "download_time": "2025-12-12 12:01:27",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.04537\", \"arxiv_url\": \"https://arxiv.org/abs/2512.04537\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04537.png\", \"original_title\": \"X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale\"}"
  }
]