<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2026-01-02</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="." class="active">‰∏≠Êñá</a>
                <a href="en/" class="">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2026-01-02</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† ËøîÂõû‰∏ªÈ°µ</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ ÊúÄÊñ∞Êó•Êä•</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ ÂÖ≥‰∫éÊàë‰ª¨</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Fighting Fire with Fire: Scalable Oral Exams</h2>
                <span class="published-time">Published: 2026-01-02 18:18:47</span>
                
                <p class="summary">The article titled "Fighting Fire with Fire: Scalable Oral Exams" explores innovative approaches to addressing the long-standing challenge of scaling oral examinations, a critical but resource-intensive component of education and certification. It posits that traditional one-on-one oral assessments, while effective, become impractical for large cohorts due to human resource limitations and time constraints. The concept of "fighting fire with fire" suggests employing sophisticated, perhaps technology-driven, methods that mirror the complexity of human interaction but enable widespread deployment. This likely involves leveraging artificial intelligence, natural language processing, and machine learning models to automate portions of the examination process, provide consistent evaluation, and manage a high volume of candidates efficiently. Such systems could analyze spoken responses, assess understanding, and offer feedback, thereby revolutionizing how educational institutions conduct and scale oral assessments without compromising quality or fairness. The proposed solutions aim to enhance accessibility, reduce operational burdens, and ensure robust evaluation in diverse academic and professional settings.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Scalable Assessment</span><span>Oral Exams</span><span>AI in Education</span><span>Automated Evaluation</span><span>Natural Language Processing</span><span>Machine Learning</span><span>Educational Technology</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.behind-the-enemy-lines.com/2025/12/fighting-fire-with-fire-scalable-oral.html" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>TinyTinyTPU: 2\nD72 systolic-array TPU-style matrix-multiply unit deployed on FPGA</h2>
                <span class="published-time">Published: 2026-01-02 19:13:24</span>
                
                <p class="summary">Researchers have developed TinyTinyTPU, a compact 2x2 systolic-array unit specifically designed for efficient matrix multiplication, emulating the architectural principles of Google's Tensor Processing Units (TPUs). This project focuses on deploying specialized hardware for accelerating machine learning computations, primarily targeting Field-Programmable Gate Arrays (FPGAs) to demonstrate feasibility and performance benefits in a customizable, reconfigurable environment. The TinyTinyTPU aims to provide a scalable and energy-efficient solution for performing the core operation fundamental to many neural network models. By leveraging systolic arrays, the design optimizes data flow and parallel processing, which are critical for high-throughput AI inference tasks. This development highlights advancements in creating custom hardware accelerators that can be tailored for specific AI workloads, offering an alternative to general-purpose processors for achieving significant performance gains in embedded or edge computing scenarios. The project underscores the ongoing innovation in hardware-software co-design for next-generation AI systems.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>FPGA</span><span>Systolic Array</span><span>Matrix Multiplication</span><span>TPU Architecture</span><span>Hardware Acceleration</span><span>Machine Learning Hardware</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/Alanma23/tinytinyTPU-co" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Parental controls aren't for parents</h2>
                <span class="published-time">Published: 2026-01-02 13:42:37</span>
                
                <p class="summary">The Hacker News story, "Parental controls aren't for parents," challenges the conventional understanding of digital parental control systems. It provocatively suggests that these technologies, while marketed for child safety, often serve broader organizational interests, including data aggregation by tech companies or institutional monitoring. The narrative implies that the underlying technical frameworks, potentially leveraging artificial intelligence for content filtering and behavioral pattern recognition, might shift control away from individual parents towards centralized platforms. This raises significant concerns regarding data privacy, digital rights, and the ethical deployment of surveillance-like features in children's online environments. The article likely scrutinizes the transparency and effectiveness of these AI-driven mechanisms, questioning whether they genuinely empower parents or inadvertently create new avenues for external oversight. It advocates for a critical examination of the technical architecture and societal implications to ensure that digital safeguards truly align with parental autonomy and children's privacy.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Parental Controls</span><span>Digital Privacy</span><span>Data Surveillance</span><span>Cybersecurity Ethics</span><span>AI-driven Filtering</span><span>Digital Rights</span><span>Child Online Safety</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://beasthacker.com/til/parental-controls-arent-for-parents.html" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Dealta 
A game-theoretic decentralized trading protocol</h2>
                <span class="published-time">Published: 2026-01-02 12:31:03</span>
                
                <p class="summary">Dealta has launched an Alpha implementation of its game-theoretic decentralized trading protocol, aiming to solve the "Physical Oracle Problem" by enabling trustless trading of physical goods. The protocol departs from existing reputation-based decentralized marketplaces, which tend towards centralization, by employing a Nash Equilibrium-based mechanism. This system ensures that honesty is the dominant strategy for all participants through strictly defined payoff matrices. A core component involves staked, pseudo-randomly selected "Brokers" who are responsible for physically verifying goods before transactions are finalized. This design is specifically intended for trading mid to high-value goods, preventing fraud like receiving incorrect items. The released solution includes a custom Layer-1 blockchain stack, comprising a Full Node that implements Hybrid Consensus (Proof-of-Work and PBFT) for instant finality. It also features an Integrated Wallet for native key management and custom trade opcodes, alongside custom DB Management for efficient indexing of trade states. This comprehensive architecture seeks to establish a highly secure and verifiable framework for decentralized commerce.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Dealta</span><span>decentralized trading</span><span>game theory</span><span>blockchain protocol</span><span>Nash Equilibrium</span><span>physical oracle problem</span><span>hybrid consensus</span><span>Layer-1 blockchain</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/orgs/Dealta-Foundation/repositories" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Grok Can't Apologize. So Why Do Headlines Keep Saying It Did?</h2>
                <span class="published-time">Published: 2026-01-02 19:30:52</span>
                
                <p class="summary">The article "Grok Can't Apologize. So Why Do Headlines Keep Saying It Did?" critically examines the anthropomorphism often attributed to artificial intelligence models, specifically xAI's Grok. It challenges the common media portrayal that suggests AI systems are capable of genuine human emotions or actions like apologizing. The core argument is that AI models, including advanced Large Language Models, operate based on programmed algorithms and vast datasets to generate responses, not from a place of sentience, consciousness, or personal understanding. Therefore, any "apology" generated by an AI is merely a linguistic output designed to mimic human communication patterns, lacking the underlying emotional context or moral reasoning. The piece underscores the importance of accurately representing AI capabilities to the public, cautioning against misinterpretations that could lead to an exaggerated perception of AI's current state and future potential, particularly concerning ethical considerations and accountability. This distinction is crucial for maintaining a realistic public discourse around AI development and deployment.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>AI ethics</span><span>AI sentience</span><span>Anthropomorphism</span><span>AI capabilities</span><span>Grok</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.readtpa.com/p/grok-cant-apologize-grok-isnt-sentient" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling</h2>
                <span class="published-time">Published: 2025-12-30T03:13:10.000Z</span>
                
                <p class="summary">Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multi-step RAG</span><span>Hypergraph-based Memory</span><span>Large Language Models</span><span>Relational Modeling</span><span>Long-Context</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.23959" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space</h2>
                <span class="published-time">Published: 2025-12-31T04:19:33.000Z</span>
                
                <p class="summary">Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled "P parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Dynamic Large Concept Models</span><span>Large Language Models</span><span>Hierarchical Language Modeling</span><span>Concept Space</span><span>Scaling Law</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Large Language Model</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24617" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models</h2>
                <span class="published-time">Published: 2025-12-30T11:51:18.000Z</span>
                
                <p class="summary">While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2%) and Gemini-3-Flash (+111.6%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Generative Multimodal Reasoning</span><span>Diffusion Models</span><span>Vision-centric Reasoning</span><span>Multimodal Large Language Models</span><span>Image-to-image Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Generative AI</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24165" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>On the Role of Discreteness in Diffusion LLMs</h2>
                <span class="published-time">Published: 2025-12-27T16:03:08.000Z</span>
                
                <p class="summary">Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Diffusion Models</span><span>Language Generation</span><span>Discrete Diffusion</span><span>Large Language Models</span><span>Text Structure</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.22630" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation</h2>
                <span class="published-time">Published: 2025-12-31T08:41:27.000Z</span>
                
                <p class="summary">In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>FlowBlending</span><span>Video Generation</span><span>Multi-Model Sampling</span><span>Inference Speedup</span><span>Model Capacity</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24724" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow</h2>
                <span class="published-time">Published: 2025-12-31T10:25:24.000Z</span>
                
                <p class="summary">Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Dream2Flow</span><span>Video Generation</span><span>Open-World Manipulation</span><span>3D Object Flow</span><span>Robotic Control</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>Generative AI</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24766" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>