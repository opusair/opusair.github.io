[
  {
    "id": "twitter_Alibaba_Qwen_1963991502440562976",
    "source": "Twitter",
    "url": "https://twitter.com/Alibaba_Qwen/status/1963991502440562976",
    "title_en": "Alibaba_Qwen_Launches Trillion-Parameter Qwen3-Max-Preview Large Model",
    "summary_en": "Alibaba's Qwen team has announced the release of their largest model to date, Qwen3-Max-Preview (Instruct), featuring over 1 trillion parameters. This groundbreaking new model is now readily accessible via Qwen Chat and Alibaba Cloud API, marking a significant milestone. Internal tests and early user feedback consistently confirm that Qwen3-Max-Preview substantially outperforms its predecessor, Qwen3-235B, across various critical metrics. These improvements include enhanced overall performance, broader knowledge acquisition, superior conversational abilities, more effective handling of agentic tasks, and precise instruction following, collectively demonstrating remarkable advancements in large language model capabilities.",
    "keywords_en": [
      "Qwen",
      "Qwen3-Max-Preview",
      "Large Language Model",
      "Trillion Parameters",
      "Artificial Intelligence",
      "Product Launch"
    ],
    "area_en": [
      "Large Language Model",
      "Product Launch",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-05T15:43:58.000Z",
    "download_time": "2025-09-08 05:30:57",
    "visual_resource": [
      "screenshot/twitter/Alibaba_Qwen_1963991502440562976.png"
    ],
    "extra_info": "{\"username\": \"Alibaba_Qwen\", \"tweet_id\": \"1963991502440562976\"}"
  },
  {
    "id": "twitter_Kimi_Moonshot_1963802687230947698",
    "source": "Twitter",
    "url": "https://twitter.com/Kimi_Moonshot/status/1963802687230947698",
    "title_en": "Kimi_Moonshot_Kimi K2-0905 Update: Enhanced Coding and Long Context",
    "summary_en": "Kimi.ai announced the Kimi K2-0905 model update, featuring significantly enhanced coding capabilities, particularly for front-end development and tool-calling. The model's context length has been extended to 256k tokens, and its integration with various agent scaffolds has been improved. Additionally, a Turbo API is available, offering 60-100 TPS and guaranteed 100% tool-call accuracy for advanced applications.",
    "keywords_en": [
      "Kimi K2-0905",
      "Large Language Model",
      "Coding Capabilities",
      "Context Length",
      "AI Agent",
      "Tool Calling"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Tech News"
    ],
    "published_time": "2025-09-05T03:13:40.000Z",
    "download_time": "2025-09-08 05:31:46",
    "visual_resource": [
      "screenshot/twitter/Kimi_Moonshot_1963802687230947698.png"
    ],
    "extra_info": "{\"username\": \"Kimi_Moonshot\", \"tweet_id\": \"1963802687230947698\"}"
  },
  {
    "id": "twitter_karpathy_1964026120191545346",
    "source": "Twitter",
    "url": "https://twitter.com/karpathy/status/1964026120191545346",
    "title_en": "karpathy_Praises OpenAI's GPT-5 Pro for Solving Complex Coding Challenges",
    "summary_en": "Renowned AI expert Andrej Karpathy tweeted praise for OpenAI's 'GPT-5 Pro' model, highlighting its exceptional capability in solving complex programming challenges. He noted that a problem he struggled with for an hour using another tool ('CC') was resolved by GPT-5 Pro in just ten minutes, providing ready-to-use code. This experience underscores the significant advancements made by OpenAI's next-generation model in code generation and problem-solving, greatly enhancing development efficiency and user experience.",
    "keywords_en": [
      "OpenAI",
      "GPT-5 Pro",
      "Code Generation",
      "Artificial Intelligence",
      "Programming Assistant",
      "Andrej Karpathy"
    ],
    "area_en": [
      "Large Language Model",
      "Generative AI",
      "Tech News"
    ],
    "published_time": "2025-09-05T17:38:51.000Z",
    "download_time": "2025-09-08 05:31:00",
    "visual_resource": [
      "screenshot/twitter/karpathy_1964026120191545346.png"
    ],
    "extra_info": "{\"username\": \"karpathy\", \"tweet_id\": \"1964026120191545346\"}"
  },
  {
    "id": "twitter_omarsar0_1964045125115662847",
    "source": "Twitter",
    "url": "https://twitter.com/omarsar0/status/1964045125115662847",
    "title_en": "omarsar0_Microsoft Unveils rStar2-Agent: 14B Math Reasoning Model Achieves Frontier Performance in 510 RL Steps",
    "summary_en": "Microsoft's latest research introduces rStar2-Agent, a 14-billion-parameter math reasoning model. This model, trained with agentic Reinforcement Learning (RL), achieves frontier-level math reasoning capabilities in just 510 RL training steps. This advancement highlights the significant potential of efficient RL training in enhancing the reasoning performance of large models, marking a new breakthrough in the field of mathematical and logical reasoning.",
    "keywords_en": [
      "Microsoft",
      "rStar2-Agent",
      "Math Reasoning",
      "Reinforcement Learning",
      "Large Model",
      "AI Agent"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Research Progress"
    ],
    "published_time": "2025-09-05T19:17:02.000Z",
    "download_time": "2025-09-08 05:31:21",
    "visual_resource": [
      "screenshot/twitter/omarsar0_1964045125115662847.png"
    ],
    "extra_info": "{\"username\": \"omarsar0\", \"tweet_id\": \"1964045125115662847\"}"
  },
  {
    "id": "twitter_arankomatsuzaki_1963817987506643350",
    "source": "Twitter",
    "url": "https://twitter.com/arankomatsuzaki/status/1963817987506643350",
    "title_en": "arankomatsuzaki_Meta Introduces SBD for LLM Inference Acceleration",
    "summary_en": "Meta has introduced a new inference accelerator called Set Block Decoding (SBD), specifically designed for Large Language Models (LLMs). SBD technology enables parallel sampling of multiple future tokens, reducing forward passes by 3 to 5 times without requiring architectural changes. This innovation is compatible with KV-cache and maintains performance comparable to Next Token Prediction (NTP) training, significantly enhancing LLM inference efficiency.",
    "keywords_en": [
      "Meta",
      "SBD",
      "LLM",
      "Inference Acceleration",
      "Parallel Sampling"
    ],
    "area_en": [
      "Large Language Model",
      "Tech News",
      "Research Progress"
    ],
    "published_time": "2025-09-05T04:14:28.000Z",
    "download_time": "2025-09-08 05:31:19",
    "visual_resource": [
      "screenshot/twitter/arankomatsuzaki_1963817987506643350.png"
    ],
    "extra_info": "{\"username\": \"arankomatsuzaki\", \"tweet_id\": \"1963817987506643350\"}"
  },
  {
    "id": "twitter_llama_index_1964009128973783135",
    "source": "Twitter",
    "url": "https://twitter.com/llama_index/status/1964009128973783135",
    "title_en": "llama_index_SemTools CLI Agents Enhance Document Search and Analysis",
    "summary_en": "LlamaIndex introduced SemTools, a command-line toolkit for document parsing and semantic search. Tested with coding agents like Claude Code on 1000 ArXiv papers, SemTools demonstrated significant improvements. The research indicates that combining Unix tools with semantic search capabilities creates highly effective knowledge workers, providing more detailed and accurate answers for document analysis tasks. This approach leverages existing Unix tooling, proving CLI access to be a powerful and efficient alternative to custom RAG infrastructure.",
    "keywords_en": [
      "Command-line agents",
      "Semantic search",
      "Document analysis",
      "SemTools",
      "LlamaIndex",
      "RAG"
    ],
    "area_en": [
      "AI Agent",
      "Natural Language Processing",
      "Tech News"
    ],
    "published_time": "2025-09-05T16:54:00.000Z",
    "download_time": "2025-09-08 05:31:14",
    "visual_resource": [
      "screenshot/twitter/llama_index_1964009128973783135.png"
    ],
    "extra_info": "{\"username\": \"llama_index\", \"tweet_id\": \"1964009128973783135\"}"
  },
  {
    "id": "FjGn2S5VmZ30hXs0Jvadqg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/FjGn2S5VmZ30hXs0Jvadqg",
    "title_en": "DeepSeek's Next Big Move Revealed: The Future of AI Agents",
    "summary_en": "Insiders reveal that DeepSeek is secretly developing an advanced AI agent model with significant self-evolution capabilities, with the highly anticipated \"R2\" version projected for release by year-end. This follows DeepSeek-V3.1's recent update, which already demonstrated substantial improvements in tool utilization and complex agent tasks, showcasing comprehensive performance gains in both programming and search agent evaluations. The forthcoming \"R2\" model is designed to execute intricate operations with minimal user prompts and continuously learn from historical actions, enabling greater autonomy. This strategic move marks a crucial and ambitious step for DeepSeek in the burgeoning AI agent domain, poised to lead new industry trends and potentially democratize access by lowering the cost barrier for sophisticated agent technology. This development firmly signals a continued and intensified industry focus on AI agent advancements within the large language model landscape throughout 2025, promising further innovation.",
    "keywords_en": [
      "DeepSeek",
      "AI Agent",
      "Large Language Model",
      "Self-evolution",
      "Programming Agent",
      "R2"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-05T14:01:18.000Z",
    "download_time": "2025-09-08T13:32:19.399254",
    "visual_resource": [
      "screenshot/wechat/wechat_image_FjGn2S5VmZ30hXs0Jvadqg.png"
    ],
    "extra_info": null
  },
  {
    "id": "dlMpKCqzbNLhqwg1RVNosg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/dlMpKCqzbNLhqwg1RVNosg",
    "title_en": "Kuaishou Keye-VL-1.5 Unveiled: Slow-Fast Dual-Track Encoding and 128K Context for a New Era of Visual Language",
    "summary_en": "The Kuaishou Keye-VL-1.5 model introduces an innovative slow-fast dual-track video encoding strategy and a progressive four-stage pre-training approach, effectively addressing the fundamental challenge of balancing spatial resolution and temporal coverage in video understanding. This advanced model extends its context length to an impressive 128K tokens, enabling it to process longer videos and more complex visual content. Furthermore, it incorporates a sophisticated post-training pipeline specifically designed to enhance inference capabilities and align with human preferences, thereby significantly improving complex video understanding and reasoning. Keye-VL-1.5 demonstrates outstanding performance in public benchmarks, achieving notable breakthroughs in video understanding tasks while simultaneously maintaining strong competitiveness in general multimodal benchmarks. This comprehensive research offers a practical and robust solution for developing next-generation multimodal models capable of sophisticated video comprehension and advanced reasoning.",
    "keywords_en": [
      "Keye-VL-1.5",
      "Video Understanding",
      "Multimodal Model",
      "Slow-Fast Dual-Track Encoding",
      "Context Length",
      "Pre-training"
    ],
    "area_en": [
      "Large Language Model",
      "Video Understanding",
      "Multimodal"
    ],
    "published_time": "2025-09-05T14:01:18.000Z",
    "download_time": "2025-09-08T13:32:21.165799",
    "visual_resource": [
      "screenshot/wechat/wechat_image_dlMpKCqzbNLhqwg1RVNosg.png"
    ],
    "extra_info": null
  },
  {
    "id": "rSE8j5rRBEJ2-F5Cy2uxuQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/rSE8j5rRBEJ2-F5Cy2uxuQ",
    "title_en": "Stanford's Latest: Qwen2.5-3B, Capable of 'Deliberate Practice', Outperforms Claude3.5",
    "summary_en": "A recent Stanford University study addresses two significant challenges in Reinforcement Learning (RL) for Machine Learning Engineering (MLE) tasks: learning bias caused by variable-duration actions and sparse reward signals. To overcome these, the research team introduced innovative solutions: \"Duration-Aware Gradient Updates\" and \"Environment Detection\" mechanisms, which significantly optimize the RL training process. Experimental results demonstrate that the Qwen2.5-3B model, with only 3 billion parameters, after RL training, surpassed the prompt-dependent Claude3.5-Sonnet in 8 out of 12 Kaggle MLE tasks, achieving an average performance improvement of 22%. Furthermore, it outperformed GPT-4o in most tasks. This research highlights the critical importance of continuous learning and iterative optimization, demonstrating that smaller models possess the potential to surpass larger, static models in complex AI tasks, offering significant insights for AI Agent development.",
    "keywords_en": [
      "Reinforcement Learning",
      "Machine Learning Engineering",
      "Qwen2.5-3B",
      "AI Agent",
      "Continuous Learning",
      "Large Language Models"
    ],
    "area_en": [
      "Machine Learning",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-05T13:55:21.000Z",
    "download_time": "2025-09-08T13:32:21.891525",
    "visual_resource": [
      "screenshot/wechat/wechat_image_rSE8j5rRBEJ2-F5Cy2uxuQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "muSPVne06AdhhpNyMrnR8Q",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/muSPVne06AdhhpNyMrnR8Q",
    "title_en": "Long Video Generation Can Now Look Back! Oxford Proposes \"Memory Stabilization\" for 12x Speedup",
    "summary_en": "Researchers from Oxford University have introduced VMem (Surfel-Indexed View Memory), a novel approach designed to address the challenges of inconsistency and high computational costs in long video generation. VMem tackles these issues by explicitly recording \"what has been seen\" into geometric surfels. Instead of relying solely on recent frames, it leverages geometric visibility to retrieve the most relevant historical views as context for subsequent generation. This method significantly enhances consistency in long-sequence video generation, particularly excelling in loop-trajectory evaluations. Furthermore, VMem drastically reduces computational overhead, achieving up to a 12-fold increase in inference speed. As an explicit, interpretable external memory mechanism, VMem offers an efficient and stable solution for long video generation, capable of being plug-and-play integrated into existing backbone networks.",
    "keywords_en": [
      "Long Video Generation",
      "Video Consistency",
      "Memory Mechanism",
      "Surfel",
      "Geometric Visibility",
      "Efficiency Improvement"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-09-05T05:37:56.000Z",
    "download_time": "2025-09-08T13:32:23.137288",
    "visual_resource": [
      "screenshot/wechat/wechat_image_muSPVne06AdhhpNyMrnR8Q.png"
    ],
    "extra_info": null
  },
  {
    "id": "83iXDiM1jJK-kEl5-LubJQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/83iXDiM1jJK-kEl5-LubJQ",
    "title_en": "Nvidia Acquires AI Coding Startup Solver",
    "summary_en": "Nvidia has recently acquired Solver, an AI coding startup specializing in developing AI Agents capable of managing entire codebases, moving beyond mere code auto-completion. This acquisition is a pivotal step in Nvidia's broader strategy to build a comprehensive software ecosystem around its leading AI hardware, aiming to reduce chip utilization costs and deepen its footprint in the rapidly evolving AI software market. Solver's technology, founded by veterans including a Siri co-founder, signals a future where AI collaborators are more deeply integrated into core software development processes. This move is expected to shorten development cycles on Nvidia's platforms and signifies a continuation of Nvidia's \"AI acquisition spree,\" further expanding its business scope from chips and data tools to advanced AI agents.",
    "keywords_en": [
      "Nvidia",
      "AI Programming",
      "AI Agent",
      "Solver",
      "Software Ecosystem",
      "Acquisition"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-05T01:47:01.000Z",
    "download_time": "2025-09-08T13:32:46.355120",
    "visual_resource": [
      "screenshot/wechat/wechat_image_83iXDiM1jJK-kEl5-LubJQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "Oc8gw7jtUoqd5QqKVbpqaQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Oc8gw7jtUoqd5QqKVbpqaQ",
    "title_en": "Breaking the Embodied AI \"Expert Dilemma\"! Peking University's New Method Enables Unitree G1 to Master Dancing and Cartwheels with a Single Framework",
    "summary_en": "Peking University and BeingBeyond team have jointly developed the innovative BumbleBee system, featuring a \"divide-refine-fuse\" three-level architecture. This groundbreaking approach is the first to effectively address the long-standing \"expert dilemma\" in traditional humanoid robot control, which suffers from over-reliance on specific expert strategies and limited generalization capabilities. BumbleBee successfully bridges the gap from optimized expert policies to versatile whole-body control within a single, unified framework. This enables humanoid robots, exemplified by the Unitree G1, to fluidly execute a wide range of complex and diverse actions, including intricate dance routines and dynamic cartwheels. Comprehensive evaluations in both simulation environments (IsaacGym, MuJoCo) and on the physical Unitree G1 platform consistently demonstrate BumbleBee's superior performance in terms of task success rate and overall stability compared to existing baseline methods. This research presents a highly efficient and generalizable new paradigm, significantly advancing the field of universal embodied artificial intelligence control.",
    "keywords_en": [
      "Embodied AI",
      "Humanoid Robots",
      "Motion Control",
      "Unified Framework",
      "BumbleBee",
      "Expert Dilemma"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Robotics",
      "AI Agent"
    ],
    "published_time": "2025-09-05T01:47:01.000Z",
    "download_time": "2025-09-08T13:32:46.599471",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Oc8gw7jtUoqd5QqKVbpqaQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "parlant",
    "source": "GitHub",
    "url": "https://github.com/emcie-co/parlant",
    "title_en": "Finally, LLM agents that actually follow instructions",
    "summary_en": "Parlant is an innovative AI agent framework designed to address the critical challenge of ensuring Large Language Models (LLMs) consistently follow instructions and maintain predictable behavior. Unlike traditional prompt engineering, Parlant enables developers to define explicit behavioral guidelines, integrate external tools, adapt to specific domains, and utilize canned responses, guaranteeing reliable agent performance. Key features include conversational journeys, dynamic guideline matching, robust tool integration, and comprehensive explainability, allowing deep insights into agent decisions. This framework is ideal for building production-ready, customer-facing AI agents across diverse sectors such as financial services, healthcare, e-commerce, and legal tech, where compliance, consistency, and reliability are paramount. Parlant empowers businesses to deploy AI agents that behave exactly as required, minimizing hallucinations and maximizing operational efficiency.",
    "keywords_en": [
      "AI Agent",
      "Large Language Model",
      "Instruction Following",
      "Behavioral Guidance",
      "Tool Integration",
      "Conversational AI",
      "Explainability",
      "AI Framework"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-08T05:17:02Z",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentDark.png?raw=true",
      "https://github.com/emcie-co/parlant/blob/develop/docs/demo.gif?raw=true"
    ],
    "extra_info": null
  },
  {
    "id": "ottomator-agents",
    "source": "GitHub",
    "url": "https://github.com/coleam00/ottomator-agents",
    "title_en": "What is the Live Agent Studio?",
    "summary_en": "The Live Agent Studio, developed by oTTomator, is a community-driven platform dedicated to exploring and implementing cutting-edge AI agents. All agents featured on the platform are open source, with their complete source code and workflow JSON files readily available in the associated GitHub repository, fostering transparency and collaborative learning. This initiative aims to establish the studio as a central hub for the latest advancements in AI agent technology, groundbreaking research, and essential development tools, ensuring users stay at the forefront of AI innovation. Users can access and utilize these powerful agents through a token-based system, which covers the underlying large language model usage costs, with initial free tokens provided to encourage exploration. Beyond its practical utility, the platform serves as a comprehensive educational resource for learning AI applications and mastering the process of building sophisticated agents, thereby fostering collaborative learning and continuous development within the broader AI community.",
    "keywords_en": [
      "AI Agent",
      "Open Source",
      "AI Platform",
      "Large Language Model",
      "Workflow",
      "AI Application Development"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-09-03T23:14:34Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "screenshot/github/ottomator-agents.png"
    ],
    "extra_info": null
  },
  {
    "id": "kilocode",
    "source": "GitHub",
    "url": "https://github.com/Kilo-Org/kilocode",
    "title_en": "🚀 Kilo Code",
    "summary_en": "Kilo Code is an open-source VS Code AI agent designed to enhance development efficiency by generating code from natural language, automating tasks, and refactoring code. It integrates the latest AI models such as Gemini 2.5 Pro, Claude 4, and GPT-5, with optional API key usage. The project incorporates features from other open-source projects like Roo Code and Cline, offering multi-mode workflows (e.g., Architect, Coder, Debugger) and an MCP Server Marketplace to extend capabilities. Kilo Code aims to provide developers with an adaptable and feature-rich AI coding assistant, streamlining the development process.",
    "keywords_en": [
      "AI Agent",
      "VS Code Extension",
      "Code Generation",
      "Natural Language Processing",
      "Automated Programming",
      "Large Language Models",
      "Code Refactoring",
      "Development Tools"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-05T22:38:14Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/Kilo-Org/kilocode/refs/heads/main/kilo.gif"
    ],
    "extra_info": null
  },
  {
    "id": "BitNet",
    "source": "GitHub",
    "url": "https://github.com/microsoft/BitNet",
    "title_en": "bitnet.cpp",
    "summary_en": "bitnet.cpp is the official inference framework from Microsoft for 1-bit Large Language Models (LLMs) like BitNet b1.58. It provides a suite of optimized kernels that enable fast and lossless inference of 1.58-bit models on both CPUs and GPUs. The framework achieves significant speedups, ranging from 1.37x to 5.07x on ARM CPUs and 2.37x to 6.17x on x86 CPUs, while also substantially reducing energy consumption. Notably, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU at speeds comparable to human reading, greatly enhancing the potential for deploying LLMs on local devices and marking a significant advancement in edge AI inference.",
    "keywords_en": [
      "1-bit LLM",
      "Inference Framework",
      "Model Optimization",
      "CPU Inference",
      "GPU Inference",
      "Edge Computing",
      "Energy Efficiency"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Deep Learning"
    ],
    "published_time": "2025-06-03T06:14:20Z",
    "download_time": "2024-05-20 10:00:00",
    "visual_resource": [
      "https://github.com/microsoft/BitNet/raw/main/assets/m2_performance.jpg",
      "https://github.com/microsoft/BitNet/raw/main/assets/intel_performance.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "llm-app",
    "source": "GitHub",
    "url": "https://github.com/pathwaycom/llm-app",
    "title_en": "Pathway AI Pipelines",
    "summary_en": "Pathway AI Pipelines provide a robust and efficient solution for rapidly deploying AI applications, specializing in high-accuracy Retrieval-Augmented Generation (RAG) and scalable enterprise AI search. These pipelines leverage the most current knowledge from various data sources, offering ready-to-use Large Language Model (LLM) App Templates that can be deployed on-cloud or on-premises. They seamlessly connect and synchronize with diverse data sources, including file systems, Google Drive, Sharepoint, S3, and Kafka, handling all data additions, deletions, and updates in real-time. A key feature is the built-in data indexing, enabling lightning-fast vector, hybrid, and full-text search capabilities, all managed in-memory with caching, thus eliminating external infrastructure dependencies. This comprehensive approach significantly simplifies the development and deployment of sophisticated AI applications, particularly for managing and querying vast document collections with continuously updated information.",
    "keywords_en": [
      "AI Pipelines",
      "RAG",
      "Large Language Models",
      "Data Indexing",
      "Vector Search",
      "Real-time Data",
      "Enterprise Search"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-07-30T12:13:38Z",
    "download_time": "2024-05-16 08:00:00",
    "visual_resource": [
      "https://github.com/pathwaycom/llm-app/blob/main/examples/pipelines/gpt_4o_multimodal_rag/gpt4o_with_pathway_comparison.gif",
      "https://github.com/pathwaycom/llm-app/blob/main/examples/pipelines/drive_alert/drive_alert_demo.gif"
    ],
    "extra_info": null
  },
  {
    "id": "XLeRobot",
    "source": "GitHub",
    "url": "https://github.com/Vector-Wangel/XLeRobot",
    "title_en": "XLeRobot 🤖",
    "summary_en": "XLeRobot is an innovative open-source, low-cost embodied AI robotics project designed to make advanced embodied AI technology accessible to a broader audience. This initiative presents a practical dual-arm mobile robot that boasts an impressive affordability, costing less than an iPhone, and remarkable ease of assembly, requiring under 4 hours. With a starting price of approximately $660, XLeRobot leverages foundational work from projects like LeRobot, SO-100, and Lekiwi. It features versatile control options, including keyboard, Xbox controller, and Switch Joy-Con, enabling intuitive interaction. The project is comprehensively supported by detailed documentation, a robust simulation environment, and clear hardware assembly guides, positioning it as an ideal platform for both practical household automation and cutting-edge robotics research.",
    "keywords_en": [
      "Embodied AI",
      "Robotics",
      "Low-cost Hardware",
      "Open-source Project",
      "Simulation",
      "Robotic Arm",
      "Home Automation",
      "Robot Control"
    ],
    "area_en": [
      "Robotics",
      "Artificial Intelligence",
      "AI Agent"
    ],
    "published_time": "2025-09-08T00:37:42Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/XLeRobot.png"
    ],
    "extra_info": null
  },
  {
    "id": "2509.04442",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.04442",
    "title_en": "Delta Activations: A Representation for Finetuned Large Language Models",
    "summary_en": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.",
    "keywords_en": [
      "Delta Activations",
      "Large Language Models",
      "Finetuning",
      "Vector Embeddings",
      "Model Representation"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-09-04T17:59:06.000Z",
    "download_time": "2025-09-07 22:32:54",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04442.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.04442\", \"arxiv_url\": \"https://arxiv.org/abs/2509.04442\"}"
  },
  {
    "id": "2509.04434",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.04434",
    "title_en": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer",
    "summary_en": "We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.",
    "keywords_en": [
      "Portrait Animation",
      "Attribute Transfer",
      "Diffusion Model",
      "Dual Reference",
      "Zero-shot"
    ],
    "area_en": [
      "Computer Vision",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-09-04T17:53:03.000Z",
    "download_time": "2025-09-07 22:32:55",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04434.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.04434\", \"arxiv_url\": \"https://arxiv.org/abs/2509.04434\"}"
  },
  {
    "id": "2509.04419",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.04419",
    "title_en": "Towards a Unified View of Large Language Model Post-Training",
    "summary_en": "Two major sources of training data exist for post-training modern language\nmodels: online (model-generated rollouts) data, and offline (human or\nother-model demonstrations) data. These two types of data are typically used by\napproaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),\nrespectively. In this paper, we show that these approaches are not in\ncontradiction, but are instances of a single optimization process. We derive a\nUnified Policy Gradient Estimator, and present the calculations of a wide\nspectrum of post-training approaches as the gradient of a common objective\nunder different data distribution assumptions and various bias-variance\ntradeoffs. The gradient estimator is constructed with four interchangeable\nparts: stabilization mask, reference policy denominator, advantage estimate,\nand likelihood gradient. Motivated by our theoretical findings, we propose\nHybrid Post-Training (HPT), an algorithm that dynamically selects different\ntraining signals. HPT is designed to yield both effective exploitation of\ndemonstration and stable exploration without sacrificing learned reasoning\npatterns. We provide extensive experiments and ablation studies to verify the\neffectiveness of our unified theoretical framework and HPT. Across six\nmathematical reasoning benchmarks and two out-of-distribution suites, HPT\nconsistently surpasses strong baselines across models of varying scales and\nfamilies.",
    "keywords_en": [
      "Large Language Model",
      "Post-training",
      "Unified Policy Gradient Estimator",
      "Hybrid Post-Training",
      "Reinforcement Learning"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Machine Learning"
    ],
    "published_time": "2025-09-04T17:40:33.000Z",
    "download_time": "2025-09-07 22:32:56",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04419.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.04419\", \"arxiv_url\": \"https://arxiv.org/abs/2509.04419\"}"
  },
  {
    "id": "2509.04406",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.04406",
    "title_en": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
    "summary_en": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.",
    "keywords_en": [
      "3D Generation",
      "Model Distillation",
      "Few-step Flow",
      "Marginal-Data Transport",
      "Velocity Distillation"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-09-04T17:24:31.000Z",
    "download_time": "2025-09-07 22:32:57",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04406.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.04406\", \"arxiv_url\": \"https://arxiv.org/abs/2509.04406\"}"
  },
  {
    "id": "2509.03059",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.03059",
    "title_en": "Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers",
    "summary_en": "Recent advances in Large Language Models (LLMs) have shown that their\nreasoning capabilities can be significantly improved through Reinforcement\nLearning with Verifiable Reward (RLVR), particularly in domains like\nmathematics and programming, where ground-truth correctness can be\nautomatically evaluated. However, extending this success to other\nreasoning-intensive domains remains challenging due to the scarcity of\nhigh-quality, verifiable datasets and the high cost of human supervision. In\nthis work, we introduce the Loong Project: an open-source framework for\nscalable synthetic data generation and verification across a diverse range of\nreasoning-intensive domains. The framework consists of two key components: (1)\nLoongBench, a curated seed dataset containing 8,729 human-vetted examples\nacross 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired\nwith executable code and rich metadata; and (2) LoongEnv, a modular synthetic\ndata generation environment that supports multiple prompting strategies to\nproduce new question-answer-code triples. Together, these components form an\nagent-environment loop that enables reinforcement learning, where an LLM-based\nagent is rewarded for generating Chain-of-Thought (CoT) solutions that align\nwith code-executed answers. Empirically, we benchmark LoongBench on a broad\nsuite of both open-source and proprietary LLMs to evaluate domain coverage and\nreveal performance bottlenecks. In addition, we conduct a comprehensive\nanalysis of synthetic data generated by LoongEnv, examining correctness,\ndifficulty, and diversity. Code and documentation are available at\nhttps://github.com/camel-ai/loong.",
    "keywords_en": [
      "Large Language Models",
      "Chain-of-Thought",
      "Synthetic Data",
      "Reinforcement Learning",
      "Verifiers"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "AI Agent"
    ],
    "published_time": "2025-09-03T06:42:40.000Z",
    "download_time": "2025-09-07 22:32:57",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03059.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.03059\", \"arxiv_url\": \"https://arxiv.org/abs/2509.03059\"}"
  },
  {
    "id": "2509.01396",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.01396",
    "title_en": "DeepResearch Arena: The First Exam of LLMs' Research Abilities via\n  Seminar-Grounded Tasks",
    "summary_en": "Deep research agents have attracted growing attention for their potential to\norchestrate multi-stage research workflows, spanning literature synthesis,\nmethodological design, and empirical verification. Despite these strides,\nevaluating their research capability faithfully is rather challenging due to\nthe difficulty of collecting frontier research questions that genuinely capture\nresearchers' attention and intellectual curiosity. To address this gap, we\nintroduce DeepResearch Arena, a benchmark grounded in academic seminars that\ncapture rich expert discourse and interaction, better reflecting real-world\nresearch environments and reducing the risk of data leakage. To automatically\nconstruct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task\nGeneration (MAHTG) system that extracts research-worthy inspirations from\nseminar transcripts. The MAHTG system further translates research-worthy\ninspirations into high-quality research tasks, ensuring the traceability of\nresearch task formulation while filtering noise. With the MAHTG system, we\ncurate DeepResearch Arena with over 10,000 high-quality research tasks from\nover 200 academic seminars, spanning 12 disciplines, such as literature,\nhistory, and science. Our extensive evaluation shows that DeepResearch Arena\npresents substantial challenges for current state-of-the-art agents, with clear\nperformance gaps observed across different models.",
    "keywords_en": [
      "LLMs",
      "Research Abilities",
      "AI Agents",
      "Benchmark",
      "Seminar-Grounded Tasks"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-01T11:42:47.000Z",
    "download_time": "2025-09-07 22:32:56",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01396.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.01396\", \"arxiv_url\": \"https://arxiv.org/abs/2509.01396\"}"
  }
]