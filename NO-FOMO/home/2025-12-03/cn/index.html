<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-03</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2025-12-03</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Prompt Injection via Poetry</h2>
                <span class="published-time">Published: 2025-12-03 18:01:11</span>
                
                <p class="summary">The article explores a novel method of 'prompt injection' where large language models (LLMs) are manipulated through poetic inputs, effectively bypassing their built-in safety and ethical guidelines. This technique leverages the LLM's capacity for creative text generation and instruction following, even for malicious purposes. Specifically, by embedding harmful requests within a poetic structure, users can trick AI systems into generating content that they are explicitly programmed to refuse, such as instructions for dangerous or illegal activities. This vulnerability underscores a significant challenge in AI security, revealing how the semantic and structural nuances of input can subvert established content filters. The phenomenon highlights the sophistication of adversarial attacks against LLMs and the difficulties in creating truly robust and context-aware AI safety mechanisms. The implications are far-reaching, emphasizing the ongoing need for advanced research into developing AI systems that are more resilient to creative forms of manipulation and better aligned with human ethical standards, particularly when faced with unconventional input methods.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Prompt Injection</span><span>Large Language Model</span><span>AI Safety</span><span>Adversarial Attacks</span><span>Natural Language Processing</span><span>AI Ethics</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.wired.com/story/poems-can-trick-ai-into-helping-you-make-a-nuclear-weapon/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Anthropic reportedly preparing for $300B IPO</h2>
                <span class="published-time">Published: 2025-12-03 09:53:27</span>
                
                <p class="summary">Anthropic, a prominent artificial intelligence research company, is reportedly commencing preparations for an initial public offering (IPO) that could potentially value the firm at an estimated $300 billion. This significant financial move signals Anthropic's strategic growth ambitions and increasing presence within the competitive AI landscape, as indicated by reports of the company engaging legal counsel Wilson Sonsini for the public offering process. The anticipated IPO, with a potential target for a 2026 debut, positions Anthropic to secure substantial capital to fuel its continued advancements in AI research and development. This development is particularly notable given the backdrop of an accelerating 'AI race' with other industry leaders, including OpenAI. Such a large-scale public offering would empower Anthropic to significantly scale its operations, intensify its efforts in developing advanced large language models, and further establish its leadership in the rapidly evolving generative AI sector, thereby shaping the future of artificial intelligence technologies.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Anthropic</span><span>IPO</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Large Language Models</span><span>Tech Industry</span><span>Valuation</span><span>OpenAI</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://vechron.com/2025/12/anthropic-hires-wilson-sonsini-ipo-2026-openai-race/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Launch HN: Phind 3 (YC S22) – Every answer is a mini-app</h2>
                <span class="published-time">Published: 2025-12-03 17:47:15</span>
                
                <p class="summary">Phind 3, an advanced AI answer engine, has officially launched, aiming to revolutionize how users interact with search results by converting every answer into a dynamic "mini-app." This innovative platform is designed to instantly build and present a complete, interactive webpage tailored to a user's specific query. These mini-apps are richly featured, incorporating various widgets such as images, charts, diagrams, and maps, providing a highly visual and engaging experience. A key differentiator of Phind 3 is its interactive nature; users can manipulate these widgets, leading to dynamic updates in the content and enabling functionalities previously unavailable in static search results. For example, querying for apartment options in a specific area can generate an interactive finding experience, complete with customizable filters and a dynamic map view. This approach significantly enhances user engagement and provides a more functional, context-aware answer, demonstrating a leap forward in AI-driven information retrieval and application generation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Answer Engine</span><span>Interactive Applications</span><span>Mini-Apps</span><span>Generative AI</span><span>Data Visualization</span><span>Human-Computer Interaction</span><span>Web Applications</span><span>Search Technology</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://news.ycombinator.com/item?id=46137548" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Are we repeating the telecoms crash with AI datacenters?</h2>
                <span class="published-time">Published: 2025-12-03 11:14:56</span>
                
                <p class="summary">The article critically examines the parallels between the current massive investment in AI datacenters and the telecoms crash of the early 2000s, raising concerns about potential market overvaluation. It delves into whether the rapid capital expenditure on AI infrastructure, particularly for GPUs and specialized hardware, is sustainable or if it represents an overheating market prone to a significant correction. The analysis likely investigates the economic drivers, such as the insatiable demand from large language models and other compute-intensive AI applications, comparing the current build-out with historical overinvestment cycles in internet backbone and fiber optics. Key considerations include the immense power requirements, intricate supply chain dynamics for advanced semiconductors, and the long-term profitability of these vast computational assets. The piece aims to draw lessons from past bubbles while also acknowledging the unique technological advancements and demand factors driving the current AI boom, ultimately questioning if the industry is heading towards a similar financial downturn due to speculative investment and potential overcapacity.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI datacenters</span><span>telecoms crash</span><span>investment bubble</span><span>infrastructure</span><span>semiconductors</span><span>capital expenditure</span><span>market correction</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://martinalderson.com/posts/are-we-really-repeating-the-telecoms-crash-with-ai-datacenters/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Reverse engineering a $1B Legal AI tool exposed 100k+ confidential files</h2>
                <span class="published-time">Published: 2025-12-03 17:44:33</span>
                
                <p class="summary">A significant security vulnerability has been identified through the reverse engineering of a prominent legal AI tool, valued at an estimated one billion dollars, resulting in the exposure of over 100,000 confidential files. This incident critically exposes potential security shortcomings within sophisticated AI-powered platforms, especially those entrusted with handling highly sensitive data in heavily regulated sectors like the legal industry. The methodical reverse engineering effort revealed a fundamental API flaw that permitted unauthorized access to an extensive repository of confidential documents, potentially impacting a large number of clients and compromising privileged information. This widespread data exposure underscores the urgent and continuous need for robust security audits, rigorous penetration testing, and secure coding practices throughout the development and deployment lifecycle of artificial intelligence systems. Such diligence is paramount, particularly for applications processing proprietary or sensitive client data. The incident serves as a stark reminder that even highly valued technological solutions must incorporate comprehensive security measures as a foundational principle to avert catastrophic data breaches, maintain client confidentiality, and preserve user trust in AI-driven services. It also prompts deeper scrutiny into the security protocols and due diligence involved in integrating third-party AI solutions within professional legal practices.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Reverse Engineering</span><span>API Vulnerability</span><span>Data Exposure</span><span>Legal AI</span><span>Cybersecurity</span><span>Confidentiality</span><span>Information Security</span><span>AI Security</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://alexschapiro.com/security/vulnerability/2025/12/02/filevine-api-100k" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GSWT: Gaussian Splatting Wang Tiles</h2>
                <span class="published-time">Published: 2025-12-03 14:40:25</span>
                
                <p class="summary">The research initiative, GSWT: Gaussian Splatting Wang Tiles, introduces a novel approach to 3D scene representation and generation by integrating Gaussian Splatting with the principles of Wang Tiles. Gaussian Splatting is a rapidly emerging technique for real-time photorealistic rendering and novel view synthesis, leveraging 3D Gaussian distributions for scene representation. Wang Tiles, on the other hand, are a method for generating infinitely non-repeating yet seamlessly tiling patterns, often used in procedural texture generation and infinite world design. By combining these two distinct methodologies, GSWT aims to develop a system capable of creating expansive, procedurally generated 3D environments that maintain visual coherence and detail without repetitive artifacts. This fusion could offer significant advantages in memory efficiency and scalability for large-scale virtual environments, enabling dynamic and diverse scene synthesis from a limited set of base elements. The potential applications span across virtual reality, game development, and architectural visualization, offering a powerful tool for generating complex and varied 3D content efficiently.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Gaussian Splatting</span><span>Wang Tiles</span><span>3D Graphics</span><span>Procedural Generation</span><span>Novel View Synthesis</span><span>Real-time Rendering</span><span>Computer Vision</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Computer Vision</span><span>Generative AI</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://yunfan.zone/gswt_webpage/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>TrendRadar</h2>
                <span class="published-time">Published: 2025-12-03T11:58:41Z</span>
                
                <p class="summary">TrendRadar is an open-source GitHub project offering a lightweight, easy-to-deploy hot news assistant designed to filter information overload and deliver truly relevant news. It aggregates trending topics from over 11 major platforms (e.g., Zhihu, Douyin, Weibo) and provides intelligent push strategies, including daily summaries, current rankings, and incremental monitoring, customizable with personal keywords and advanced filtering. The platform features real-time trend analysis, a personalized hotness algorithm, and supports multi-channel notifications via WeChat, Feishu, DingTalk, Telegram, Email, ntfy, Bark, and Slack. It also boasts multi-platform adaptation through GitHub Pages and Docker deployments for data persistence. A key innovation in v3.0.0 is the AI analysis feature, powered by the MCP (Model Context Protocol), enabling natural language querying and deep insights into news data through 13 analytical tools, suitable for investors, content creators, and PR professionals seeking efficient information tracking.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>News Aggregation</span><span>Hotspot Monitoring</span><span>AI News Analysis</span><span>MCP Protocol</span><span>Real-time Notifications</span><span>GitHub Actions Deployment</span><span>Docker Containerization</span><span>Content Filtering</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/sansan0/TrendRadar" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent Development Kit (ADK) for Go</h2>
                <span class="published-time">Published: 2025-12-03T14:09:25Z</span>
                
                <p class="summary">The Agent Development Kit (ADK) for Go is an open-source, code-first toolkit designed to streamline the building, evaluation, and deployment of sophisticated AI agents. This flexible and modular framework applies robust software development principles to AI agent creation, simplifying the orchestration of workflows from simple tasks to complex multi-agent systems. While optimized for Google's Gemini, ADK maintains model and deployment agnosticism, ensuring broad compatibility with various AI models and frameworks. Leveraging Go's inherent strengths in concurrency and performance, this version is particularly well-suited for developers creating high-performance, cloud-native agent applications, with strong support for platforms like Google Cloud Run. Its key technical features include an idiomatic Go design, a rich ecosystem for integrating pre-built or custom tools to expand agent capabilities, and a code-first development approach that offers ultimate flexibility, testability, and version control. ADK facilitates the design of scalable applications through modular multi-agent systems, enabling developers to define agent logic and orchestration directly in Go.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Agent Development</span><span>Go Programming</span><span>Cloud-Native</span><span>Modular AI Systems</span><span>Agent Orchestration</span><span>Tool Ecosystem</span><span>Software Development Kit</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/google/adk-go" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</h2>
                <span class="published-time">Published: 2025-12-02T09:25:14.000Z</span>
                
                <p class="summary">We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>DeepSeek-V3.2</span><span>Large Language Models</span><span>Sparse Attention</span><span>Reinforcement Learning</span><span>AI Agent</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Deep Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.02556" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation</h2>
                <span class="published-time">Published: 2025-11-28T08:25:59.000Z</span>
                
                <p class="summary">Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Video Generation</span><span>Block Diffusion</span><span>Temporal Consistency</span><span>World Models</span><span>Long Video Benchmarks</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Computer Vision</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.22973" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds</h2>
                <span class="published-time">Published: 2025-11-30T20:58:13.000Z</span>
                
                <p class="summary">While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Autonomous Agents</span><span>LLM/VLM Agents</span><span>Simulator</span><span>Open-ended World Simulation</span><span>Multi-agent Systems</span></div>
                    <div class="area"><span class="label">Areas：</span><span>AI Agent</span><span>Large Language Model</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.01078" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration</h2>
                <span class="published-time">Published: 2025-11-26T18:59:46.000Z</span>
                
                <p class="summary">Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Tool Orchestration</span><span>Large Language Models</span><span>AI Agents</span><span>Reinforcement Learning</span><span>Efficiency</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.21689" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Guided Self-Evolving LLMs with Minimal Human Supervision</h2>
                <span class="published-time">Published: 2025-12-02T07:06:11.000Z</span>
                
                <p class="summary">AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Self-Evolving LLMs</span><span>Human Supervision</span><span>Self-Play</span><span>In-context grounding</span><span>Curriculum learning</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.02472" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence</h2>
                <span class="published-time">Published: 2025-12-02T10:29:51.000Z</span>
                
                <p class="summary">Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Video Generation</span><span>Rule-based Reasoning</span><span>Vision Foundation Models</span><span>RULER-Bench</span><span>Evaluation Benchmarks</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Computer Vision</span><span>Generative AI</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.02622" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>