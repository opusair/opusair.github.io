[
  {
    "id": "twitter_GoogleDeepMind_1935005262097551811",
    "source": "Twitter",
    "url": "https://x.com/GoogleDeepMind/status/1935005262097551811",
    "title_en": "GoogleDeepMind_Gemini 2.5 Series Models Updated and Generally Available",
    "summary_en": "Google DeepMind has announced significant updates to its Gemini model series, making Gemini 2.5 Flash and Pro widely available for developers. These powerful models are now ready for building and scaling production-grade AI applications, offering robust capabilities. Furthermore, a preview of Gemini 2.5 Flash-Lite has been introduced. This new model is highlighted as the fastest within the 2.5 family, designed to provide exceptionally quick responses at the lowest possible cost, thereby significantly lowering the entry barrier and operational expenses for AI development and deployment.",
    "keywords_en": [
      "Gemini",
      "Large Language Model",
      "AI Applications",
      "Product Launch",
      "Google DeepMind",
      "Flash-Lite"
    ],
    "area_en": [
      "Product Launch",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-06-17T16:02:57.000Z",
    "download_time": "2025-06-18 08:44:49",
    "visual_resource": [
      "screenshot/twitter/GoogleDeepMind_1935005262097551811.png"
    ],
    "extra_info": "{\"username\": \"GoogleDeepMind\", \"tweet_id\": \"1935005262097551811\"}"
  },
  {
    "id": "twitter_gdb_1934795523481063601",
    "source": "Twitter",
    "url": "https://twitter.com/gdb/status/1934795523481063601",
    "title_en": "gdb_ChatGPT Image Generation Now Available on WhatsApp",
    "summary_en": "Greg Brockman announced that ChatGPT's image generation feature is now universally available to WhatsApp users. Users can directly access AI image generation services within WhatsApp by calling the 1-800-ChatGPT number. This significantly expands ChatGPT's application scenarios, making its generative AI capabilities more conveniently accessible to a wide mobile user base and marking a significant step in integrating AI technology into instant messaging platforms.",
    "keywords_en": [
      "ChatGPT",
      "Image Generation",
      "WhatsApp",
      "OpenAI",
      "Generative AI",
      "Product Launch"
    ],
    "area_en": [
      "Generative AI",
      "Product Launch",
      "Industry News"
    ],
    "published_time": "2025-06-17T02:09:34.000Z",
    "download_time": "2025-06-18 08:27:57",
    "visual_resource": [
      "screenshot/twitter/gdb_1934795523481063601.png"
    ],
    "extra_info": "{\"username\": \"gdb\", \"tweet_id\": \"1934795523481063601\"}"
  },
  {
    "id": "twitter_SakanaAILabs_1934767254715117812",
    "source": "Twitter",
    "url": "https://twitter.com/SakanaAILabs/status/1934767254715117812",
    "title_en": "SakanaAILabs_Introduces ALE-Bench and ALE-Agent for Automating Hard Optimization Problems",
    "summary_en": "Sakana AI introduces ALE-Bench benchmark and ALE-Agent, aiming to automate solutions for NP-hard optimization problems. ALE-Bench, developed with AtCoder, is a coding benchmark focusing on open-ended optimization challenges requiring long-horizon and creative reasoning. Their end-to-end agent, ALE-Agent, achieved an impressive 21st place (top 2%) among 1,000 human participants in the May 2025 AtCoder Heuristic Competition, demonstrating AI's strong capability in real-world hard optimization problems like logistics and routing.",
    "keywords_en": [
      "ALE-Bench",
      "ALE-Agent",
      "Hard Optimization Problems",
      "Algorithm Engineering",
      "AI Agent",
      "AtCoder"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Research Progress"
    ],
    "published_time": "2025-06-17T00:17:14.000Z",
    "download_time": "2025-06-18 08:27:48",
    "visual_resource": [
      "screenshot/twitter/SakanaAILabs_1934767254715117812.png"
    ],
    "extra_info": "{\"username\": \"SakanaAILabs\", \"tweet_id\": \"1934767254715117812\"}"
  },
  {
    "id": "twitter_fabianstelzer_1935038388782113197",
    "source": "Twitter",
    "url": "https://twitter.com/fabianstelzer/status/1935038388782113197",
    "title_en": "fabianstelzer_Agent-Generated Long Videos Experiment and Future Authorship Model",
    "summary_en": "Fabianstelzer is experimenting with agents to generate longer videos using tools like Flux Ultra, Kling 2.1, and MMAudio, along with automated stitching. Using \"roman legionnaire travel log\" as a prompt, he suggests that future video authorship will shift from creating films directly to creating agents that can create films, indicating a profound change in content creation paradigms.",
    "keywords_en": [
      "AI Agent",
      "Video Generation",
      "AI Creation",
      "Flux Ultra",
      "Kling 2.1",
      "MMAudio"
    ],
    "area_en": [
      "AI Agent",
      "Generative AI",
      "Tech News"
    ],
    "published_time": "2025-06-17T18:14:37.000Z",
    "download_time": "2025-06-18 08:27:59",
    "visual_resource": [
      "screenshot/twitter/fabianstelzer_1935038388782113197.png"
    ],
    "extra_info": "{\"username\": \"fabianstelzer\", \"tweet_id\": \"1935038388782113197\"}"
  },
  {
    "id": "twitter_UnslothAI_1934983471912591612",
    "source": "Twitter",
    "url": "https://twitter.com/UnslothAI/status/1934983471912591612",
    "title_en": "UnslothAI_Launches Comprehensive Reinforcement Learning Guide for LLMs",
    "summary_en": "Unsloth AI has released a comprehensive guide on Reinforcement Learning (RL) for Large Language Models (LLMs). The guide delves into RL's objectives and its pivotal role in building intelligent AI agents, explains why models like o3, Claude 4, and R1 utilize RL, and details techniques such as GRPO, RLHF, and DPO, along with reward functions. It also provides practical instructions for training local R1 models using Unsloth.",
    "keywords_en": [
      "Reinforcement Learning",
      "Large Language Models",
      "AI Agents",
      "RLHF",
      "DPO",
      "Unsloth"
    ],
    "area_en": [
      "Machine Learning",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-17T14:36:24.000Z",
    "download_time": "2025-06-18 08:28:31",
    "visual_resource": [
      "screenshot/twitter/UnslothAI_1934983471912591612.png"
    ],
    "extra_info": "{\"username\": \"UnslothAI\", \"tweet_id\": \"1934983471912591612\"}"
  },
  {
    "id": "twitter_ylecun_1935108028891861393",
    "source": "Twitter",
    "url": "https://x.com/ylecun/status/1935108028891861393",
    "title_en": "ylecun_Discussion on AI Long-Term Task Success Rate and Error Rate",
    "summary_en": "Prominent AI scientist Yann LeCun quotes Benjamin Todd's explanation for why AIs perform poorly in long-duration tasks. The theory posits that if an AI has a 10% chance of error every 10 minutes, the success rate decreases exponentially with task length: 53% for 1 hour, but only 0.002% for 10 hours. This \"constant error rate\" theory has been tested and shown to fit the data well, highlighting the limitations of AI in complex, sustained operations.",
    "keywords_en": [
      "AI performance",
      "error rate",
      "task duration",
      "success rate",
      "AI limitations"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Research Progress"
    ],
    "published_time": "2025-06-17T22:51:21.000Z",
    "download_time": "2025-06-18 08:38:24",
    "visual_resource": [
      "screenshot/twitter/ylecun_1935108028891861393.png"
    ],
    "extra_info": "{\"username\": \"ylecun\", \"tweet_id\": \"1935108028891861393\"}"
  },
  {
    "id": "OuBPwSRg0xBVZKzAHGOfLw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/OuBPwSRg0xBVZKzAHGOfLw",
    "title_en": "Horizon Robotics Open-Sources \"3D Physical World Generation\" Engine, One-Click Generation of Full URDF Assets in 5 Minutes, Seamlessly Supporting {Gym&Isaac}!",
    "summary_en": "Horizon Robotics has officially open-sourced EmbodiedGen, an innovative interactive platform designed for 3D physical world generation. This groundbreaking engine facilitates the low-cost and scalable creation of high-quality, controllable, and highly realistic 3D assets, all in the widely adopted URDF format. A key feature is its ability to embed precise physical properties and real-world scale into these assets, making them directly compatible with popular physics simulation environments such as OpenAI Gym and NVIDIA Isaac. This seamless integration is crucial for advanced training and rigorous evaluation of embodied AI systems. EmbodiedGen is presented as a comprehensive toolkit, comprising six essential modules, including image-to-3D, text-to-3D, and texture generation. By harnessing the power of generative AI, the platform effectively addresses critical generalization and evaluation challenges inherent in embodied intelligence research. Its practical applications span diverse areas, from efficient 3D scene generation to robust real-to-simulation transfers, thereby significantly boosting the efficiency of 3D asset creation and advancing the frontier of simulation-based research.",
    "keywords_en": [
      "EmbodiedGen",
      "3D Generation",
      "Physical World",
      "URDF",
      "Generative AI",
      "Embodied Intelligence"
    ],
    "area_en": [
      "Generative AI",
      "Robotics",
      "AI Agent"
    ],
    "published_time": "2025-06-17T23:45:27.000Z",
    "download_time": "2025-06-18T17:15:09.020274",
    "visual_resource": [
      "screenshot/wechat/wechat_image_OuBPwSRg0xBVZKzAHGOfLw.png"
    ],
    "extra_info": null
  },
  {
    "id": "O8_jdzQevn5yjVxeuMi2Lg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/O8_jdzQevn5yjVxeuMi2Lg",
    "title_en": "DeepSeek-R1 Tops Web Programming Crowdsourced Ranking, Surpassing Claude 4",
    "summary_en": "The latest report indicates that DeepSeek's new R1-0528 version has surpassed Claude Opus 4, previously recognized as the \"world's strongest coding model,\" to claim the global top spot in web programming capabilities. The article demonstrates DeepSeek-R1's robust performance through practical tests, showcasing its strong ability to generate code for solar system animations, Three.js simulations, and AGI-themed websites, although there is still room for improvement in certain complex tasks like the Tetris game. As an open-source model, DeepSeek-R1 not only excels in programming rankings but is also rated as the best open-source text model currently available, ranking sixth overall. It is notably more accessible and user-friendly for domestic users. The article also briefly mentions the breakthrough of Kimi's new model in achieving open-source SOTA for code.",
    "keywords_en": [
      "DeepSeek-R1",
      "Web Programming",
      "Large Language Model",
      "Open-source Model",
      "Code Generation"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-06-17T07:42:00.000Z",
    "download_time": "2025-06-18T17:15:07.402767",
    "visual_resource": [
      "screenshot/wechat/wechat_image_O8_jdzQevn5yjVxeuMi2Lg.png"
    ],
    "extra_info": null
  },
  {
    "id": "2vBomVVLDz8phY2j94BS8g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/2vBomVVLDz8phY2j94BS8g",
    "title_en": "First AI Achieves Perfect Score in Gaokao Math, Impressing Renowned Teachers with Astonishing Results",
    "summary_en": "ByteDance's Doubao AI Education Edition, integrated into the Doubao AI Study App, achieved remarkable scores in the National Gaokao Math exams, scoring 144 on Paper I and a perfect 150 on Paper II. The full score on Paper II was rigorously validated by six senior math teachers with over a decade of average teaching experience, underscoring the AI's exceptional capabilities. The model demonstrated superior multimodal understanding, information integration, logical reasoning, and formula rendering, excelling particularly in challenging multiple-choice and final problems with concise thought processes and stable solutions. The article highlights the AI's consistent performance, evidenced by multiple public tests. Renowned educators lauded its potential as a powerful teaching assistant and learning companion, capable of providing high-quality, personalized explanations. This advancement is expected to foster rigorous reasoning skills in students and address educational resource disparities, signaling a promising future for AI in education.",
    "keywords_en": [
      "Gaokao Math",
      "AI Problem Solving",
      "Doubao Large Model",
      "Multimodal AI",
      "AI Education",
      "Model Stability"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Multimodal"
    ],
    "published_time": "2025-06-17T05:41:28.000Z",
    "download_time": "2025-06-18T17:15:05.932065",
    "visual_resource": [
      "screenshot/wechat/wechat_image_2vBomVVLDz8phY2j94BS8g.png"
    ],
    "extra_info": null
  },
  {
    "id": "ZAvHLKYu5J7eZUlzMWe9AA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ZAvHLKYu5J7eZUlzMWe9AA",
    "title_en": "Transformer Eight Founders' Startup: AI Sweeps NP-Hard Problem Competition, Top 2% Player is an AI Agent!",
    "summary_en": "Sakana AI, in collaboration with AtCoder, has introduced ALE-Bench, a new benchmark designed to evaluate AI's proficiency in solving NP-hard problems. Their ALE-Agent, built upon Gemini 2.5 Pro, demonstrated exceptional performance in the AtCoder Heuristic Competition. Competing against over a thousand human participants, the agent secured the 21st position, placing it within the top 2%. This achievement signifies a significant breakthrough for AI in tackling complex real-world optimization challenges such as logistics pathfinding and task scheduling. The ALE-Agent leverages domain-specific knowledge prompting and generates diverse solutions to achieve its results. While the agent still faces limitations in debugging and analyzing code complexity, its rapid, iterative optimization capabilities highlight immense potential. This development suggests a future where AI could match or even surpass top human experts in algorithm engineering.",
    "keywords_en": [
      "NP-hard problems",
      "AI agent",
      "Algorithm engineering",
      "AtCoder",
      "ALE-Bench",
      "Optimization problems"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-06-17T05:41:28.000Z",
    "download_time": "2025-06-18T17:15:05.283908",
    "visual_resource": [
      "screenshot/wechat/wechat_image_ZAvHLKYu5J7eZUlzMWe9AA.png"
    ],
    "extra_info": null
  },
  {
    "id": "sIa9qIzPuykCysAVgeGxew",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/sIa9qIzPuykCysAVgeGxew",
    "title_en": "First Comprehensive Survey on Speech Large Models Accepted by ACL 2025 Main Conference",
    "summary_en": "A comprehensive survey paper titled \"Recent Advances in Speech Language Models: A Survey,\" authored by a Chinese University of Hong Kong team, has been accepted by the ACL 2025 main conference, marking the first systematic review in this field. The survey positions Speech Large Models (SpeechLM) as the next frontier in AI, aiming to overcome the limitations of traditional speech interaction, such as information loss, severe latency, and error accumulation, by enabling end-to-end natural human-machine speech dialogue. The paper meticulously details SpeechLM's technical architecture, comprising speech tokenizers, language models, and vocoders, along with its training strategies, including pre-training and instruction tuning. It explores advanced interaction paradigms like full-duplex communication, highlighting SpeechLM's broad applications in natural dialogue, personalized assistants, and emotional speech generation. Furthermore, the survey discusses evaluation methodologies and future challenges, such as optimization, real-time processing, safety, and support for low-resource languages. This seminal work anticipates that SpeechLM will usher in a new era of speech AI, fundamentally transforming human-computer interaction by enabling more natural and intuitive communication.",
    "keywords_en": [
      "Speech Large Models",
      "Speech Interaction",
      "Human-Computer Interaction",
      "Survey",
      "Multimodal"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Multimodal"
    ],
    "published_time": "2025-06-17T04:51:20.000Z",
    "download_time": "2025-06-18T17:15:14.403757",
    "visual_resource": [
      "screenshot/wechat/wechat_image_sIa9qIzPuykCysAVgeGxew.png"
    ],
    "extra_info": null
  },
  {
    "id": "u_vREBzJ7Qeckn4OEWtoMQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/u_vREBzJ7Qeckn4OEWtoMQ",
    "title_en": "XPeng, the First New Energy Vehicle Company to Transform into an AI Company, Showcases Next-Gen Autonomous Driving Model at Global AI Summit",
    "summary_en": "XPeng, the first new energy vehicle company to transform into an AI company, showcased its next-generation autonomous driving foundation model and development progress at CVPR 2025, a global computer vision summit. The article details the VLA+VLM models integrated into the XPeng G7, highlighting its L3 intelligent driving strategy built upon \"massive computing power, large models, and big data.\" XPeng has established a comprehensive cloud-to-edge AI system, including a 72-billion-parameter cloud foundation model and a 10 EFLOPS intelligent computing cluster, validating the Scaling Laws in autonomous driving. Its world foundation model possesses visual understanding, chain-of-thought reasoning, and action generation capabilities, continuously evolving through reinforcement learning. This demonstrates a leading human-like driving performance, signaling a new paradigm shift towards \"AI-developed cars\" in intelligent driving. The company's deep investment in AI infrastructure and proprietary technology stack positions it at the forefront of the industry, aiming to extend this advanced AI capability to future applications like AI robots and flying cars.",
    "keywords_en": [
      "XPeng",
      "Autonomous Driving",
      "Large Models",
      "Foundation Models",
      "World Model"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Computer Vision"
    ],
    "published_time": "2025-06-17T04:51:20.000Z",
    "download_time": "2025-06-18T17:15:10.059113",
    "visual_resource": [
      "screenshot/wechat/wechat_image_u_vREBzJ7Qeckn4OEWtoMQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "jan",
    "source": "GitHub",
    "url": "https://github.com/menloresearch/jan",
    "title_en": "Jan - Local AI Assistant",
    "summary_en": "Jan is a robust desktop AI assistant engineered for entirely offline operation, placing a strong emphasis on user privacy and data security. It empowers users to download and execute various large language models (LLMs) such as Llama, Gemma, and Qwen directly on their local machines, ensuring conversations remain private. Beyond local capabilities, Jan also facilitates seamless integration with leading cloud AI providers like OpenAI, Anthropic, Mistral, and Groq. The platform features an OpenAI-compatible API, allowing other applications to easily connect and leverage its functionalities. Users can also create and deploy specialized custom AI assistants tailored to specific tasks. Developed using Node.js, Rust, and Tauri, Jan provides a comprehensive, secure, and versatile local AI interaction environment, making advanced AI accessible without compromising personal data.",
    "keywords_en": [
      "Local AI",
      "Offline Assistant",
      "Large Language Model",
      "Privacy Protection",
      "Desktop Application",
      "Tauri",
      "Llama.cpp",
      "AI Assistant"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-18T07:03:55Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://github.com/menloresearch/jan/raw/main/JanBanner.png"
    ],
    "extra_info": null
  },
  {
    "id": "anthropic-cookbook",
    "source": "GitHub",
    "url": "https://github.com/anthropics/anthropic-cookbook",
    "title_en": "Anthropic Cookbook",
    "summary_en": "The Anthropic Cookbook is a collection of code and guides designed for developers building applications with Claude. It offers readily integratable code snippets covering areas such as text classification, Retrieval Augmented Generation (RAG), summarization, tool use, multimodal capabilities, and advanced techniques. The project aims to assist developers in leveraging the Claude API, providing Python examples and concepts adaptable to other programming languages, thereby enhancing the efficiency of AI application development.",
    "keywords_en": [
      "Claude",
      "LLM Applications",
      "Code Examples",
      "API Development",
      "Natural Language Processing",
      "Multimodal",
      "AI Agent",
      "Retrieval Augmented Generation"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-13T19:28:20Z",
    "download_time": "2024-05-16 10:00:00",
    "visual_resource": [
      "screenshot/github/anthropic-cookbook.png"
    ],
    "extra_info": null
  },
  {
    "id": "ragflow",
    "source": "GitHub",
    "url": "https://github.com/infiniflow/ragflow",
    "title_en": "RAGFlow",
    "summary_en": "RAGFlow is an open-source Retrieval-Augmented Generation (RAG) engine, leveraging deep document understanding to deliver highly accurate and truthful question-answering for businesses of any scale. It seamlessly integrates with Large Language Models (LLMs) to process and extract knowledge from diverse, complex formatted data, providing well-founded, traceable citations that significantly reduce hallucinations. Its core strengths include sophisticated deep document understanding, intelligent template-based chunking for precise information retrieval, and broad compatibility with heterogeneous data sources like Word, Excel, PDFs, and web pages. RAGFlow offers an automated and streamlined RAG workflow, supporting configurable LLMs and embedding models, along with intuitive APIs for effortless integration into existing systems, making it a robust platform for building reliable and efficient knowledge-based AI applications.",
    "keywords_en": [
      "Retrieval-Augmented Generation",
      "Deep Document Understanding",
      "Large Language Model",
      "Knowledge Extraction",
      "Automated Workflow",
      "Docker"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-06-18T08:02:36Z",
    "download_time": "2024-05-23 10:00:00",
    "visual_resource": [
      "screenshot/github/ragflow.png"
    ],
    "extra_info": null
  },
  {
    "id": "DeepEP",
    "source": "GitHub",
    "url": "https://github.com/deepseek-ai/DeepEP",
    "title_en": "DeepEP",
    "summary_en": "DeepEP is a cutting-edge communication library specifically designed for Mixture-of-Experts (MoE) and expert parallelism, providing high-throughput and low-latency all-to-all GPU kernels, alongside support for low-precision operations such as FP8. It features optimized kernels for asymmetric-domain bandwidth forwarding, crucial for aligning with the DeepSeek-V3 gating algorithm, making it highly effective for both training and inference prefilling tasks. The library also offers fine-grained SM number control. For latency-sensitive inference decoding, DeepEP includes a set of pure RDMA low-latency kernels. A notable innovation is its hook-based communication-computation overlapping method, which uniquely avoids occupying any Streaming Multiprocessor (SM) resources, thereby significantly boosting the communication efficiency and overall performance of MoE models in various demanding scenarios.",
    "keywords_en": [
      "MoE",
      "Expert Parallelism",
      "Communication Library",
      "GPU Kernels",
      "Low Latency",
      "High Throughput",
      "Deep Learning",
      "Model Training"
    ],
    "area_en": [
      "Deep Learning",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-18T08:04:42Z",
    "download_time": "2024-07-30 12:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/figures/normal.png",
      "https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/figures/low-latency.png"
    ],
    "extra_info": null
  },
  {
    "id": "awesome-llm-apps",
    "source": "GitHub",
    "url": "https://github.com/Shubhamsaboo/awesome-llm-apps",
    "title_en": "🌟 Awesome LLM Apps",
    "summary_en": "The GitHub repository \"Awesome LLM Apps\" is a curated collection of Large Language Model (LLM) applications, incorporating various technologies such as Retrieval Augmented Generation (RAG), AI Agents, Multi-agent Teams, MCP (Multimodal Control Policy), and Voice Agents. It showcases practical applications built using models from OpenAI, Anthropic, Google, as well as open-source models like DeepSeek, Qwen, and Llama. This project aims to help developers explore the application potential of LLMs across different domains and foster the growth of the open-source LLM application ecosystem.",
    "keywords_en": [
      "Large Language Model",
      "AI Agent",
      "Retrieval Augmented Generation",
      "Multi-agent System",
      "Voice AI",
      "LLM Applications",
      "Open Source Models",
      "Model Fine-tuning"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-15T16:08:42Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://github.com/Shubhamsaboo/awesome-llm-apps/raw/main/docs/banner/unwind_black.png",
      "https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&type=Date"
    ],
    "extra_info": null
  },
  {
    "id": "continue",
    "source": "GitHub",
    "url": "https://github.com/continuedev/continue",
    "title_en": "Continue",
    "summary_en": "Continue is an open-source platform designed to revolutionize developer workflows by providing highly customizable AI code assistants. Through its robust VS Code and JetBrains extensions, it seamlessly integrates advanced AI capabilities directly into the integrated development environment. Key features include an 'Agent' for substantial codebase changes, 'Chat' for instant LLM assistance, 'Autocomplete' for inline code suggestions, and 'Edit' for convenient in-file modifications. This comprehensive suite allows developers to leverage large language models for enhanced productivity, streamlining tasks from code generation and debugging to refactoring and knowledge retrieval, all without context switching. Continue also offers a centralized hub for models, rules, and prompts, fostering a collaborative ecosystem where developers can create, share, and utilize bespoke AI programming tools, ultimately accelerating software development cycles and improving code quality.",
    "keywords_en": [
      "AI Code Assistant",
      "IDE Integration",
      "Large Language Model",
      "Intelligent Programming",
      "Code Completion",
      "Developer Tools"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-18T00:04:06Z",
    "download_time": "2024-07-30 10:30:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/continuedev/continue/main/media/readme.png",
      "https://raw.githubusercontent.com/continuedev/continue/main/docs/static/img/agent.gif",
      "https://raw.githubusercontent.com/continuedev/continue/main/docs/static/img/chat.gif"
    ],
    "extra_info": null
  },
  {
    "id": "2506.13585",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.13585",
    "title_en": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention",
    "summary_en": "We introduce MiniMax-M1, the world's first open-weight, large-scale\nhybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid\nMixture-of-Experts (MoE) architecture combined with a lightning attention\nmechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters\nactivated per token. The M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning\nattention mechanism in MiniMax-M1 enables efficient scaling of test-time\ncompute. These properties make M1 particularly suitable for complex tasks that\nrequire processing long inputs and thinking extensively. MiniMax-M1 is trained\nusing large-scale reinforcement learning (RL) on diverse problems including\nsandbox-based, real-world software engineering environments. In addition to\nM1's inherent efficiency advantage for RL training, we propose CISPO, a novel\nRL algorithm to further enhance RL efficiency. CISPO clips importance sampling\nweights rather than token updates, outperforming other competitive RL variants.\nCombining hybrid-attention and CISPO enables MiniMax-M1's full RL training on\n512 H800 GPUs to complete in only three weeks, with a rental cost of just\n$534,700. We release two versions of MiniMax-M1 models with 40K and 80K\nthinking budgets respectively, where the 40K model represents an intermediate\nphase of the 80K training. Experiments on standard benchmarks show that our\nmodels are comparable or superior to strong open-weight models such as the\noriginal DeepSeek-R1 and Qwen3-235B, with particular strengths in complex\nsoftware engineering, tool utilization, and long-context tasks. We publicly\nrelease MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.",
    "keywords_en": [
      "MiniMax-M1",
      "Hybrid-attention",
      "Lightning Attention",
      "Reinforcement Learning",
      "Long-context"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-06-16T15:08:02.000Z",
    "download_time": "2025-06-18 01:56:47",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13585.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.13585\", \"arxiv_url\": \"https://arxiv.org/abs/2506.13585\"}"
  },
  {
    "id": "2506.11763",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.11763",
    "title_en": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
    "summary_en": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.",
    "keywords_en": [
      "Deep Research Agents",
      "Benchmark",
      "LLM-based Agents",
      "Information Retrieval",
      "Evaluation Methodology"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-13T13:17:32.000Z",
    "download_time": "2025-06-18 01:56:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11763.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.11763\", \"arxiv_url\": \"https://arxiv.org/abs/2506.11763\"}"
  },
  {
    "id": "2506.13654",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.13654",
    "title_en": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning",
    "summary_en": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,\nin days and weeks) egocentric videos, which leverages a structured\nChain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained\nvia reinforcement learning (RL). Inspired by human problem-solving strategies,\nCoTT decomposes complex reasoning into modular steps, with the RL agent\ninvoking specific tools, one per step, to iteratively and collaboratively\nanswer sub-questions tackling such tasks as temporal retrieval and multi-modal\nunderstanding. We design a two-stage training paradigm involving supervised\nfinetuning (SFT) of a pretrained language model using CoTT data and RL to\nenable our agent to dynamically propose step-by-step tools for long-range\nreasoning. To facilitate training, we construct a dataset called Ego-R1 Data,\nwhich consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our\nEgo-R1 agent is evaluated on a newly curated week-long video QA benchmark,\nEgo-R1 Bench, which contains human-verified QA pairs from hybrid sources.\nExtensive results demonstrate that the dynamic, tool-augmented chain-of-thought\nreasoning by our Ego-R1 Agent can effectively tackle the unique challenges of\nunderstanding ultra-long egocentric videos, significantly extending the time\ncoverage from few hours to a week.",
    "keywords_en": [
      "Egocentric Video",
      "Video Reasoning",
      "Chain-of-Tool-Thought",
      "Reinforcement Learning",
      "AI Agent"
    ],
    "area_en": [
      "Video Understanding",
      "AI Agent",
      "Multimodal"
    ],
    "published_time": "2025-06-16T16:17:08.000Z",
    "download_time": "2025-06-18 01:56:47",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13654.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.13654\", \"arxiv_url\": \"https://arxiv.org/abs/2506.13654\"}"
  },
  {
    "id": "2506.09482",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.09482",
    "title_en": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference\n  Autoregression",
    "summary_en": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.",
    "keywords_en": [
      "TransDiff",
      "Image generation",
      "Autoregressive Transformer",
      "Diffusion models",
      "Multi-Reference Autoregression"
    ],
    "area_en": [
      "Computer Vision",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-06-11T07:50:31.000Z",
    "download_time": "2025-06-18 01:56:43",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09482.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.09482\", \"arxiv_url\": \"https://arxiv.org/abs/2506.09482\"}"
  },
  {
    "id": "2506.13759",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.13759",
    "title_en": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
    "summary_en": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
    "keywords_en": [
      "Discrete Diffusion",
      "Large Language Models",
      "Multimodal Models",
      "Parallel Generation",
      "Survey"
    ],
    "area_en": [
      "Large Language Model",
      "Multimodal",
      "Generative AI"
    ],
    "published_time": "2025-06-16T17:59:08.000Z",
    "download_time": "2025-06-18 01:56:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13759.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.13759\", \"arxiv_url\": \"https://arxiv.org/abs/2506.13759\"}"
  },
  {
    "id": "2506.12299",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.12299",
    "title_en": "QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety",
    "summary_en": "The recent advancements in Large Language Models(LLMs) have had a significant\nimpact on a wide range of fields, from general domains to specialized areas.\nHowever, these advancements have also significantly increased the potential for\nmalicious users to exploit harmful and jailbreak prompts for malicious attacks.\nAlthough there have been many efforts to prevent harmful prompts and jailbreak\nprompts, protecting LLMs from such malicious attacks remains an important and\nchallenging task. In this paper, we propose QGuard, a simple yet effective\nsafety guard method, that utilizes question prompting to block harmful prompts\nin a zero-shot manner. Our method can defend LLMs not only from text-based\nharmful prompts but also from multi-modal harmful prompt attacks. Moreover, by\ndiversifying and modifying guard questions, our approach remains robust against\nthe latest harmful prompts without fine-tuning. Experimental results show that\nour model performs competitively on both text-only and multi-modal harmful\ndatasets. Additionally, by providing an analysis of question prompting, we\nenable a white-box analysis of user inputs. We believe our method provides\nvaluable insights for real-world LLM services in mitigating security risks\nassociated with harmful prompts.",
    "keywords_en": [
      "Large Language Models",
      "LLM Safety",
      "Zero-shot",
      "Multimodal",
      "Harmful Prompts"
    ],
    "area_en": [
      "Large Language Model",
      "Multimodal",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-14T01:23:50.000Z",
    "download_time": "2025-06-18 01:56:51",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12299.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.12299\", \"arxiv_url\": \"https://arxiv.org/abs/2506.12299\"}"
  }
]