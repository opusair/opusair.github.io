<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-10-02</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="." class="active">‰∏≠Êñá</a>
                <a href="en/" class="">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-10-02</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† ËøîÂõû‰∏ªÈ°µ</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ ÊúÄÊñ∞Êó•Êä•</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ ÂÖ≥‰∫éÊàë‰ª¨</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Gemini 3.0 Pro ‚Äì early tests</h2>
                <span class="published-time">Published: 2025-10-02 18:26:57</span>
                
                <p class="summary">Initial reports have emerged regarding the early testing phase of Gemini 3.0 Pro, Google's next-generation artificial intelligence model. These preliminary evaluations are crucial for assessing the model's enhanced capabilities across various benchmarks, including its performance in complex reasoning, advanced multimodal understanding, and sophisticated language generation tasks. The 'Pro' designation suggests a strategic focus on advanced applications, potentially targeting enterprise solutions, high-demand computational scenarios, and developers requiring state-of-the-art performance. Industry observers are keenly anticipating further details on its architectural improvements, scalability, and efficiency, especially in comparison to previous iterations of the Gemini family and competing models. The early test phase is vital for identifying potential areas for optimization, validating the model's stability and robustness, and ensuring its readiness for deployment across a diverse range of real-world applications before a wider public release. This period helps to refine its capabilities and solidify its competitive position in the rapidly evolving landscape of large language and multimodal models, setting expectations for its impact on AI development.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini 3.0 Pro</span><span>Large Language Model</span><span>Multimodal AI</span><span>AI Model Evaluation</span><span>Generative AI</span><span>AI Performance</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/chetaslua/status/1973694615518880236" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAI's H1 2025: $4.3B in income, $13.5B in loss</h2>
                <span class="published-time">Published: 2025-10-02 18:37:28</span>
                
                <p class="summary">OpenAI, a prominent artificial intelligence research and deployment company, reportedly achieved a significant revenue of $4.3 billion during the first half of 2025. This financial performance underscores the rapid commercialization success and expansion of its AI services, including its widely adopted large language models and other generative AI technologies. Despite this substantial income, the company is also reported to have incurred considerable losses, totaling $13.5 billion, within the same period. This substantial difference between revenue and loss highlights OpenAI's aggressive and ongoing investment strategy in cutting-edge AI research and development. It reflects massive expenditures on advanced computing infrastructure, talent acquisition, and foundational model training, crucial for sustaining its competitive edge in the fast-evolving AI landscape and pushing the boundaries of machine learning innovation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>OpenAI</span><span>Financial Performance</span><span>AI Industry</span><span>Generative AI</span><span>Revenue Growth</span><span>AI Investment</span><span>Large Language Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.techinasia.com/news/openais-revenue-rises-16-to-4-3b-in-h1-2025" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Daniel Stenberg on 22 curl bugs found by AI and fixed</h2>
                <span class="published-time">Published: 2025-10-02 13:29:55</span>
                
                <p class="summary">Daniel Stenberg, the lead developer of the widely-used curl project, has confirmed the successful identification and subsequent resolution of 22 bugs discovered through an artificial intelligence system. This significant development highlights the increasing utility of AI in enhancing software quality assurance and security. The AI system's ability to pinpoint these vulnerabilities underscores its potential as a powerful tool in automated bug detection, complementing traditional testing and manual review processes. The findings not only improve the robustness and reliability of curl, a critical component in countless applications and systems, but also offer valuable insights into the practical application of AI in identifying complex code flaws. This event serves as a notable case study demonstrating how AI technologies can contribute directly to the stability and security of foundational open-source software. The collaboration between AI-driven analysis and human expertise in patching these issues represents an important step forward in modern software development practices.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>curl</span><span>AI bug detection</span><span>software vulnerabilities</span><span>code quality</span><span>security patches</span><span>software development</span><span>automated analysis</span><span>open source</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mastodon.social/@bagder/115241241075258997" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Two Amazon delivery drones crash into crane in commercial area of Tolleson, AZ</h2>
                <span class="published-time">Published: 2025-10-02 14:52:49</span>
                
                <p class="summary">Two Amazon Prime MK30 delivery drones recently crashed into a crane located in a commercial area of Tolleson, Arizona. This incident, extensively reported by news outlets like The Verge and ABC15, brings to the forefront significant operational challenges and safety considerations inherent in advanced autonomous drone delivery systems. The collision underscores the intricate complexities involved in deploying Unmanned Aerial Vehicles (UAVs) within urban or semi-urban environments, particularly concerning the efficacy of their obstacle detection, avoidance, and overall navigation capabilities. Ongoing investigations are anticipated to thoroughly examine the drones' integrated navigation systems, sensor array performance, and underlying flight control algorithms to pinpoint the precise root cause of this dual collision. Such occurrences are instrumental in catalyzing the refinement of critical safety protocols, bolstering regulatory frameworks, and advancing the foundational artificial intelligence and robotics technologies that govern safe and reliable autonomous aerial operations, ultimately aiming to foster public trust and widespread adoption of future drone delivery services. The incident has reportedly prompted Amazon to temporarily suspend its drone delivery operations in the affected region.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Drone technology</span><span>Autonomous flight</span><span>Aviation safety</span><span>Delivery drones</span><span>UAV</span><span>Obstacle avoidance</span><span>AI systems</span><span>Robotics</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>Artificial Intelligence</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.abc15.com/news/region-west-valley/tolleson/two-amazon-delivery-drones-crash-into-crane-in-commercial-area-of-tolleson" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>NL Judge: Meta must respect user's choice of recommendation system</h2>
                <span class="published-time">Published: 2025-10-02 11:32:19</span>
                
                <p class="summary">A Dutch court has ruled that Meta, the parent company of Facebook and Instagram, must provide users with the option to choose their preferred recommendation system. This landmark decision stems from a lawsuit initiated by Bits of Freedom, a digital rights organization. The ruling mandates Meta to move beyond purely personalized, data-driven algorithmic feeds, empowering users to opt for alternative or non-personalized content delivery mechanisms. This judgment has significant implications for how large social media platforms design and implement their recommendation algorithms, emphasizing user autonomy and control over their digital experience. It signals a growing legal push to regulate the pervasive influence of artificial intelligence in content curation, potentially paving the way for similar requirements in other jurisdictions. Compliance will necessitate considerable adjustments to Meta's underlying machine learning infrastructure, highlighting a crucial intersection between user rights, technological design, and platform governance.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Recommendation systems</span><span>Algorithmic control</span><span>User choice</span><span>Data privacy</span><span>Platform regulation</span><span>Digital rights</span><span>Machine learning ethics</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.bitsoffreedom.nl/2025/10/02/judge-in-the-bits-of-freedom-vs-meta-lawsuit-meta-must-respect-users-choice/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Meta will listen into AI conversations to personalize ads</h2>
                <span class="published-time">Published: 2025-10-02 12:36:39</span>
                
                <p class="summary">Meta is reportedly planning to leverage user interactions with its AI systems to personalize advertisements, a development that signals a significant shift in data collection and targeted advertising strategies. This approach would involve analyzing conversations users have with Meta's AI assistants to infer preferences, interests, and needs, subsequently tailoring ad content. While Meta's stated goal is to enhance user experience through more relevant ads, the initiative immediately raises substantial privacy concerns regarding the extent of data collection from private conversations and its potential implications for user trust and data security. Critics are likely to question the ethical boundaries of such a practice, especially concerning the processing of sensitive conversational data. This move underscores the ongoing tension between technological advancement in AI, commercial interests in advertising, and individual privacy rights, potentially prompting increased regulatory scrutiny on how AI-driven platforms handle user information. The strategy could also reshape the competitive landscape for personalized advertising, pushing other tech giants to explore similar data exploitation methods.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Personalized Advertising</span><span>AI Ethics</span><span>Data Privacy</span><span>Conversational AI</span><span>Targeted Advertising</span><span>Data Monetization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.theregister.com/2025/10/01/meta_ai_use_informs_ads/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>Tunix: A JAX-native LLM Post-Training Library</h2>
                <span class="published-time">Published: 2025-10-02T06:29:55Z</span>
                
                <p class="summary">Tunix (Tune-in-JAX) is an early-stage, JAX-native library designed to streamline the post-training of Large Language Models, leveraging JAX for accelerated computation and seamless integration with Flax NNX. It offers comprehensive support across key methodologies including Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), and Knowledge Distillation. For SFT, Tunix facilitates both full weights fine-tuning and Parameter-Efficient Fine-Tuning (PEFT) via LoRA/Q-LoRA layers. Its RL capabilities encompass Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and Token-level Group Sequence Policy Optimization (GSPO-token), alongside Direct Preference Optimization (DPO) for preference alignment. The library also provides diverse Knowledge Distillation strategies, such as logit matching, attention transfer, and feature pooling. Tunix emphasizes modularity, enabling easy customization, and efficiency, with native support for distributed training strategies like DP, FSDP, and TP on accelerators like TPUs. Currently under active development, Tunix aims to expand its advanced algorithms, scalability, and agentic RL training features, and is notably collaborating with the GRL framework for scalable LLM RL experiments on TPUs.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>JAX</span><span>LLM Post-Training</span><span>Supervised Fine-Tuning</span><span>Reinforcement Learning</span><span>Knowledge Distillation</span><span>Parameter-Efficient Fine-Tuning</span><span>Distributed Training</span><span>TPU</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/google/tunix" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Pathway Live Data Framework</h2>
                <span class="published-time">Published: 2025-10-02T10:56:49Z</span>
                
                <p class="summary">Pathway is a Python ETL framework designed for stream processing, real-time analytics, LLM pipelines, and Retrieval-Augmented Generation (RAG). It provides an intuitive Python API that integrates seamlessly with existing Python ML libraries, making it suitable for both development and production environments, handling both batch and streaming data with the same codebase. Underpinning Pathway is a high-performance, scalable Rust engine that leverages Differential Dataflow for incremental computation, enabling multithreading, multiprocessing, and distributed computations. Key features include a wide range of connectors (Kafka, GDrive, PostgreSQL, Airbyte), support for stateful transformations, persistence for computation state, and built-in consistency mechanisms. The framework also offers dedicated LLM tooling with wrappers, parsers, embedders, and an in-memory real-time Vector Index, facilitating the rapid development and deployment of live LLM and RAG applications. It can be easily deployed with Docker and Kubernetes, offering superior performance compared to traditional streaming technologies like Flink and Spark, and is available under a BSL 1.1 License.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>ETL Framework</span><span>Stream Processing</span><span>Real-time Analytics</span><span>LLM Pipelines</span><span>RAG</span><span>Python</span><span>Rust Engine</span><span>Incremental Computation</span><span>Kubernetes</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/pathwaycom/pathway" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Claude Agent SDK for Python</h2>
                <span class="published-time">Published: 2025-09-30T19:59:14Z</span>
                
                <p class="summary">The Claude Agent SDK for Python provides a robust toolkit for interacting with Claude Code, enabling the development of sophisticated AI agent applications. It offers an asynchronous `query()` function for basic conversational interactions and a `ClaudeSDKClient` for more advanced, bidirectional conversations. Key features include the ability to define custom in-process tools as Python functions, which streamlines development, enhances performance by eliminating IPC overhead, and simplifies deployment compared to external MCP servers. The SDK also supports hooks, allowing developers to inject deterministic processing and automated feedback into the Claude agent loop at specific points, such as pre-tool use validation. Prerequisites include Python 3.10+, Node.js, and Claude Code. The SDK provides comprehensive error handling and type definitions for robust application building, making it a powerful solution for integrating Claude's agentic capabilities into Python projects.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Python SDK</span><span>Claude Agent</span><span>AI Agent</span><span>Asynchronous Programming</span><span>Custom Tools</span><span>MCP Servers</span><span>Hooks</span><span>Natural Language Processing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/anthropics/claude-agent-sdk-python" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search</h2>
                <span class="published-time">Published: 2025-09-29T20:00:29.000Z</span>
                
                <p class="summary">Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reinforcement Learning</span><span>Monte Carlo Tree Search</span><span>Large Language Models</span><span>Systematic Exploration</span><span>Reasoning Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Machine Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.25454" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GEM: A Gym for Agentic LLMs</h2>
                <span class="published-time">Published: 2025-10-01T15:55:57.000Z</span>
                
                <p class="summary">The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Agentic LLMs</span><span>Environment Simulator</span><span>Reinforcement Learning</span><span>LLM Training</span><span>Benchmarking</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.01051" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights</h2>
                <span class="published-time">Published: 2025-09-26T21:22:54.000Z</span>
                
                <p class="summary">Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Post-training quantization</span><span>Large Language Models</span><span>Low-Precision Weights</span><span>Sinkhorn-Normalized Quantization</span><span>Calibration-Free Quantization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Machine Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.22944" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Code2Video: A Code-centric Paradigm for Educational Video Generation</h2>
                <span class="published-time">Published: 2025-10-01T17:56:48.000Z</span>
                
                <p class="summary">While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Educational Video Generation</span><span>Generative AI</span><span>AI Agent</span><span>Code-centric Framework</span><span>Vision-Language Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>AI Agent</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.01174" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ACON: Optimizing Context Compression for Long-horizon LLM Agents</h2>
                <span class="published-time">Published: 2025-10-01T07:43:49.000Z</span>
                
                <p class="summary">Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>AI Agents</span><span>Context Compression</span><span>Long-horizon Tasks</span><span>Optimization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.00615" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration</h2>
                <span class="published-time">Published: 2025-10-01T02:41:11.000Z</span>
                
                <p class="summary">Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video Generation</span><span>Subject Consistency</span><span>Cross-Modal Integration</span><span>Diffusion Transformer</span><span>Multimodal LLM</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.00438" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>