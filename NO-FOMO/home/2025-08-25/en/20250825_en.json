[
  {
    "id": "twitter_Gradio_1960023019239133503",
    "source": "Twitter",
    "url": "https://twitter.com/Gradio/status/1960023019239133503",
    "title_en": "Gradio_VibeVoice: Microsoft Research Launches Multi-Speaker Audio Conversation Generation Framework",
    "summary_en": "Gradio showcased VibeVoice, a framework developed by Microsoft Research for generating expressive, long-form, multi-speaker audio conversations. VibeVoice enables the creation of podcasts from text, is MIT licensed, and can synthesize speech up to 90 minutes long with up to four distinct speakers, offering a powerful tool for audio content creation.",
    "keywords_en": [
      "VibeVoice",
      "Microsoft Research",
      "Audio Generation",
      "Multi-speaker",
      "Speech Synthesis",
      "Open Source"
    ],
    "area_en": [
      "Natural Language Processing",
      "Generative AI",
      "Open Source"
    ],
    "published_time": "2025-08-25T16:54:37.000Z",
    "download_time": "2025-08-26 06:28:56",
    "visual_resource": [
      "screenshot/twitter/Gradio_1960023019239133503.png"
    ],
    "extra_info": "{\"username\": \"Gradio\", \"tweet_id\": \"1960023019239133503\"}"
  },
  {
    "id": "twitter_omarsar0_1960084088133398718",
    "source": "Twitter",
    "url": "https://twitter.com/omarsar0/status/1960084088133398718",
    "title_en": "omarsar0_Unified MCP Launched: Connecting AI Agents to Apps",
    "summary_en": "omarsar0 announced the launch of Rube, a Unified MCP (Multimodal Control Protocol) server designed to provide a universal connection platform for AI agents. This server enables seamless integration of AI agents with various user applications, supporting collaboration with popular IDEs, Claude Code, and other MCP clients. Rube demonstrates powerful capabilities, such as researching YouTube videos and automatically generating comprehensive content strategy documents, significantly expanding the application scope and efficiency of AI agents.",
    "keywords_en": [
      "Unified MCP",
      "AI Agents",
      "Rube",
      "Application Integration",
      "Content Strategy",
      "Multimodal"
    ],
    "area_en": [
      "AI Agent",
      "Product Launch",
      "Tech News"
    ],
    "published_time": "2025-08-25T20:57:17.000Z",
    "download_time": "2025-08-26 06:28:54",
    "visual_resource": [
      "screenshot/twitter/omarsar0_1960084088133398718.png"
    ],
    "extra_info": "{\"username\": \"omarsar0\", \"tweet_id\": \"1960084088133398718\"}"
  },
  {
    "id": "twitter_DSPyOSS_1960000178179527110",
    "source": "Twitter",
    "url": "https://twitter.com/DSPyOSS/status/1960000178179527110",
    "title_en": "DSPyOSS_DSPy 3.0 Releases GEPA Optimizer, Boosting Performance by 40%",
    "summary_en": "DSPyOSS announced the official release of DSPy 3.0, introducing the GEPA optimizer. According to Connor Shorten, dspy.GEPA achieved a significant 40% performance increase with just 500 metric calls. GEPA optimizes prompts (a 100-line illustrated process) to improve model performance, demonstrating strong potential, especially in optimizing Listwise Rerankers. This update aims to help users more effectively monitor and guide their optimization runs.",
    "keywords_en": [
      "DSPy",
      "GEPA",
      "Prompt Optimization",
      "Performance Boost",
      "Listwise Reranker",
      "Open Source"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Open Source",
      "Tech News"
    ],
    "published_time": "2025-08-25T15:23:52.000Z",
    "download_time": "2025-08-26 06:29:14",
    "visual_resource": [
      "screenshot/twitter/DSPyOSS_1960000178179527110.png"
    ],
    "extra_info": "{\"username\": \"DSPyOSS\", \"tweet_id\": \"1960000178179527110\"}"
  },
  {
    "id": "twitter_ctnzr_1960075010938429809",
    "source": "Twitter",
    "url": "https://twitter.com/ctnzr/status/1960075010938429809",
    "title_en": "ctnzr_NVIDIA NVFP4 Progress in LLM Pretraining",
    "summary_en": "Bryan Catanzaro of NVIDIA announced significant progress in large language model (LLM) pretraining, specifically highlighting advancements with NVFP4 technology. This innovation allows LLM training to achieve the precision of 16-bit operations while benefiting from the speed and efficiency of 4-bit computation. This breakthrough is expected to substantially enhance the performance and efficiency of LLM development, potentially accelerating the future evolution and deployment of advanced AI models.",
    "keywords_en": [
      "LLM",
      "Pretraining",
      "NVFP4",
      "NVIDIA",
      "Floating Point"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Tech News"
    ],
    "published_time": "2025-08-25T20:21:13.000Z",
    "download_time": "2025-08-26 06:29:24",
    "visual_resource": [
      "screenshot/twitter/ctnzr_1960075010938429809.png"
    ],
    "extra_info": "{\"username\": \"ctnzr\", \"tweet_id\": \"1960075010938429809\"}"
  },
  {
    "id": "twitter_corbtt_1960102502764036270",
    "source": "Twitter",
    "url": "https://twitter.com/corbtt/status/1960102502764036270",
    "title_en": "corbtt_ART and LangGraph Integration for RL Agent Training",
    "summary_en": "Kyle Corbitt announced the official integration between ART and LangGraph. This integration enables LangGraph agents to be trained with reinforcement learning, automatically improving their reasoning, tool use, and adaptability. This development is expected to significantly enhance agent performance in complex tasks, offering a new avenue for the development and optimization of AI agents.",
    "keywords_en": [
      "ART",
      "LangGraph",
      "Reinforcement Learning",
      "AI Agent",
      "Product Integration"
    ],
    "area_en": [
      "AI Agent",
      "Machine Learning",
      "Product Launch"
    ],
    "published_time": "2025-08-25T22:10:28.000Z",
    "download_time": "2025-08-26 06:29:13",
    "visual_resource": [
      "screenshot/twitter/corbtt_1960102502764036270.png"
    ],
    "extra_info": "{\"username\": \"corbtt\", \"tweet_id\": \"1960102502764036270\"}"
  },
  {
    "id": "twitter_Clad3815_1959856362059387098",
    "source": "Twitter",
    "url": "https://twitter.com/Clad3815/status/1959856362059387098",
    "title_en": "Clad3815_GPT-5 Successfully Challenges Pokémon Crystal and Defeats Final Boss",
    "summary_en": "Clad3815 shared the impressive progress of GPT-5 in the Pokémon Crystal game. GPT-5 successfully defeated the final boss, RED, in just 9,517 steps, significantly fewer than the 27,040 steps required by the previous model 'o3', demonstrating its powerful strategic planning capabilities. Despite being under-leveled, GPT-5 easily won with superior strategy, highlighting the model's immense progress in just a few months. The stream will continue to showcase GPT-5's further performance, such as catching legendary Pokémon and completing the Pokédex.",
    "keywords_en": [
      "GPT-5",
      "Pokémon Crystal",
      "AI Gaming",
      "Strategy",
      "Large Language Model",
      "OpenAI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-08-25T05:52:23.000Z",
    "download_time": "2025-08-26 06:30:14",
    "visual_resource": [
      "screenshot/twitter/Clad3815_1959856362059387098.png"
    ],
    "extra_info": "{\"username\": \"Clad3815\", \"tweet_id\": \"1959856362059387098\"}"
  },
  {
    "id": "RnM3MhxXbtsLWtZzpMFCRQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/RnM3MhxXbtsLWtZzpMFCRQ",
    "title_en": "While Other Robot Dogs 'Parkour', This One 'Works': The Era of Human-Language-Understanding 'Working Dogs' is Approaching!",
    "summary_en": "ODYSSEY is a unified mobile manipulation framework designed for agile quadruped robots equipped with robotic arms, seamlessly integrating high-level task planning with low-level whole-body control. This innovative framework introduces a hierarchical planner driven by a vision-language model, enabling long-horizon command decomposition and precise action execution, thereby addressing egocentric perception challenges in language-conditioned tasks. Furthermore, ODYSSEY presents the first comprehensive long-horizon mobile manipulation benchmark, encompassing diverse indoor and outdoor scenarios. Utilizing this system, robot dogs can comprehend human instructions and execute practical tasks such as yard cleanup and coffee placement. The platform demonstrates a remarkable success rate of up to 70% across eight distinct long-term tasks, signaling the imminent arrival of an era where \"working dogs\" can understand human speech and perform complex, useful functions.",
    "keywords_en": [
      "Robot Dog",
      "Mobile Manipulation",
      "Vision-Language Model",
      "Task Planning",
      "Whole-Body Control",
      "Benchmark"
    ],
    "area_en": [
      "Robotics",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-25T23:50:19.000Z",
    "download_time": "2025-08-26T14:31:07.189013",
    "visual_resource": [
      "screenshot/wechat/wechat_image_RnM3MhxXbtsLWtZzpMFCRQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "sZ9FzK2Do5dzqG22HNFnig",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/sZ9FzK2Do5dzqG22HNFnig",
    "title_en": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models",
    "summary_en": "Shanghai AI Lab, in collaboration with multiple institutions, has released an 82-page survey titled \"Speed Always Wins: A Survey on Efficient Architectures for Large Language Models.\" This comprehensive review delves into the computational and storage resource consumption bottlenecks faced by Large Language Models (LLMs) despite their continuous performance improvements. The survey specifically addresses the efficiency challenges of the Transformer architecture, particularly the O(N^2) complexity of its self-attention mechanism. It systematically categorizes and summarizes seven directions for efficient architectural improvements, including linear sequence modeling, sparse sequence modeling, efficient full attention, sparse expert models, hybrid model architectures, diffusion language models, and their applications in multimodal contexts. The paper emphasizes the critical importance of these innovations for advancing AI under computational constraints, offering key technological pathways for the widespread deployment and application of LLMs.",
    "keywords_en": [
      "Large Language Models",
      "Efficient Architectures",
      "Transformer",
      "Computational Optimization",
      "Sparse Expert Models",
      "Linear Sequence Modeling"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Deep Learning"
    ],
    "published_time": "2025-08-25T16:18:06.000Z",
    "download_time": "2025-08-26T14:30:57.052135",
    "visual_resource": [
      "screenshot/wechat/wechat_image_sZ9FzK2Do5dzqG22HNFnig.png"
    ],
    "extra_info": null
  },
  {
    "id": "4pJVWkncLdF7Jp8Jq60_6w",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/4pJVWkncLdF7Jp8Jq60_6w",
    "title_en": "Key Highlights of DeepSeek-V3.1",
    "summary_en": "DeepSeek officially unveiled its new generation hybrid inference model, DeepSeek-V3.1, featuring several key advancements. A primary highlight is its innovative support for both rapid non-thinking responses and more deliberate, chain-of-thought-driven answers, offering flexibility for diverse applications. This dual-mode capability contributes to significantly improved inference efficiency, with V3.1-Think reducing output tokens by 20-50% compared to its predecessor, DeepSeek-R1, while maintaining performance. Furthermore, V3.1 demonstrates substantially enhanced Agent capabilities, a result of extensive post-training optimization. It shows marked improvements in tool utilization, code repair (SWE), and complex search tasks, positioning it as a crucial step towards the AI Agent era. Built upon the DeepSeek-V3 pre-trained model, V3.1 extends its context window to an impressive 128K tokens and integrates UE8M0 FP8 precision, optimized for next-generation domestic chips. While benchmark comparisons indicate overall performance gains over previous DeepSeek versions, it still aims to surpass top competitors like Qwen3-235B. Additionally, DeepSeek-V3.1 offers revised API pricing, with reduced output costs, underscoring its commitment to accessibility and continued innovation in the large language model landscape.",
    "keywords_en": [
      "DeepSeek-V3.1",
      "Hybrid Inference",
      "Agent Capability",
      "Inference Efficiency",
      "Large Language Model",
      "Context Extension"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-25T15:46:33.000Z",
    "download_time": "2025-08-26T14:31:02.980940",
    "visual_resource": [
      "screenshot/wechat/wechat_image_4pJVWkncLdF7Jp8Jq60_6w.png"
    ],
    "extra_info": null
  },
  {
    "id": "mOE3_cI_ACc0NUzJsl8vHQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/mOE3_cI_ACc0NUzJsl8vHQ",
    "title_en": "Waver: A High-Performance Foundation Model for Unified Image and Video Generation",
    "summary_en": "ByteDance has introduced Waver, a high-performance foundation model designed for unified image and video generation. This model is capable of directly generating 5-10 second native 720p videos, which can then be upscaled to 1080p, and supports text-to-video, image-to-video, and text-to-image generation within a single framework. Waver enhances modality alignment and accelerates training convergence through its Hybrid Stream DiT architecture, complemented by rigorous data filtering and multimodal large language model-based video quality assessment to ensure high-quality training data. It demonstrates exceptional performance in complex motion capture, outperforming open-source solutions and matching or exceeding state-of-the-art commercial models. Although Waver exhibits some limitations, such as potential detail deformation in high-motion scenarios and occasional lack of rich visual detail, its core architecture, comprising a Task-Unified DiT and Cascade Refiner, alongside support for longer, narrative, and diverse artistic style videos, showcases its robust and versatile video generation capabilities.",
    "keywords_en": [
      "Waver",
      "Video Generation",
      "Image Generation",
      "Foundation Model",
      "Multimodal",
      "Generative AI"
    ],
    "area_en": [
      "Generative AI",
      "Large Language Model",
      "Multimodal"
    ],
    "published_time": "2025-08-25T13:56:49.000Z",
    "download_time": "2025-08-26T14:31:38.698315",
    "visual_resource": [
      "screenshot/wechat/wechat_image_mOE3_cI_ACc0NUzJsl8vHQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "FPLEIX8xr5_z_9kdl8oTIA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/FPLEIX8xr5_z_9kdl8oTIA",
    "title_en": "First GPT-5 Video Agent Creates Full Film with One Sentence! Full Process Automation, Zero Barrier to Become a Director",
    "summary_en": "According to Xinzhiyuan, Video Ocean, the world's first video AI Agent integrated with GPT-5, pioneers a new paradigm of \"generating minute-level viral videos with a single sentence.\" This platform intelligently integrates and automates the entire creative process, from initial creative commands to storyboarding, visual generation, voiceovers, subtitles, and editing. It liberates creators from tedious operations, allowing them to focus solely on creative expression. Video Ocean not only significantly boosts content production efficiency by tenfold but also effortlessly handles commercial-grade video production demands, enabling users to build professional-level film content with zero barriers. Its core innovation lies in applying AI Agents to the entire creative workflow, rather than functioning as a single-purpose tool, aiming to break down creative barriers and realize the vision of \"filmmaking for everyone.\"",
    "keywords_en": [
      "Video Ocean",
      "AI Agent",
      "Video Generation",
      "Workflow Automation",
      "Creative Director"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Generative AI",
      "AI Agent"
    ],
    "published_time": "2025-08-25T03:03:52.000Z",
    "download_time": "2025-08-26T14:31:41.584618",
    "visual_resource": [
      "screenshot/wechat/wechat_image_FPLEIX8xr5_z_9kdl8oTIA.png"
    ],
    "extra_info": null
  },
  {
    "id": "ZGMaRu4S69_GKWMVL9PL5A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ZGMaRu4S69_GKWMVL9PL5A",
    "title_en": "Can Large Models Generate High-Performance Kernels for Different Hardware Platforms? Nanjing University and Zhejiang University Propose Cross-Platform Kernel Generation Evaluation Framework MultiKernelBench",
    "summary_en": "Nanjing University and Zhejiang University have jointly released MultiKernelBench, a novel open-source evaluation framework addressing the challenge of large language models (LLMs) automatically generating high-performance deep learning kernels for diverse hardware platforms like GPUs, NPUs, and TPUs. Existing benchmarks primarily focus on single platforms with coarse evaluation dimensions. MultiKernelBench pioneers cross-platform support, encompassing mainstream architectures from NVIDIA, Huawei, and Google, alongside a fine-grained task system, end-to-end automated evaluation, and category-aware prompting strategies. Initial assessments reveal that current LLMs still exhibit significant shortcomings in kernel generation success rates and performance on non-CUDA platforms. However, MultiKernelBench provides a crucial tool for advancing LLMs towards becoming \"all-round players\" in this domain, with its open-source release fostering community collaboration and future development.",
    "keywords_en": [
      "Large Language Models",
      "Kernel Generation",
      "Heterogeneous Computing",
      "Evaluation Framework",
      "Deep Learning",
      "MultiKernelBench"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-08-25T02:46:11.000Z",
    "download_time": "2025-08-26T14:31:44.484669",
    "visual_resource": [
      "screenshot/wechat/wechat_image_ZGMaRu4S69_GKWMVL9PL5A.png"
    ],
    "extra_info": null
  },
  {
    "id": "DeepCode",
    "source": "GitHub",
    "url": "https://github.com/HKUDS/DeepCode",
    "title_en": "DeepCode: Open Agentic Coding",
    "summary_en": "DeepCode, an innovative AI-powered open agentic coding platform developed by the Data Intelligence Lab at the University of Hong Kong, revolutionizes software development by automating complex code generation and implementation tasks. Leveraging a sophisticated multi-agent system, it efficiently translates diverse inputs, from intricate research papers to natural language descriptions, into high-quality, production-ready code. Its core capabilities, such as Paper2Code, streamline the implementation of complex algorithms; Text2Web facilitates rapid frontend web development; and Text2Backend automates robust server-side code generation. This comprehensive approach significantly accelerates the entire development lifecycle, from initial concept to deployable code, while boosting overall efficiency and enabling seamless research reproducibility. DeepCode provides flexible interaction through both command-line and intuitive web interfaces, and ensures broad tool compatibility through its adherence to the Model Context Protocol (MCP) standard, making it a powerful solution for modern coding challenges.",
    "keywords_en": [
      "Multi-Agent System",
      "Code Generation",
      "AI Agent",
      "Paper2Code",
      "Frontend Development",
      "Backend Development",
      "Retrieval-Augmented Generation"
    ],
    "area_en": [
      "AI Agent",
      "Generative AI",
      "Large Language Model"
    ],
    "published_time": "2025-08-23T14:44:18Z",
    "download_time": "2024-05-23 12:00:00",
    "visual_resource": [
      "https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif",
      "https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif",
      "https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "airi",
    "source": "GitHub",
    "url": "https://github.com/moeru-ai/airi",
    "title_en": "Project AIRI",
    "summary_en": "Project AIRI aims to recreate the concept of Neuro-sama, developing an advanced AI virtual human or digital companion designed for seamless real-time interaction with users. This innovative project leverages a suite of modern web technologies, including WebGPU, WebAudio, Web Workers, WebAssembly, and WebSocket, ensuring broad compatibility and accessibility. Furthermore, it integrates native performance capabilities through NVIDIA CUDA and Apple Metal, allowing for efficient operation not only within web browsers but also on desktop and mobile devices. AIRI supports both VRM and Live2D models, providing rich visual representation. Its core functionalities encompass sophisticated in-game interaction, multi-platform chat capabilities, advanced client-side speech recognition and synthesis, and an evolving memory system. The overarching goal of Project AIRI is to empower individuals to easily own, customize, and interact with their own personalized digital life, offering a new paradigm for human-AI companionship.",
    "keywords_en": [
      "AI Virtual Human",
      "Digital Companion",
      "Web Technologies",
      "Large Language Models",
      "Real-time Interaction",
      "VTuber",
      "Live2D",
      "VRM"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-26T06:18:12Z",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "https://github.com/moeru-ai/airi/raw/main/docs/content/public/banner-light-1280x640.avif",
      "https://github.com/moeru-ai/airi/raw/main/docs/content/public/readme-image-pc-preview.avif"
    ],
    "extra_info": null
  },
  {
    "id": "verifiers",
    "source": "GitHub",
    "url": "https://github.com/willccbb/verifiers",
    "title_en": "Verifiers",
    "summary_en": "Verifiers is a comprehensive library engineered for developing reinforcement learning environments and training large language model (LLM) agents. It incorporates an asynchronous GRPO implementation, built around the `transformers` Trainer, and is further enhanced by `prime-rl` for efficient, large-scale FSDP training. This versatile toolkit extends beyond RL training, enabling the creation of robust LLM evaluations, the construction of synthetic data pipelines, and the implementation of sophisticated agent harnesses. Its core architecture supports various environment types, including single-turn, tool-use, and multi-turn interactions, complemented by flexible rubric and parser components. With optimized GPU training capabilities and seamless integration with vLLM, Verifiers provides a reliable and scalable foundation for advancing LLM-driven reinforcement learning research and practical applications.",
    "keywords_en": [
      "LLM Reinforcement Learning",
      "AI Agent",
      "Environment Building",
      "Asynchronous GRPO",
      "Large Model Training",
      "vLLM",
      "Tool Calling",
      "Multi-turn Interaction"
    ],
    "area_en": [
      "Machine Learning",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-26T03:44:01Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/verifiers.png"
    ],
    "extra_info": null
  },
  {
    "id": "nn-zero-to-hero",
    "source": "GitHub",
    "url": "https://github.com/karpathy/nn-zero-to-hero",
    "title_en": "Neural Networks: Zero to Hero",
    "summary_en": "This GitHub repository serves as the official companion for the \"Neural Networks: Zero to Hero\" course, providing comprehensive resources through a series of YouTube video lectures and corresponding Jupyter Notebooks. The course meticulously guides learners from the absolute basics of neural networks and backpropagation, exemplified by the `micrograd` project, to the intricate development of character-level language models with `makemore`, culminating in the construction of a Generative Pre-trained Transformer (GPT) from scratch. It delves into essential concepts such as PyTorch tensor operations, efficient neural network evaluation, model training methodologies, hyperparameter tuning, and advanced techniques like Batch Normalization and manual backpropagation. This educational initiative is designed to equip individuals with a deep, hands-on understanding of modern deep learning architectures, particularly focusing on their practical application in natural language processing and large language models.",
    "keywords_en": [
      "Neural Networks",
      "Deep Learning",
      "Natural Language Processing",
      "Large Language Models",
      "Backpropagation",
      "PyTorch",
      "GPT",
      "Machine Learning"
    ],
    "area_en": [
      "Deep Learning",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2024-02-20T17:19:51Z",
    "download_time": "2024-02-20 17:20:00",
    "visual_resource": [
      "screenshot/github/nn-zero-to-hero.png"
    ],
    "extra_info": null
  },
  {
    "id": "system_prompts_leaks",
    "source": "GitHub",
    "url": "https://github.com/asgeirtj/system_prompts_leaks",
    "title_en": "System Prompts Leaks",
    "summary_en": "This GitHub repository compiles system message instructions from various publicly deployed chatbots, aiming to collect and share these \"leaked\" system prompts. It serves as a resource for researchers and developers to understand the underlying behaviors and prompt engineering practices of different AI models, facilitating the analysis and learning of how large language models adhere to instructions in real-world applications.",
    "keywords_en": [
      "System Prompts",
      "Chatbots",
      "Large Language Models",
      "Prompt Engineering",
      "Information Collection"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-24T15:06:03Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://api.star-history.com/svg?repos=asgeirtj/system_prompts_leaks&type=Date"
    ],
    "extra_info": null
  },
  {
    "id": "generative-ai-for-beginners",
    "source": "GitHub",
    "url": "https://github.com/microsoft/generative-ai-for-beginners",
    "title_en": "Generative AI For Beginners",
    "summary_en": "This GitHub repository presents 'Generative AI For Beginners', a comprehensive 21-lesson course meticulously developed by Microsoft Cloud Advocates. It is specifically designed to equip beginners with the essential knowledge and practical skills required to build robust Generative AI applications from the ground up. The curriculum delves into core concepts, including the fundamentals of Large Language Models (LLMs), advanced prompt engineering techniques, and the implementation of Retrieval-Augmented Generation (RAG) frameworks. Furthermore, it covers the development of sophisticated AI agents and explores the use of open-source models. The course provides extensive hands-on code examples, primarily in Python and TypeScript, enabling learners to apply theoretical knowledge directly. Participants can leverage either Azure OpenAI Service or the OpenAI API for practical exercises. This resource serves as an invaluable and systematic learning pathway for individuals eager to master Generative AI development and deploy real-world AI solutions.",
    "keywords_en": [
      "Generative AI",
      "Large Language Models",
      "Prompt Engineering",
      "RAG",
      "AI Agents",
      "Python",
      "TypeScript",
      "Course"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Generative AI",
      "Large Language Model"
    ],
    "published_time": "2025-08-21T14:28:29Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/microsoft/Generative-AI-For-Beginners/master/images/repo-thumbnailv4-fixed.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.16279",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.16279",
    "title_en": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications",
    "summary_en": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.",
    "keywords_en": [
      "Agentic Applications",
      "AgentScope",
      "Large Language Models",
      "Developer-Centric Framework",
      "Tool Use"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-22T10:35:56.000Z",
    "download_time": "2025-08-25 20:32:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16279.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.16279\", \"arxiv_url\": \"https://arxiv.org/abs/2508.16279\"}"
  },
  {
    "id": "2508.16153",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.16153",
    "title_en": "AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs",
    "summary_en": "In this paper, we introduce a novel learning paradigm for adaptive Large\nLanguage Model (LLM) agents that eliminates the need for fine-tuning the\nunderlying LLMs. Existing approaches are often either rigid, relying on static,\nhandcrafted reflection workflows, or computationally intensive, requiring\ngradient updates of LLM model parameters. In contrast, our method enables\nlow-cost continual adaptation via memory-based online reinforcement learning.\nWe formalise this as a Memory-augmented Markov Decision Process (M-MDP),\nequipped with a neural case-selection policy to guide action decisions. Past\nexperiences are stored in an episodic memory, either differentiable or\nnon-parametric. The policy is continually updated based on environmental\nfeedback through a memory rewriting mechanism, whereas policy improvement is\nachieved through efficient memory reading (retrieval). We instantiate our agent\nmodel in the deep research setting, namely AgentFly, which attains top-1 on\nGAIA validation (87.88% Pass@3) and 79.40% on the test set. It reaches\n66.6% F1 and 80.4% PM on the DeepResearcher dataset, outperforming the\nstate-of-the-art training-based method, while case-based memory adds 4.7% to\n9.6% absolute points on out-of-distribution tasks. Our approach offers a\nscalable and efficient pathway for developing generalist LLM agents capable of\ncontinuous, real-time learning without gradient updates, advancing machine\nlearning towards open-ended skill acquisition and deep research scenarios. The\ncode is available at https://github.com/Agent-on-the-Fly/AgentFly.",
    "keywords_en": [
      "LLM agents",
      "reinforcement learning",
      "memory-augmented",
      "continual adaptation",
      "fine-tuning"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-08-22T07:25:30.000Z",
    "download_time": "2025-08-25 20:32:37",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16153.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.16153\", \"arxiv_url\": \"https://arxiv.org/abs/2508.16153\"}"
  },
  {
    "id": "2508.14029",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.14029",
    "title_en": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains\n  RLVR",
    "summary_en": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na key paradigm for post-training Large Language Models (LLMs), particularly for\ncomplex reasoning tasks. However, vanilla RLVR training has been shown to\nimprove Pass@1 performance at the expense of policy entropy, leading to reduced\ngeneration diversity and limiting the Pass@k performance, which typically\nrepresents the upper bound of LLM reasoning capability. In this paper, we\nsystematically analyze the policy's generation diversity from the perspective\nof training problems and find that augmenting and updating training problems\nhelps mitigate entropy collapse during training. Based on these observations,\nwe propose an online Self-play with Variational problem Synthesis (SvS)\nstrategy for RLVR training, which uses the policy's correct solutions to\nsynthesize variational problems while ensuring their reference answers remain\nidentical to the originals. This self-improving strategy effectively maintains\npolicy entropy during training and substantially improves Pass@k compared with\nstandard RLVR, sustaining prolonged improvements and achieving absolute gains\nof 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and\nAIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model\nsizes from 3B to 32B consistently demonstrate the generalizability and\nrobustness of SvS.",
    "keywords_en": [
      "Reinforcement Learning with Verifiable Rewards",
      "Large Language Models",
      "Self-play",
      "Variational Problem Synthesis",
      "Policy Entropy"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-19T17:42:45.000Z",
    "download_time": "2025-08-25 20:32:38",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14029.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.14029\", \"arxiv_url\": \"https://arxiv.org/abs/2508.14029\"}"
  },
  {
    "id": "2508.13013",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.13013",
    "title_en": "EgoTwin: Dreaming Body and View in First Person",
    "summary_en": "While exocentric video synthesis has achieved great progress, egocentric\nvideo generation remains largely underexplored, which requires modeling\nfirst-person view content along with camera motion patterns induced by the\nwearer's body movements. To bridge this gap, we introduce a novel task of joint\negocentric video and human motion generation, characterized by two key\nchallenges: 1) Viewpoint Alignment: the camera trajectory in the generated\nvideo must accurately align with the head trajectory derived from human motion;\n2) Causal Interplay: the synthesized human motion must causally align with the\nobserved visual dynamics across adjacent video frames. To address these\nchallenges, we propose EgoTwin, a joint video-motion generation framework built\non the diffusion transformer architecture. Specifically, EgoTwin introduces a\nhead-centric motion representation that anchors the human motion to the head\njoint and incorporates a cybernetics-inspired interaction mechanism that\nexplicitly captures the causal interplay between video and motion within\nattention operations. For comprehensive evaluation, we curate a large-scale\nreal-world dataset of synchronized text-video-motion triplets and design novel\nmetrics to assess video-motion consistency. Extensive experiments demonstrate\nthe effectiveness of the EgoTwin framework.",
    "keywords_en": [
      "Egocentric Video Generation",
      "Human Motion Generation",
      "Diffusion Transformer",
      "Video-Motion Generation",
      "Viewpoint Alignment"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-08-18T15:33:09.000Z",
    "download_time": "2025-08-25 20:32:37",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13013.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.13013\", \"arxiv_url\": \"https://arxiv.org/abs/2508.13013\"}"
  },
  {
    "id": "2508.10390",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.10390",
    "title_en": "Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts",
    "summary_en": "Evaluating jailbreak attacks is challenging when prompts are not overtly\nharmful or fail to induce harmful outputs. Unfortunately, many existing\nred-teaming datasets contain such unsuitable prompts. To evaluate attacks\naccurately, these datasets need to be assessed and cleaned for maliciousness.\nHowever, existing malicious content detection methods rely on either manual\nannotation, which is labor-intensive, or large language models (LLMs), which\nhave inconsistent accuracy in harmful types. To balance accuracy and\nefficiency, we propose a hybrid evaluation framework named MDH (Malicious\ncontent Detection based on LLMs with Human assistance) that combines LLM-based\nannotation with minimal human oversight, and apply it to dataset cleaning and\ndetection of jailbroken responses. Furthermore, we find that well-crafted\ndeveloper messages can significantly boost jailbreak success, leading us to\npropose two new strategies: D-Attack, which leverages context simulation, and\nDH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,\njudgements, and detection results will be released in github repository:\nhttps://github.com/AlienZhang1996/DH-CoT.",
    "keywords_en": [
      "Jailbreaking",
      "Large Language Models",
      "Harmful Prompts",
      "Red-teaming",
      "Dataset Cleaning"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-14T06:46:56.000Z",
    "download_time": "2025-08-25 20:32:35",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10390.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.10390\", \"arxiv_url\": \"https://arxiv.org/abs/2508.10390\"}"
  },
  {
    "id": "2508.08240",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.08240",
    "title_en": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for\n  Long-Horizon Tasks",
    "summary_en": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/",
    "keywords_en": [
      "Quadruped Robots",
      "Mobile Manipulation",
      "Long-Horizon Tasks",
      "Vision-Language Model",
      "Whole-Body Control"
    ],
    "area_en": [
      "Robotics",
      "Multimodal",
      "AI Agent"
    ],
    "published_time": "2025-08-11T17:54:31.000Z",
    "download_time": "2025-08-25 20:32:37",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08240.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.08240\", \"arxiv_url\": \"https://arxiv.org/abs/2508.08240\"}"
  }
]