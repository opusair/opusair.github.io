<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-08-22</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }

        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.3s ease, transform 0.2s ease;
            border: 2px solid transparent;
            font-size: 0.9em;
        }

        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }

        .language-switch a.active {
            background: var(--secondary-color);
            border-color: var(--border-color);
        }

        @media (max-width: 768px) {
            .language-switch {
                position: static;
                justify-content: center;
                margin-bottom: 20px;
            }
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../">‰∏≠Êñá</a>
                <a href="./" class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-08-22</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Twitter</h2>

            <article class="item-card">
                <h2>ZoubinGhahrama1_Google Gemini AI Energy Efficiency Significantly Improved</h2>
                <span class="published-time">Published: 2025-08-22T07:09:09.000Z</span>
                <img src="../screenshot/twitter/ZoubinGhahrama1_1958788516030161185.png" alt="ZoubinGhahrama1_Google Gemini AI Energy Efficiency Significantly Improved">
                <p class="summary">Zoubin Ghahramani, Google's AI lead, retweeted, highlighting Google's excellence in AI efficiency. Michael Terrell revealed that the energy consumption for a median Gemini AI text prompt has dropped 33x in just 12 months, a monumental achievement. This progress, attributed to Googlers' efforts and focus on efficiency, holds significant implications for the AI field.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Google</span><span>Gemini AI</span><span>Energy Efficiency</span><span>Artificial Intelligence</span><span>Optimization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Tech News</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/ZoubinGhahrama1/status/1958788516030161185" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>demishassabis_Google DeepMindÂèëÂ∏ÉGenie 3ÔºöÊñáÊú¨ÁîüÊàê‰∫§‰∫íÂºè3D‰∏ñÁïå</h2>
                <span class="published-time">Published: 2025-08-22T01:05:05.000Z</span>
                <img src="../screenshot/twitter/demishassabis_1958696898488840414.png" alt="demishassabis_Google DeepMindÂèëÂ∏ÉGenie 3ÔºöÊñáÊú¨ÁîüÊàê‰∫§‰∫íÂºè3D‰∏ñÁïå">
                <p class="summary">Google DeepMindËÅîÂêàÂàõÂßã‰∫∫Demis HassabisËΩ¨ÂèëÊé®ÊñáÔºåÊè≠Á§∫‰∫ÜGoogle DeepMindÊúÄÊñ∞Êé®Âá∫ÁöÑGenie 3Ê®°Âûã„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üÈÄöËøáÊñáÊú¨Êåá‰ª§ÁîüÊàê‰∫§‰∫íÂºèÁöÑ3D‰∏ñÁïåÔºåÁî®Êà∑ÂèØÂÆûÊó∂ÂØºËà™Âπ∂‰∏éÂú∫ÊôØ‰∏≠ÁöÑÁâ©‰ΩìËøõË°å‰∫íÂä®Ôºå‰æãÂ¶ÇÊâìÂºÄËΩ¶Èó®ÊàñÁ©øËøáËä±Âùõ„ÄÇGenie 3ÁöÑÂèëÂ∏ÉÊ†áÂøóÁùÄAIÂú®ÁîüÊàêÊ≤âÊµ∏ÂºèËôöÊãüÁéØÂ¢ÉÊñπÈù¢ÂèñÂæóÈáçÂ§ßËøõÂ±ïÔºå‰∏∫Êú™Êù•ËôöÊãüÁé∞ÂÆûÂíåÊ∏∏ÊàèÂºÄÂèëÂ∏¶Êù•Êó†ÈôêÂèØËÉΩ„ÄÇ</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Google DeepMind</span><span>Genie 3</span><span>‰∫§‰∫íÂºè3D‰∏ñÁïå</span><span>ÊñáÊú¨ÁîüÊàê</span><span>ËôöÊãüÁéØÂ¢É</span><span>AIÊ®°Âûã</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>‰∫∫Â∑•Êô∫ËÉΩ</span><span>ÁîüÊàêÂºèAI</span><span>Â§öÊ®°ÊÄÅ</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/demishassabis/status/1958696898488840414" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>yupp_ai_Yupp Integrates DeepSeek v3.1 Models</h2>
                <span class="published-time">Published: 2025-08-22T16:51:28.000Z</span>
                <img src="../screenshot/twitter/yupp_ai_1958935061677711451.png" alt="yupp_ai_Yupp Integrates DeepSeek v3.1 Models">
                <p class="summary">Yupp AI has officially announced the successful integration of DeepSeek v3.1 Thinking and Chat models into its platform. This significant update brings the latest DeepSeek v3.1 edition, which is distinguished by its built-in hybrid thinking capabilities. These advancements are specifically designed to provide users with quicker, more accurate answers and to power stronger, highly tool-savvy AI agents. The Yupp team has already conducted thorough internal testing, utilizing various prompts on their platform to validate the new models' enhanced performance and capabilities. This strategic integration marks a notable improvement in Yupp's service offerings and overall AI functionality.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Yupp</span><span>DeepSeek v3.1</span><span>Large Language Model</span><span>AI Agent</span><span>Product Integration</span><span>Hybrid Thinking</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>Product Launch</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/yupp_ai/status/1958935061677711451" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>gdb_OpenAI's Custom LLM Achieves Biology Breakthrough, 50x Increase in Yamanaka Factor Efficiency</h2>
                <span class="published-time">Published: 2025-08-22T16:26:54.000Z</span>
                <img src="../screenshot/twitter/gdb_1958928877415510134.png" alt="gdb_OpenAI's Custom LLM Achieves Biology Breakthrough, 50x Increase in Yamanaka Factor Efficiency">
                <p class="summary">OpenAI, in collaboration with RetroBiosciences, announced a significant breakthrough in biology achieved by their custom large language model, gpt-4b micro. This model successfully designed novel variants of the Nobel-winning Yamanaka factors, demonstrating a 50-fold increase in in vitro cellular reprogramming efficiency compared to standard OSKM proteins. This groundbreaking research highlights AI's immense potential to accelerate scientific discovery and drug development.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>OpenAI</span><span>Large Language Model</span><span>Biology</span><span>Yamanaka Factors</span><span>Cell Reprogramming</span><span>AI for Science</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Research Progress</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/gdb/status/1958928877415510134" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>elonmusk_xAIÂÆ£Â∏ÉColossus 2Â∞ÜÊàêÂÖ®ÁêÉÈ¶ñ‰∏™ÂçÉÂÖÜÁì¶Á∫ßAIË∂ÖÁÆó</h2>
                <span class="published-time">Published: 2025-08-22T11:01:02.000Z</span>
                <img src="../screenshot/twitter/elonmusk_1958846872157921546.png" alt="elonmusk_xAIÂÆ£Â∏ÉColossus 2Â∞ÜÊàêÂÖ®ÁêÉÈ¶ñ‰∏™ÂçÉÂÖÜÁì¶Á∫ßAIË∂ÖÁÆó">
                <p class="summary">ÂüÉÈöÜ¬∑È©¨ÊñØÂÖãÂÆ£Â∏ÉÔºåxAIÊ≠£Âú®ÊûÑÂª∫ÁöÑColossus 2Â∞ÜÊàê‰∏∫ÂÖ®ÁêÉÈ¶ñ‰∏™ÂçÉÂÖÜÁì¶Á∫ßAIËÆ≠ÁªÉË∂ÖÁ∫ßËÆ°ÁÆóÊú∫ÔºåÊ†áÂøóÁùÄAIÁÆóÂäõÂèëÂ±ïËøàÂêëÊñ∞ÈáåÁ®ãÁ¢ë„ÄÇÊà¥Â∞îÂÖ¨Âè∏ÂàõÂßã‰∫∫ËøàÂÖãÂ∞î¬∑Êà¥Â∞î‰πüËØÅÂÆû‰∫ÜÂØπxAIÁöÑËÆøÈóÆÔºåÂπ∂Ë°®Ëææ‰∫ÜÂØπxAIÊàêÂ∞±ÁöÑËµûËµèÂíåÊîØÊåÅÔºåÂº∫Ë∞É‰∫ÜÊà¥Â∞îÂú®ÂÖ∂‰∏≠ÁöÑÂèÇ‰∏éÔºåÈ¢ÑÁ§∫ÁùÄÊú™Êù•AIÂü∫Á°ÄËÆæÊñΩÁöÑÂ∑®Â§ßËøõÊ≠•„ÄÇ</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>xAI</span><span>Colossus 2</span><span>AIË∂ÖÁÆó</span><span>ÂüÉÈöÜ¬∑È©¨ÊñØÂÖã</span><span>ËøàÂÖãÂ∞î¬∑Êà¥Â∞î</span><span>ÁÆóÂäõ</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>‰∫∫Â∑•Êô∫ËÉΩ</span><span>ÊäÄÊúØÂä®ÊÄÅ</span><span>‰∫ßÂìÅÂèëÂ∏É</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/elonmusk/status/1958846872157921546" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AnthropicAI_Research on Filtering CBRN Weapon Information in Pretraining Data</h2>
                <span class="published-time">Published: 2025-08-22T16:19:09.000Z</span>
                <img src="../screenshot/twitter/AnthropicAI_1958926929626898449.png" alt="AnthropicAI_Research on Filtering CBRN Weapon Information in Pretraining Data">
                <p class="summary">Anthropic has announced new research focusing on filtering dangerous information, specifically related to chemical, biological, radiological, and nuclear (CBRN) weapons, from their models' pretraining data. The initiative aims to effectively remove such sensitive content without compromising the models' performance on harmless tasks, thereby enhancing AI safety and responsible development.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Anthropic</span><span>AI Safety</span><span>Data Filtering</span><span>Pretraining</span><span>CBRN Weapons</span><span>Large Language Model</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Research Progress</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/AnthropicAI/status/1958926929626898449" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>Where Are Autonomous Deep Research Agents Heading? A Four-Step Process Guide | Huawei's Latest</h2>
                <span class="published-time">Published: 2025-08-22T15:39:43.000Z</span>
                <img src="../screenshot/wechat/wechat_image_RM4ZxyEuwZ8OIGPMavPQYA.png" alt="Where Are Autonomous Deep Research Agents Heading? A Four-Step Process Guide | Huawei's Latest">
                <p class="summary">Huawei's Noah's Ark Lab, in collaboration with the University of Hong Kong, has published a systematic review focusing on Deep Research Agents, adopting a novel process-oriented perspective. This comprehensive survey meticulously outlines four critical stages: strategic planning, precise query construction, efficient web exploration, and structured report generation. The paper delves into a comparative analysis of single-agent versus multi-agent systems, elucidating their respective strengths and weaknesses. Furthermore, it explores advanced optimization techniques, such as parameter tuning and reinforcement learning, alongside robust evaluation methodologies, including benchmark testing. This review serves as a crucial operational blueprint for developing sophisticated deep research agents, while also identifying significant future challenges, such as seamless tool integration, maintaining absolute factual integrity in dynamic environments, and effectively processing diverse multimodal information. It offers invaluable guidance and insights for both AI researchers and engineers navigating this evolving field.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Deep Research Agents</span><span>Process-Oriented</span><span>Multi-Agent Systems</span><span>Report Generation</span><span>Huawei Noah's Ark Lab</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/RM4ZxyEuwZ8OIGPMavPQYA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Breaking the Long Video Generation Bottleneck: Nanjing University and TeleAI Jointly Launch New AI Generation Paradigm MMPL, Enabling 'One-Take' Creativity</h2>
                <span class="published-time">Published: 2025-08-22T15:02:41.000Z</span>
                <img src="../screenshot/wechat/wechat_image_oyF51mtGt3x0FEzC9xwm-g.png" alt="Breaking the Long Video Generation Bottleneck: Nanjing University and TeleAI Jointly Launch New AI Generation Paradigm MMPL, Enabling 'One-Take' Creativity">
                <p class="summary">Nanjing University and TeleAI have jointly introduced MMPL (Macro-from-Micro Planning), a novel autoregressive generation paradigm designed to overcome critical bottlenecks in current AI long video generation, such as visual drift, quality degradation, and low efficiency. Drawing inspiration from the film industry's "storyboarding and multi-group parallel shooting" mechanism, MMPL pioneers a unique dual-layer architecture centered on "macro planning and micro execution." This innovative approach leverages sparse anchor point planning and highly parallel content population, which effectively mitigates the cumulative error problem inherent in sequential generation and dramatically boosts overall generation efficiency. The paradigm significantly enhances the visual quality, temporal consistency, and stability of long videos, achieving near real-time preview speeds (up to 32 FPS) for minute-long content. MMPL represents a revolutionary advancement in AI video creation, pushing the boundaries of what's possible and moving AI closer to an "AI director" capability.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Long Video Generation</span><span>MMPL</span><span>AI Director</span><span>Macro Planning</span><span>Parallel Generation</span><span>Video Consistency</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/oyF51mtGt3x0FEzC9xwm-g" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DeepSeek's Single Remark Triggers Surge in Chinese Chip Stocks! What Exactly is the UE8M0 FP8 Behind It?</h2>
                <span class="published-time">Published: 2025-08-22T05:49:09.000Z</span>
                <img src="../screenshot/wechat/wechat_image_4omztEOfepiBe0kjh8ji1A.png" alt="DeepSeek's Single Remark Triggers Surge in Chinese Chip Stocks! What Exactly is the UE8M0 FP8 Behind It?">
                <p class="summary">Following the release of DeepSeek V3.1, its subtle yet impactful mention of "UE8M0 FP8" parameter precision and "next-generation domestic chips" ignited a collective surge in Chinese chip stocks. This article thoroughly analyzes the technical advantages of UE8M0 FP8, an advanced micro-scaling format, which includes substantial bandwidth savings, reduced power consumption, and significantly enhanced data throughput. These characteristics make it exceptionally well-suited for integration with emerging domestic AI accelerators. This strategic alignment is widely interpreted as a crucial advancement towards robust software-hardware synergy within China's artificial intelligence landscape. Such collaboration is expected to dramatically improve the performance-to-cost ratio of indigenous chips and fundamentally lessen the nation's dependence on foreign computing infrastructure, ultimately fostering a powerful, localized technological ecosystem reminiscent of the historical "Wintel alliance."</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>DeepSeek</span><span>Domestic Chips</span><span>UE8M0 FP8</span><span>Software-Hardware Synergy</span><span>AI Accelerator</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/4omztEOfepiBe0kjh8ji1A" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>Build and deploy AI agent workflows in minutes.</h2>
                <span class="published-time">Published: 2025-08-23T00:03:36Z</span>
                <img src="https://github.com/simstudioai/sim/raw/main/apps/sim/public/static/demo.gif" alt="Build and deploy AI agent workflows in minutes.">
                <p class="summary">Sim is a platform dedicated to building and deploying AI agent workflows. It offers flexible cloud-hosted and various self-hosted options, including NPM packages, Docker Compose, Dev Containers, and manual configurations. The project utilizes an advanced tech stack such as Bun, Next.js, PostgreSQL (integrated with pgvector), and Drizzle ORM, and supports integration with Ollama for running local AI models. Sim aims to simplify the development and management of AI agent applications, providing users with efficient and scalable solutions to rapidly implement complex AI workflows.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agent</span><span>Workflow</span><span>Deployment</span><span>Self-hosted</span><span>PostgreSQL</span><span>Ollama</span><span>Full-stack Development</span><span>Low-code</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/simstudioai/sim" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Project AIRI</h2>
                <span class="published-time">Published: 2025-08-23T05:36:10Z</span>
                <img src="https://github.com/moeru-ai/airi/blob/main/docs/content/public/banner-light-1280x640.avif?raw=true" alt="Project AIRI">
                <p class="summary">Project AIRI aims to recreate AI virtual characters like Neuro-sama, building a digital companion capable of interacting, playing games, and chatting with users. Leveraging Web technologies such as WebGPU and WebAssembly, the project achieves cross-browser and mobile device compatibility, while its desktop version supports NVIDIA CUDA and Apple Metal. AIRI integrates various large language model APIs, supports VRM and Live2D models, and features speech recognition, speech synthesis, and memory systems. It enables virtual characters to play games (e.g., Minecraft, Factorio) and interact on social platforms, striving to empower users to easily own personalized digital lives.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI VTuber</span><span>Digital Life</span><span>Large Language Model</span><span>Web Technologies</span><span>Virtual Avatar</span><span>AI Agent</span><span>Cross-platform</span><span>Voice Interaction</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/moeru-ai/airi" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Intelligent Data Query System Based on Large Models and RAG</h2>
                <span class="published-time">Published: 2025-08-22T10:12:30Z</span>
                <img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="Intelligent Data Query System Based on Large Models and RAG">
                <p class="summary">SQLBot is an intelligent data querying system built upon advanced large language models (LLMs) and Retrieval-Augmented Generation (RAG) technology. Its primary objective is to facilitate efficient data querying and insightful analysis by transforming natural language questions into high-quality SQL queries (text-to-SQL). The system boasts several key advantages: it's ready for immediate use, requiring only basic configuration of LLMs and data sources; it offers easy integration, allowing seamless embedding into existing third-party business systems or invocation by popular AI application development platforms such as n8n, MaxKB, Dify, and Coze. This broad compatibility empowers diverse applications with intelligent data querying capabilities. Moreover, SQLBot prioritizes security and control, providing a robust resource isolation mechanism based on workspaces and enabling fine-grained data permission management to safeguard sensitive information.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>RAG</span><span>Intelligent Data Query</span><span>Text2SQL</span><span>Data Analysis</span><span>Data Security</span><span>Docker</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/dataease/SQLBot" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>bitnet.cpp</h2>
                <span class="published-time">Published: 2025-06-03T06:14:20Z</span>
                <img src="https://github.com/microsoft/BitNet/raw/main/assets/header_model_release.png" alt="bitnet.cpp">
                <p class="summary">bitnet.cpp is Microsoft's official inference framework specifically designed for 1-bit Large Language Models (LLMs), such as BitNet b1.58. It provides a comprehensive suite of highly optimized kernels, enabling fast and lossless inference on both CPU and GPU architectures, with future NPU support planned. The framework demonstrates impressive performance gains, achieving speedups of 1.37x to 5.07x on ARM CPUs and 2.37x to 6.17x on x86 CPUs. Concurrently, it significantly reduces energy consumption by 55.4% to 70.0% on ARM and 71.9% to 82.2% on x86. A key achievement is its ability to run a 100B BitNet b1.58 model on a single CPU, delivering speeds of 5-7 tokens per second, which is comparable to human reading. This breakthrough substantially enhances the potential for deploying and running large language models efficiently on local and edge devices, making advanced AI more accessible.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>1-bit LLM</span><span>Inference Framework</span><span>CPU Optimization</span><span>GPU Optimization</span><span>Edge Computing</span><span>Quantization</span><span>BitNet</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/microsoft/BitNet" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Intern-S1: A Scientific Multimodal Foundation Model</h2>
                <span class="published-time">Published: 2025-08-21T17:58:00.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15763.png" alt="Intern-S1: A Scientific Multimodal Foundation Model">
                <p class="summary">In recent years, a plethora of open-source foundation models have emerged,
achieving remarkable progress in some widely attended fields, with performance
being quite close to that of closed-source models. However, in high-value but
more challenging scientific professional fields, either the fields still rely
on expert models, or the progress of general foundation models lags
significantly compared to those in popular areas, far from sufficient for
transforming scientific research and leaving substantial gap between
open-source models and closed-source models in these scientific domains. To
mitigate this gap and explore a step further toward Artificial General
Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped
with general understanding and reasoning capabilities with expertise to analyze
multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)
model with 28 billion activated parameters and 241 billion total parameters,
continually pre-trained on 5T tokens, including over 2.5T tokens from
scientific domains. In the post-training stage, Intern-S1 undergoes offline and
then online reinforcement learning (RL) in InternBootCamp, where we propose
Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks
simultaneously. Through integrated innovations in algorithms, data, and
training systems, Intern-S1 achieved top-tier performance in online RL
training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates
competitive performance on general reasoning tasks among open-source models and
significantly outperforms open-source models in scientific domains, surpassing
closed-source state-of-the-art models in professional tasks, such as molecular
synthesis planning, reaction condition prediction, predicting thermodynamic
stabilities for crystals. Our models are available at
https://huggingface.co/internlm/Intern-S1.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Scientific Multimodal Foundation Model</span><span>Mixture-of-Experts</span><span>Reinforcement Learning</span><span>Artificial General Intelligence</span><span>Scientific Domains</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.15763" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Mobile-Agent-v3: Foundamental Agents for GUI Automation</h2>
                <span class="published-time">Published: 2025-08-21T00:39:12.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15144.png" alt="Mobile-Agent-v3: Foundamental Agents for GUI Automation">
                <p class="summary">This paper introduces GUI-Owl, a foundational GUI agent model that achieves
state-of-the-art performance among open-source end-to-end models on ten GUI
benchmarks across desktop and mobile environments, covering grounding, question
answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B
achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose
Mobile-Agent-v3, a general-purpose GUI agent framework that further improves
performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new
state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates
three key innovations: (1) Large-scale Environment Infrastructure: a
cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,
enabling our Self-Evolving GUI Trajectory Production framework. This generates
high-quality interaction data via automated query generation and correctness
validation, leveraging GUI-Owl to refine trajectories iteratively, forming a
self-improving loop. It supports diverse data pipelines and reduces manual
annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI
grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports
end-to-end decision-making and can act as a modular component in multi-agent
systems. (3) Scalable Environment RL: we develop a scalable reinforcement
learning framework with fully asynchronous training for real-world alignment.
We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for
online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are
open-sourced at https://github.com/X-PLUG/MobileAgent.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>GUI Automation</span><span>AI Agent</span><span>Reinforcement Learning</span><span>Large Model</span><span>Benchmarks</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.15144" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on
  Challenging Queries</h2>
                <span class="published-time">Published: 2025-08-21T17:55:54.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15760.png" alt="LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on
  Challenging Queries">
                <p class="summary">Tool calling has emerged as a critical capability for AI agents to interact
with the real world and solve complex tasks. While the Model Context Protocol
(MCP) provides a powerful standardized framework for tool integration, there is
a significant gap in benchmarking how well AI agents can effectively solve
multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In
this work, we present LiveMCP-101, a benchmark of 101 carefully curated
real-world queries, refined through iterative LLM rewriting and manual review,
that require coordinated use of multiple MCP tools including web search, file
operations, mathematical reasoning, and data analysis. Moreover, we introduce a
novel evaluation approach that leverages ground-truth execution plans rather
than raw API outputs, better reflecting the evolving nature of real-world
environments. Experiments show that even frontier LLMs achieve a success rate
below 60\%, highlighting major challenges in tool orchestration. Detailed
ablations and error analysis further reveal distinct failure modes and
inefficiencies in token usage, pointing to concrete directions for advancing
current models. LiveMCP-101 sets a rigorous standard for evaluating real-world
agent capabilities, advancing toward autonomous AI systems that reliably
execute complex tasks through tool use.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI agents</span><span>tool calling</span><span>Model Context Protocol</span><span>benchmarking</span><span>tool orchestration</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.15760" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Waver: Wave Your Way to Lifelike Video Generation</h2>
                <span class="published-time">Published: 2025-08-21T17:56:10.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15761.png" alt="Waver: Wave Your Way to Lifelike Video Generation">
                <p class="summary">We present Waver, a high-performance foundation model for unified image and
video generation. Waver can directly generate videos with durations ranging
from 5 to 10 seconds at a native resolution of 720p, which are subsequently
upscaled to 1080p. The model simultaneously supports text-to-video (T2V),
image-to-video (I2V), and text-to-image (T2I) generation within a single,
integrated framework. We introduce a Hybrid Stream DiT architecture to enhance
modality alignment and accelerate training convergence. To ensure training data
quality, we establish a comprehensive data curation pipeline and manually
annotate and train an MLLM-based video quality model to filter for the
highest-quality samples. Furthermore, we provide detailed training and
inference recipes to facilitate the generation of high-quality videos. Building
on these contributions, Waver excels at capturing complex motion, achieving
superior motion amplitude and temporal consistency in video synthesis. Notably,
it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial
Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming
existing open-source models and matching or surpassing state-of-the-art
commercial solutions. We hope this technical report will help the community
more efficiently train high-quality video generation models and accelerate
progress in video generation technologies. Official page:
https://github.com/FoundationVision/Waver.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video Generation</span><span>Foundation Model</span><span>Multimodal</span><span>DiT Architecture</span><span>High-Quality Video</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.15761" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>A Survey on Large Language Model Benchmarks</h2>
                <span class="published-time">Published: 2025-08-21T08:43:35.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15361.png" alt="A Survey on Large Language Model Benchmarks">
                <p class="summary">In recent years, with the rapid development of the depth and breadth of large
language models' capabilities, various corresponding evaluation benchmarks have
been emerging in increasing numbers. As a quantitative assessment tool for
model performance, benchmarks are not only a core means to measure model
capabilities but also a key element in guiding the direction of model
development and promoting technological innovation. We systematically review
the current status and development of large language model benchmarks for the
first time, categorizing 283 representative benchmarks into three categories:
general capabilities, domain-specific, and target-specific. General capability
benchmarks cover aspects such as core linguistics, knowledge, and reasoning;
domain-specific benchmarks focus on fields like natural sciences, humanities
and social sciences, and engineering technology; target-specific benchmarks pay
attention to risks, reliability, agents, etc. We point out that current
benchmarks have problems such as inflated scores caused by data contamination,
unfair evaluation due to cultural and linguistic biases, and lack of evaluation
on process credibility and dynamic environments, and provide a referable design
paradigm for future benchmark innovation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Benchmarks</span><span>Model Evaluation</span><span>Survey</span><span>Data Contamination</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.15361" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery
  Generated by AI Scientists</h2>
                <span class="published-time">Published: 2025-08-20T23:16:41.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15126.png" alt="aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery
  Generated by AI Scientists">
                <p class="summary">Recent advances in large language models (LLMs) have enabled AI agents to
autonomously generate scientific proposals, conduct experiments, author papers,
and perform peer reviews. Yet this flood of AI-generated research content
collides with a fragmented and largely closed publication ecosystem.
Traditional journals and conferences rely on human peer review, making them
difficult to scale and often reluctant to accept AI-generated research content;
existing preprint servers (e.g. arXiv) lack rigorous quality-control
mechanisms. Consequently, a significant amount of high-quality AI-generated
research lacks appropriate venues for dissemination, hindering its potential to
advance scientific progress. To address these challenges, we introduce aiXiv, a
next-generation open-access platform for human and AI scientists. Its
multi-agent architecture allows research proposals and papers to be submitted,
reviewed, and iteratively refined by both human and AI scientists. It also
provides API and MCP interfaces that enable seamless integration of
heterogeneous human and AI scientists, creating a scalable and extensible
ecosystem for autonomous scientific discovery. Through extensive experiments,
we demonstrate that aiXiv is a reliable and robust platform that significantly
enhances the quality of AI-generated research proposals and papers after
iterative revising and reviewing on aiXiv. Our work lays the groundwork for a
next-generation open-access ecosystem for AI scientists, accelerating the
publication and dissemination of high-quality AI-generated research content.
Code is available at https://github.com/aixiv-org. Website is available at
https://forms.gle/DxQgCtXFsJ4paMtn8.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Scientists</span><span>Open Access</span><span>Scientific Discovery</span><span>Large Language Models</span><span>AI Agents</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.15126" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>