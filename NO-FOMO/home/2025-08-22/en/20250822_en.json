[
  {
    "id": "twitter_ZoubinGhahrama1_1958788516030161185",
    "source": "Twitter",
    "url": "https://x.com/ZoubinGhahrama1/status/1958788516030161185",
    "title_en": "ZoubinGhahrama1_Google Gemini AI Energy Efficiency Significantly Improved",
    "summary_en": "Zoubin Ghahramani, Google's AI lead, retweeted, highlighting Google's excellence in AI efficiency. Michael Terrell revealed that the energy consumption for a median Gemini AI text prompt has dropped 33x in just 12 months, a monumental achievement. This progress, attributed to Googlers' efforts and focus on efficiency, holds significant implications for the AI field.",
    "keywords_en": [
      "Google",
      "Gemini AI",
      "Energy Efficiency",
      "Artificial Intelligence",
      "Optimization"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-08-22T07:09:09.000Z",
    "download_time": "2025-08-23 07:51:00",
    "visual_resource": [
      "screenshot/twitter/ZoubinGhahrama1_1958788516030161185.png"
    ],
    "extra_info": "{\"username\": \"ZoubinGhahrama1\", \"tweet_id\": \"1958788516030161185\"}"
  },
  {
    "id": "twitter_demishassabis_1958696898488840414",
    "source": "Twitter",
    "url": "https://twitter.com/demishassabis/status/1958696898488840414",
    "title_en": "demishassabis_Google DeepMind发布Genie 3：文本生成交互式3D世界",
    "summary_en": "Google DeepMind联合创始人Demis Hassabis转发推文，揭示了Google DeepMind最新推出的Genie 3模型。该模型能够通过文本指令生成交互式的3D世界，用户可实时导航并与场景中的物体进行互动，例如打开车门或穿过花坛。Genie 3的发布标志着AI在生成沉浸式虚拟环境方面取得重大进展，为未来虚拟现实和游戏开发带来无限可能。",
    "keywords_en": [
      "Google DeepMind",
      "Genie 3",
      "交互式3D世界",
      "文本生成",
      "虚拟环境",
      "AI模型"
    ],
    "area_en": [
      "人工智能",
      "生成式AI",
      "多模态"
    ],
    "published_time": "2025-08-22T01:05:05.000Z",
    "download_time": "2025-08-23 08:34:25",
    "visual_resource": [
      "screenshot/twitter/demishassabis_1958696898488840414.png"
    ],
    "extra_info": "{\"username\": \"demishassabis\", \"tweet_id\": \"1958696898488840414\"}"
  },
  {
    "id": "twitter_yupp_ai_1958935061677711451",
    "source": "Twitter",
    "url": "https://twitter.com/yupp_ai/status/1958935061677711451",
    "title_en": "yupp_ai_Yupp Integrates DeepSeek v3.1 Models",
    "summary_en": "Yupp AI has officially announced the successful integration of DeepSeek v3.1 Thinking and Chat models into its platform. This significant update brings the latest DeepSeek v3.1 edition, which is distinguished by its built-in hybrid thinking capabilities. These advancements are specifically designed to provide users with quicker, more accurate answers and to power stronger, highly tool-savvy AI agents. The Yupp team has already conducted thorough internal testing, utilizing various prompts on their platform to validate the new models' enhanced performance and capabilities. This strategic integration marks a notable improvement in Yupp's service offerings and overall AI functionality.",
    "keywords_en": [
      "Yupp",
      "DeepSeek v3.1",
      "Large Language Model",
      "AI Agent",
      "Product Integration",
      "Hybrid Thinking"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Product Launch"
    ],
    "published_time": "2025-08-22T16:51:28.000Z",
    "download_time": "2025-08-25 01:19:06",
    "visual_resource": [
      "screenshot/twitter/yupp_ai_1958935061677711451.png"
    ],
    "extra_info": "{\"username\": \"yupp_ai\", \"tweet_id\": \"1958935061677711451\"}"
  },
  {
    "id": "twitter_gdb_1958928877415510134",
    "source": "Twitter",
    "url": "https://x.com/gdb/status/1958928877415510134",
    "title_en": "gdb_OpenAI's Custom LLM Achieves Biology Breakthrough, 50x Increase in Yamanaka Factor Efficiency",
    "summary_en": "OpenAI, in collaboration with RetroBiosciences, announced a significant breakthrough in biology achieved by their custom large language model, gpt-4b micro. This model successfully designed novel variants of the Nobel-winning Yamanaka factors, demonstrating a 50-fold increase in in vitro cellular reprogramming efficiency compared to standard OSKM proteins. This groundbreaking research highlights AI's immense potential to accelerate scientific discovery and drug development.",
    "keywords_en": [
      "OpenAI",
      "Large Language Model",
      "Biology",
      "Yamanaka Factors",
      "Cell Reprogramming",
      "AI for Science"
    ],
    "area_en": [
      "Large Language Model",
      "Research Progress",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-22T16:26:54.000Z",
    "download_time": "2025-08-23 07:52:38",
    "visual_resource": [
      "screenshot/twitter/gdb_1958928877415510134.png"
    ],
    "extra_info": "{\"username\": \"gdb\", \"tweet_id\": \"1958928877415510134\"}"
  },
  {
    "id": "twitter_elonmusk_1958846872157921546",
    "source": "Twitter",
    "url": "https://twitter.com/elonmusk/status/1958846872157921546",
    "title_en": "elonmusk_xAI宣布Colossus 2将成全球首个千兆瓦级AI超算",
    "summary_en": "埃隆·马斯克宣布，xAI正在构建的Colossus 2将成为全球首个千兆瓦级AI训练超级计算机，标志着AI算力发展迈向新里程碑。戴尔公司创始人迈克尔·戴尔也证实了对xAI的访问，并表达了对xAI成就的赞赏和支持，强调了戴尔在其中的参与，预示着未来AI基础设施的巨大进步。",
    "keywords_en": [
      "xAI",
      "Colossus 2",
      "AI超算",
      "埃隆·马斯克",
      "迈克尔·戴尔",
      "算力"
    ],
    "area_en": [
      "人工智能",
      "技术动态",
      "产品发布"
    ],
    "published_time": "2025-08-22T11:01:02.000Z",
    "download_time": "2025-08-23 08:36:08",
    "visual_resource": [
      "screenshot/twitter/elonmusk_1958846872157921546.png"
    ],
    "extra_info": "{\"username\": \"elonmusk\", \"tweet_id\": \"1958846872157921546\"}"
  },
  {
    "id": "twitter_AnthropicAI_1958926929626898449",
    "source": "Twitter",
    "url": "https://twitter.com/AnthropicAI/status/1958926929626898449",
    "title_en": "AnthropicAI_Research on Filtering CBRN Weapon Information in Pretraining Data",
    "summary_en": "Anthropic has announced new research focusing on filtering dangerous information, specifically related to chemical, biological, radiological, and nuclear (CBRN) weapons, from their models' pretraining data. The initiative aims to effectively remove such sensitive content without compromising the models' performance on harmless tasks, thereby enhancing AI safety and responsible development.",
    "keywords_en": [
      "Anthropic",
      "AI Safety",
      "Data Filtering",
      "Pretraining",
      "CBRN Weapons",
      "Large Language Model"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-08-22T16:19:09.000Z",
    "download_time": "2025-08-25 01:19:30",
    "visual_resource": [
      "screenshot/twitter/AnthropicAI_1958926929626898449.png"
    ],
    "extra_info": "{\"username\": \"AnthropicAI\", \"tweet_id\": \"1958926929626898449\"}"
  },
  {
    "id": "RM4ZxyEuwZ8OIGPMavPQYA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/RM4ZxyEuwZ8OIGPMavPQYA",
    "title_en": "Where Are Autonomous Deep Research Agents Heading? A Four-Step Process Guide | Huawei's Latest",
    "summary_en": "Huawei's Noah's Ark Lab, in collaboration with the University of Hong Kong, has published a systematic review focusing on Deep Research Agents, adopting a novel process-oriented perspective. This comprehensive survey meticulously outlines four critical stages: strategic planning, precise query construction, efficient web exploration, and structured report generation. The paper delves into a comparative analysis of single-agent versus multi-agent systems, elucidating their respective strengths and weaknesses. Furthermore, it explores advanced optimization techniques, such as parameter tuning and reinforcement learning, alongside robust evaluation methodologies, including benchmark testing. This review serves as a crucial operational blueprint for developing sophisticated deep research agents, while also identifying significant future challenges, such as seamless tool integration, maintaining absolute factual integrity in dynamic environments, and effectively processing diverse multimodal information. It offers invaluable guidance and insights for both AI researchers and engineers navigating this evolving field.",
    "keywords_en": [
      "Deep Research Agents",
      "Process-Oriented",
      "Multi-Agent Systems",
      "Report Generation",
      "Huawei Noah's Ark Lab",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-22T15:39:43.000Z",
    "download_time": "2025-08-23T20:41:16.019994",
    "visual_resource": [
      "screenshot/wechat/wechat_image_RM4ZxyEuwZ8OIGPMavPQYA.png"
    ],
    "extra_info": null
  },
  {
    "id": "oyF51mtGt3x0FEzC9xwm-g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/oyF51mtGt3x0FEzC9xwm-g",
    "title_en": "Breaking the Long Video Generation Bottleneck: Nanjing University and TeleAI Jointly Launch New AI Generation Paradigm MMPL, Enabling 'One-Take' Creativity",
    "summary_en": "Nanjing University and TeleAI have jointly introduced MMPL (Macro-from-Micro Planning), a novel autoregressive generation paradigm designed to overcome critical bottlenecks in current AI long video generation, such as visual drift, quality degradation, and low efficiency. Drawing inspiration from the film industry's \"storyboarding and multi-group parallel shooting\" mechanism, MMPL pioneers a unique dual-layer architecture centered on \"macro planning and micro execution.\" This innovative approach leverages sparse anchor point planning and highly parallel content population, which effectively mitigates the cumulative error problem inherent in sequential generation and dramatically boosts overall generation efficiency. The paradigm significantly enhances the visual quality, temporal consistency, and stability of long videos, achieving near real-time preview speeds (up to 32 FPS) for minute-long content. MMPL represents a revolutionary advancement in AI video creation, pushing the boundaries of what's possible and moving AI closer to an \"AI director\" capability.",
    "keywords_en": [
      "Long Video Generation",
      "MMPL",
      "AI Director",
      "Macro Planning",
      "Parallel Generation",
      "Video Consistency"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Large Language Model"
    ],
    "published_time": "2025-08-22T15:02:41.000Z",
    "download_time": "2025-08-23T20:41:21.143938",
    "visual_resource": [
      "screenshot/wechat/wechat_image_oyF51mtGt3x0FEzC9xwm-g.png"
    ],
    "extra_info": null
  },
  {
    "id": "4omztEOfepiBe0kjh8ji1A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/4omztEOfepiBe0kjh8ji1A",
    "title_en": "DeepSeek's Single Remark Triggers Surge in Chinese Chip Stocks! What Exactly is the UE8M0 FP8 Behind It?",
    "summary_en": "Following the release of DeepSeek V3.1, its subtle yet impactful mention of \"UE8M0 FP8\" parameter precision and \"next-generation domestic chips\" ignited a collective surge in Chinese chip stocks. This article thoroughly analyzes the technical advantages of UE8M0 FP8, an advanced micro-scaling format, which includes substantial bandwidth savings, reduced power consumption, and significantly enhanced data throughput. These characteristics make it exceptionally well-suited for integration with emerging domestic AI accelerators. This strategic alignment is widely interpreted as a crucial advancement towards robust software-hardware synergy within China's artificial intelligence landscape. Such collaboration is expected to dramatically improve the performance-to-cost ratio of indigenous chips and fundamentally lessen the nation's dependence on foreign computing infrastructure, ultimately fostering a powerful, localized technological ecosystem reminiscent of the historical \"Wintel alliance.\"",
    "keywords_en": [
      "DeepSeek",
      "Domestic Chips",
      "UE8M0 FP8",
      "Software-Hardware Synergy",
      "AI Accelerator"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-08-22T05:49:09.000Z",
    "download_time": "2025-08-23T20:41:11.798774",
    "visual_resource": [
      "screenshot/wechat/wechat_image_4omztEOfepiBe0kjh8ji1A.png"
    ],
    "extra_info": null
  },
  {
    "id": "sim",
    "source": "GitHub",
    "url": "https://github.com/simstudioai/sim",
    "title_en": "Build and deploy AI agent workflows in minutes.",
    "summary_en": "Sim is a platform dedicated to building and deploying AI agent workflows. It offers flexible cloud-hosted and various self-hosted options, including NPM packages, Docker Compose, Dev Containers, and manual configurations. The project utilizes an advanced tech stack such as Bun, Next.js, PostgreSQL (integrated with pgvector), and Drizzle ORM, and supports integration with Ollama for running local AI models. Sim aims to simplify the development and management of AI agent applications, providing users with efficient and scalable solutions to rapidly implement complex AI workflows.",
    "keywords_en": [
      "AI Agent",
      "Workflow",
      "Deployment",
      "Self-hosted",
      "PostgreSQL",
      "Ollama",
      "Full-stack Development",
      "Low-code"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-23T00:03:36Z",
    "download_time": "2024-07-30 08:00:00",
    "visual_resource": [
      "https://github.com/simstudioai/sim/raw/main/apps/sim/public/static/demo.gif",
      "https://github.com/simstudioai/sim/raw/main/apps/sim/public/logo/reverse/text/large.png"
    ],
    "extra_info": null
  },
  {
    "id": "airi",
    "source": "GitHub",
    "url": "https://github.com/moeru-ai/airi",
    "title_en": "Project AIRI",
    "summary_en": "Project AIRI aims to recreate AI virtual characters like Neuro-sama, building a digital companion capable of interacting, playing games, and chatting with users. Leveraging Web technologies such as WebGPU and WebAssembly, the project achieves cross-browser and mobile device compatibility, while its desktop version supports NVIDIA CUDA and Apple Metal. AIRI integrates various large language model APIs, supports VRM and Live2D models, and features speech recognition, speech synthesis, and memory systems. It enables virtual characters to play games (e.g., Minecraft, Factorio) and interact on social platforms, striving to empower users to easily own personalized digital lives.",
    "keywords_en": [
      "AI VTuber",
      "Digital Life",
      "Large Language Model",
      "Web Technologies",
      "Virtual Avatar",
      "AI Agent",
      "Cross-platform",
      "Voice Interaction"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-23T05:36:10Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://github.com/moeru-ai/airi/blob/main/docs/content/public/banner-light-1280x640.avif?raw=true",
      "https://github.com/moeru-ai/airi/blob/main/docs/content/public/readme-image-pc-preview.avif?raw=true",
      "https://repobeats.axiom.co/api/embed/a1d6fe2c13ea2bb53a5154435a71e2431f70c2ee.svg"
    ],
    "extra_info": null
  },
  {
    "id": "SQLBot",
    "source": "GitHub",
    "url": "https://github.com/dataease/SQLBot",
    "title_en": "Intelligent Data Query System Based on Large Models and RAG",
    "summary_en": "SQLBot is an intelligent data querying system built upon advanced large language models (LLMs) and Retrieval-Augmented Generation (RAG) technology. Its primary objective is to facilitate efficient data querying and insightful analysis by transforming natural language questions into high-quality SQL queries (text-to-SQL). The system boasts several key advantages: it's ready for immediate use, requiring only basic configuration of LLMs and data sources; it offers easy integration, allowing seamless embedding into existing third-party business systems or invocation by popular AI application development platforms such as n8n, MaxKB, Dify, and Coze. This broad compatibility empowers diverse applications with intelligent data querying capabilities. Moreover, SQLBot prioritizes security and control, providing a robust resource isolation mechanism based on workspaces and enabling fine-grained data permission management to safeguard sensitive information.",
    "keywords_en": [
      "Large Language Model",
      "RAG",
      "Intelligent Data Query",
      "Text2SQL",
      "Data Analysis",
      "Data Security",
      "Docker"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-22T10:12:30Z",
    "download_time": "2024-07-30 15:30:00",
    "visual_resource": [
      "https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png"
    ],
    "extra_info": null
  },
  {
    "id": "BitNet",
    "source": "GitHub",
    "url": "https://github.com/microsoft/BitNet",
    "title_en": "bitnet.cpp",
    "summary_en": "bitnet.cpp is Microsoft's official inference framework specifically designed for 1-bit Large Language Models (LLMs), such as BitNet b1.58. It provides a comprehensive suite of highly optimized kernels, enabling fast and lossless inference on both CPU and GPU architectures, with future NPU support planned. The framework demonstrates impressive performance gains, achieving speedups of 1.37x to 5.07x on ARM CPUs and 2.37x to 6.17x on x86 CPUs. Concurrently, it significantly reduces energy consumption by 55.4% to 70.0% on ARM and 71.9% to 82.2% on x86. A key achievement is its ability to run a 100B BitNet b1.58 model on a single CPU, delivering speeds of 5-7 tokens per second, which is comparable to human reading. This breakthrough substantially enhances the potential for deploying and running large language models efficiently on local and edge devices, making advanced AI more accessible.",
    "keywords_en": [
      "1-bit LLM",
      "Inference Framework",
      "CPU Optimization",
      "GPU Optimization",
      "Edge Computing",
      "Quantization",
      "BitNet"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Deep Learning"
    ],
    "published_time": "2025-06-03T06:14:20Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://github.com/microsoft/BitNet/raw/main/assets/header_model_release.png",
      "https://github.com/microsoft/BitNet/raw/main/assets/m2_performance.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "2508.15763",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.15763",
    "title_en": "Intern-S1: A Scientific Multimodal Foundation Model",
    "summary_en": "In recent years, a plethora of open-source foundation models have emerged,\nachieving remarkable progress in some widely attended fields, with performance\nbeing quite close to that of closed-source models. However, in high-value but\nmore challenging scientific professional fields, either the fields still rely\non expert models, or the progress of general foundation models lags\nsignificantly compared to those in popular areas, far from sufficient for\ntransforming scientific research and leaving substantial gap between\nopen-source models and closed-source models in these scientific domains. To\nmitigate this gap and explore a step further toward Artificial General\nIntelligence (AGI), we introduce Intern-S1, a specialized generalist equipped\nwith general understanding and reasoning capabilities with expertise to analyze\nmultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)\nmodel with 28 billion activated parameters and 241 billion total parameters,\ncontinually pre-trained on 5T tokens, including over 2.5T tokens from\nscientific domains. In the post-training stage, Intern-S1 undergoes offline and\nthen online reinforcement learning (RL) in InternBootCamp, where we propose\nMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks\nsimultaneously. Through integrated innovations in algorithms, data, and\ntraining systems, Intern-S1 achieved top-tier performance in online RL\ntraining.On comprehensive evaluation benchmarks, Intern-S1 demonstrates\ncompetitive performance on general reasoning tasks among open-source models and\nsignificantly outperforms open-source models in scientific domains, surpassing\nclosed-source state-of-the-art models in professional tasks, such as molecular\nsynthesis planning, reaction condition prediction, predicting thermodynamic\nstabilities for crystals. Our models are available at\nhttps://huggingface.co/internlm/Intern-S1.",
    "keywords_en": [
      "Scientific Multimodal Foundation Model",
      "Mixture-of-Experts",
      "Reinforcement Learning",
      "Artificial General Intelligence",
      "Scientific Domains"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-21T17:58:00.000Z",
    "download_time": "2025-08-23 03:50:37",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15763.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.15763\", \"arxiv_url\": \"https://arxiv.org/abs/2508.15763\"}"
  },
  {
    "id": "2508.15144",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.15144",
    "title_en": "Mobile-Agent-v3: Foundamental Agents for GUI Automation",
    "summary_en": "This paper introduces GUI-Owl, a foundational GUI agent model that achieves\nstate-of-the-art performance among open-source end-to-end models on ten GUI\nbenchmarks across desktop and mobile environments, covering grounding, question\nanswering, planning, decision-making, and procedural knowledge. GUI-Owl-7B\nachieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose\nMobile-Agent-v3, a general-purpose GUI agent framework that further improves\nperformance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new\nstate-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates\nthree key innovations: (1) Large-scale Environment Infrastructure: a\ncloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,\nenabling our Self-Evolving GUI Trajectory Production framework. This generates\nhigh-quality interaction data via automated query generation and correctness\nvalidation, leveraging GUI-Owl to refine trajectories iteratively, forming a\nself-improving loop. It supports diverse data pipelines and reduces manual\nannotation. (2) Diverse Foundational Agent Capabilities: by integrating UI\ngrounding, planning, action semantics, and reasoning patterns, GUI-Owl supports\nend-to-end decision-making and can act as a modular component in multi-agent\nsystems. (3) Scalable Environment RL: we develop a scalable reinforcement\nlearning framework with fully asynchronous training for real-world alignment.\nWe also introduce Trajectory-aware Relative Policy Optimization (TRPO) for\nonline RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are\nopen-sourced at https://github.com/X-PLUG/MobileAgent.",
    "keywords_en": [
      "GUI Automation",
      "AI Agent",
      "Reinforcement Learning",
      "Large Model",
      "Benchmarks"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-21T00:39:12.000Z",
    "download_time": "2025-08-23 03:50:40",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15144.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.15144\", \"arxiv_url\": \"https://arxiv.org/abs/2508.15144\"}"
  },
  {
    "id": "2508.15760",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.15760",
    "title_en": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on\n  Challenging Queries",
    "summary_en": "Tool calling has emerged as a critical capability for AI agents to interact\nwith the real world and solve complex tasks. While the Model Context Protocol\n(MCP) provides a powerful standardized framework for tool integration, there is\na significant gap in benchmarking how well AI agents can effectively solve\nmulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. In\nthis work, we present LiveMCP-101, a benchmark of 101 carefully curated\nreal-world queries, refined through iterative LLM rewriting and manual review,\nthat require coordinated use of multiple MCP tools including web search, file\noperations, mathematical reasoning, and data analysis. Moreover, we introduce a\nnovel evaluation approach that leverages ground-truth execution plans rather\nthan raw API outputs, better reflecting the evolving nature of real-world\nenvironments. Experiments show that even frontier LLMs achieve a success rate\nbelow 60\\%, highlighting major challenges in tool orchestration. Detailed\nablations and error analysis further reveal distinct failure modes and\ninefficiencies in token usage, pointing to concrete directions for advancing\ncurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-world\nagent capabilities, advancing toward autonomous AI systems that reliably\nexecute complex tasks through tool use.",
    "keywords_en": [
      "AI agents",
      "tool calling",
      "Model Context Protocol",
      "benchmarking",
      "tool orchestration"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-21T17:55:54.000Z",
    "download_time": "2025-08-23 03:50:35",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15760.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.15760\", \"arxiv_url\": \"https://arxiv.org/abs/2508.15760\"}"
  },
  {
    "id": "2508.15761",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.15761",
    "title_en": "Waver: Wave Your Way to Lifelike Video Generation",
    "summary_en": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.",
    "keywords_en": [
      "Video Generation",
      "Foundation Model",
      "Multimodal",
      "DiT Architecture",
      "High-Quality Video"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Multimodal"
    ],
    "published_time": "2025-08-21T17:56:10.000Z",
    "download_time": "2025-08-23 03:50:41",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15761.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.15761\", \"arxiv_url\": \"https://arxiv.org/abs/2508.15761\"}"
  },
  {
    "id": "2508.15361",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.15361",
    "title_en": "A Survey on Large Language Model Benchmarks",
    "summary_en": "In recent years, with the rapid development of the depth and breadth of large\nlanguage models' capabilities, various corresponding evaluation benchmarks have\nbeen emerging in increasing numbers. As a quantitative assessment tool for\nmodel performance, benchmarks are not only a core means to measure model\ncapabilities but also a key element in guiding the direction of model\ndevelopment and promoting technological innovation. We systematically review\nthe current status and development of large language model benchmarks for the\nfirst time, categorizing 283 representative benchmarks into three categories:\ngeneral capabilities, domain-specific, and target-specific. General capability\nbenchmarks cover aspects such as core linguistics, knowledge, and reasoning;\ndomain-specific benchmarks focus on fields like natural sciences, humanities\nand social sciences, and engineering technology; target-specific benchmarks pay\nattention to risks, reliability, agents, etc. We point out that current\nbenchmarks have problems such as inflated scores caused by data contamination,\nunfair evaluation due to cultural and linguistic biases, and lack of evaluation\non process credibility and dynamic environments, and provide a referable design\nparadigm for future benchmark innovation.",
    "keywords_en": [
      "Large Language Models",
      "Benchmarks",
      "Model Evaluation",
      "Survey",
      "Data Contamination"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Large Language Model"
    ],
    "published_time": "2025-08-21T08:43:35.000Z",
    "download_time": "2025-08-23 03:50:33",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15361.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.15361\", \"arxiv_url\": \"https://arxiv.org/abs/2508.15361\"}"
  },
  {
    "id": "2508.15126",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.15126",
    "title_en": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery\n  Generated by AI Scientists",
    "summary_en": "Recent advances in large language models (LLMs) have enabled AI agents to\nautonomously generate scientific proposals, conduct experiments, author papers,\nand perform peer reviews. Yet this flood of AI-generated research content\ncollides with a fragmented and largely closed publication ecosystem.\nTraditional journals and conferences rely on human peer review, making them\ndifficult to scale and often reluctant to accept AI-generated research content;\nexisting preprint servers (e.g. arXiv) lack rigorous quality-control\nmechanisms. Consequently, a significant amount of high-quality AI-generated\nresearch lacks appropriate venues for dissemination, hindering its potential to\nadvance scientific progress. To address these challenges, we introduce aiXiv, a\nnext-generation open-access platform for human and AI scientists. Its\nmulti-agent architecture allows research proposals and papers to be submitted,\nreviewed, and iteratively refined by both human and AI scientists. It also\nprovides API and MCP interfaces that enable seamless integration of\nheterogeneous human and AI scientists, creating a scalable and extensible\necosystem for autonomous scientific discovery. Through extensive experiments,\nwe demonstrate that aiXiv is a reliable and robust platform that significantly\nenhances the quality of AI-generated research proposals and papers after\niterative revising and reviewing on aiXiv. Our work lays the groundwork for a\nnext-generation open-access ecosystem for AI scientists, accelerating the\npublication and dissemination of high-quality AI-generated research content.\nCode is available at https://github.com/aixiv-org. Website is available at\nhttps://forms.gle/DxQgCtXFsJ4paMtn8.",
    "keywords_en": [
      "AI Scientists",
      "Open Access",
      "Scientific Discovery",
      "Large Language Models",
      "AI Agents"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-20T23:16:41.000Z",
    "download_time": "2025-08-23 03:50:41",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15126.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.15126\", \"arxiv_url\": \"https://arxiv.org/abs/2508.15126\"}"
  }
]