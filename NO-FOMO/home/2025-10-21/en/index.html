<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-10-21</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-10-21</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Is Sora the beginning of the end for OpenAI?</h2>
                <span class="published-time">Published: 2025-10-21 16:01:17</span>
                
                <p class="summary">The emergence of Sora, OpenAI's groundbreaking text-to-video generative AI model, has sparked considerable debate regarding its long-term implications for the company's trajectory and the broader artificial intelligence landscape. While Sora showcases impressive capabilities in synthesizing realistic and complex video sequences from textual prompts, questions are arising about its potential to disrupt OpenAI's current market dominance, particularly given the intense competition in the generative AI space. The discussion often revolves around whether such advanced specialized models could lead to a fragmentation of OpenAI's focus or if they represent a natural, necessary evolution of its research agenda. Analysts are examining how Sora's development might influence investor confidence, talent retention, and OpenAI's strategic positioning amidst a rapidly evolving technological environment where new AI paradigms are frequently introduced, potentially challenging established leaders.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Sora</span><span>OpenAI</span><span>Generative AI</span><span>Video Generation</span><span>AI Competition</span><span>Deep Learning</span><span>AI Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Video Understanding</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://calnewport.com/is-sora-the-beginning-of-the-end-for-openai/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAI Set to Challenge Google with New ChatGPT Atlas Browser</h2>
                <span class="published-time">Published: 2025-10-21 17:03:34</span>
                
                <p class="summary">OpenAI is reportedly preparing to launch a new web browser, tentatively named "ChatGPT Atlas Browser," marking a significant strategic move to directly challenge Google's long-standing dominance in the internet browsing and search ecosystem. This initiative signals OpenAI's ambitious expansion of its artificial intelligence capabilities from conversational models into broader consumer-facing applications that deeply integrate web-based information retrieval and enhanced user interaction. The introduction of an AI-powered browser is anticipated to fundamentally redefine how users access and interact with online content, potentially offering a more intelligent, personalized, and efficient browsing experience through advanced features like AI-driven summarization, content contextualization, and predictive navigation. Industry observers suggest that this development will significantly intensify competition within the tech industry, compelling Google and other major players to accelerate their own AI integrations across their browser and search offerings. This potential shift indicates a pivotal moment in the evolution of digital information access, where AI agents could become central to how individuals navigate the internet.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>OpenAI</span><span>ChatGPT Atlas Browser</span><span>Web Browser</span><span>AI Competition</span><span>Google</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Search Engine</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.bloomberg.com/news/articles/2025-10-21/openai-set-to-challenge-google-with-new-chatgpt-atlas-browser" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Neural audio codecs: how to get audio into LLMs</h2>
                <span class="published-time">Published: 2025-10-21 12:55:59</span>
                
                <p class="summary">This article delves into the critical challenge of seamlessly integrating audio data into Large Language Models (LLMs) and highlights neural audio codecs as an essential technological solution. Traditional LLMs are inherently designed to process discrete text tokens, making the direct input of continuous, high-dimensional audio signals highly inefficient and complex. Neural audio codecs overcome this limitation by converting raw audio waveforms into discrete, compressed token sequences. This transformation typically involves an encoder that maps audio to a latent representation, followed by a quantizer that discretizes this representation into a predefined vocabulary of audio tokens. These tokens are then readily compatible with transformer-based LLM architectures, effectively bridging the modality gap. The advancement of these codecs is paramount for developing truly multimodal AI systems, allowing LLMs to process, understand, and generate audio content. This innovation significantly expands the scope and applicability of LLMs, fostering more natural human-computer interfaces and enabling novel applications in areas such as advanced speech recognition, audio synthesis, and music generation, thereby enhancing the LLM's capacity to interact with and interpret a broader spectrum of real-world information.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Neural Audio Codecs</span><span>Large Language Models (LLMs)</span><span>Audio Processing</span><span>Multimodal AI</span><span>Speech Representation</span><span>Deep Learning</span><span>Tokenization</span><span>Transformer Architectures</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://kyutai.org/next/codec-explainer" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Binary Retrieval-Augmented Reward Mitigates Hallucinations</h2>
                <span class="published-time">Published: 2025-10-21 16:14:28</span>
                
                <p class="summary">This research paper presents a novel methodology called Binary Retrieval-Augmented Reward, specifically engineered to address and mitigate the prevalent issue of hallucinations in large language models (LLMs). The approach centers on augmenting LLM training with a retrieval mechanism that queries external knowledge bases for factual information relevant to the generation task. A key innovation is the application of a binary reward signal, which is directly derived from the consistency and accuracy of the LLM's output when evaluated against the retrieved information. This binary feedback mechanism is hypothesized to steer the model more effectively during the learning process, encouraging the generation of responses that are factually grounded and coherent, thereby significantly reducing instances of fabricated or incorrect content. The proposed method represents a critical advancement in enhancing the reliability and trustworthiness of AI systems, particularly as LLMs are deployed in sensitive applications requiring high factual accuracy. This work contributes to the ongoing efforts to improve the robustness and factual fidelity of generative AI.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Retrieval-Augmented Generation</span><span>Large Language Models</span><span>Hallucinations</span><span>Reward Models</span><span>Factual Consistency</span><span>AI Safety</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://arxiv.org/abs/2510.17733" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LLMs can get "brain rot"</h2>
                <span class="published-time">Published: 2025-10-21 14:24:26</span>
                
                <p class="summary">The concept of "brain rot" in Large Language Models (LLMs) describes a critical and concerning phenomenon where these advanced artificial intelligence systems undergo a noticeable degradation in their performance, coherence, or accumulated knowledge over time. This effect is frequently observed in contexts such as continuous self-training loops, recursive fine-tuning processes, or when LLMs are exposed to datasets that are increasingly composed of synthetic content or data generated by other AI models. The degradation typically manifests as a decline in factual accuracy, a reduction in sophisticated reasoning capabilities, or an increase in the generation of repetitive and less diverse textual outputs. This "brain rot" poses substantial challenges to the long-term reliability, trustworthiness, and robustness of LLMs, as models may inadvertently "forget" previously learned information or distort critical insights. Researchers are actively engaged in understanding the precise mechanisms that lead to this degradation, aiming to develop effective mitigation strategies. The goal is to prevent catastrophic forgetting and ensure the sustainable development and deployment of AI systems that can maintain high performance and consistency over extended periods, addressing a fundamental concern for the future of AI applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Model Degradation</span><span>Catastrophic Forgetting</span><span>AI Robustness</span><span>Self-training</span><span>Synthetic Data</span><span>Performance Drift</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://llm-brain-rot.github.io/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Katakate ‚Äì Dozens of VMs per node for safe code exec</h2>
                <span class="published-time">Published: 2025-10-21 15:22:25</span>
                
                <p class="summary">Katakate is an open-source project unveiled on Hacker News, designed to provide scalable and secure infrastructure for executing code within lightweight virtual machines. It enables the deployment of dozens of VMs per node, primarily targeting the execution of AI-generated code, CI/CD runners, and off-chain AI Decentralized Applications (DApps). A core objective is to bypass the inherent security dangers and operational complexities often associated with Docker-in-Docker environments. Katakate distinguishes itself with a user-friendly command-line interface and a Python SDK, making it accessible for AI engineers who typically prefer to minimize engagement with complex VM orchestration and networking configurations. Emphasizing a "defense-in-depth" security philosophy, the project is presented as an an easy-to-host solution for personal infrastructure. The creator is actively soliciting community feedback and contributions, highlighting a clear and exciting development roadmap.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Virtual Machines</span><span>Code Execution</span><span>Security</span><span>AI Infrastructure</span><span>CICD</span><span>Orchestration</span><span>Virtualization</span><span>AI-generated Code</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/Katakate/k7" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>A private, multi-model, 100% local, full-featured alternative to Notebook LM</h2>
                <span class="published-time">Published: 2025-10-21T19:54:59Z</span>
                
                <p class="summary">Open Notebook presents itself as a privacy-focused, open-source, and 100% local alternative to Google's Notebook LM, addressing the need for secure and self-controlled knowledge acquisition. This robust platform enables users to maintain complete sovereignty over their research data, supporting an extensive array of over 16 AI providers, including industry leaders like OpenAI, Anthropic, and local options such as Ollama and LM Studio. This multi-model compatibility ensures flexibility and significant cost optimization for AI interactions. Open Notebook is adept at organizing diverse multi-modal content, ranging from PDFs and videos to audio files and web pages, integrating them into a unified research environment. Its advanced features include intelligent full-text and vector search capabilities across all content, context-aware AI conversations powered by the user's research, and a professional multi-speaker podcast generation tool. Furthermore, the platform offers a comprehensive REST API for seamless custom integrations and flexible deployment options via Docker, preventing vendor lock-in. Open Notebook provides an indispensable tool for individuals and organizations prioritizing data privacy, advanced AI capabilities, and customizable research workflows in their pursuit of knowledge.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Privacy-focused AI</span><span>Open Source</span><span>Notebook Alternative</span><span>Multi-model AI</span><span>Podcast Generation</span><span>Vector Search</span><span>AI Chat</span><span>Docker Deployment</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/lfnovo/open-notebook" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Claude Cookbooks</h2>
                <span class="published-time">Published: 2025-10-21T19:46:28Z</span>
                
                <p class="summary">The Claude Cookbooks offer a comprehensive collection of code and guides designed to assist developers in building applications with the Claude API. This repository provides readily copy-able code snippets, primarily in Python, though the underlying concepts are adaptable to any programming language. It serves as an essential resource for both new and experienced Claude API users, recommending foundational courses for beginners. The cookbooks cover a wide array of technical capabilities, including text and data classification, Retrieval Augmented Generation (RAG) for enhancing responses with external knowledge, and various summarization techniques. Beyond core NLP functionalities, the repository delves into advanced topics such as tool use and integration, demonstrating how to connect Claude with external tools for tasks like customer service, calculator operations, and SQL queries. It also features extensive third-party integrations, covering vector databases (e.g., Pinecone), Wikipedia, web pages, and internet search services. Multimodal capabilities are highlighted with guides on Claude's vision features for image interpretation, chart analysis, form content extraction, and even generating images in conjunction with Stable Diffusion. Advanced techniques like sub-agents, PDF processing, automated evaluations, JSON mode, moderation filters, and prompt caching are also explored, making this an invaluable resource for developing robust and efficient AI applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Claude API</span><span>AI Development</span><span>Large Language Model</span><span>Retrieval Augmented Generation</span><span>Tool Use</span><span>Multimodal AI</span><span>Prompt Engineering</span><span>API Integration</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/anthropics/claude-cookbooks" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ebook2audiobook</h2>
                <span class="published-time">Published: 2025-10-20T22:12:43Z</span>
                
                <p class="summary">ebook2audiobook is a robust CPU/GPU converter designed to transform digital eBooks into audiobooks, complete with chapters and metadata. It leverages various advanced text-to-speech (TTS) engines, including XTTSv2, Bark, Vits, Fairseq, YourTTS, and Tacotron, to generate high-quality audio. A key feature is its optional voice cloning capability, allowing users to personalize audiobook voices. The tool boasts extensive language support, catering to over 1110 languages, with English as the default. It is engineered for efficiency, capable of running on systems with as little as 4GB RAM. The project provides flexible deployment options, including local execution via Gradio web interface or command-line (headless) mode, as well as remote execution on platforms like Hugging Face Spaces, Google Colab, and Kaggle. Docker support is also integrated, offering pre-built containers for both CPU and NVIDIA GPU accelerated conversions, simplifying setup and deployment for diverse environments. This tool emphasizes responsible use, intended for non-DRM, legally acquired eBooks.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Audiobook Conversion</span><span>Text-to-Speech</span><span>Voice Cloning</span><span>XTTSv2</span><span>Multi-language Support</span><span>Deep Learning</span><span>Docker</span><span>Gradio</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/DrewThomasson/ebook2audiobook" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>DeepAnalyze: Agentic Large Language Models for Autonomous Data Science</h2>
                <span class="published-time">Published: 2025-10-19T15:13:42.000Z</span>
                
                <p class="summary">Autonomous data science, from raw data sources to analyst-grade deep research reports, has been a long-standing challenge, and is now becoming feasible with the emergence of powerful large language models (LLMs). Recent workflow-based data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B, the first agentic LLM designed for autonomous data science, capable of automatically completing the end-to-end pipeline from data sources to analyst-grade deep research reports. To tackle high-complexity data science tasks, we propose a curriculum-based agentic training paradigm that emulates the learning trajectory of human data scientists, enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments. We also introduce a data-grounded trajectory synthesis framework that constructs high-quality training data. Through agentic training, DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data question answering and specialized analytical tasks to open-ended data research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze outperforms previous workflow-based agents built on most advanced proprietary LLMs. The model, code, and training data of DeepAnalyze are open-sourced, paving the way toward autonomous data science.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Autonomous Data Science</span><span>Agentic Large Language Models</span><span>DeepAnalyze</span><span>Curriculum-based Training</span><span>Data-grounded Trajectory Synthesis</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.16872" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Glyph: Scaling Context Windows via Visual-Text Compression</h2>
                <span class="published-time">Published: 2025-10-20T17:58:56.000Z</span>
                
                <p class="summary">Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Context Windows</span><span>Visual-Text Compression</span><span>Vision-Language Models</span><span>Long-Context Modeling</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Multimodal</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.17800" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI</h2>
                <span class="published-time">Published: 2025-10-19T05:23:43.000Z</span>
                
                <p class="summary">The rapid evolution of agentic AI marks a new phase in artificial intelligence, where Large Language Models (LLMs) no longer merely respond but act, reason, and adapt. This survey traces the paradigm shift in building agentic AI: from Pipeline-based systems, where planning, tool use, and memory are orchestrated by external logic, to the emerging Model-native paradigm, where these capabilities are internalized within the model's parameters. We first position Reinforcement Learning (RL) as the algorithmic engine enabling this paradigm shift. By reframing learning from imitating static data to outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task across language, vision and embodied domains. Building on this, the survey systematically reviews how each capability -- Planning, Tool use, and Memory -- has evolved from externally scripted modules to end-to-end learned behaviors. Furthermore, it examines how this paradigm shift has reshaped major agent applications, specifically the Deep Research agent emphasizing long-horizon reasoning and the GUI agent emphasizing embodied interaction. We conclude by discussing the continued internalization of agentic capabilities like Multi-agent collaboration and Reflection, alongside the evolving roles of the system and model layers in future agentic AI. Together, these developments outline a coherent trajectory toward model-native agentic AI as an integrated learning and interaction framework, marking the transition from constructing systems that apply intelligence to developing models that grow intelligence through experience.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Agentic AI</span><span>Model-native</span><span>Large Language Models</span><span>Reinforcement Learning</span><span>Paradigm Shift</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.16720" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</h2>
                <span class="published-time">Published: 2025-10-20T17:48:26.000Z</span>
                
                <p class="summary">Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Computer Use Agents</span><span>Foundation Model</span><span>Hybrid Action</span><span>Multimodal Agents</span><span>Reinforcement Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Multimodal</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.17790" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ConsistEdit: Highly Consistent and Precise Training-free Visual Editing</h2>
                <span class="published-time">Published: 2025-10-20T17:59:52.000Z</span>
                
                <p class="summary">Recent advances in training-free attention control methods have enabled flexible and efficient text-guided editing capabilities for existing generation models. However, current approaches struggle to simultaneously deliver strong editing strength while preserving consistency with the source. This limitation becomes particularly critical in multi-round and video editing, where visual errors can accumulate over time. Moreover, most existing methods enforce global consistency, which limits their ability to modify individual attributes such as texture while preserving others, thereby hindering fine-grained editing. Recently, the architectural shift from U-Net to MM-DiT has brought significant improvements in generative performance and introduced a novel mechanism for integrating text and vision modalities. These advancements pave the way for overcoming challenges that previous methods failed to resolve. Through an in-depth analysis of MM-DiT, we identify three key insights into its attention mechanisms. Building on these, we propose ConsistEdit, a novel attention control method specifically tailored for MM-DiT. ConsistEdit incorporates vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of the query, key, and value tokens to produce consistent, prompt-aligned edits. Extensive experiments demonstrate that ConsistEdit achieves state-of-the-art performance across a wide range of image and video editing tasks, including both structure-consistent and structure-inconsistent scenarios. Unlike prior methods, it is the first approach to perform editing across all inference steps and attention layers without handcraft, significantly enhancing reliability and consistency, which enables robust multi-round and multi-region editing. Furthermore, it supports progressive adjustment of structural consistency, enabling finer control.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>ConsistEdit</span><span>Visual Editing</span><span>Attention Control</span><span>MM-DiT</span><span>Image and Video Editing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Deep Learning</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.17803" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agentic Reinforcement Learning for Search is Unsafe</h2>
                <span class="published-time">Published: 2025-10-20T11:19:37.000Z</span>
                
                <p class="summary">Agentic reinforcement learning (RL) trains large language models to autonomously call tools during reasoning, with search as the most common application. These models excel at multi-step reasoning tasks, but their safety properties are not well understood. In this study, we show that RL-trained search models inherit refusal from instruction tuning and often deflect harmful requests by turning them into safe queries. However, this safety is fragile. Two simple attacks, one that forces the model to begin response with search (Search attack), another that encourages models to repeatedly search (Multi-search attack), trigger cascades of harmful searches and answers. Across two model families (Qwen, Llama) with both local and web search, these attacks lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query safety by 82.4%. The attacks succeed by triggering models to generate harmful, request-mirroring search queries before they can generate the inherited refusal tokens. This exposes a core weakness of current RL training: it rewards continued generation of effective queries without accounting for their harmfulness. As a result, RL search models have vulnerabilities that users can easily exploit, making it urgent to develop safety-aware agentic RL pipelines optimising for safe search.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Agentic Reinforcement Learning</span><span>Large Language Models</span><span>AI Safety</span><span>Search Models</span><span>Adversarial Attacks</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.17431" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>