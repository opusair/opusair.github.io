<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-01</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-12-01</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>DeepSeek-v3.2: Pushing the frontier of open large language models</h2>
                <span class="published-time">Published: 2025-12-01 15:48:03</span>
                
                <p class="summary">DeepSeek-v3.2 marks a notable stride in the development of open large language models, aiming to expand the frontiers of accessible, high-performance AI. Released by DeepSeek AI, this version is designed to elevate the capabilities of open-source LLMs, likely through sophisticated architectural innovations, optimized training regimes, and a focus on enhanced reasoning and generation. The availability of DeepSeek-v3.2 on platforms like Hugging Face signifies a commitment to the open-source community, enabling researchers and developers to leverage, experiment with, and build upon advanced AI technology. This release positions itself as a strong contender in the landscape of LLMs, striving to offer competitive performance compared to closed-source alternatives across a spectrum of tasks, including complex problem-solving and nuanced conversational interactions. By fostering greater transparency and collaborative engagement, DeepSeek-v3.2 contributes to democratizing advanced AI, providing a powerful resource for innovation and diverse applications in the broader artificial intelligence domain.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>DeepSeek-v3.2</span><span>Large Language Models</span><span>Open-source AI</span><span>AI Research</span><span>Generative AI</span><span>Model Architecture</span><span>Natural Language Processing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/paper.pdf" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Search tool that only returns content created before ChatGPT's public release</h2>
                <span class="published-time">Published: 2025-12-01 04:06:06</span>
                
                <p class="summary">A novel search tool, dubbed "Slop Evader," has been introduced with the specific objective of filtering web content to display only results published prior to the public release of ChatGPT. This initiative directly addresses growing concerns among users and researchers regarding the proliferation of AI-generated content on the internet and its potential impact on information quality and authenticity. By restricting search results to the pre-ChatGPT era, the tool aims to offer a pristine digital environment, free from the influence of modern large language models. This allows users to access information, articles, and discussions that are unequivocally human-authored, providing a valuable resource for those seeking original content or conducting research uninfluenced by contemporary generative AI paradigms. The tool underscores a nascent demand for mechanisms to navigate and curate information in an increasingly AI-saturated online landscape.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Search engine</span><span>Content filtering</span><span>Generative AI</span><span>Information quality</span><span>Pre-ChatGPT era</span><span>AI detection</span><span>Web search</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://tegabrain.com/Slop-Evader" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Cara Hunter on the deepfake video that nearly ended her political career</h2>
                <span class="published-time">Published: 2025-12-01 19:32:32</span>
                
                <p class="summary">Cara Hunter, a notable political figure, experienced a severe threat to her public career following the circulation of a highly explicit deepfake video. This digitally fabricated content, designed to falsely portray Hunter, exemplifies the increasing weaponization of artificial intelligence in political spheres. The incident illuminates the escalating challenges posed by synthetic media, particularly its capacity to disseminate misinformation and inflict substantial reputational harm on individuals. This specific case underscores the critical need for multifaceted approaches to counter deepfakes, encompassing advanced technological detection mechanisms, widespread public education initiatives, and the implementation of stringent legal and regulatory frameworks aimed at safeguarding individuals from such malicious digital assaults. The experience of Cara Hunter serves as a compelling and cautionary tale, illustrating how sophisticated AI technologies, when maliciously deployed, can yield devastating personal and professional ramifications, thereby intensifying ongoing debates concerning digital ethics, online safety, and the integrity of public discourse in the digital age.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Deepfake</span><span>Artificial Intelligence</span><span>Synthetic Media</span><span>Misinformation</span><span>Digital Ethics</span><span>Video Manipulation</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.theguardian.com/society/ng-interactive/2025/dec/01/it-was-extremely-pornographic-cara-hunter-on-the-deepfake-video-that-nearly-ended-her-political-career" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning</h2>
                <span class="published-time">Published: 2025-12-01 08:54:31</span>
                
                <p class="summary">DeepSeek AI has unveiled DeepSeekMath-V2, an advanced large language model specifically engineered to significantly enhance mathematical reasoning capabilities through a novel self-verifiable approach. This new iteration aims to address the common challenges faced by existing large language models concerning accuracy and reliability when tackling complex mathematical problem-solving. By integrating sophisticated mechanisms for self-verification, DeepSeekMath-V2 is designed not only to generate potential solutions but also to critically evaluate and validate them, potentially through iterative refinement, formal proof generation, or consistency checks. This development represents a crucial step towards building more robust and trustworthy artificial intelligence systems for demanding scientific and engineering domains. The model's emphasis on verifiable reasoning is anticipated to substantially improve performance in fields requiring high precision and certainty, such as formal mathematics, theoretical physics, and intricate computational tasks, offering a new paradigm for AI-driven mathematical discovery and problem-solving.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Mathematical Reasoning</span><span>Large Language Model</span><span>Self-Verification</span><span>AI Research</span><span>Deep Learning</span><span>Math AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/deepseek-ai/DeepSeek-Math-V2" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Why I'm Betting Against the AGI Hype</h2>
                <span class="published-time">Published: 2025-12-01 17:09:06</span>
                
                <p class="summary">This article explores the growing skepticism surrounding the immediate realization of Artificial General Intelligence (AGI), challenging the prevailing narrative of rapid progress often amplified by media and tech evangelists. The author argues that despite significant advancements in specific AI domains, particularly with large language models, these systems still lack fundamental aspects of human-like understanding, common sense, and true generalization capabilities. The piece delves into the inherent limitations of current AI architectures, suggesting that incremental improvements in narrow AI do not necessarily pave the way for AGI as quickly as some predict. It emphasizes the complex philosophical and technical hurdles that remain, such as achieving genuine consciousness, emotional intelligence, and real-world embodiment, which are often overlooked in the hype cycle. The author concludes by advocating for a more pragmatic and long-term view of AI development, urging a focus on robust and beneficial narrow AI applications rather than premature expectations for AGI.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AGI</span><span>AI Hype</span><span>Artificial General Intelligence</span><span>AI Development</span><span>Skepticism</span><span>AI Limitations</span><span>Future of AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.notesfromthecircus.com/p/why-im-betting-against-the-agi-hype" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Google, Nvidia, and OpenAI</h2>
                <span class="published-time">Published: 2025-12-01 15:18:42</span>
                
                <p class="summary">This article, titled 'Google, Nvidia, and OpenAI', is expected to analyze the complex and competitive landscape involving these three pivotal entities shaping the future of artificial intelligence. It likely delves into Google's expansive AI ecosystem, encompassing cutting-edge research, powerful cloud computing services, and proprietary hardware like TPUs, positioning it as a key competitor to OpenAI. The piece would highlight Nvidia's indispensable role as the dominant provider of high-performance GPUs, which are foundational for training and deploying advanced AI models across the industry, including for both Google and OpenAI. Furthermore, the analysis would scrutinize OpenAI's rapid advancements in generative AI and large language models, examining its market impact, strategic alliances, and ongoing rivalry with established tech giants. The objective is to unravel the strategic implications of their collaborations, market competitions, and technological breakthroughs, offering insights into the evolving AI industry and its potential future directions.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Artificial Intelligence</span><span>Large Language Models</span><span>Generative AI</span><span>AI Hardware</span><span>GPUs</span><span>Cloud Computing</span><span>AI Ecosystem</span><span>Strategic Partnerships</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://stratechery.com/2025/google-nvidia-and-openai/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>TrendRadar</h2>
                <span class="published-time">Published: 2025-11-29T11:59:33Z</span>
                
                <p class="summary">TrendRadar is an open-source GitHub project designed to aggregate, filter, and analyze real-time hot news from over 11 mainstream platforms, deployable in as little as 30 seconds. It offers intelligent push strategies (daily, current, incremental), precise content filtering using customizable keywords, and real-time trend analysis to understand topic evolution. The platform supports multi-channel notifications (WeChat, Feishu, Telegram, Email, Slack, etc.) and multi-terminal adaptation, including GitHub Pages and Docker deployment for data persistence. A key feature is its AI smart analysis, built on the Model Context Protocol (MCP), enabling natural language queries, in-depth trend analysis, sentiment analysis, and cross-platform data insights, making it an efficient tool for investors, content creators, and businesses to cut through information overload.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>News Aggregation</span><span>Trend Analysis</span><span>AI Agent</span><span>Content Filtering</span><span>Real-time Notifications</span><span>Docker Deployment</span><span>Model Context Protocol</span><span>GitHub Actions</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/sansan0/TrendRadar" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent Development Kit (ADK) for Go</h2>
                <span class="published-time">Published: 2025-12-01T07:40:37Z</span>
                
                <p class="summary">The Agent Development Kit (ADK) for Go is an open-source, code-first toolkit designed to streamline the building, evaluating, and deploying of sophisticated AI agents. Developed by Google, ADK applies software development principles to AI agent creation, offering a flexible and modular framework for orchestrating agent workflows from simple tasks to complex multi-agent systems. While optimized for Google's Gemini, it maintains model and deployment agnosticism, ensuring compatibility across various AI models and frameworks. This Go version leverages the language's strengths in concurrency and performance, making it ideal for developers creating cloud-native agent applications, particularly in environments like Google Cloud Run. Key features include idiomatic Go design, a rich tool ecosystem for diverse agent capabilities, and a code-first approach that enhances flexibility, testability, and versioning. ADK-Go facilitates the creation of scalable applications through modular multi-agent system design, enabling easy containerization and deployment.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agent</span><span>Go</span><span>Toolkit</span><span>Cloud-native</span><span>Multi-Agent Systems</span><span>Google Cloud</span><span>Software Development</span><span>AI Framework</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Artificial Intelligence</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/google/adk-go" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>‚û§ Cursor Free VIP</h2>
                <span class="published-time">Published: 2025-09-16T03:47:39Z</span>
                
                <p class="summary">Cursor Free VIP is a cross-platform utility tool designed for educational and research purposes, aiming to help users manage and reset configurations of the Cursor AI IDE. It supports Windows, macOS, and Linux operating systems across various architectures, offering automated installation scripts for ease of use. Key features include the ability to reset Cursor's settings and multi-language support, enhancing accessibility for a global user base. The project emphasizes its role as a learning aid, explicitly stating it does not generate fake email accounts or facilitate unauthorized OAuth access, and encourages users to support the original Cursor project. It provides detailed configuration options for various system paths and timing parameters, and includes a comprehensive changelog. Users are advised to run the script with administrator privileges and ensure the Cursor IDE is closed for optimal performance. This tool presents itself as a valuable resource for developers and researchers exploring the Cursor AI environment.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Cursor IDE</span><span>AI Development Tool</span><span>Configuration Management</span><span>Cross-Platform Utility</span><span>Automation Script</span><span>Software Research</span><span>Developer Productivity</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/yeongpin/cursor-free-vip" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models</h2>
                <span class="published-time">Published: 2025-11-28T16:17:53.000Z</span>
                
                <p class="summary">This work explores the challenge of building "Machines that Can Remember", framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: sparsity, random-access flexibility, and length generalization. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Ultra-long context modeling</span><span>Large Language Models</span><span>Hierarchical Sparse Attention</span><span>Length generalization</span><span>Transformers</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.23319" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement</h2>
                <span class="published-time">Published: 2025-11-28T18:59:01.000Z</span>
                
                <p class="summary">Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multi-person video generation</span><span>Diffusion Transformer</span><span>Identity-aware attention</span><span>Talking video synthesis</span><span>Interactivity refinement</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.23475" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</h2>
                <span class="published-time">Published: 2025-11-27T18:52:07.000Z</span>
                
                <p class="summary">The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the "scale-at-all-costs" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Image Generation</span><span>Foundation Model</span><span>Diffusion Transformer</span><span>Efficient AI</span><span>Consumer-grade Hardware</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.22699" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>REASONEDIT: Towards Reasoning-Enhanced Image Editing Models</h2>
                <span class="published-time">Published: 2025-11-27T17:02:48.000Z</span>
                
                <p class="summary">Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Image Editing</span><span>Reasoning</span><span>Multimodal Large Language Models</span><span>Diffusion Models</span><span>AI Agent</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Multimodal</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.22625" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning</h2>
                <span class="published-time">Published: 2025-11-27T16:01:22.000Z</span>
                
                <p class="summary">Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>DeepSeekMath-V2</span><span>Mathematical Reasoning</span><span>Self-Verification</span><span>Large Language Models</span><span>Theorem Proving</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.22570" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action</h2>
                <span class="published-time">Published: 2025-11-27T06:03:53.000Z</span>
                
                <p class="summary">To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Embodied Agent</span><span>Vision-Language-Action</span><span>Reasoning</span><span>Action Degeneration</span><span>DualVLA</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Multimodal</span><span>Robotics</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.22134" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>