<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-06-20</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2025-06-20</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Twitter</h2>

            <article class="item-card">
                <h2>gdb_OpenAI Codex Averages 10K PRs Daily, AI Transforms Software Engineering</h2>
                <span class="published-time">Published: 2025-06-20T01:37:12.000Z</span>
                <img src="../screenshot/twitter/gdb_1935874544931324325.png" alt="gdb_OpenAI Codex Averages 10K PRs Daily, AI Transforms Software Engineering">
                <p class="summary">OpenAI co-founder Greg Brockman quoted Anjney Midha's tweet, revealing that OpenAI Codex has merged 345,000 pull requests on GitHub over the past 35 days, averaging 10,000 per day. This impressive statistic highlights the increasing impact of artificial intelligence in software engineering, signaling that AI is profoundly transforming software development paradigms.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>OpenAI</span><span>Codex</span><span>GitHub</span><span>Pull Request</span><span>Software Engineering</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Tech News</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/gdb/status/1935874544931324325" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DeepLearningAI_Apple Updates Foundation Models, Enhancing On-Device and Server AI Performance</h2>
                <span class="published-time">Published: 2025-06-20T18:00:02.000Z</span>
                <img src="../screenshot/twitter/DeepLearningAI_1936121879552537056.png" alt="DeepLearningAI_Apple Updates Foundation Models, Enhancing On-Device and Server AI Performance">
                <p class="summary">Apple has refreshed its Apple Foundation Models (AFM) with new versions for on-device and server use, aiming to improve performance in tasks like image understanding and multilingual reasoning. The company also released a Foundation Models API for developers. The on-device AFM, utilizing a compact 3B-parameter transformer, outperformed similar-sized rivals in some tasks. Conversely, the server-side AFM, leveraging a custom mixture-of-experts architecture, showed mixed results when compared against top competitors such as GPT-4o.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Apple</span><span>Foundation Models</span><span>Large Language Model</span><span>On-device AI</span><span>Server AI</span><span>Mixture-of-Experts</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Product Launch</span><span>Tech News</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/DeepLearningAI/status/1936121879552537056" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>reach_vb_DeepMind Launches Magenta Real-time Music Generation Model</h2>
                <span class="published-time">Published: 2025-06-20T21:14:25.000Z</span>
                <img src="../screenshot/twitter/reach_vb_1936182860228034902.png" alt="reach_vb_DeepMind Launches Magenta Real-time Music Generation Model">
                <p class="summary">DeepMind has launched Magenta Real-time, an Apache 2.0 licensed music generation model with over 800 million parameters. Trained on approximately 190,000 hours of instrumental stock music, it adapts MusicLM for real-time generation using 2-second audio chunks conditioned on 10-second prior context. The model introduces MusicCoCa, a joint music-text embedding model, and supports style embeddings for real-time genre/instrument morphing. It achieves 1.25-second generation for 2-second audio on free-tier Colab TPUs, with model weights available on Hugging Face.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>DeepMind</span><span>Magenta Real-time</span><span>Music Generation</span><span>Real-time Generation</span><span>Transformer Model</span><span>MusicCoCa</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Product Launch</span><span>Open Source</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/reach_vb/status/1936182860228034902" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiniMax__AI_Launches Breakthrough Voice Generation Technology</h2>
                <span class="published-time">Published: 2025-06-20T17:27:21.000Z</span>
                <img src="../screenshot/twitter/MiniMax__AI_1936113656372379680.png" alt="MiniMax__AI_Launches Breakthrough Voice Generation Technology">
                <p class="summary">During "MiniMax Week," MiniMax unveiled "Audio Dessert," a breakthrough in voice generation technology. This innovation enables users to create speech with any prompt, any voice, and any emotion. It offers full customizability and multilingual support, significantly enhancing the flexibility and expressiveness of voice synthesis and poised to revolutionize AI voice applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Voice Generation</span><span>MiniMax</span><span>Audio Dessert</span><span>Voice Design</span><span>AI Voice</span><span>Technology Breakthrough</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Product Launch</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/MiniMax__AI/status/1936113656372379680" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>osanseviero_Google AI Releases Medical AI Model MedGemma</h2>
                <span class="published-time">Published: 2025-06-20T16:00:01.000Z</span>
                <img src="../screenshot/twitter/osanseviero_1936096973691539652.png" alt="osanseviero_Google AI Releases Medical AI Model MedGemma">
                <p class="summary">Google AI Developers have officially launched MedGemma, a new collection of Gemma 3 variant models specifically optimized for medical text and image comprehension tasks. MedGemma is available in two distinct versions: a 4B multimodal model, capable of processing both text and images, and a 27B text-only model. This strategic release aims to significantly accelerate the development and deployment of AI projects within the healthcare sector, providing researchers and developers with powerful, efficient, and specialized AI tools to advance medical applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>MedGemma</span><span>Gemma</span><span>Medical AI</span><span>Multimodal</span><span>Google AI</span><span>Healthcare AI</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/osanseviero/status/1936096973691539652" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>meshcapade_Showcases Future of Digital Human Modeling at CVPR2025</h2>
                <span class="published-time">Published: 2025-06-20T14:15:29.000Z</span>
                <img src="../screenshot/twitter/meshcapade_1936065369287925838.png" alt="meshcapade_Showcases Future of Digital Human Modeling at CVPR2025">
                <p class="summary">Meshcapade showcased its production-ready digital human technology at CVPR 2025, revealing the future of human modeling. This technology enables real-time avatar generation from images and video, utilizes SMPL-based motion and shape estimation, and offers fully on-premise, privacy-compliant pipelines. Its applications span robotics, autonomous vehicles, artificial intelligence, and fashion.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Digital Humans</span><span>Computer Vision</span><span>SMPL</span><span>Avatar Generation</span><span>Robotics</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Computer Vision</span><span>Robotics</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/meshcapade/status/1936065369287925838" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>Andrej Karpathy's Latest Speech Goes Viral: The Era of Software 3.0 Has Arrived!</h2>
                <span class="published-time">Published: 2025-06-20T16:02:17.000Z</span>
                <img src="../screenshot/wechat/wechat_image_AEc0uKb9vUOpqoX_iPJ4MQ.png" alt="Andrej Karpathy's Latest Speech Goes Viral: The Era of Software 3.0 Has Arrived!">
                <p class="summary">Andrej Karpathy, a founding member of OpenAI and former Director of AI at Tesla, recently delivered a highly influential speech titled "Software in the era of AI" at Y Combinator's AI Startup School, which has garnered widespread attention and discussion. In his address, Karpathy articulated a profound paradigm shift occurring in software development, asserting that the era of "Software 3.0" is now upon us, fundamentally driven by artificial intelligence. As a figure who significantly shaped the golden age of deep learning, his pronouncements consistently spark extensive discussion and reflection within the technical community, revealing the deep transformations AI is bringing to the industry. This latest presentation further solidifies his role as a visionary, offering critical insights into how AI is reshaping the very foundations of software engineering and development, moving from explicit programming to more data-driven and model-centric approaches. His perspective is essential for comprehending the future direction of AI-powered systems and their integration into evolving software paradigms.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Andrej Karpathy</span><span>Software 3.0</span><span>AI Era</span><span>Deep Learning</span><span>Software Development</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Deep Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/AEc0uKb9vUOpqoX_iPJ4MQ" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>The Agentic AI Moment: Multi-Agent Systems Drive the 'One-Person Company' Revolution</h2>
                <span class="published-time">Published: 2025-06-20T10:39:07.000Z</span>
                <img src="../screenshot/wechat/wechat_image_8JfR11MUxZneJBDnLDCX_g.png" alt="The Agentic AI Moment: Multi-Agent Systems Drive the 'One-Person Company' Revolution">
                <p class="summary">The article underscores the rapid advancement and burgeoning era of Agentic AI, a technology enabling AI to independently perceive environments, utilize various tools, and execute complex tasks, thereby evolving AI capabilities from simple "question-answering" to proactive "doing." Amazon Web Services (AWS) is at the forefront, offering a comprehensive full-stack technical framework, including Amazon Bedrock, Amazon Q Developer, and Amazon Transform. These tools empower enterprises to rapidly develop AI applications, modernize legacy codebases, and achieve substantial productivity improvements. Real-world examples, such as Fosun Pharma's accelerated medical writing and HOHO Information's efficient document processing, illustrate significant cost reductions and efficiency gains in knowledge-intensive domains. The article posits that Agentic AI is a pivotal turning point, potentially leading to the rise of "one-person companies" and fundamentally reshaping traditional IT systems. AWS has strategically positioned Agentic AI as a future multi-billion dollar business, actively driving its innovation and widespread adoption.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Agent</span><span>Generative AI</span><span>Amazon Web Services</span><span>Large Language Models</span><span>Productivity</span></div>
                    <div class="area"><span class="label">Areas：</span><span>AI Agent</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/8JfR11MUxZneJBDnLDCX_g" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DeepMind Uncovers R1's Secret: 'Aha Moments' Revealed and Quantifiable for the First Time!</h2>
                <span class="published-time">Published: 2025-06-20T13:24:45.000Z</span>
                <img src="../screenshot/wechat/wechat_image_sS8ZyUWlo5mNfmu0b9PnGw.png" alt="DeepMind Uncovers R1's Secret: 'Aha Moments' Revealed and Quantifiable for the First Time!">
                <p class="summary">A joint research team from the University of Tokyo and Google DeepMind has for the first time visualized the internal thought processes of reasoning models like DeepSeek-R1 using "reasoning graphs," unveiling the mechanisms behind their impressive intelligence. The study reveals that the models' self-correction "aha moments" manifest as distinct cyclic structures within these graphs, quantifiable as an average of approximately five "reconsiderations" per problem. Reasoning graphs also demonstrate that advanced reasoning models, compared to foundational models, exhibit more loops, larger graph diameters, and small-world network characteristics. These topological structures are crucial for their efficient inference and error correction capabilities. Furthermore, high-quality training data significantly expands the reasoning graph's diameter. This groundbreaking research offers a novel perspective for understanding and enhancing AI's reasoning abilities, suggesting that future AI architectures will be designed based on the topological essence of intelligence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Reasoning Models</span><span>Reasoning Graphs</span><span>Aha Moments</span><span>Topological Structure</span><span>Small-World Networks</span><span>Self-Correction</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/sS8ZyUWlo5mNfmu0b9PnGw" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SIGGRAPH 2025 | Large Avatar Model: Single-Image Second-Level Creation of Hyper-Realistic 3D Interactive Digital Humans with Cross-Platform Real-time Driven Rendering</h2>
                <span class="published-time">Published: 2025-06-20T10:39:07.000Z</span>
                <img src="../screenshot/wechat/wechat_image_41mpwUeWFARsXRWuDrCBZA.png" alt="SIGGRAPH 2025 | Large Avatar Model: Single-Image Second-Level Creation of Hyper-Realistic 3D Interactive Digital Humans with Cross-Platform Real-time Driven Rendering">
                <p class="summary">Alibaba Tongyi Lab introduces the Large Avatar Model (LAM), designed to generate animatable, hyper-realistic 3D Gaussian digital humans from a single image within seconds. This model overcomes the limitations of traditional methods, which rely on multi-view data and complex post-processing, by employing novel techniques such as normalized space Gaussian sphere generation, a multi-modal feature interaction Transformer, and mesh subdivision. LAM achieves lightweight, cross-platform, real-time driven rendering, supporting WebGL with up to 120 FPS on mobile devices. It directly integrates with traditional graphics pipelines, eliminating the need for neural post-processing. Furthermore, LAM can be combined with large models for text-driven generation and 3D style transfer. This technology has been applied to build low-latency, low-cost interactive conversational digital human solutions, which are now fully open-sourced, offering a new paradigm for applications in virtual meetings, game development, and beyond.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Digital Human</span><span>3D Gaussian</span><span>Single-Image Generation</span><span>Real-time Rendering</span><span>Cross-Platform</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Computer Vision</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/41mpwUeWFARsXRWuDrCBZA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RAG Efficiency Soars 30% with Just 2 Lines of Code! Applicable to Various Tasks, Scalable to Billions of Data</h2>
                <span class="published-time">Published: 2025-06-20T16:02:17.000Z</span>
                <img src="../screenshot/wechat/wechat_image_r7jt1BDGP3ycVuCnscf4IQ.png" alt="RAG Efficiency Soars 30% with Just 2 Lines of Code! Applicable to Various Tasks, Scalable to Billions of Data">
                <p class="summary">A groundbreaking new open-source method, PSP (Proximity graph with Spherical Pathway), developed by a collaborative team from Zhejiang University and vector retrieval expert Fu Cong, has achieved a remarkable 30% boost in RAG (Retrieval-Augmented Generation) vector retrieval efficiency with just two lines of code modification. This innovative approach directly tackles the long-standing "metric mismatch" problem prevalent in maximum inner product search. PSP theoretically demonstrates that existing graph index structures, primarily designed for Euclidean distances, can be leveraged to find globally optimal maximum inner product solutions through a refined greedy search algorithm. Furthermore, PSP introduces an adaptive early stopping strategy, which intelligently determines when to conclude a search, thereby significantly accelerating retrieval speed and minimizing computational redundancy. The method exhibits robust generalization capabilities, proving effective across diverse data modalities such as text-to-text, image-to-image, and text-to-image search, as well as recommendation systems. Crucially, PSP showcases exceptional scalability, demonstrating near-logarithmic time complexity, making it highly suitable for deployment in applications involving billions of data points. This advancement represents a significant leap forward for vector retrieval, a core technological component underpinning many cutting-edge AI products, promising enhanced performance and broader applicability.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>RAG</span><span>Vector Retrieval</span><span>PSP</span><span>Inner Product Search</span><span>Euclidean Distance</span><span>Graph Index</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/r7jt1BDGP3ycVuCnscf4IQ" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Tsinghua University Releases Sundial: The First Trillion-Timestamp Pre-trained Generative Time Series Large Model | ICML Oral</h2>
                <span class="published-time">Published: 2025-06-20T04:05:05.000Z</span>
                <img src="../screenshot/wechat/wechat_image_y3sc2e2lmW1sqfnoK-ZdDA.png" alt="Tsinghua University Releases Sundial: The First Trillion-Timestamp Pre-trained Generative Time Series Large Model | ICML Oral">
                <p class="summary">Tsinghua University has unveiled Sundial, a generative time series large model, with its research accepted as an Oral paper at ICML 2025. This pioneering model marks the first to achieve trillion-timestamp pre-training and has established TimeBench, the largest time series dataset in the field. Sundial effectively addresses the non-determinism and mode collapse issues inherent in time series forecasting by incorporating a flow-matching-based prediction loss function. This innovation enables the model to generate multiple prediction trajectories and provide robust probabilistic forecasting capabilities. Compared to traditional methods and existing deep learning models, Sundial demonstrates exceptional zero-shot prediction performance across various benchmarks, boasting millisecond-level inference speeds. This breakthrough offers an out-of-the-box, highly efficient forecasting solution for diverse sectors such as meteorology, finance, and the Internet of Things, significantly expanding the potential applications of time series models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Time Series Large Model</span><span>Generative</span><span>Flow Matching</span><span>Zero-shot Prediction</span><span>Probabilistic Forecasting</span><span>Mode Collapse</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Deep Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/y3sc2e2lmW1sqfnoK-ZdDA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>Build a Large Language Model (From Scratch)</h2>
                <span class="published-time">Published: 2025-06-22T21:36:39Z</span>
                <img src="../screenshot/github/LLMs-from-scratch.png" alt="Build a Large Language Model (From Scratch)">
                <p class="summary">This GitHub repository serves as the official code companion for the book "Build a Large Language Model (From Scratch)," guiding users through the process of developing, pretraining, and finetuning GPT-like large language models from the ground up. The project offers a step-by-step coding approach to deeply understand LLM mechanics, including functionalities for loading and finetuning pretrained model weights. Its methodology mirrors that of large-scale foundational models, yet the code is designed to run efficiently on conventional laptops without requiring specialized hardware, making it ideal for educational purposes and developers seeking a profound understanding of LLMs.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Large Language Model</span><span>GPT</span><span>Build From Scratch</span><span>Pretraining</span><span>Finetuning</span><span>Natural Language Processing</span><span>Deep Learning</span><span>Code Implementation</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Deep Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/rasbt/LLMs-from-scratch" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AI Engineering Hub 🚀</h2>
                <span class="published-time">Published: 2025-06-22T06:10:52Z</span>
                <img src="https://github.com/patchy631/ai-engineering-hub/raw/main/assets/ai-eng-hub.gif" alt="AI Engineering Hub 🚀">
                <p class="summary">The "AI Engineering Hub" GitHub repository serves as a comprehensive resource for individuals looking to advance their skills in the rapidly evolving field of AI engineering. It offers in-depth tutorials focusing on critical areas such as Large Language Models (LLMs) and Retrieval-Augmented Generation (RAGs), alongside practical examples of real-world AI agent applications. Designed for beginners, practitioners, and researchers alike, this hub provides hands-on experience and scalable solutions to implement and adapt in various projects. By offering a blend of theoretical understanding and practical application, the repository empowers users to stay at the forefront of AI innovation, fostering experimentation and enabling successful deployment of advanced AI systems. It is an essential toolkit for anyone aiming to master the complexities of modern AI development.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Engineering</span><span>Large Language Models</span><span>Retrieval-Augmented Generation</span><span>AI Agents</span><span>Tutorials</span><span>Hands-on Experience</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/patchy631/ai-engineering-hub" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Claude Code</h2>
                <span class="published-time">Published: 2025-06-18T20:29:20Z</span>
                <img src="https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif" alt="Claude Code">
                <p class="summary">Claude Code, an innovative agentic coding tool developed by Anthropic, revolutionizes the development workflow by operating directly within the terminal. It possesses a deep understanding of the entire codebase, enabling developers to significantly accelerate their coding process. Through intuitive natural language commands, Claude Code can efficiently execute routine programming tasks, provide clear explanations for complex code segments, and seamlessly manage various Git workflows, from committing changes to branching. This powerful AI assistant is designed for flexible integration, usable directly in the terminal, within popular Integrated Development Environments (IDEs), or even by tagging @claude on GitHub. By automating repetitive actions and simplifying complex operations, Claude Code offers a sophisticated yet user-friendly AI-powered programming experience, ultimately streamlining development cycles and boosting overall productivity for software engineers across diverse projects.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Intelligent Coding Tool</span><span>AI Agent</span><span>Natural Language Programming</span><span>Code Understanding</span><span>Git Workflow</span><span>Terminal Tool</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/anthropics/claude-code" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Gitingest</h2>
                <span class="published-time">Published: 2025-06-22T23:41:14Z</span>
                <img src="https://github.com/cyclotruc/gitingest/raw/main/docs/frontpage.png" alt="Gitingest">
                <p class="summary">Gitingest is an innovative tool designed to transform the content of any Git repository into a prompt-friendly text digest for Large Language Models (LLMs). It generates text summaries of code context from Git repository URLs or local directories, offering smart formatting optimized for LLM prompts, along with statistics on file/directory structure, extract size, and token count. Gitingest is available as a command-line tool, a Python package, and browser extensions, significantly streamlining the process for LLMs to consume and analyze codebases, thereby enhancing code understanding and analysis efficiency.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Git Repository</span><span>LLM</span><span>Code Digest</span><span>Text Processing</span><span>Developer Tool</span><span>Prompt Engineering</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/cyclotruc/gitingest" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Web Development for Beginners - A Curriculum</h2>
                <span class="published-time">Published: 2025-05-29T17:34:21Z</span>
                <img src="https://github.com/microsoft/Web-Dev-For-Beginners/raw/main/images/background.png" alt="Web Development for Beginners - A Curriculum">
                <p class="summary">This comprehensive 12-week web development curriculum, meticulously crafted by Microsoft Cloud Advocates, provides a robust foundation in JavaScript, CSS, and HTML. It features 24 immersive, hands-on lessons, guiding learners through the creation of diverse projects like interactive terrariums, practical browser extensions, and engaging space games. The pedagogical approach emphasizes active learning through integrated quizzes, collaborative discussions, and practical assignments, ensuring optimal skill development and knowledge retention. This project-based methodology is designed to bridge the gap between theoretical understanding and practical application, preparing students for real-world web development challenges. Moreover, the team has recently launched an exciting new curriculum focused on Generative AI for JavaScript, offering an advanced learning path for developers keen on exploring cutting-edge artificial intelligence applications within the web ecosystem. This expansion highlights the commitment to providing relevant and forward-thinking educational resources.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Web Development</span><span>Front-end Development</span><span>JavaScript</span><span>HTML</span><span>CSS</span><span>Project-based Learning</span><span>Programming Education</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/microsoft/Web-Dev-For-Beginners" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>n8n - Secure Workflow Automation for Technical Teams</h2>
                <span class="published-time">Published: 2025-06-23T06:59:40Z</span>
                <img src="https://user-images.githubusercontent.com/10284570/173569848-c624317f-42b1-45a6-ab09-f0ea3c247648.png" alt="n8n - Secure Workflow Automation for Technical Teams">
                <p class="summary">n8n is a workflow automation platform designed for technical teams, blending the flexibility of code with the speed of no-code. It boasts over 400 integrations, native AI capabilities supporting the creation of AI agent workflows based on LangChain, and operates under a fair-code license, granting users full control over their data and deployments. n8n offers enterprise-grade features like advanced permissions, SSO, and air-gapped deployments. Supported by an active community providing numerous templates and assistance, it stands as an ideal choice for building robust automation solutions.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Workflow Automation</span><span>Low-Code</span><span>No-Code</span><span>AI Integration</span><span>Data Control</span><span>Self-Hosting</span><span>Enterprise-Grade</span><span>LangChain</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/n8n-io/n8n" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain
  Perspective</h2>
                <span class="published-time">Published: 2025-06-17T20:24:00.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14965.png" alt="Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain
  Perspective">
                <p class="summary">Reinforcement learning (RL) has emerged as a promising approach to improve
large language model (LLM) reasoning, yet most open efforts focus narrowly on
math and code, limiting our understanding of its broader applicability to
general reasoning. A key challenge lies in the lack of reliable, scalable RL
reward signals across diverse reasoning domains. We introduce Guru, a curated
RL reasoning corpus of 92K verifiable examples spanning six reasoning
domains--Math, Code, Science, Logic, Simulation, and Tabular--each built
through domain-specific reward design, deduplication, and filtering to ensure
reliability and effectiveness for RL training. Based on Guru, we systematically
revisit established findings in RL for LLM reasoning and observe significant
variation across domains. For example, while prior work suggests that RL
primarily elicits existing knowledge from pretrained models, our results reveal
a more nuanced pattern: domains frequently seen during pretraining (Math, Code,
Science) easily benefit from cross-domain RL training, while domains with
limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain
training to achieve meaningful performance gains, suggesting that RL is likely
to facilitate genuine skill acquisition. Finally, we present Guru-7B and
Guru-32B, two models that achieve state-of-the-art performance among open
models RL-trained with publicly available data, outperforming best baselines by
7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We
also show that our models effectively improve the Pass@k performance of their
base models, particularly on complex tasks less likely to appear in pretraining
data. We release data, models, training and evaluation code to facilitate
general-purpose reasoning at: https://github.com/LLM360/Reasoning360</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Reinforcement Learning</span><span>Large Language Models</span><span>Reasoning</span><span>Cross-Domain</span><span>General-purpose Reasoning</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.14965" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show-o2: Improved Native Unified Multimodal Models</h2>
                <span class="published-time">Published: 2025-06-18T15:39:15.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15564.png" alt="Show-o2: Improved Native Unified Multimodal Models">
                <p class="summary">This paper presents improved native unified multimodal models, i.e.,
Show-o2, that leverage autoregressive modeling and flow matching. Built upon a
3D causal variational autoencoder space, unified visual representations are
constructed through a dual-path of spatial (-temporal) fusion, enabling
scalability across image and video modalities while ensuring effective
multimodal understanding and generation. Based on a language model,
autoregressive modeling and flow matching are natively applied to the language
head and flow head, respectively, to facilitate text token prediction and
image/video generation. A two-stage training recipe is designed to effectively
learn and scale to larger models. The resulting Show-o2 models demonstrate
versatility in handling a wide range of multimodal understanding and generation
tasks across diverse modalities, including text, images, and videos. Code and
models are released at https://github.com/showlab/Show-o.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Multimodal Models</span><span>Autoregressive Modeling</span><span>Flow Matching</span><span>Unified Visual Representations</span><span>Multimodal Understanding and Generation</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Multimodal</span><span>Deep Learning</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.15564" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Improved Iterative Refinement for Chart-to-Code Generation via
  Structured Instruction</h2>
                <span class="published-time">Published: 2025-06-15T14:10:16.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14837.png" alt="Improved Iterative Refinement for Chart-to-Code Generation via
  Structured Instruction">
                <p class="summary">Recently, multimodal large language models (MLLMs) have attracted increasing
research attention due to their powerful visual understanding capabilities.
While they have achieved impressive results on various vision tasks, their
performance on chart-to-code generation remains suboptimal. This task requires
MLLMs to generate executable code that can reproduce a given chart, demanding
not only precise visual understanding but also accurate translation of visual
elements into structured code. Directly prompting MLLMs to perform this complex
task often yields unsatisfactory results. To address this challenge, we propose
{ChartIR}, an iterative refinement method based on structured instruction.
First, we distinguish two tasks: visual understanding and code translation. To
accomplish the visual understanding component, we design two types of
structured instructions: description and difference. The description
instruction captures the visual elements of the reference chart, while the
difference instruction characterizes the discrepancies between the reference
chart and the generated chart. These instructions effectively transform visual
features into language representations, thereby facilitating the subsequent
code translation process. Second, we decompose the overall chart generation
pipeline into two stages: initial code generation and iterative refinement,
enabling progressive enhancement of the final output. Experimental results show
that, compared to other method, our method achieves superior performance on
both the open-source model Qwen2-VL and the closed-source model GPT-4o.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Chart-to-Code Generation</span><span>Iterative Refinement</span><span>Structured Instruction</span><span>Multimodal Large Language Models</span><span>Visual Understanding</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Multimodal</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.14837" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation</h2>
                <span class="published-time">Published: 2025-06-18T13:35:47.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15455.png" alt="RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation">
                <p class="summary">Recent Large Language Models (LLMs) have reported high accuracy on reasoning
benchmarks. However, it is still unclear whether the observed results arise
from true reasoning or from statistical recall of the training set. Inspired by
the ladder of causation (Pearl, 2009) and its three levels (associations,
interventions and counterfactuals), this paper introduces RE-IMAGINE, a
framework to characterize a hierarchy of reasoning ability in LLMs, alongside
an automated pipeline to generate problem variations at different levels of the
hierarchy. By altering problems in an intermediate symbolic representation,
RE-IMAGINE generates arbitrarily many problems that are not solvable using
memorization alone. Moreover, the framework is general and can work across
reasoning domains, including math, code, and logic. We demonstrate our
framework on four widely-used benchmarks to evaluate several families of LLMs,
and observe reductions in performance when the models are queried with problem
variations. These assessments indicate a degree of reliance on statistical
recall for past performance, and open the door to further research targeting
skills across the reasoning hierarchy.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Large Language Models</span><span>Reasoning evaluation</span><span>Symbolic benchmark synthesis</span><span>Problem variations</span><span>Statistical recall</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.15455" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>