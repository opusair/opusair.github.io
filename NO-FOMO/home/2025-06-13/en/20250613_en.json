[
  {
    "id": "twitter_AnthropicAI_1933630785879507286",
    "source": "Twitter",
    "url": "https://twitter.com/AnthropicAI/status/1933630785879507286",
    "title_en": "AnthropicAI_揭秘Claude多智能体研究系统构建",
    "summary_en": "Anthropic在其工程博客上发布新文章，详细阐述了如何通过并行协作的多个智能体构建Claude的研发能力。文章分享了该多智能体研究系统在开发过程中的成功经验、遇到的挑战以及工程难题，为AI系统设计与优化提供了宝贵见解。",
    "keywords_en": [
      "Claude",
      "多智能体",
      "研究系统",
      "Anthropic",
      "AI工程"
    ],
    "area_en": [
      "智能体",
      "大模型",
      "研究进展"
    ],
    "published_time": "2025-06-13T21:01:19.000Z",
    "download_time": "2025-06-16 05:13:01",
    "visual_resource": [
      "screenshot/twitter/AnthropicAI_1933630785879507286.png"
    ],
    "extra_info": "{\"username\": \"AnthropicAI\", \"tweet_id\": \"1933630785879507286\"}"
  },
  {
    "id": "twitter_jxmnop_1933359925415325980",
    "source": "Twitter",
    "url": "https://twitter.com/jxmnop/status/1933359925415325980",
    "title_en": "jxmnop_LLM自编辑与强化学习框架SEAL",
    "summary_en": "推文探讨了大型语言模型（LLM）通过强化学习（RL）实现自我编辑的可能性。Jyo Pari介绍了一个名为SEAL的框架，该框架允许LLM生成自身的训练数据（自编辑），并根据新输入更新其权重。这种自编辑能力通过RL学习，以模型下游性能作为奖励信号，预示着LLM自我改进的新范式。",
    "keywords_en": [
      "大型语言模型",
      "强化学习",
      "自编辑",
      "SEAL框架",
      "模型训练",
      "AI自我改进"
    ],
    "area_en": [
      "大模型",
      "机器学习",
      "研究进展"
    ],
    "published_time": "2025-06-13T03:05:00.000Z",
    "download_time": "2025-06-16 05:13:19",
    "visual_resource": [
      "screenshot/twitter/jxmnop_1933359925415325980.png"
    ],
    "extra_info": "{\"username\": \"jxmnop\", \"tweet_id\": \"1933359925415325980\"}"
  },
  {
    "id": "twitter_TheTuringPost_1933478813062869156",
    "source": "Twitter",
    "url": "https://twitter.com/TheTuringPost/status/1933478813062869156",
    "title_en": "TheTuringPost_ReMA: Meta-learning and RL Empowering Multi-Agent LLMs",
    "summary_en": "TuringPost introduces Reinforced Meta-thinking Agents (ReMA), a framework combining meta-learning and reinforcement learning to significantly enhance Large Language Models' (LLM) effectiveness, especially in multi-agent collaboration. ReMA divides problem-solving into meta-thinking (planning) and reasoning (execution). It utilizes a Multi-Agent Meta-thinking Reasoning Process (MAMRP) with high-level agents for meta-planning and low-level agents for reasoning, allowing dynamic plan adjustments. ReMA demonstrates notable performance improvements on both math and LLM-as-a-Judge benchmarks.",
    "keywords_en": [
      "Meta-learning",
      "Reinforcement Learning",
      "Large Language Model",
      "AI Agent",
      "ReMA",
      "Multi-Agent"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-06-13T10:57:25.000Z",
    "download_time": "2025-06-16 05:13:20",
    "visual_resource": [
      "screenshot/twitter/TheTuringPost_1933478813062869156.png"
    ],
    "extra_info": "{\"username\": \"TheTuringPost\", \"tweet_id\": \"1933478813062869156\"}"
  },
  {
    "id": "twitter_jerryjliu0_1933627680265810205",
    "source": "Twitter",
    "url": "https://twitter.com/jerryjliu0/status/1933627680265810205",
    "title_en": "jerryjliu0_LlamaParse推出用例预设，增强文档解析能力",
    "summary_en": "Jerry Liu宣布LlamaParse推出“用例预设”功能，这是一系列专业的解析代理，能将不同文档类型（如表单、技术文档、发票）渲染成预定义格式。例如，表单可输出为结构化2D表格，技术文档可输出XML，发票可输出行项目。此外，LlamaParse还更新了文档页面，详细介绍了通用预设（快速、平衡、高级）和用例预设。",
    "keywords_en": [
      "LlamaParse",
      "用例预设",
      "文档解析",
      "AI代理",
      "结构化数据",
      "LlamaIndex"
    ],
    "area_en": [
      "产品发布",
      "生成式AI",
      "技术动态"
    ],
    "published_time": "2025-06-13T20:48:58.000Z",
    "download_time": "2025-06-16 05:13:27",
    "visual_resource": [
      "screenshot/twitter/jerryjliu0_1933627680265810205.png"
    ],
    "extra_info": "{\"username\": \"jerryjliu0\", \"tweet_id\": \"1933627680265810205\"}"
  },
  {
    "id": "twitter_vipulved_1933647581370069401",
    "source": "Twitter",
    "url": "https://twitter.com/vipulved/status/1933647581370069401",
    "title_en": "vipulved_Predicts End of Hand-Written Code",
    "summary_en": "Prominent technologist Vipul Ved Prakash boldly predicts that within the next 12 months, the era of hand-written code will come to an end. This statement suggests the rapid advancement of artificial intelligence in code generation, indicating that AI-assisted programming and fully automated code generation are poised to become mainstream. This shift is expected to fundamentally transform software development, prompting significant industry discussion and reflection on future programming paradigms.",
    "keywords_en": [
      "Hand-written code",
      "Code generation",
      "Artificial Intelligence",
      "Future of programming",
      "Software development"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Generative AI",
      "Tech News"
    ],
    "published_time": "2025-06-13T22:08:03.000Z",
    "download_time": "2025-06-16 05:13:47",
    "visual_resource": [
      "screenshot/twitter/vipulved_1933647581370069401.png"
    ],
    "extra_info": "{\"username\": \"vipulved\", \"tweet_id\": \"1933647581370069401\"}"
  },
  {
    "id": "twitter_GoogleDeepMind_1933549777192460771",
    "source": "Twitter",
    "url": "https://x.com/GoogleDeepMind/status/1933549777192460771",
    "title_en": "GoogleDeepMind_DeepMind and Primordial Soup's Film \"ANCESTRA\" Debuts at Tribeca",
    "summary_en": "Google DeepMind announced the debut of \"ANCESTRA,\" the first film from its partnership with Darren Aronofsky's Primordial Soup, at the Tribeca Film Festival. Directed by Eliza McNitt, the film integrates traditional filmmaking with DeepMind's generative video model, Veo, showcasing the potential of AI in cinematic art creation.",
    "keywords_en": [
      "Google DeepMind",
      "Primordial Soup",
      "ANCESTRA",
      "Veo",
      "Tribeca Film Festival",
      "Generative Video"
    ],
    "area_en": [
      "Generative AI",
      "Product Launch",
      "Tech News"
    ],
    "published_time": "2025-06-13T15:39:25.000Z",
    "download_time": "2025-06-16 05:45:40",
    "visual_resource": [
      "screenshot/twitter/GoogleDeepMind_1933549777192460771.png"
    ],
    "extra_info": "{\"username\": \"GoogleDeepMind\", \"tweet_id\": \"1933549777192460771\"}"
  },
  {
    "id": "TDX_zZ9jrz-SJImHAVoWdg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/TDX_zZ9jrz-SJImHAVoWdg",
    "title_en": "Official Announcement | Milvus 2.6 Officially Open-Sourced: 72% Memory Reduction, 4x Faster than ES",
    "summary_en": "Milvus 2.6 has been officially open-sourced, marking a significant milestone in its 2025 product roadmap, aiming to deliver a more efficient, powerful, and economical vector database solution. The new version introduces key improvements in cost reduction and efficiency, notably through the RaBitQ 1-bit quantization technology, achieving a substantial 72% memory reduction and significant QPS improvements. Regarding search capabilities, Milvus 2.6 enhances Analyzer/Tokenizer functionalities, adds Phrase Match, and introduces Decay Function for time-decay re-ranking, thereby improving hybrid search accuracy and timeliness. Architecturally, the update includes tiered storage for hot and cold data, a Streaming Service for real-time data processing, and support for 100k collections, significantly boosting system stability, scalability, and overall performance. Milvus 2.6 is dedicated to addressing the efficiency and stability challenges of data retrieval in the AI era, providing a robust foundation for large-scale AI applications.",
    "keywords_en": [
      "Milvus 2.6",
      "Vector Database",
      "Memory Optimization",
      "Search Enhancement",
      "Architecture Optimization",
      "Real-time Processing"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-06-13T11:21:11.000Z",
    "download_time": "2025-06-16T13:32:00.877596",
    "visual_resource": [
      "screenshot/wechat/wechat_image_TDX_zZ9jrz-SJImHAVoWdg.png"
    ],
    "extra_info": null
  },
  {
    "id": "AET1QTzrrbLX0WxY-vL8HA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/AET1QTzrrbLX0WxY-vL8HA",
    "title_en": "iFlytek Leads the Way as Intelligent Interaction Enters True Human-Computer Interaction Era",
    "summary_en": "At its latest press conference, iFlytek unveiled a new paradigm for intelligent interaction, signaling a shift from basic command-based human-computer interaction to a deep intelligent collaboration era. Through AIUI platform upgrades and the empowerment of the Spark Interaction Large Model, iFlytek has achieved full-duplex human-like dialogue, emotional perception and empathy, human-like memory systems, and rapid response times, significantly enhancing user experience, particularly with a surge in interaction frequency in children's scenarios. Furthermore, iFlytek launched its Robot Superbrain Platform, integrating the Spark Large Model to enable multimodal perception and intelligent action, and introduced a smart voice backpack to empower existing robots. This represents more than just product feature upgrades; it's a comprehensive re-shaping of the hardware industry ecosystem with AI interaction at its core, transforming human-machine relationships from \"command-execution\" to \"collaborative partnership,\" thereby accelerating the creation of a smarter intelligent world.",
    "keywords_en": [
      "Intelligent Interaction",
      "Human-Computer Interaction",
      "iFlytek",
      "Large Language Model",
      "Robotics",
      "AIUI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Robotics"
    ],
    "published_time": "2025-06-13T05:07:52.000Z",
    "download_time": "2025-06-16T13:31:55.450023",
    "visual_resource": [
      "screenshot/wechat/wechat_image_AET1QTzrrbLX0WxY-vL8HA.png"
    ],
    "extra_info": null
  },
  {
    "id": "fZW9_bHXfXzbogtHPpsnew",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/fZW9_bHXfXzbogtHPpsnew",
    "title_en": "AI Self-Correction for Enhanced Safety in Large Reasoning Models: Achieving 'Aha-Moments' and Reducing Risk by 9.6%",
    "summary_en": "Developed by researchers from UC Santa Cruz, UC Berkeley, Cisco Research, and Yale University, the SafeKey framework addresses the limited generalization of Large Reasoning Models (LRMs) against “jailbreak” attacks. By identifying the “Key Sentence” phenomenon and “Dormant Safety Signals,” SafeKey introduces two innovative mechanisms: the “Dual-Path Safety Head” to amplify internal safety signals before key sentence generation, and “Query-Mask Modeling” to compel models to rely on their self-generated understanding for safe decisions. This framework aims to reinforce the “safety aha-moment” during key sentence generation, significantly enhancing LRM safety and robustness without impacting core capabilities. Experimental results demonstrate SafeKey's effectiveness in reducing the risk rate by 9.6%, offering an efficient and generalizable solution for secure LRM deployment.",
    "keywords_en": [
      "Large Reasoning Models",
      "Model Safety",
      "Jailbreak Attacks",
      "SafeKey",
      "Safety Robustness",
      "Key Sentence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Machine Learning"
    ],
    "published_time": "2025-06-13T05:07:52.000Z",
    "download_time": "2025-06-16T13:31:59.045896",
    "visual_resource": [
      "screenshot/wechat/wechat_image_fZW9_bHXfXzbogtHPpsnew.png"
    ],
    "extra_info": null
  },
  {
    "id": "KSKsp2gnOGiUlFvFkW6-UQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/KSKsp2gnOGiUlFvFkW6-UQ",
    "title_en": "Evaluating Image Editing Models' Reasoning Capabilities from a Knowledge Type Perspective: All Models Underperform in Procedural Reasoning",
    "summary_en": "A collaborative research team, including members from Southeast University, Max Planck Institute for Informatics, and UC Berkeley, has introduced KRIS-Bench, a groundbreaking benchmark designed to systematically evaluate the reasoning capabilities of image editing models. Uniquely, KRIS-Bench assesses models from a knowledge type perspective, categorizing challenges into factual, conceptual, and procedural knowledge. It features 22 diverse editing tasks, ranging from basic to advanced, with output scoring facilitated by multimodal large models and human calibration. Crucially, the evaluations conducted using KRIS-Bench indicate that existing image editing models, both closed and open-source, exhibit significant limitations, particularly in \"procedural reasoning.\" This research endeavors to propel image editing models beyond simple pixel manipulation, fostering their evolution into \"visual sages\" capable of human-like cognitive understanding, thereby enabling them to grasp the underlying physical, chemical, and causal principles inherent in image editing tasks.",
    "keywords_en": [
      "Image Editing Models",
      "Reasoning Capability",
      "KRIS-Bench",
      "Knowledge Types",
      "Procedural Reasoning",
      "Cognitive Evaluation"
    ],
    "area_en": [
      "Computer Vision",
      "Multimodal",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-13T05:07:52.000Z",
    "download_time": "2025-06-16T13:31:59.552420",
    "visual_resource": [
      "screenshot/wechat/wechat_image_KSKsp2gnOGiUlFvFkW6-UQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "Qo0xE6BtG4RpMIaeWCWsZQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Qo0xE6BtG4RpMIaeWCWsZQ",
    "title_en": "DeepSeek Engineer Open-Sources Lightweight Nano-vLLM, Achieving Near-Original Throughput with Just 1200 Lines of Code",
    "summary_en": "DeepSeek engineer Yu Xingkai has recently open-sourced Nano-vLLM, a lightweight large language model (LLM) inference engine, demonstrating a significant breakthrough in efficiency. This project remarkably achieves throughput performance comparable to the high-performance vLLM, all while being implemented with just 1200 lines of Python code. Nano-vLLM is designed to offer an easily readable and highly efficient alternative for LLM deployment, boasting core features like rapid offline inference, a remarkably concise codebase, and advanced optimization functionalities including Prefix caching, Torch compilation, and CUDA graphs. Benchmark tests, conducted on an RTX 4070 using the Qwen3-0.6B model, clearly indicate that Nano-vLLM's performance is nearly on par with the original vLLM. This innovative development provides a promising new approach and a valuable tool for optimizing LLM inference, underscoring the potential for streamlined and highly performant deep learning systems.",
    "keywords_en": [
      "vLLM",
      "Nano-vLLM",
      "LLM Inference",
      "Open-source",
      "Throughput",
      "Deep Learning Systems"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-13T04:32:14.000Z",
    "download_time": "2025-06-16T13:32:09.800159",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Qo0xE6BtG4RpMIaeWCWsZQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "jiFidVIjR3SVfgxjpHlAzA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/jiFidVIjR3SVfgxjpHlAzA",
    "title_en": "Unstructured Data Meetup Japan Edition: Don't Miss It on July 5th!",
    "summary_en": "The Unstructured Data Meetup Japan edition, initiated by leading vector database provider Zilliz, is scheduled for July 5th at the AWS office in Tokyo. This technical sharing event focuses on unstructured data and Generative AI, targeting developers, AI engineers, data scientists, and AI product managers. The meetup aims to foster collaboration and explore cutting-edge advancements in vector databases, large language models, AI applications, and big data. Key presentations include Zilliz's introduction to Milvus and Zilliz Cloud, SmartNews's insights into Milvus applications for search, recommendation, and content understanding, and Mercari's deep dive into AI and LLM integration within their retrieval systems, covering offline data labeling, query expansion, and embedding technologies. This gathering will facilitate the exchange of the latest developments in unstructured data and generative AI fields.",
    "keywords_en": [
      "Unstructured Data",
      "Generative AI",
      "Vector Database",
      "Large Language Model",
      "AI Applications",
      "Technical Sharing"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-06-13T11:21:11.000Z",
    "download_time": "2025-06-16T13:31:54.247438",
    "visual_resource": [
      "screenshot/wechat/wechat_image_jiFidVIjR3SVfgxjpHlAzA.png"
    ],
    "extra_info": null
  },
  {
    "id": "ai-hedge-fund",
    "source": "GitHub",
    "url": "https://github.com/virattt/ai-hedge-fund",
    "title_en": "AI Hedge Fund",
    "summary_en": "This project presents a robust proof-of-concept for an AI-powered hedge fund, meticulously designed to explore the advanced application of artificial intelligence in sophisticated trading decision-making. The system boasts a highly collaborative multi-agent architecture, integrating specialized AI agents focused on critical financial aspects such as disciplined valuation, market sentiment analysis, in-depth fundamental data interpretation, precise technical indicator analysis, and comprehensive risk management. Furthermore, it simulates the distinct investment philosophies of twelve renowned financial masters, offering diverse strategic perspectives. The project provides flexible deployment options via Poetry and Docker, features seamless integration with various Large Language Models (LLMs) for enhanced analytical capabilities, and includes robust backtesting functionalities for performance evaluation. Primarily intended for educational and research purposes, this sophisticated system rigorously simulates trading decisions without engaging in any actual financial transactions, emphasizing its role as a learning and development tool.",
    "keywords_en": [
      "Artificial Intelligence",
      "Hedge Fund",
      "AI Agent",
      "Algorithmic Trading",
      "FinTech",
      "Large Language Model",
      "Backtesting",
      "Investment Decision"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-06-15T17:21:13Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "screenshot/github/ai-hedge-fund.png"
    ],
    "extra_info": null
  },
  {
    "id": "deepeval",
    "source": "GitHub",
    "url": "https://github.com/confident-ai/deepeval",
    "title_en": "The LLM Evaluation Framework",
    "summary_en": "DeepEval is a robust, open-source LLM evaluation framework, akin to Pytest but specialized for large language model outputs. It incorporates the latest research to assess LLM performance across various metrics, including G-Eval, RAGAS, hallucination, answer relevancy, and conversational metrics, with evaluations running locally on your machine. This framework is crucial for validating RAG pipelines, chatbots, and AI agents, enabling developers to determine optimal models and prompts, prevent prompt drifting, and ensure application reliability. DeepEval also supports building custom metrics, generating synthetic datasets, seamless CI/CD integration, and red-teaming for over 40 safety vulnerabilities. Furthermore, it allows easy benchmarking of any LLM on popular benchmarks like MMLU and HumanEval, providing a comprehensive solution for LLM quality assurance and performance optimization throughout the development lifecycle.",
    "keywords_en": [
      "Large Language Model",
      "LLM Evaluation",
      "Evaluation Framework",
      "RAG",
      "AI Agent",
      "Quality Assurance",
      "Continuous Integration",
      "Performance Testing"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-13T07:55:16Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://github.com/confident-ai/deepeval/blob/main/docs/static/img/deepeval.png",
      "https://github.com/confident-ai/deepeval/blob/main/assets/demo.gif?raw=true"
    ],
    "extra_info": null
  },
  {
    "id": "all-rag-techniques",
    "source": "GitHub",
    "url": "https://github.com/FareedKhan-dev/all-rag-techniques",
    "title_en": "All RAG Techniques: A Simpler, Hands-On Approach ✨",
    "summary_en": "This GitHub repository offers a practical, hands-on guide to Retrieval-Augmented Generation (RAG) techniques. It meticulously breaks down over 20 advanced RAG methods, such as semantic chunking, reranking, Graph RAG, Multimodal RAG, and Corrective RAG, all implemented from scratch using pure Python libraries like OpenAI and NumPy, without relying on frameworks like LangChain. The project aims to provide readable, modifiable, and educational code, enabling developers to grasp the fundamental workings of RAG. It also covers handling large datasets with knowledge graphs, serving as a valuable resource for building efficient and interpretable RAG systems.",
    "keywords_en": [
      "Retrieval-Augmented Generation",
      "RAG",
      "Large Language Models",
      "Natural Language Processing",
      "Knowledge Graphs",
      "Vector Databases",
      "Python",
      "Machine Learning"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-13T02:13:12Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "screenshot/github/all-rag-techniques.png"
    ],
    "extra_info": null
  },
  {
    "id": "self-llm",
    "source": "GitHub",
    "url": "https://github.com/datawhalechina/self-llm",
    "title_en": "Open-source Large Language Model Usage Guide",
    "summary_en": "This project serves as a dedicated open-source large language model tutorial for beginners in China, primarily focusing on Linux platforms. It provides comprehensive, full-process guidance covering environment setup, local deployment, and efficient fine-tuning for a wide range of open-source LLMs. The initiative aims to simplify the complex procedures of deploying, using, and applying these models, including mainstream ones like LLaMA, ChatGLM, and InternLM. Technical aspects covered include command-line invocation, online demo deployment, and integration with frameworks like LangChain. The tutorial's objective is to enable more students and researchers to effectively leverage open-source large models, fostering their adoption in academic and practical settings, and facilitating the development of customized, domain-specific LLMs.",
    "keywords_en": [
      "Open-source LLM",
      "Environment Configuration",
      "Local Deployment",
      "Model Fine-tuning",
      "LangChain",
      "Linux"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-06-12T08:52:13Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://contrib.rocks/image?repo=datawhalechina/self-llm"
    ],
    "extra_info": null
  },
  {
    "id": "youtube-transcript-api",
    "source": "GitHub",
    "url": "https://github.com/jdepoix/youtube-transcript-api",
    "title_en": "✨ YouTube Transcript API ✨",
    "summary_en": "The YouTube Transcript API is a Python library designed for efficient retrieval of YouTube video transcripts and subtitles, including automatically generated ones. This API eliminates the need for headless browsers, supports subtitle translation, and offers flexible API interfaces and command-line tools. It addresses IP blocking issues by supporting proxy configurations to ensure service stability and provides various output formats. The project offers a convenient solution for video content analysis, multilingual processing, and automated data extraction.",
    "keywords_en": [
      "YouTube",
      "Transcript",
      "Subtitles",
      "API",
      "Python",
      "Video Processing",
      "Data Extraction",
      "Proxy"
    ],
    "area_en": [
      "Video Understanding",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-11T22:24:04Z",
    "download_time": "2024-07-09 07:00:00",
    "visual_resource": [
      "screenshot/github/youtube-transcript-api.png"
    ],
    "extra_info": null
  },
  {
    "id": "ai-agents-for-beginners",
    "source": "GitHub",
    "url": "https://github.com/microsoft/ai-agents-for-beginners",
    "title_en": "AI Agents for Beginners - A Course",
    "summary_en": "This GitHub repository presents \"AI Agents for Beginners - A Course,\" comprising 11 lessons designed to teach the fundamentals of building AI agents. The curriculum covers AI agent concepts, exploration of agentic frameworks, various agentic design patterns (such as tool use, RAG, planning, multi-agent, and metacognition), and deploying AI agents in production environments. The course provides Python code samples compatible with Azure AI Foundry and GitHub Model Catalogs, leveraging Microsoft's Azure AI Agent Service, Semantic Kernel, and AutoGen frameworks. With multi-language support, this course is ideal for beginners seeking a systematic approach to AI agent development.",
    "keywords_en": [
      "AI Agents",
      "Generative AI",
      "Agentic Frameworks",
      "Semantic Kernel",
      "AutoGen",
      "RAG",
      "Design Patterns",
      "Azure AI"
    ],
    "area_en": [
      "AI Agent",
      "Generative AI",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-11T11:04:34Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/images/repo-thumbnail.png"
    ],
    "extra_info": null
  },
  {
    "id": "2506.09513",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.09513",
    "title_en": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
    "summary_en": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
    "keywords_en": [
      "Medical Reasoning",
      "Large Language Models",
      "Dataset",
      "Multi-Agent",
      "Chain-of-Thought"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-11T08:36:55.000Z",
    "download_time": "2025-06-15 22:32:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09513.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.09513\", \"arxiv_url\": \"https://arxiv.org/abs/2506.09513\"}"
  },
  {
    "id": "2506.10910",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.10910",
    "title_en": "Magistral",
    "summary_en": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.",
    "keywords_en": [
      "Magistral",
      "Reinforcement Learning",
      "Large Language Models",
      "Reasoning Model",
      "Multimodal Understanding"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-12T17:22:37.000Z",
    "download_time": "2025-06-15 22:32:24",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10910.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.10910\", \"arxiv_url\": \"https://arxiv.org/abs/2506.10910\"}"
  },
  {
    "id": "2506.10540",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.10540",
    "title_en": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
    "summary_en": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
    "keywords_en": [
      "Multi-agent",
      "Animated storytelling",
      "Video generation",
      "Monte Carlo Tree Search",
      "AI-generated animation"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Generative AI"
    ],
    "published_time": "2025-06-12T10:06:21.000Z",
    "download_time": "2025-06-15 22:32:24",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10540.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.10540\", \"arxiv_url\": \"https://arxiv.org/abs/2506.10540\"}"
  },
  {
    "id": "2506.10857",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.10857",
    "title_en": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "summary_en": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
    "keywords_en": [
      "Multi-step reasoning",
      "Long narrative videos",
      "Benchmark",
      "Large models",
      "Video understanding"
    ],
    "area_en": [
      "Large Language Model",
      "Video Understanding",
      "Multimodal"
    ],
    "published_time": "2025-06-12T16:17:17.000Z",
    "download_time": "2025-06-15 22:32:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10857.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.10857\", \"arxiv_url\": \"https://arxiv.org/abs/2506.10857\"}"
  },
  {
    "id": "2506.09344",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.09344",
    "title_en": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
    "summary_en": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
    "keywords_en": [
      "Unified Multimodal Model",
      "Perception and Generation",
      "Mixture-of-Experts",
      "Audio Generation",
      "Image Generation"
    ],
    "area_en": [
      "Multimodal",
      "Generative AI",
      "Large Language Model"
    ],
    "published_time": "2025-06-11T02:50:49.000Z",
    "download_time": "2025-06-15 22:32:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09344.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.09344\", \"arxiv_url\": \"https://arxiv.org/abs/2506.09344\"}"
  },
  {
    "id": "2506.10357",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.10357",
    "title_en": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
    "summary_en": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/",
    "keywords_en": [
      "Multimodal Large Language Models",
      "Minecraft Agents",
      "Generalist Agent",
      "Mixture-of-Experts",
      "Reinforcement Learning"
    ],
    "area_en": [
      "AI Agent",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-06-12T05:29:40.000Z",
    "download_time": "2025-06-15 22:32:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10357.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.10357\", \"arxiv_url\": \"https://arxiv.org/abs/2506.10357\"}"
  }
]