<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-08-11</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }

        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.3s ease, transform 0.2s ease;
            border: 2px solid transparent;
            font-size: 0.9em;
        }

        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }

        .language-switch a.active {
            background: var(--secondary-color);
            border-color: var(--border-color);
        }

        @media (max-width: 768px) {
            .language-switch {
                position: static;
                justify-content: center;
                margin-bottom: 20px;
            }
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../">‰∏≠Êñá</a>
                <a href="./" class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-08-11</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Twitter</h2>

            <article class="item-card">
                <h2>ericmitchellai_GPT-5 Hallucination Significantly Improved</h2>
                <span class="published-time">Published: 2025-08-11T02:59:23.000Z</span>
                <img src="../screenshot/twitter/ericmitchellai_1954739395719807370.png" alt="ericmitchellai_GPT-5 Hallucination Significantly Improved">
                <p class="summary">The tweet highlights GPT-5 as the first model series that virtually eliminates hallucinations, particularly when processing business logic, models, or research notes. This shift indicates a move from benchmark-maxxing to real-world utility optimization in AI development. Although GPT-5 is not flawless, its error rate has significantly decreased, signaling a major breakthrough in large language models' reliability.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>GPT-5</span><span>Hallucination</span><span>Large Language Model</span><span>Reliability</span><span>Real-world Utility</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Tech News</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/ericmitchellai/status/1954739395719807370" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>percyliang_HELM Adds GPT-5 Models, Highlights Evaluation Challenges</h2>
                <span class="published-time">Published: 2025-08-11T21:03:37.000Z</span>
                <img src="../screenshot/twitter/percyliang_1955012250567315854.png" alt="percyliang_HELM Adds GPT-5 Models, Highlights Evaluation Challenges">
                <p class="summary">Percy Liang announced that HELM v1.12.0 now includes GPT-5 and GPT-5 mini. Interestingly, GPT-5 mini tops the leaderboard ahead of GPT-5 on Omni-MATH, primarily because GPT-5 consumes more reasoning tokens and is harder to control, often exceeding the 14096 token budget. This highlights the inherent difficulties in conducting fair and accurate model evaluations.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>GPT-5</span><span>GPT-5 mini</span><span>HELM</span><span>Model Evaluation</span><span>Large Language Model</span><span>Reasoning Tokens</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Tech News</span><span>Research Progress</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/percyliang/status/1955012250567315854" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>fchollet_Discussion on Reasons for Deficiency in Frontier Multimodal Models' Visual Capabilities</h2>
                <span class="published-time">Published: 2025-08-11T20:50:30.000Z</span>
                <img src="../screenshot/twitter/fchollet_1955008947431395488.png" alt="fchollet_Discussion on Reasons for Deficiency in Frontier Multimodal Models' Visual Capabilities">
                <p class="summary">Prominent AI researcher Fran√ßois Chollet highlights a significant performance gap: while frontier large language models demonstrate superhuman text-based world knowledge and frontier image/video models excel with superhuman vision-based knowledge, current frontier Vision-Language Models (VLMs) are notably underperforming. He posits that this discrepancy primarily stems from the relative scarcity of high-quality image-text paired data, in stark contrast to the abundant availability of text data and individual image/video data. This data imbalance is presented as a key bottleneck for VLM advancement.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multimodal</span><span>Vision-Language Models</span><span>Data Scarcity</span><span>Large Models</span><span>Image-Text Pairs</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Large Language Model</span><span>Research Progress</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/fchollet/status/1955008947431395488" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AIatMeta_Meta FAIR Team Wins Algonauts Brain Modeling Competition with TRIBE Model</h2>
                <span class="published-time">Published: 2025-08-11T11:20:02.000Z</span>
                <img src="../screenshot/twitter/AIatMeta_1954865388749205984.png" alt="AIatMeta_Meta FAIR Team Wins Algonauts Brain Modeling Competition with TRIBE Model">
                <p class="summary">Meta FAIR's Brain & AI team secured first place at the 2025 Algonauts brain modeling competition. Their 1-billion-parameter TRIBE (Trimodal Brain Encoder) model is the first deep neural network designed to predict brain responses to stimuli across multiple modalities, cortical areas, and individuals. This innovative approach integrates pretrained representations from Meta's foundational models, including Llama 3.2 for text, Wav2Vec2-BERT for audio, and V-JEPA 2 for video, to accurately predict extensive spatio-temporal fMRI brain responses to movies.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Meta FAIR</span><span>TRIBE model</span><span>Brain Modeling</span><span>Multimodal</span><span>Deep Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Deep Learning</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/AIatMeta/status/1954865388749205984" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>polynoamial_OpenAI Reasoning System Wins Gold in International Programming and Math Competitions</h2>
                <span class="published-time">Published: 2025-08-11T18:01:25.000Z</span>
                <img src="../screenshot/twitter/polynoamial_1954966398989635668.png" alt="polynoamial_OpenAI Reasoning System Wins Gold in International Programming and Math Competitions">
                <p class="summary">OpenAI researchers Noam Brown and Sheryl Hsu announced that OpenAI's reasoning system achieved a gold medal in the 2025 International Olympiad in Informatics (IOI), ranking first among AI participants. Notably, this same model, which previously secured gold in the International Math Olympiad (IMO), is also considered OpenAI's best competitive coding model. This achievement signifies a major breakthrough for OpenAI in AI reasoning and general capabilities.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>OpenAI</span><span>Artificial Intelligence</span><span>Reasoning System</span><span>Competitive Programming</span><span>Math Olympiad</span><span>IOI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Research Progress</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/polynoamial/status/1954966398989635668" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AnimaAnandkumar_LeanDojo Major Update: Lean+LLM for Formal Math Reasoning</h2>
                <span class="published-time">Published: 2025-08-11T05:37:00.000Z</span>
                <img src="../screenshot/twitter/AnimaAnandkumar_1954779061529743412.png" alt="AnimaAnandkumar_LeanDojo Major Update: Lean+LLM for Formal Math Reasoning">
                <p class="summary">Prof. Anima Anandkumar announced a major update to LeanDojo, introducing the Lean+LLM framework for verified mathematical reasoning. This update includes Lean4Code, an integrated development environment; LeanCopilot, a neural network inference system supporting local/cloud LLMs and provers like DeepSeek/Kimina; and LeanAgent, a lifelong learning framework. Highlighting Lean's critical role in AI's success in the International Math Olympiad, LeanDojo, as the first open-source framework, aims to integrate Lean with LLMs to address hallucination in long proofs and ensure rigorous mathematical verification.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LeanDojo</span><span>Formal Verification</span><span>Large Language Model</span><span>Theorem Proving</span><span>Mathematical Reasoning</span><span>Open Source Project</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Open Source</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/AnimaAnandkumar/status/1954779061529743412" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>2025 Global Large Model Application Report: Red Ocean Competition Erodes Loyalty, Users Juggle 4.7 Platforms!</h2>
                <span class="published-time">Published: 2025-08-11T16:02:05.000Z</span>
                <img src="../screenshot/wechat/wechat_image_fQbM7aP4mkZ8Vam_h3JnRA.png" alt="2025 Global Large Model Application Report: Red Ocean Competition Erodes Loyalty, Users Juggle 4.7 Platforms!">
                <p class="summary">The "2025 Global Large Model Application Report" reveals that in the first half of 2025, large models have transitioned from technological frontiers to practical production, with 45% of enterprises deploying them in production environments. Engineering R&D, customer support, and marketing emerge as primary application scenarios. The report highlights that users, on average, utilize 4.7 different large models, indicating a red ocean competitive market with low brand loyalty. Key challenges include insufficient model knowledge, hallucination issues, and high costs. While OpenAI maintains its lead, Google's Gemini and Deepseek show significant progress. Chinese large models face geographical deployment restrictions in overseas markets. NVIDIA dominates the training hardware market. The report emphasizes that reliability, cost, and intelligence levels remain major hurdles for AI adoption.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Models</span><span>AI Applications</span><span>Market Competition</span><span>User Loyalty</span><span>Enterprise Deployment</span><span>Challenges</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/fQbM7aP4mkZ8Vam_h3JnRA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>41 SOTA Benchmarks! Zhipu AI's Latest Open-Source GLM-4.5V Tested: Guessing Locations from Images, Video to Code in Seconds</h2>
                <span class="published-time">Published: 2025-08-11T14:22:04.000Z</span>
                <img src="../screenshot/wechat/wechat_image_nQfs4g4RkJ5n-rlJK7ctTQ.png" alt="41 SOTA Benchmarks! Zhipu AI's Latest Open-Source GLM-4.5V Tested: Guessing Locations from Images, Video to Code in Seconds">
                <p class="summary">Zhipu AI's newly open-sourced GLM-4.5V visual reasoning model has achieved State-of-the-Art (SOTA) performance across 41 public visual multimodal benchmarks, demonstrating exceptional multimodal capabilities. Built upon the GLM-4.5 foundation, this 106B-parameter model excels in high-accuracy image-based location guessing (GeoGuessr), advanced grounding capabilities like identifying elements in complex artworks, and groundbreaking video understanding with direct web page code replication. Furthermore, GLM-4.5V showcases robust spatial relationship comprehension and intricate chart analysis. Optimized through a three-stage training strategy, it supports 64K multimodal long contexts and offers cost-effective API services. This release signifies a pivotal shift for multimodal large models from mere capability validation to widespread practical deployment, setting a new direction for AI visual reasoning.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Zhipu AI</span><span>GLM-4.5V</span><span>Multimodal Large Model</span><span>Visual Reasoning</span><span>Video Understanding</span><span>Code Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Large Language Model</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/nQfs4g4RkJ5n-rlJK7ctTQ" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>TPAMI 2025 | DAPT: Decouple Before Align to Solve Visual-Language Model "Information Asymmetry" and Significantly Improve Prompt Tuning Performance</h2>
                <span class="published-time">Published: 2025-08-11T14:00:27.000Z</span>
                <img src="../screenshot/wechat/wechat_image_Pq530y9HR2iOMHycZIXAPA.png" alt="TPAMI 2025 | DAPT: Decouple Before Align to Solve Visual-Language Model "Information Asymmetry" and Significantly Improve Prompt Tuning Performance">
                <p class="summary">A TPAMI 2025 paper introduces DAPT (Decouple before Align Prompt Tuning), a novel framework addressing the "information asymmetry" challenge in Visual-Language Model (VLM) prompt tuning. DAPT's core idea is to first decouple image foreground and background, then symmetrically align them with corresponding text descriptions, effectively mitigating model attention bias towards irrelevant visual information. The framework incorporates visual pull-push regularization to compel the model to focus on regions of interest. Extensive experiments demonstrate DAPT's superior performance, achieving new State-of-the-Art (SOTA) results across 11 tasks, including few-shot learning, data-efficient learning, and base-to-novel generalization. Notably, DAPT achieves performance comparable to or exceeding existing methods using only 50% of the training data, offering a highly efficient and robust paradigm for VLM utilization and multimodal alignment.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>DAPT</span><span>Prompt Tuning</span><span>Visual-Language Models</span><span>Information Asymmetry</span><span>Decouple-Align</span><span>Few-shot Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Multimodal</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/Pq530y9HR2iOMHycZIXAPA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Sam Altman's Removal of GPT-4o Triggers AI "Withdrawal," While Elon Musk Announces Grok 4 Global Free Access!</h2>
                <span class="published-time">Published: 2025-08-11T14:00:27.000Z</span>
                <img src="../screenshot/wechat/wechat_image_DL8Ot-_qaXdr-BJyvSldCA.png" alt="Sam Altman's Removal of GPT-4o Triggers AI "Withdrawal," While Elon Musk Announces Grok 4 Global Free Access!">
                <p class="summary">This article analyzes the contrasting product strategies of OpenAI and xAI regarding AI models. OpenAI's decision to discontinue GPT-4o sparked significant user "withdrawal symptoms," prompting Sam Altman to acknowledge the error. OpenAI's rationale is to design AI with "restraint" and "clear boundaries" to prevent user over-reliance or addiction, prioritizing users' long-term well-being over unlimited gratification. In stark contrast, Elon Musk announced Grok 4's global free access, employing strategies like "spicy mode" and anthropomorphic characters to maximize user engagement and retention through high-stimulation and frequent interaction. This divergence highlights the ethical debate between AI empowerment and potential addiction, reflecting different corporate philosophies on human-AI relationships and the future of AI development.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>GPT-4o</span><span>Grok 4</span><span>AI Ethics</span><span>User Addiction</span><span>Product Strategy</span><span>Large Language Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/DL8Ot-_qaXdr-BJyvSldCA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Inference Cost Plummets 75%! GPT-OSS Achieves 4x Inference Speed with New Data Type, Enabling 120B Parameter Models on 80GB GPUs</h2>
                <span class="published-time">Published: 2025-08-11T06:49:20.000Z</span>
                <img src="../screenshot/wechat/wechat_image_ezkb5TEvN1y2WmaTWk6_Qw.png" alt="Inference Cost Plummets 75%! GPT-OSS Achieves 4x Inference Speed with New Data Type, Enabling 120B Parameter Models on 80GB GPUs">
                <p class="summary">OpenAI's gpt-oss model has adopted the innovative MXFP4 data type, leading to a remarkable 75% reduction in inference costs and a fourfold increase in token generation speed. This advancement allows MXFP4 to decrease memory footprint to just one-quarter of equivalent BF16 models, making it feasible to run massive 120-billion-parameter large language models on more accessible 80GB GPUs, thereby significantly improving hardware efficiency and accessibility. The technology achieves this by optimizing both weight storage and memory bandwidth, delivering extreme data compression while meticulously preserving model precision, effectively mitigating the common accuracy degradation issues associated with low-precision quantization. This breakthrough not only substantially lowers the operational expenses for deploying large-scale AI models but also introduces a powerful new paradigm for efficient AI inference, promising a future of more cost-effective and high-performance artificial intelligence applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>MXFP4</span><span>Inference Cost</span><span>Large Language Model</span><span>Data Type</span><span>Quantization</span><span>Memory Optimization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/ezkb5TEvN1y2WmaTWk6_Qw" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Inspur Information Unveils New-Generation AI Supernode: Making 64 Cards Act as One, Supporting Simultaneous Operation of Four Domestic Open-Source Models</h2>
                <span class="published-time">Published: 2025-08-11T06:49:20.000Z</span>
                <img src="../screenshot/wechat/wechat_image_F5hCckvyEybGZCJg8NamWA.png" alt="Inspur Information Unveils New-Generation AI Supernode: Making 64 Cards Act as One, Supporting Simultaneous Operation of Four Domestic Open-Source Models">
                <p class="summary">Inspur Information has unveiled its new-generation AI supernode, "YuanNao SD200," designed to address the explosive growth in computation and communication demands posed by trillion-parameter large models and the Agentic AI paradigm. This innovative supernode integrates 64 GPU cards into a unified memory and addressing compute domain. Utilizing a 3D Mesh open architecture and remote GPU virtual mapping technology, it achieves an ultra-large resource pool of 4TB VRAM and 64TB system memory, offering sub-100-nanosecond low-latency communication. The YuanNao SD200 supports the simultaneous operation of four domestic open-source models, including DeepSeek R1 and Kimi K2, on a single machine, demonstrating super-linear scaling performance for trillion-parameter large model inference and real-time multi-agent collaboration. Its open hardware and software ecosystem is compatible with mainstream frameworks, reducing migration costs and aiming to promote AI intelligence equalization.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Inspur Information</span><span>AI Supernode</span><span>Large Model Inference</span><span>AI Agent</span><span>Open Architecture</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/F5hCckvyEybGZCJg8NamWA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</h2>
                <span class="published-time">Published: 2025-08-08T17:21:06.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06471.png" alt="GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models">
                <p class="summary">We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>GLM-4.5</span><span>Large Language Model</span><span>AI Agent</span><span>Reasoning</span><span>Coding</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.06471" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Memp: Exploring Agent Procedural Memory</h2>
                <span class="published-time">Published: 2025-08-08T16:20:56.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06433.png" alt="Memp: Exploring Agent Procedural Memory">
                <p class="summary">Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Agent</span><span>Procedural Memory</span><span>Large Language Models</span><span>Memory Update</span><span>Trajectory Distillation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.06433" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy
  Optimization</h2>
                <span class="published-time">Published: 2025-08-07T17:49:56.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05731.png" alt="InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy
  Optimization">
                <p class="summary">The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>GUI Grounding</span><span>Multimodal Large Language Models</span><span>Adaptive Exploration Policy Optimization</span><span>Reinforcement Learning</span><span>Semantic Alignment</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.05731" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OS Agents: A Survey on MLLM-based Agents for General Computing Devices
  Use</h2>
                <span class="published-time">Published: 2025-08-06T14:33:45.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04482.png" alt="OS Agents: A Survey on MLLM-based Agents for General Computing Devices
  Use">
                <p class="summary">The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>OS Agents</span><span>Multimodal Large Language Models</span><span>AI Agent</span><span>Graphical User Interface</span><span>Survey</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Multimodal</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.04482" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</h2>
                <span class="published-time">Published: 2025-08-04T06:06:06.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02095.png" alt="VLM4D: Towards Spatiotemporal Awareness in Vision Language Models">
                <p class="summary">Vision language models (VLMs) have shown remarkable capabilities in
integrating linguistic and visual reasoning but remain fundamentally limited in
understanding dynamic spatiotemporal interactions. Humans effortlessly track
and reason about object movements, rotations, and perspective shifts-abilities
essential for robust dynamic real-world understanding yet notably lacking in
current VLMs. In this paper, we introduce VLM4D, the first benchmark
specifically designed to evaluate the spatiotemporal reasoning capabilities of
VLMs. Our benchmark comprises diverse real-world and synthetic videos
accompanied by carefully curated question-answer pairs emphasizing
translational and rotational motions, perspective awareness, and motion
continuity. Through comprehensive evaluations of state-of-the-art open and
closed-source VLMs, we identify significant performance gaps compared to human
baselines, highlighting fundamental deficiencies in existing models. Extensive
analysis reveals that VLMs struggle particularly with integrating multiple
visual cues and maintaining temporal coherence. We further explore promising
directions, such as leveraging 4D feature field reconstruction and targeted
spatiotemporal supervised fine-tuning, demonstrating their effectiveness in
enhancing spatiotemporal comprehension. Our work aims to encourage deeper
exploration into improving VLMs' spatial and temporal grounding, paving the way
towards more capable and reliable visual intelligence for dynamic environments.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Vision Language Models</span><span>Spatiotemporal Awareness</span><span>Benchmark</span><span>Spatiotemporal Reasoning</span><span>Video Understanding</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Computer Vision</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.02095" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MeshLLM: Empowering Large Language Models to Progressively Understand
  and Generate 3D Mesh</h2>
                <span class="published-time">Published: 2025-08-02T07:37:37.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01242.png" alt="MeshLLM: Empowering Large Language Models to Progressively Understand
  and Generate 3D Mesh">
                <p class="summary">We present MeshLLM, a novel framework that leverages large language models
(LLMs) to understand and generate text-serialized 3D meshes. Our approach
addresses key limitations in existing methods, including the limited dataset
scale when catering to LLMs' token length and the loss of 3D structural
information during mesh serialization. We introduce a Primitive-Mesh
decomposition strategy, which divides 3D meshes into structurally meaningful
subunits. This enables the creation of a large-scale dataset with 1500k+
samples, almost 50 times larger than previous methods, which aligns better with
the LLM scaling law principles. Furthermore, we propose inferring face
connectivity from vertices and local mesh assembly training strategies,
significantly enhancing the LLMs' ability to capture mesh topology and spatial
structures. Experiments show that MeshLLM outperforms the state-of-the-art
LLaMA-Mesh in both mesh generation quality and shape understanding,
highlighting its great potential in processing text-serialized 3D meshes.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>3D Mesh</span><span>Mesh Generation</span><span>Shape Understanding</span><span>MeshLLM</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Generative AI</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.01242" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>