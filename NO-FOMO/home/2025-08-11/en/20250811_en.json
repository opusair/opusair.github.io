[
  {
    "id": "twitter_ericmitchellai_1954739395719807370",
    "source": "Twitter",
    "url": "https://x.com/ericmitchellai/status/1954739395719807370",
    "title_en": "ericmitchellai_GPT-5 Hallucination Significantly Improved",
    "summary_en": "The tweet highlights GPT-5 as the first model series that virtually eliminates hallucinations, particularly when processing business logic, models, or research notes. This shift indicates a move from benchmark-maxxing to real-world utility optimization in AI development. Although GPT-5 is not flawless, its error rate has significantly decreased, signaling a major breakthrough in large language models' reliability.",
    "keywords_en": [
      "GPT-5",
      "Hallucination",
      "Large Language Model",
      "Reliability",
      "Real-world Utility"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Tech News"
    ],
    "published_time": "2025-08-11T02:59:23.000Z",
    "download_time": "2025-08-12 02:46:19",
    "visual_resource": [
      "screenshot/twitter/ericmitchellai_1954739395719807370.png"
    ],
    "extra_info": "{\"username\": \"ericmitchellai\", \"tweet_id\": \"1954739395719807370\"}"
  },
  {
    "id": "twitter_percyliang_1955012250567315854",
    "source": "Twitter",
    "url": "https://x.com/percyliang/status/1955012250567315854",
    "title_en": "percyliang_HELM Adds GPT-5 Models, Highlights Evaluation Challenges",
    "summary_en": "Percy Liang announced that HELM v1.12.0 now includes GPT-5 and GPT-5 mini. Interestingly, GPT-5 mini tops the leaderboard ahead of GPT-5 on Omni-MATH, primarily because GPT-5 consumes more reasoning tokens and is harder to control, often exceeding the 14096 token budget. This highlights the inherent difficulties in conducting fair and accurate model evaluations.",
    "keywords_en": [
      "GPT-5",
      "GPT-5 mini",
      "HELM",
      "Model Evaluation",
      "Large Language Model",
      "Reasoning Tokens"
    ],
    "area_en": [
      "Large Language Model",
      "Tech News",
      "Research Progress"
    ],
    "published_time": "2025-08-11T21:03:37.000Z",
    "download_time": "2025-08-12 02:56:20",
    "visual_resource": [
      "screenshot/twitter/percyliang_1955012250567315854.png"
    ],
    "extra_info": "{\"username\": \"percyliang\", \"tweet_id\": \"1955012250567315854\"}"
  },
  {
    "id": "twitter_fchollet_1955008947431395488",
    "source": "Twitter",
    "url": "https://x.com/fchollet/status/1955008947431395488",
    "title_en": "fchollet_Discussion on Reasons for Deficiency in Frontier Multimodal Models' Visual Capabilities",
    "summary_en": "Prominent AI researcher Fran√ßois Chollet highlights a significant performance gap: while frontier large language models demonstrate superhuman text-based world knowledge and frontier image/video models excel with superhuman vision-based knowledge, current frontier Vision-Language Models (VLMs) are notably underperforming. He posits that this discrepancy primarily stems from the relative scarcity of high-quality image-text paired data, in stark contrast to the abundant availability of text data and individual image/video data. This data imbalance is presented as a key bottleneck for VLM advancement.",
    "keywords_en": [
      "Multimodal",
      "Vision-Language Models",
      "Data Scarcity",
      "Large Models",
      "Image-Text Pairs"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-08-11T20:50:30.000Z",
    "download_time": "2025-08-12 02:48:15",
    "visual_resource": [
      "screenshot/twitter/fchollet_1955008947431395488.png"
    ],
    "extra_info": "{\"username\": \"fchollet\", \"tweet_id\": \"1955008947431395488\"}"
  },
  {
    "id": "twitter_AIatMeta_1954865388749205984",
    "source": "Twitter",
    "url": "https://x.com/AIatMeta/status/1954865388749205984",
    "title_en": "AIatMeta_Meta FAIR Team Wins Algonauts Brain Modeling Competition with TRIBE Model",
    "summary_en": "Meta FAIR's Brain & AI team secured first place at the 2025 Algonauts brain modeling competition. Their 1-billion-parameter TRIBE (Trimodal Brain Encoder) model is the first deep neural network designed to predict brain responses to stimuli across multiple modalities, cortical areas, and individuals. This innovative approach integrates pretrained representations from Meta's foundational models, including Llama 3.2 for text, Wav2Vec2-BERT for audio, and V-JEPA 2 for video, to accurately predict extensive spatio-temporal fMRI brain responses to movies.",
    "keywords_en": [
      "Meta FAIR",
      "TRIBE model",
      "Brain Modeling",
      "Multimodal",
      "Deep Learning"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Deep Learning",
      "Multimodal"
    ],
    "published_time": "2025-08-11T11:20:02.000Z",
    "download_time": "2025-08-12 02:45:52",
    "visual_resource": [
      "screenshot/twitter/AIatMeta_1954865388749205984.png"
    ],
    "extra_info": "{\"username\": \"AIatMeta\", \"tweet_id\": \"1954865388749205984\"}"
  },
  {
    "id": "twitter_polynoamial_1954966398989635668",
    "source": "Twitter",
    "url": "https://x.com/polynoamial/status/1954966398989635668",
    "title_en": "polynoamial_OpenAI Reasoning System Wins Gold in International Programming and Math Competitions",
    "summary_en": "OpenAI researchers Noam Brown and Sheryl Hsu announced that OpenAI's reasoning system achieved a gold medal in the 2025 International Olympiad in Informatics (IOI), ranking first among AI participants. Notably, this same model, which previously secured gold in the International Math Olympiad (IMO), is also considered OpenAI's best competitive coding model. This achievement signifies a major breakthrough for OpenAI in AI reasoning and general capabilities.",
    "keywords_en": [
      "OpenAI",
      "Artificial Intelligence",
      "Reasoning System",
      "Competitive Programming",
      "Math Olympiad",
      "IOI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Research Progress",
      "AI Agent"
    ],
    "published_time": "2025-08-11T18:01:25.000Z",
    "download_time": "2025-08-12 04:04:27",
    "visual_resource": [
      "screenshot/twitter/polynoamial_1954966398989635668.png"
    ],
    "extra_info": "{\"username\": \"polynoamial\", \"tweet_id\": \"1954966398989635668\"}"
  },
  {
    "id": "twitter_AnimaAnandkumar_1954779061529743412",
    "source": "Twitter",
    "url": "https://x.com/AnimaAnandkumar/status/1954779061529743412",
    "title_en": "AnimaAnandkumar_LeanDojo Major Update: Lean+LLM for Formal Math Reasoning",
    "summary_en": "Prof. Anima Anandkumar announced a major update to LeanDojo, introducing the Lean+LLM framework for verified mathematical reasoning. This update includes Lean4Code, an integrated development environment; LeanCopilot, a neural network inference system supporting local/cloud LLMs and provers like DeepSeek/Kimina; and LeanAgent, a lifelong learning framework. Highlighting Lean's critical role in AI's success in the International Math Olympiad, LeanDojo, as the first open-source framework, aims to integrate Lean with LLMs to address hallucination in long proofs and ensure rigorous mathematical verification.",
    "keywords_en": [
      "LeanDojo",
      "Formal Verification",
      "Large Language Model",
      "Theorem Proving",
      "Mathematical Reasoning",
      "Open Source Project"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Open Source"
    ],
    "published_time": "2025-08-11T05:37:00.000Z",
    "download_time": "2025-08-12 04:05:48",
    "visual_resource": [
      "screenshot/twitter/AnimaAnandkumar_1954779061529743412.png"
    ],
    "extra_info": "{\"username\": \"AnimaAnandkumar\", \"tweet_id\": \"1954779061529743412\"}"
  },
  {
    "id": "fQbM7aP4mkZ8Vam_h3JnRA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/fQbM7aP4mkZ8Vam_h3JnRA",
    "title_en": "2025 Global Large Model Application Report: Red Ocean Competition Erodes Loyalty, Users Juggle 4.7 Platforms!",
    "summary_en": "The \"2025 Global Large Model Application Report\" reveals that in the first half of 2025, large models have transitioned from technological frontiers to practical production, with 45% of enterprises deploying them in production environments. Engineering R&D, customer support, and marketing emerge as primary application scenarios. The report highlights that users, on average, utilize 4.7 different large models, indicating a red ocean competitive market with low brand loyalty. Key challenges include insufficient model knowledge, hallucination issues, and high costs. While OpenAI maintains its lead, Google's Gemini and Deepseek show significant progress. Chinese large models face geographical deployment restrictions in overseas markets. NVIDIA dominates the training hardware market. The report emphasizes that reliability, cost, and intelligence levels remain major hurdles for AI adoption.",
    "keywords_en": [
      "Large Models",
      "AI Applications",
      "Market Competition",
      "User Loyalty",
      "Enterprise Deployment",
      "Challenges"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Generative AI"
    ],
    "published_time": "2025-08-11T16:02:05.000Z",
    "download_time": "2025-08-12T11:19:18.770421",
    "visual_resource": [
      "screenshot/wechat/wechat_image_fQbM7aP4mkZ8Vam_h3JnRA.png"
    ],
    "extra_info": null
  },
  {
    "id": "nQfs4g4RkJ5n-rlJK7ctTQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/nQfs4g4RkJ5n-rlJK7ctTQ",
    "title_en": "41 SOTA Benchmarks! Zhipu AI's Latest Open-Source GLM-4.5V Tested: Guessing Locations from Images, Video to Code in Seconds",
    "summary_en": "Zhipu AI's newly open-sourced GLM-4.5V visual reasoning model has achieved State-of-the-Art (SOTA) performance across 41 public visual multimodal benchmarks, demonstrating exceptional multimodal capabilities. Built upon the GLM-4.5 foundation, this 106B-parameter model excels in high-accuracy image-based location guessing (GeoGuessr), advanced grounding capabilities like identifying elements in complex artworks, and groundbreaking video understanding with direct web page code replication. Furthermore, GLM-4.5V showcases robust spatial relationship comprehension and intricate chart analysis. Optimized through a three-stage training strategy, it supports 64K multimodal long contexts and offers cost-effective API services. This release signifies a pivotal shift for multimodal large models from mere capability validation to widespread practical deployment, setting a new direction for AI visual reasoning.",
    "keywords_en": [
      "Zhipu AI",
      "GLM-4.5V",
      "Multimodal Large Model",
      "Visual Reasoning",
      "Video Understanding",
      "Code Generation"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Video Understanding"
    ],
    "published_time": "2025-08-11T14:22:04.000Z",
    "download_time": "2025-08-12T11:19:17.678695",
    "visual_resource": [
      "screenshot/wechat/wechat_image_nQfs4g4RkJ5n-rlJK7ctTQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "Pq530y9HR2iOMHycZIXAPA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Pq530y9HR2iOMHycZIXAPA",
    "title_en": "TPAMI 2025 | DAPT: Decouple Before Align to Solve Visual-Language Model \"Information Asymmetry\" and Significantly Improve Prompt Tuning Performance",
    "summary_en": "A TPAMI 2025 paper introduces DAPT (Decouple before Align Prompt Tuning), a novel framework addressing the \"information asymmetry\" challenge in Visual-Language Model (VLM) prompt tuning. DAPT's core idea is to first decouple image foreground and background, then symmetrically align them with corresponding text descriptions, effectively mitigating model attention bias towards irrelevant visual information. The framework incorporates visual pull-push regularization to compel the model to focus on regions of interest. Extensive experiments demonstrate DAPT's superior performance, achieving new State-of-the-Art (SOTA) results across 11 tasks, including few-shot learning, data-efficient learning, and base-to-novel generalization. Notably, DAPT achieves performance comparable to or exceeding existing methods using only 50% of the training data, offering a highly efficient and robust paradigm for VLM utilization and multimodal alignment.",
    "keywords_en": [
      "DAPT",
      "Prompt Tuning",
      "Visual-Language Models",
      "Information Asymmetry",
      "Decouple-Align",
      "Few-shot Learning"
    ],
    "area_en": [
      "Computer Vision",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-08-11T14:00:27.000Z",
    "download_time": "2025-08-12T11:19:18.570970",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Pq530y9HR2iOMHycZIXAPA.png"
    ],
    "extra_info": null
  },
  {
    "id": "DL8Ot-_qaXdr-BJyvSldCA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/DL8Ot-_qaXdr-BJyvSldCA",
    "title_en": "Sam Altman's Removal of GPT-4o Triggers AI \"Withdrawal,\" While Elon Musk Announces Grok 4 Global Free Access!",
    "summary_en": "This article analyzes the contrasting product strategies of OpenAI and xAI regarding AI models. OpenAI's decision to discontinue GPT-4o sparked significant user \"withdrawal symptoms,\" prompting Sam Altman to acknowledge the error. OpenAI's rationale is to design AI with \"restraint\" and \"clear boundaries\" to prevent user over-reliance or addiction, prioritizing users' long-term well-being over unlimited gratification. In stark contrast, Elon Musk announced Grok 4's global free access, employing strategies like \"spicy mode\" and anthropomorphic characters to maximize user engagement and retention through high-stimulation and frequent interaction. This divergence highlights the ethical debate between AI empowerment and potential addiction, reflecting different corporate philosophies on human-AI relationships and the future of AI development.",
    "keywords_en": [
      "GPT-4o",
      "Grok 4",
      "AI Ethics",
      "User Addiction",
      "Product Strategy",
      "Large Language Models"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-08-11T14:00:27.000Z",
    "download_time": "2025-08-12T11:19:17.415365",
    "visual_resource": [
      "screenshot/wechat/wechat_image_DL8Ot-_qaXdr-BJyvSldCA.png"
    ],
    "extra_info": null
  },
  {
    "id": "ezkb5TEvN1y2WmaTWk6_Qw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ezkb5TEvN1y2WmaTWk6_Qw",
    "title_en": "Inference Cost Plummets 75%! GPT-OSS Achieves 4x Inference Speed with New Data Type, Enabling 120B Parameter Models on 80GB GPUs",
    "summary_en": "OpenAI's gpt-oss model has adopted the innovative MXFP4 data type, leading to a remarkable 75% reduction in inference costs and a fourfold increase in token generation speed. This advancement allows MXFP4 to decrease memory footprint to just one-quarter of equivalent BF16 models, making it feasible to run massive 120-billion-parameter large language models on more accessible 80GB GPUs, thereby significantly improving hardware efficiency and accessibility. The technology achieves this by optimizing both weight storage and memory bandwidth, delivering extreme data compression while meticulously preserving model precision, effectively mitigating the common accuracy degradation issues associated with low-precision quantization. This breakthrough not only substantially lowers the operational expenses for deploying large-scale AI models but also introduces a powerful new paradigm for efficient AI inference, promising a future of more cost-effective and high-performance artificial intelligence applications.",
    "keywords_en": [
      "MXFP4",
      "Inference Cost",
      "Large Language Model",
      "Data Type",
      "Quantization",
      "Memory Optimization"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-11T06:49:20.000Z",
    "download_time": "2025-08-12T11:19:34.469670",
    "visual_resource": [
      "screenshot/wechat/wechat_image_ezkb5TEvN1y2WmaTWk6_Qw.png"
    ],
    "extra_info": null
  },
  {
    "id": "F5hCckvyEybGZCJg8NamWA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/F5hCckvyEybGZCJg8NamWA",
    "title_en": "Inspur Information Unveils New-Generation AI Supernode: Making 64 Cards Act as One, Supporting Simultaneous Operation of Four Domestic Open-Source Models",
    "summary_en": "Inspur Information has unveiled its new-generation AI supernode, \"YuanNao SD200,\" designed to address the explosive growth in computation and communication demands posed by trillion-parameter large models and the Agentic AI paradigm. This innovative supernode integrates 64 GPU cards into a unified memory and addressing compute domain. Utilizing a 3D Mesh open architecture and remote GPU virtual mapping technology, it achieves an ultra-large resource pool of 4TB VRAM and 64TB system memory, offering sub-100-nanosecond low-latency communication. The YuanNao SD200 supports the simultaneous operation of four domestic open-source models, including DeepSeek R1 and Kimi K2, on a single machine, demonstrating super-linear scaling performance for trillion-parameter large model inference and real-time multi-agent collaboration. Its open hardware and software ecosystem is compatible with mainstream frameworks, reducing migration costs and aiming to promote AI intelligence equalization.",
    "keywords_en": [
      "Inspur Information",
      "AI Supernode",
      "Large Model Inference",
      "AI Agent",
      "Open Architecture"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-11T06:49:20.000Z",
    "download_time": "2025-08-12T11:19:33.366966",
    "visual_resource": [
      "screenshot/wechat/wechat_image_F5hCckvyEybGZCJg8NamWA.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.06471",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.06471",
    "title_en": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
    "summary_en": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.",
    "keywords_en": [
      "GLM-4.5",
      "Large Language Model",
      "AI Agent",
      "Reasoning",
      "Coding"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-08T17:21:06.000Z",
    "download_time": "2025-08-11 20:20:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06471.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.06471\", \"arxiv_url\": \"https://arxiv.org/abs/2508.06471\"}"
  },
  {
    "id": "2508.06433",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.06433",
    "title_en": "Memp: Exploring Agent Procedural Memory",
    "summary_en": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.",
    "keywords_en": [
      "Agent",
      "Procedural Memory",
      "Large Language Models",
      "Memory Update",
      "Trajectory Distillation"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-08T16:20:56.000Z",
    "download_time": "2025-08-11 20:20:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06433.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.06433\", \"arxiv_url\": \"https://arxiv.org/abs/2508.06433\"}"
  },
  {
    "id": "2508.05731",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.05731",
    "title_en": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy\n  Optimization",
    "summary_en": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.",
    "keywords_en": [
      "GUI Grounding",
      "Multimodal Large Language Models",
      "Adaptive Exploration Policy Optimization",
      "Reinforcement Learning",
      "Semantic Alignment"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-07T17:49:56.000Z",
    "download_time": "2025-08-11 20:20:43",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05731.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.05731\", \"arxiv_url\": \"https://arxiv.org/abs/2508.05731\"}"
  },
  {
    "id": "2508.04482",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.04482",
    "title_en": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use",
    "summary_en": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.",
    "keywords_en": [
      "OS Agents",
      "Multimodal Large Language Models",
      "AI Agent",
      "Graphical User Interface",
      "Survey"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "AI Agent"
    ],
    "published_time": "2025-08-06T14:33:45.000Z",
    "download_time": "2025-08-11 20:20:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04482.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.04482\", \"arxiv_url\": \"https://arxiv.org/abs/2508.04482\"}"
  },
  {
    "id": "2508.02095",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.02095",
    "title_en": "VLM4D: Towards Spatiotemporal Awareness in Vision Language Models",
    "summary_en": "Vision language models (VLMs) have shown remarkable capabilities in\nintegrating linguistic and visual reasoning but remain fundamentally limited in\nunderstanding dynamic spatiotemporal interactions. Humans effortlessly track\nand reason about object movements, rotations, and perspective shifts-abilities\nessential for robust dynamic real-world understanding yet notably lacking in\ncurrent VLMs. In this paper, we introduce VLM4D, the first benchmark\nspecifically designed to evaluate the spatiotemporal reasoning capabilities of\nVLMs. Our benchmark comprises diverse real-world and synthetic videos\naccompanied by carefully curated question-answer pairs emphasizing\ntranslational and rotational motions, perspective awareness, and motion\ncontinuity. Through comprehensive evaluations of state-of-the-art open and\nclosed-source VLMs, we identify significant performance gaps compared to human\nbaselines, highlighting fundamental deficiencies in existing models. Extensive\nanalysis reveals that VLMs struggle particularly with integrating multiple\nvisual cues and maintaining temporal coherence. We further explore promising\ndirections, such as leveraging 4D feature field reconstruction and targeted\nspatiotemporal supervised fine-tuning, demonstrating their effectiveness in\nenhancing spatiotemporal comprehension. Our work aims to encourage deeper\nexploration into improving VLMs' spatial and temporal grounding, paving the way\ntowards more capable and reliable visual intelligence for dynamic environments.",
    "keywords_en": [
      "Vision Language Models",
      "Spatiotemporal Awareness",
      "Benchmark",
      "Spatiotemporal Reasoning",
      "Video Understanding"
    ],
    "area_en": [
      "Multimodal",
      "Computer Vision",
      "Video Understanding"
    ],
    "published_time": "2025-08-04T06:06:06.000Z",
    "download_time": "2025-08-11 20:20:47",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02095.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.02095\", \"arxiv_url\": \"https://arxiv.org/abs/2508.02095\"}"
  },
  {
    "id": "2508.01242",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.01242",
    "title_en": "MeshLLM: Empowering Large Language Models to Progressively Understand\n  and Generate 3D Mesh",
    "summary_en": "We present MeshLLM, a novel framework that leverages large language models\n(LLMs) to understand and generate text-serialized 3D meshes. Our approach\naddresses key limitations in existing methods, including the limited dataset\nscale when catering to LLMs' token length and the loss of 3D structural\ninformation during mesh serialization. We introduce a Primitive-Mesh\ndecomposition strategy, which divides 3D meshes into structurally meaningful\nsubunits. This enables the creation of a large-scale dataset with 1500k+\nsamples, almost 50 times larger than previous methods, which aligns better with\nthe LLM scaling law principles. Furthermore, we propose inferring face\nconnectivity from vertices and local mesh assembly training strategies,\nsignificantly enhancing the LLMs' ability to capture mesh topology and spatial\nstructures. Experiments show that MeshLLM outperforms the state-of-the-art\nLLaMA-Mesh in both mesh generation quality and shape understanding,\nhighlighting its great potential in processing text-serialized 3D meshes.",
    "keywords_en": [
      "Large Language Models",
      "3D Mesh",
      "Mesh Generation",
      "Shape Understanding",
      "MeshLLM"
    ],
    "area_en": [
      "Large Language Model",
      "Generative AI",
      "Multimodal"
    ],
    "published_time": "2025-08-02T07:37:37.000Z",
    "download_time": "2025-08-11 20:20:42",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01242.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.01242\", \"arxiv_url\": \"https://arxiv.org/abs/2508.01242\"}"
  }
]