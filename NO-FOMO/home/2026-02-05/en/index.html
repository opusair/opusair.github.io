<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2026-02-05</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2026-02-05</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Claude Opus 4.6</h2>
                <span class="published-time">Published: 2026-02-05 17:38:53</span>
                
                <p class="summary">Anthropic has officially unveiled Claude Opus 4.6, representing the latest significant advancement in its flagship large language model series. This new iteration is expected to deliver substantial improvements in reasoning capabilities, complex problem-solving, and overall comprehension, building upon the robust foundation of its predecessors. Opus 4.6 is engineered for enhanced performance across a diverse range of benchmarks, demonstrating increased accuracy in tasks requiring deep understanding and nuanced response generation. Industry insights suggest a particular focus on refining the model's ability to manage intricate conversational flows and multi-turn interactions with greater coherence and contextual awareness. Additionally, Anthropic maintains its commitment to responsible AI development by integrating advanced safety mechanisms and ethical guidelines into Opus 4.6, ensuring the model operates within strict safety parameters while providing powerful and reliable AI capabilities for users. This release highlights Anthropic's continuous efforts to innovate and expand the boundaries of AI research and application, making more capable and safer AI accessible.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Claude Opus</span><span>Large Language Model</span><span>AI Models</span><span>Anthropic</span><span>Generative AI</span><span>Natural Language Processing</span><span>AI Development</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.anthropic.com/news/claude-opus-4-6" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GPT-5.3-Codex</h2>
                <span class="published-time">Published: 2026-02-05 18:08:08</span>
                
                <p class="summary">OpenAI has introduced GPT-5.3-Codex, a significant advancement in their series of large language models specifically engineered for code-related tasks. This new iteration builds upon the foundational capabilities of the GPT architecture and the specialized programming prowess of the Codex models, promising enhanced performance in generating, understanding, and debugging code across multiple programming languages. GPT-5.3-Codex is anticipated to offer developers more efficient tools for automating software development workflows, improving code quality, and accelerating the pace of innovation in software development. Its advanced reasoning and context-awareness within programming environments are expected to push the boundaries of what AI can achieve in software engineering, making complex coding tasks more accessible and productive for a wider range of users. The model's improvements are expected to manifest in areas such as intelligent code completion, automated unit test generation, and seamless translation of natural language requests into functional code, thereby significantly enhancing developer productivity. This release underscores OpenAI's ongoing commitment to developing specialized AI agents for practical applications and industry-specific challenges.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>Code Generation</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Software Development</span><span>Neural Networks</span><span>AI Agent</span><span>OpenAI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://openai.com/index/introducing-gpt-5-3-codex/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Orchestrate teams of Claude Code sessions</h2>
                <span class="published-time">Published: 2026-02-05 17:49:54</span>
                
                <p class="summary">Anthropic has rolled out a significant enhancement to its Claude Code platform, enabling the orchestration of teams comprising multiple AI-powered coding sessions. Detailed in the `agent-teams` documentation, this new functionality is designed to revolutionize collaborative software development by allowing developers to manage and coordinate several Claude Code instances working in concert on intricate programming projects. The system facilitates a structured approach to leveraging distributed artificial intelligence, where AI agents can be assigned specific tasks, communicate their progress, and integrate their generated code seamlessly. This orchestration capability aims to accelerate various stages of the development lifecycle, including initial code generation, iterative debugging, and overall project completion, by harnessing collective AI intelligence. It marks a pivotal move towards more sophisticated, AI-driven software engineering paradigms, offering a highly efficient and scalable method for deploying large language models within complex coding environments, thereby optimizing resource utilization and project timelines.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Code Generation</span><span>Orchestration</span><span>Collaborative AI</span><span>Software Development</span><span>Large Language Model</span><span>AI in Programming</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://code.claude.com/docs/en/agent-teams" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>We tasked Opus 4.6 using agent teams to build a C Compiler</h2>
                <span class="published-time">Published: 2026-02-05 19:07:51</span>
                
                <p class="summary">Anthropic has reported a significant milestone in artificial intelligence development, detailing an experiment where their advanced Opus 4.6 model, orchestrated through sophisticated agent teams, successfully constructed a C compiler. This pioneering initiative showcases the rapidly expanding capabilities of large language models when applied to highly complex software engineering challenges. The methodology involved deploying multiple AI agents, working collaboratively to break down the daunting task of compiler development into manageable sub-problems, mirroring the distributed efforts of human software teams. This achievement underscores the growing potential of AI in automating intricate coding tasks, significantly enhancing productivity in software development, and pushing the boundaries of what autonomous AI systems can accomplish. The project particularly highlights the model's ability to understand intricate specifications, plan execution steps, generate robust code, debug, and seamlessly integrate components for a sophisticated system. Such advancements are critical, paving the way for future applications in autonomous software creation, maintenance, and complex system design, potentially transforming the landscape of software engineering.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agent</span><span>C Compiler</span><span>Code Generation</span><span>Large Language Model</span><span>Software Engineering</span><span>Opus 4.6</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.anthropic.com/engineering/building-c-compiler" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Anthropic's Claude Opus 4.6 uncovers 500 zero-day flaws in open-source code</h2>
                <span class="published-time">Published: 2026-02-05 18:25:05</span>
                
                <p class="summary">Anthropic's advanced large language model, Claude Opus 4.6, has reportedly identified a substantial number of 500 zero-day flaws across various open-source codebases. This significant achievement highlights the escalating capabilities of artificial intelligence in proactively detecting critical software vulnerabilities before they can be exploited. The successful identification of such a high volume of previously unknown security defects underscores the potential of sophisticated AI agents to revolutionize cybersecurity practices, moving beyond traditional static and dynamic analysis tools. This development could lead to more robust and secure software ecosystems, particularly within the widely used open-source community. Furthermore, it demonstrates Claude Opus 4.6's profound understanding and analytical prowess in complex code structures, positioning it as a powerful tool for automated vulnerability research and patching efforts. The findings suggest a future where AI plays a pivotal role in maintaining the integrity and safety of global software infrastructure, offering a new paradigm for threat detection and prevention.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Claude Opus 4.6</span><span>Zero-day flaws</span><span>Open-source security</span><span>Vulnerability detection</span><span>Large Language Model</span><span>AI in cybersecurity</span><span>Code analysis</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.axios.com/2026/02/05/anthropic-claude-opus-46-software-hunting" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models</h2>
                <span class="published-time">Published: 2026-02-05 18:21:53</span>
                
                <p class="summary">A recent study introduces the concept of psychometric jailbreaks as a novel methodology to uncover internal inconsistencies within advanced artificial intelligence models, particularly those at the frontier of development, such as large language models. These specialized 'jailbreaks' involve carefully designed prompts and interaction patterns that probe the models' ethical alignments, policy adherence, and behavioral coherence under various cognitive stress tests. The research reveals that despite extensive safety training and alignment efforts, these sophisticated models often harbor 'internal conflicts,' manifesting as contradictory responses, inconsistent value judgments, or shifts in persona when subjected to specific psychometric probes. This phenomenon highlights a critical challenge in ensuring the robust reliability and ethical integrity of next-generation AI systems, suggesting that current alignment techniques may not fully eliminate deep-seated inconsistencies. The findings underscore the urgent need for more comprehensive evaluation methods and advanced architectural designs to resolve these internal conflicts, thereby enhancing the trustworthiness and predictability of AI in sensitive applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Psychometric Jailbreaks</span><span>Frontier Models</span><span>AI Safety</span><span>Large Language Models</span><span>Model Alignment</span><span>AI Ethics</span><span>Behavioral Consistency</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://arxiv.org/abs/2512.04124" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Protein Autoregressive Modeling via Multiscale Structure Generation</h2>
                <span class="published-time">Published: 2026-02-04T18:59:49.000Z</span>
                
                <p class="summary">We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Protein Autoregressive Modeling</span><span>Multiscale Structure Generation</span><span>Protein Backbone Generation</span><span>Autoregressive Transformer</span><span>Zero-shot Generalization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Deep Learning</span><span>Machine Learning</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.04883" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Rethinking the Trust Region in LLM Reinforcement Learning</h2>
                <span class="published-time">Published: 2026-02-04T18:59:04.000Z</span>
                
                <p class="summary">Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLM Reinforcement Learning</span><span>Proximal Policy Optimization</span><span>Policy Divergence</span><span>DPPO</span><span>LLM Fine-tuning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.04879" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Horizon-LM: A RAM-Centric Architecture for LLM Training</h2>
                <span class="published-time">Published: 2026-02-04T18:04:46.000Z</span>
                
                <p class="summary">The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2times higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLM Training</span><span>RAM-Centric Architecture</span><span>Host Memory</span><span>Memory Capacity</span><span>Training Throughput</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.04816" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models</h2>
                <span class="published-time">Published: 2026-02-04T17:51:05.000Z</span>
                
                <p class="summary">Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Omni-modal Large Language Models</span><span>Token Compression</span><span>Modality-Asymmetric</span><span>Multimodal Understanding</span><span>Computational Efficiency</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Multimodal</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.04804" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>From Data to Behavior: Predicting Unintended Model Behaviors Before Training</h2>
                <span class="published-time">Published: 2026-02-04T16:37:17.000Z</span>
                
                <p class="summary">Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>unintended model behaviors</span><span>bias detection</span><span>pre-training</span><span>data analysis</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.04735" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ERNIE 5.0 Technical Report</h2>
                <span class="published-time">Published: 2026-02-04T16:18:15.000Z</span>
                
                <p class="summary">In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>ERNIE 5.0</span><span>Multimodal AI</span><span>Mixture-of-Experts</span><span>Autoregressive Model</span><span>Elastic Training</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Deep Learning</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.04705" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>