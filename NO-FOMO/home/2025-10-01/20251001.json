[
  {
    "id": "hackernews_45442470",
    "source": "Hacker News",
    "url": "https://www.washingtonpost.com/technology/interactive/2025/openai-training-data-sora/",
    "title": "OpenAI won't say whose content trained its video tool. We found some clues.",
    "summary": "A recent investigation explores the undisclosed training data used for OpenAI's video generation tool, Sora. Despite OpenAI's lack of transparency regarding the origins of its training content, researchers and journalists are actively seeking clues to identify potential sources. The inquiry aims to understand the dataset composition, which is critical for evaluating the model's biases, intellectual property implications, and the ethical considerations surrounding generative AI development. Early findings suggest a diverse range of content might have been utilized, though specific creators or copyright holders remain unconfirmed. This ongoing effort highlights the growing demand for greater accountability and transparency from AI developers regarding their data practices, especially as advanced generative models become more prevalent and influential across various industries. The investigation underscores the challenges in tracing digital content back to its original creators when it's incorporated into large-scale AI training datasets, prompting broader discussions on data provenance in the AI era.",
    "keywords": [
      "OpenAI",
      "Sora",
      "Generative AI",
      "Training Data",
      "Video Generation",
      "AI Ethics",
      "Data Provenance"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Video Understanding"
    ],
    "published_time": "2025-10-01 19:48:23",
    "download_time": "2025-10-01 20:04:01",
    "extra_info": "{\"score\": 3, \"by\": \"kgwgk\", \"descendants\": 0, \"story_id\": 45442470}"
  },
  {
    "id": "hackernews_45440431",
    "source": "Hacker News",
    "url": "https://www.opentslm.com/",
    "title": "OpenTSLM: Language models that understand time series",
    "summary": "OpenTSLM presents a groundbreaking initiative focused on developing language models specifically designed to understand and process time series data. This project, spearheaded by StanfordBDHG, re-contextualizes the highly successful transformer architectures, traditionally applied in natural language processing, for temporal sequence analysis. By treating time series points as tokens within a sequence, OpenTSLM aims to leverage the contextual understanding and predictive power inherent in large language models to tackle complex challenges in diverse domains. The project provides an open-source framework and an accompanying whitepaper that details its innovative methodology and potential applications. It explores how these specialized language models can improve performance in critical time series tasks such as forecasting, anomaly detection, and classification, offering a robust and scalable solution that could significantly advance the field of temporal data analysis. This paradigm shift promises enhanced model interpretability and more accurate predictive capabilities across various industries, from finance and healthcare to industrial monitoring and scientific research.",
    "keywords": [
      "Time Series Analysis",
      "Language Models",
      "Deep Learning",
      "Transformer Networks",
      "Machine Learning",
      "Forecasting",
      "Anomaly Detection"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-10-01 17:25:33",
    "download_time": "2025-10-01 20:02:41",
    "extra_info": "{\"score\": 92, \"by\": \"rjakob\", \"descendants\": 25, \"story_id\": 45440431}"
  },
  {
    "id": "hackernews_45437893",
    "source": "Hacker News",
    "url": "https://www.alephic.com/writing/the-magic-of-claude-code",
    "title": "Unix philosophy and filesystem access makes Claude Code amazing",
    "summary": "The article underscores the transformative impact of adopting the Unix philosophy, particularly concerning sophisticated filesystem access, on the performance and utility of Claude Code. By enabling Claude to interact with project files and directories akin to a human developer operating within a command-line environment, its proficiency in comprehending, generating, and debugging code within intricate software architectures is substantially elevated. This methodological integration allows the AI to perform a wider array of development tasks that demand extensive contextual awareness and iterative problem-solving, moving beyond mere code snippet generation towards managing comprehensive development workflows. Such an approach fosters a more dynamic and effective collaborative ecosystem between AI and human developers, capitalizing on the inherent simplicity, modularity, and composability of Unix principles. This paradigm shift not only enhances Claude's ability to tackle complex programming challenges but also redefines its role as an indispensable, versatile tool in modern software engineering, promising remarkable advancements in AI-driven code development and project management.",
    "keywords": [
      "Unix philosophy",
      "Filesystem access",
      "Claude Code",
      "AI agent",
      "Code generation",
      "Software development",
      "Large Language Models",
      "Development workflow"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-10-01 14:05:45",
    "download_time": "2025-10-01 20:02:41",
    "extra_info": "{\"score\": 138, \"by\": \"noahbrier\", \"descendants\": 98, \"story_id\": 45437893}"
  },
  {
    "id": "hackernews_45437735",
    "source": "Hacker News",
    "url": "https://cursor.com/changelog/1-7",
    "title": "Cursor 1.7",
    "summary": "Cursor 1.7 marks a significant update for the AI-powered code editor, primarily focused on bolstering developer productivity and refining the intelligent coding experience. This iteration brings substantial enhancements to its core AI capabilities, including more accurate code generation, smarter context-aware suggestions, and improved debugging assistance through interactive AI chat. Users can expect a more fluid and responsive interface due to extensive performance optimizations, resulting in quicker startup times and reduced latency during intensive coding sessions. The update also introduces advanced refactoring tools, alongside more robust integration with popular version control systems like Git, facilitating seamless collaboration and project management. Furthermore, improvements to syntax highlighting and code navigation contribute to a more intuitive development environment. These enhancements are designed to empower developers to leverage AI assistance more effectively, making complex tasks simpler and accelerating the overall software development lifecycle. Cursor 1.7 solidifies its position as a leading intelligent development platform, offering a more stable, efficient, and feature-rich tool for modern software engineering.",
    "keywords": [
      "AI code editor",
      "Software development",
      "Integrated Development Environment",
      "Code generation",
      "Debugging tools",
      "Developer productivity",
      "Version control",
      "Programming"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-10-01 13:51:03",
    "download_time": "2025-10-01 20:03:34",
    "extra_info": "{\"score\": 124, \"by\": \"mustaphah\", \"descendants\": 126, \"story_id\": 45437735}"
  },
  {
    "id": "hackernews_45439721",
    "source": "Hacker News",
    "url": "https://fossa.com/blog/fossabot-dependency-upgrade-ai-agent/",
    "title": "Fossabot: AI code review for Dependabot/Renovate on breaking changes and impacts",
    "summary": "Fossabot introduces an advanced AI-powered code review solution specifically designed to enhance the dependency upgrade process managed by popular tools like Dependabot and Renovate. This innovative AI agent aims to streamline the often-complex task of updating software dependencies by automatically identifying and analyzing potential breaking changes and their downstream impacts within a codebase. By leveraging artificial intelligence and machine learning techniques, Fossabot provides developers with more profound insights than traditional static analysis tools, predicting how dependency updates might affect an application's functionality and stability. Its primary goal is to mitigate the inherent risks associated with integrating new library or framework versions, significantly reducing the manual effort typically required to review pull requests generated by automated dependency bots. This capability is crucial for maintaining robust software health, ensuring security, and optimizing performance. Fossabot empowers development teams to proactively address compatibility issues and facilitates smoother, more reliable update cycles, ultimately accelerating the adoption of critical updates while minimizing the risk of introducing regressions or vulnerabilities.",
    "keywords": [
      "AI Code Review",
      "Dependency Management",
      "Automated Code Review",
      "Breaking Changes",
      "Impact Analysis",
      "AI Agent",
      "DevOps"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-10-01 16:30:16",
    "download_time": "2025-10-01 20:02:40",
    "extra_info": "{\"score\": 74, \"by\": \"robszumski\", \"descendants\": 9, \"story_id\": 45439721}"
  },
  {
    "id": "hackernews_45437594",
    "source": "Hacker News",
    "url": "https://app.chartdb.io/ai",
    "title": "Show HN: ChartDB Agent â€“ Cursor for DB schema design",
    "summary": "ChartDB has announced the launch of its new product, ChartDB Agent, a tool designed to revolutionize database schema design through natural language processing and artificial intelligence. This new offering builds upon the success of ChartDB OSS, an open-source solution that generates Entity-Relationship (ER) diagrams from existing databases without requiring direct access. The ChartDB Agent empowers users to design databases from scratch or implement schema modifications by simply describing their requirements in plain English. Key functionalities include the ability to generate complete schemas from natural language input, brainstorm new tables, columns, and relationships with AI-driven suggestions, and visually refine designs within an interactive ER diagram interface. Furthermore, the agent ensures deterministic export of SQL scripts, facilitating seamless integration into development workflows. ChartDB Agent is available for immediate trial without requiring a signup, with an option to sign up for use with personal databases, and the team is actively soliciting user feedback.",
    "keywords": [
      "Database Schema Design",
      "Natural Language Processing",
      "AI Agent",
      "Entity-Relationship Diagrams",
      "SQL Script Generation",
      "Database Tools"
    ],
    "area": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "AI Agent"
    ],
    "published_time": "2025-10-01 13:38:36",
    "download_time": "2025-10-01 20:03:10",
    "extra_info": "{\"score\": 99, \"by\": \"guyb3\", \"descendants\": 28, \"story_id\": 45437594}"
  },
  {
    "id": "MoneyPrinterTurbo",
    "source": "GitHub",
    "url": "https://github.com/harry0703/MoneyPrinterTurbo",
    "title": "MoneyPrinterTurbo",
    "summary": "MoneyPrinterTurbo is an innovative open-source project that automates the entire process of short video creation. By simply providing a video topic or keywords, the system intelligently generates video scripts, curates relevant visual assets, produces synchronized subtitles, selects appropriate background music, and finally synthesizes a high-definition short video. The project boasts a robust MVC architecture, offering both a user-friendly Web interface and a comprehensive API for integration. It supports various high-definition video dimensions, including vertical 9:16 and horizontal 16:9, and features batch video generation capabilities. Users can customize video clip durations, select from multiple languages for scripts and voice synthesis, and fine-tune subtitle appearance. Furthermore, MoneyPrinterTurbo integrates with a wide array of large language models, such as OpenAI, Moonshot, Azure, DeepSeek, and Google Gemini, for advanced script generation and leverages high-quality, copyright-free video materials, making it an efficient solution for streamlined digital content creation.",
    "keywords": [
      "AI Video Generation",
      "Short Video Creation",
      "Large Language Models",
      "Speech Synthesis",
      "Subtitle Generation",
      "Content Automation",
      "Generative AI",
      "Video Editing"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Multimodal"
    ],
    "published_time": "2025-05-16T03:03:36Z",
    "download_time": "2024-05-16 06:40:40",
    "extra_info": null
  },
  {
    "id": "claude-agent-sdk-python",
    "source": "GitHub",
    "url": "https://github.com/anthropics/claude-agent-sdk-python",
    "title": "Claude Agent SDK for Python",
    "summary": "The Claude Agent SDK for Python provides a robust toolkit for developers to integrate Claude's agent capabilities into their Python applications. This SDK offers core functionalities through its `query()` function for single-turn interactions and the `ClaudeSDKClient` for more complex, bidirectional conversations. Key technical features include asynchronous programming support, enabling efficient handling of AI agent responses, and the ability to define custom tools as in-process MCP (Message-Passing Protocol) servers. This significantly simplifies development by eliminating subprocess management and improving performance compared to external servers. Additionally, the SDK supports customizable hooks, allowing developers to inject deterministic processing and automated feedback at various points in the Claude agent's operational loop, enhancing control and safety. The SDK facilitates building sophisticated AI agent applications by providing structured methods for interaction, tool integration, and error handling, making it a critical component for leveraging Claude's intelligence in a programmatic manner.",
    "keywords": [
      "Claude Agent SDK",
      "Python",
      "AI Agent",
      "Asynchronous Programming",
      "Custom Tools",
      "Hooks",
      "MCP Servers",
      "API"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-09-30T19:59:14Z",
    "download_time": "2024-05-16 12:00:00",
    "extra_info": null
  },
  {
    "id": "lobe-chat",
    "source": "GitHub",
    "url": "https://github.com/lobehub/lobe-chat",
    "title": "Lobe Chat",
    "summary": "Lobe Chat is an open-source, modern-designed UI/framework for ChatGPT and other Large Language Models (LLMs), offering a comprehensive suite of features for building advanced conversational AI applications. It supports speech synthesis, multi-modal interactions, and an extensible plugin system based on function calling, enabling seamless integration with external tools and data sources. Users can deploy a private chat application for various LLMs like OpenAI, Claude, Gemini, Groq, and Ollama with a single click. Key functionalities include a Model Context Protocol (MCP) plugin marketplace, a dedicated desktop application, smart internet search for real-time knowledge, Chain of Thought visualization for AI reasoning, branching conversations, and support for Claude Artifacts. Furthermore, Lobe Chat facilitates file uploads for knowledge bases, integrates with 42 multi-model service providers (including local LLM support via Ollama), provides visual recognition capabilities for image understanding, and offers TTS & STT for voice conversations. It also enables text-to-image generation using models like DALL-E 3, features an Agent Market (GPTs), supports both local and remote databases with CRDT for synchronization, allows multi-user management, and is available as a Progressive Web App (PWA) with mobile device adaptation and custom themes.",
    "keywords": [
      "ChatGPT framework",
      "LLMs UI",
      "Function Calling",
      "AI Agents",
      "Multi-modal AI",
      "Self-hosting",
      "Plugin System",
      "Knowledge Base"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-10-01T19:41:54Z",
    "download_time": "2024-07-30 08:35:00",
    "extra_info": null
  },
  {
    "id": "2509.24002",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.24002",
    "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP",
    "summary": "MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of 127 high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only 52.56% pass@1 and 33.86% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below 30% pass@1 and 15% pass^4. On average, LLMs require 16.2 execution turns and 17.4 tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
    "keywords": [
      "MCP",
      "LLM benchmarks",
      "AI agents",
      "Stress-testing",
      "CRUD operations"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-28T17:53:27.000Z",
    "download_time": "2025-10-01 13:04:35",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.24002\", \"arxiv_url\": \"https://arxiv.org/abs/2509.24002\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24002.png\", \"original_title\": \"MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\\n  Use\"}"
  },
  {
    "id": "2509.26507",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.26507",
    "title": "The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain",
    "summary": "The relationship between computing systems and the brain has served asmotivation for pioneering theoreticians since John von Neumann and Alan Turing.Uniform, scale-free biological networks, such as the brain, have powerfulproperties, including generalizing over time, which is the main barrier forMachine Learning on the path to Universal Reasoning Models. We introduce `Dragon Hatchling' (BDH), a new Large Language Modelarchitecture based on a scale-free biologically inspired network of locally-interacting neuron particles. BDH couples strong theoreticalfoundations and inherent interpretability without sacrificing Transformer-likeperformance.BDH is a practical, performant state-of-the-art attention-based state spacesequence learning architecture. In addition to being a graph model, BDH admitsa GPU-friendly formulation. It exhibits Transformer-like scaling laws:empirically BDH rivals GPT2 performance on language and translation tasks, atthe same number of parameters (10M to 1B), for the same training data.BDH can be represented as a brain model. The working memory of BDH duringinference entirely relies on synaptic plasticity with Hebbian learning usingspiking neurons. We confirm empirically that specific, individual synapsesstrengthen connection whenever BDH hears or reasons about a specific conceptwhile processing language inputs. The neuron interaction network of BDH is agraph of high modularity with heavy-tailed degree distribution. The BDH modelis biologically plausible, explaining one possible mechanism which humanneurons could use to achieve speech.BDH is designed for interpretability. Activation vectors of BDH are sparseand positive. We demonstrate monosemanticity in BDH on language tasks.Interpretability of state, which goes beyond interpretability of neurons andmodel parameters, is an inherent feature of the BDH architecture.",
    "keywords": [
      "Large Language Models",
      "Biologically Inspired AI",
      "Brain Models",
      "Transformer Architecture",
      "Interpretability"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-09-30T16:49:01.000Z",
    "download_time": "2025-10-01 13:04:47",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.26507\", \"arxiv_url\": \"https://arxiv.org/abs/2509.26507\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26507.png\", \"original_title\": \"The Dragon Hatchling: The Missing Link between the Transformer and\\n  Models of the Brain\"}"
  },
  {
    "id": "2509.25760",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.25760",
    "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
    "summary": "While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.",
    "keywords": [
      "Large Language Models",
      "Reinforcement Learning",
      "Truthfulness",
      "Hallucinations",
      "Abstention"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Machine Learning"
    ],
    "published_time": "2025-09-30T04:25:17.000Z",
    "download_time": "2025-10-01 13:04:42",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.25760\", \"arxiv_url\": \"https://arxiv.org/abs/2509.25760\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25760.png\", \"original_title\": \"TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning\"}"
  },
  {
    "id": "2509.26490",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.26490",
    "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications",
    "summary": "As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/.",
    "keywords": [
      "LLM Agents",
      "Benchmarking",
      "Interactive Tasks",
      "Real-world Applications",
      "VitaBench"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-30T16:33:49.000Z",
    "download_time": "2025-10-01 13:04:36",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.26490\", \"arxiv_url\": \"https://arxiv.org/abs/2509.26490\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26490.png\", \"original_title\": \"VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\\n  Real-world Applications\"}"
  },
  {
    "id": "2509.25182",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.25182",
    "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder",
    "summary": "We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.",
    "keywords": [
      "DC-VideoGen",
      "video generation",
      "deep compression",
      "video autoencoder",
      "diffusion model"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-09-29T17:59:31.000Z",
    "download_time": "2025-10-01 13:04:36",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.25182\", \"arxiv_url\": \"https://arxiv.org/abs/2509.25182\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25182.png\", \"original_title\": \"DC-VideoGen: Efficient Video Generation with Deep Compression Video\\n  Autoencoder\"}"
  },
  {
    "id": "2509.22646",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.22646",
    "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs",
    "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.",
    "keywords": [
      "AI-generated videos",
      "Deepfake detection",
      "Multimodal LLMs",
      "Video generation reward",
      "Human perception"
    ],
    "area": [
      "Generative AI",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-09-26T17:59:54.000Z",
    "download_time": "2025-10-01 13:04:37",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.22646\", \"arxiv_url\": \"https://arxiv.org/abs/2509.22646\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22646.png\", \"original_title\": \"Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\\n  LLMs\"}"
  }
]