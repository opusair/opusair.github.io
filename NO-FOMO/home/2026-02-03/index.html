<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2026-02-03</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="." class="active">‰∏≠Êñá</a>
                <a href="en/" class="">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2026-02-03</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† ËøîÂõû‰∏ªÈ°µ</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ ÊúÄÊñ∞Êó•Êä•</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ ÂÖ≥‰∫éÊàë‰ª¨</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Qwen3-Coder-Next</h2>
                <span class="published-time">Published: 2026-02-03 16:01:50</span>
                
                <p class="summary">Qwen3-Coder-Next is introduced as the latest iteration in the Qwen series of large language models, specifically engineered for advanced code generation and software development tasks. Developed by Qwen.ai, this model is anticipated to build upon the capabilities of its predecessors, offering enhanced performance in understanding complex programming languages, debugging, and generating high-quality, efficient code. The 'Next' in its name suggests significant improvements in architectural design or training methodologies, potentially leading to better contextual understanding, fewer hallucinations in code output, and broader language support. This release aims to further empower developers by streamlining coding workflows, automating repetitive tasks, and assisting in the creation of more robust and reliable software solutions across various platforms. Its introduction signifies a continued push towards more sophisticated AI assistants in the software engineering domain, promising to integrate seamlessly into existing development environments and boost productivity, thereby accelerating innovation in software development processes.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>Code Generation</span><span>AI for Programming</span><span>Software Development</span><span>Generative AI</span><span>Qwen</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Generative AI</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://qwen.ai/blog?id=qwen3-coder-next" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent Skills</h2>
                <span class="published-time">Published: 2026-02-03 14:09:54</span>
                
                <p class="summary">Agent Skills refers to the specialized competencies and functionalities that empower artificial intelligence agents to perform complex tasks and interact effectively within dynamic environments. This concept encompasses the design, acquisition, and management of distinct capabilities, allowing AI agents to learn, adapt, and execute actions autonomously. These skills often involve integrating various AI techniques, such as natural language understanding, problem-solving, decision-making, and interaction protocols. The development of robust agent skills is crucial for advancing AI's practical applications, enabling agents to achieve higher levels of autonomy, efficiency, and intelligence in diverse domains, from task automation and virtual assistance to complex scientific research and human-computer collaboration. Platforms focusing on agent skills typically aim to provide frameworks or tools for developers to build, train, and deploy agents with a versatile set of abilities, thereby enhancing their overall performance and utility in real-world scenarios.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Autonomous Systems</span><span>Agent Development</span><span>Intelligent Automation</span><span>AI Capabilities</span><span>Skill Acquisition</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://agentskills.io/home" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Xcode 26.3 unlocks the power of agentic coding</h2>
                <span class="published-time">Published: 2026-02-03 18:04:08</span>
                
                <p class="summary">Apple has announced the release of Xcode 26.3, introducing groundbreaking capabilities that fundamentally transform the software development process through 'agentic coding.' This update integrates advanced artificial intelligence agents directly into the integrated development environment, empowering developers with intelligent automation and enhanced productivity features. Agentic coding in Xcode 26.3 is designed to assist with various development tasks, including intelligent code completion, automated debugging, proactive error detection, and streamlined refactoring suggestions, aiming to reduce repetitive work and accelerate development cycles. The new features leverage sophisticated AI models to understand code context, anticipate developer needs, and provide highly relevant, context-aware assistance. This marks a significant step towards a more autonomous and efficient coding paradigm, allowing developers to focus more on architectural design and complex problem-solving while AI agents handle routine tasks. The integration promises to improve code quality, enforce best practices, and significantly enhance the overall developer experience within the Apple ecosystem.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Xcode</span><span>Agentic Coding</span><span>AI Development</span><span>Developer Tools</span><span>Code Generation</span><span>Integrated Development Environment</span><span>Software Development</span><span>Apple</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.apple.com/newsroom/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: I built "AI Wattpad" to eval LLMs on fiction</h2>
                <span class="published-time">Published: 2026-02-03 17:08:43</span>
                
                <p class="summary">A new platform, Narrator, has been developed to evaluate Large Language Models (LLMs) on their ability to generate engaging serialized fiction. The creator, a long-time webfiction reader, identified a gap in current LLM evaluation methods, which often fail to assess the holistic nature of creative writing. Unlike fragmented benchmarks that test isolated capabilities like brainstorming, writing, or memory, Narrator focuses on real reader engagement to rank LLMs. This approach acknowledges that creative writing is a complex pipeline requiring consistent narrative, good prose, and strong memory across extended content. The platform aims to provide a more comprehensive and reader-centric evaluation of LLMs' performance in generating compelling long-form fictional content, addressing the limitations of existing, more academic evaluation landscapes.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLMs</span><span>fiction generation</span><span>AI evaluation</span><span>creative writing</span><span>serialized fiction</span><span>reader engagement</span><span>benchmarks</span><span>narrative consistency</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Generative AI</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://narrator.sh/llm-leaderboard" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>How does misalignment scale with model intelligence and task complexity?</h2>
                <span class="published-time">Published: 2026-02-03 00:28:06</span>
                
                <p class="summary">A recent inquiry delves into the intricate relationship between AI misalignment, model intelligence, and task complexity, addressing a fundamental concern within AI safety research. The central question revolves around how the challenge of ensuring AI systems act in accordance with human intent scales as these models become more capable and are deployed in increasingly complex environments. This research area is critical for understanding the potential for unintended consequences and control issues as AI advances. It explores whether misalignment issues intensify, transform, or become more tractable with greater intelligence. The investigation likely considers different facets of misalignment, such as goal misspecification, emergent behaviors, and value drift, examining how these manifest across varying levels of AI sophistication. The findings could offer crucial insights for developing more robust alignment strategies and safe AI architectures, ultimately influencing the trajectory of advanced artificial intelligence development.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Alignment</span><span>AI Safety</span><span>Model Intelligence</span><span>Task Complexity</span><span>Misalignment</span><span>Autonomous Systems</span><span>AI Ethics</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://alignment.anthropic.com/2026/hot-mess-of-ai/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LNAI \b Define AI coding tool configs once, sync to Claude, Cursor, Codex, etc.</h2>
                <span class="published-time">Published: 2026-02-03 08:45:57</span>
                
                <p class="summary">LNAI is an innovative project designed to streamline the configuration management for various AI coding tools. Its primary function is to enable developers to define their preferred settings, preferences, and prompt engineering instructions once, and then automatically synchronize these configurations across multiple AI assistants and platforms. This centralized approach aims to reduce redundancy and ensure consistency in how developers interact with tools like Claude, Cursor, and Codex. By abstracting the configuration layer, LNAI enhances productivity, minimizes setup time, and helps maintain a uniform coding environment regardless of the specific AI coding tool being utilized. This project addresses the challenge of managing disparate configurations in an increasingly fragmented landscape of AI-powered development aids, offering a unified solution for workflow optimization and standard adherence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI coding tools</span><span>Configuration management</span><span>Developer tools</span><span>Workflow automation</span><span>Large Language Models</span><span>Code generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/KrystianJonca/lnai" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Kimi K2.5: Visual Agentic Intelligence</h2>
                <span class="published-time">Published: 2026-02-02T16:17:38.000Z</span>
                
                <p class="summary">We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multimodal Agentic Model</span><span>Agentic Intelligence</span><span>Agent Swarm</span><span>Text-Vision Pre-training</span><span>Parallel Agent Orchestration</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Multimodal</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.02276" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</h2>
                <span class="published-time">Published: 2026-02-02T18:59:04.000Z</span>
                
                <p class="summary">We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reinforcement Learning</span><span>Reward Model</span><span>Policy Optimization</span><span>Environment Adaptation</span><span>Large Language Model Agents</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Machine Learning</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.02488" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards</h2>
                <span class="published-time">Published: 2026-02-02T04:37:11.000Z</span>
                
                <p class="summary">Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present PISCES, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, PISCES uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, PISCES is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that PISCES outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Text-to-Video Generation</span><span>Optimal Transport</span><span>Reward-based Learning</span><span>Annotation-free</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.01624" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss</h2>
                <span class="published-time">Published: 2026-02-02T18:59:42.000Z</span>
                
                <p class="summary">Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>PixelGen</span><span>Pixel Diffusion</span><span>Latent Diffusion</span><span>Perceptual Loss</span><span>Image Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Deep Learning</span><span>Computer Vision</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.02493" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</h2>
                <span class="published-time">Published: 2026-01-29T17:58:40.000Z</span>
                
                <p class="summary">Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by "reasoning-then-tool-call" for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multimodal Large Language Models</span><span>Deep Research</span><span>Multimodal Search</span><span>Reinforcement Learning</span><span>Tool Calling</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Large Language Model</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.22060" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics</h2>
                <span class="published-time">Published: 2026-02-02T17:04:36.000Z</span>
                
                <p class="summary">Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Model Steering</span><span>Parameter Dynamics</span><span>Preference-Utility Analysis</span><span>Activation Manifold</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.02343" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>