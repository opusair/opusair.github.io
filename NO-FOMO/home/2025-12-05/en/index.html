<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-05</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-12-05</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Show HN: SerpApi MCP Server</h2>
                <span class="published-time">Published: 2025-12-05 18:30:06</span>
                
                <p class="summary">The Hacker News story introduces the 'SerpApi MCP Server,' a new project from SerpApi, a company recognized for its robust real-time search engine results APIs. The accompanying GitHub repository confirms 'MCP' stands for 'Multi-Cloud Proxy,' indicating this server is a critical infrastructure component. SerpApi's core operations involve complex web scraping and large-scale data extraction, demanding sophisticated proxy management, distributed architectures, and efficient request handling across diverse cloud environments. The MCP Server likely plays a pivotal role in optimizing these aspects, providing a robust solution for managing proxies across multiple cloud providers to ensure reliable and scalable data acquisition. This 'Show HN' initiative suggests an open-source contribution, offering valuable insights into the technical solutions employed by leading data extraction services. The project is particularly relevant for developers, engineers, and organizations focused on building scalable web scraping tools, distributed systems, or exploring the backend mechanics of large-scale data collection. It underscores SerpApi's commitment to technological advancement and community sharing in the domain of web data acquisition, indirectly supporting the data demands of AI and Machine Learning applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Web Scraping</span><span>Proxy Management</span><span>Distributed Systems</span><span>API</span><span>Cloud Infrastructure</span><span>Open Source</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/serpapi/serpapi-mcp" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Jony Ive's OpenAI Device Barred from Using 'Io' Name</h2>
                <span class="published-time">Published: 2025-12-05 16:48:44</span>
                
                <p class="summary">A collaborative project between renowned designer Jony Ive and artificial intelligence powerhouse OpenAI is reportedly facing a branding challenge, as their forthcoming device has been legally barred from utilizing the name 'Io'. This development suggests a potential conflict over intellectual property, trademark infringement, or a similar naming dispute, which could impact the product's market entry and branding strategy. Jony Ive, celebrated for his iconic industrial designs during his tenure at Apple, is venturing into new hardware territory with OpenAI, a leader in advanced AI research and development. This partnership is anticipated to produce an innovative AI-powered hardware device, aiming to blend cutting-edge artificial intelligence with sophisticated design and user experience. The prohibition from using the 'Io' designation underscores the intricate legal and commercial hurdles involved in launching high-profile technology products. This situation may necessitate a rebranding effort, potentially causing delays in the device's official unveiling or market availability, and highlights the broader complexities surrounding intellectual property in the rapidly evolving tech landscape.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Hardware</span><span>Product Design</span><span>OpenAI</span><span>Branding</span><span>Intellectual Property</span><span>Tech Innovation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.macrumors.com/2025/12/05/openai-device-barred-from-io-name/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Gemini 3 Pro: the frontier of vision AI</h2>
                <span class="published-time">Published: 2025-12-05 16:15:10</span>
                
                <p class="summary">Google's latest release, Gemini 3 Pro, is positioned as a significant advancement at the forefront of vision AI, pushing the boundaries of what artificial intelligence can achieve in understanding and processing visual information. This iteration of the Gemini model family is expected to feature enhanced capabilities in areas such as object recognition, image analysis, video comprehension, and multimodal reasoning, integrating seamlessly with other data types. The announcement likely details architectural improvements and new techniques that enable more robust and nuanced interpretations of complex visual data, aiming to set new industry standards. Developers are anticipated to gain access to powerful tools for building next-generation applications in diverse fields, from augmented reality to automated inspection systems, leveraging Gemini 3 Pro's advanced visual intelligence to unlock novel functionalities and drive innovation across the AI landscape. The release underscores Google's ongoing commitment to leading AI research and development.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini 3 Pro</span><span>Vision AI</span><span>Multimodal AI</span><span>Computer Vision</span><span>Deep Learning</span><span>AI Research</span><span>Google AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Computer Vision</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://blog.google/technology/developers/gemini-3-pro-vision/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>NeurIPS 2025 Best Paper Awards</h2>
                <span class="published-time">Published: 2025-12-05 01:15:42</span>
                
                <p class="summary">The NeurIPS 2025 organizing committee has officially announced the recipients of its prestigious Best Paper Awards, recognizing groundbreaking research presented at the conference. These annual accolades celebrate exceptional contributions that significantly push the boundaries of machine learning and artificial intelligence, encompassing a broad spectrum of topics from fundamental theoretical advancements to innovative practical applications. The selected papers are meticulously chosen based on stringent criteria, including their originality, technical depth, empirical significance, clarity of presentation, and potential transformative impact on future research directions within the AI community. This year's honorees are anticipated to feature leading-edge work across diverse domains, such as novel deep learning architectures, advanced reinforcement learning algorithms, critical ethical AI considerations, and the development of more robust, efficient, and interpretable intelligent systems. This highly anticipated announcement underscores the vibrant and rapidly evolving research landscape within the global AI community, acknowledging the profound efforts of researchers who are consistently shaping the future of intelligent systems and driving technological progress in the field. These awards serve as a crucial benchmark for the most impactful and influential studies emerging from one of the premier conferences dedicated to neural information processing systems.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>NeurIPS</span><span>Machine Learning</span><span>AI Research</span><span>Best Paper Awards</span><span>Deep Learning</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>Next AI Draw.io</h2>
                <span class="published-time">Published: 2025-12-05T14:45:57Z</span>
                
                <p class="summary">Next AI Draw.io is a cutting-edge Next.js web application that seamlessly integrates advanced AI capabilities with draw.io diagrams, enabling users to create, modify, and enhance complex visualizations through natural language commands. Leveraging Large Language Models (LLMs), the platform supports direct diagram manipulation, image-based replication, and real-time refinement via an interactive chat interface. Key features include comprehensive diagram version history, specialized support for generating AWS, GCP, and Azure architecture diagrams with appropriate icons, and dynamic animated connectors for improved visualization. The application is built with Next.js, Vercel AI SDK, and react-drawio, representing diagrams as modifiable XML. It boasts multi-provider AI support, including AWS Bedrock, OpenAI, Anthropic, Google AI, and DeepSeek, offering flexibility for various LLM backends. Next AI Draw.io streamlines the diagramming process, making it accessible and efficient for technical and non-technical users alike, particularly valuable for documenting system architectures and general visual communication.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI-Powered Diagramming</span><span>Large Language Model</span><span>Draw.io</span><span>Next.js</span><span>Architecture Diagrams</span><span>Natural Language Processing</span><span>Generative AI</span><span>Vercel AI SDK</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/DayuanJiang/next-ai-draw-io" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>SIMA 2: A Generalist Embodied Agent for Virtual Worlds</h2>
                <span class="published-time">Published: 2025-12-04T13:46:11.000Z</span>
                
                <p class="summary">We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Embodied Agent</span><span>Generalist Agent</span><span>Virtual Worlds</span><span>Foundation Model</span><span>Self-improvement</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Multimodal</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.04797" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction</h2>
                <span class="published-time">Published: 2025-12-04T16:57:02.000Z</span>
                
                <p class="summary">The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Autonomous Agents</span><span>Agentic Tasks</span><span>Scalable Infrastructure</span><span>Interaction Signals</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.04987" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>TV2TV: A Unified Framework for Interleaved Language and Video Generation</h2>
                <span class="published-time">Published: 2025-12-04T18:59:09.000Z</span>
                
                <p class="summary">Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before "acting in pixels" to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Interleaved Language and Video</span><span>Generative AI</span><span>Mixture-of-Transformers</span><span>Video Generation</span><span>Prompt Alignment</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.05103" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</h2>
                <span class="published-time">Published: 2025-12-04T18:59:52.000Z</span>
                
                <p class="summary">Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an Agentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reward Models</span><span>Multimodal</span><span>AI Agent</span><span>Tool Use</span><span>Visual Reasoning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>AI Agent</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.05111" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</h2>
                <span class="published-time">Published: 2025-12-04T18:59:53.000Z</span>
                
                <p class="summary">Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Text-to-Image Generation</span><span>Chain-of-Thought</span><span>Multimodal Large Language Models</span><span>Generative AI</span><span>Rare Concept Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.05112" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs</h2>
                <span class="published-time">Published: 2025-12-04T12:35:10.000Z</span>
                
                <p class="summary">Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Low-bit Quantization</span><span>Post-Training Quantization</span><span>SignRoundV2</span><span>Quantization Scales</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.04746" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>