<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-10-28</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-10-28</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Using AI to negotiate a $195k hospital bill down to $33k</h2>
                <span class="published-time">Published: 2025-10-28 15:58:58</span>
                
                <p class="summary">An individual successfully leveraged artificial intelligence to dramatically reduce a substantial hospital bill, negotiating an initial charge of $195,000 down to $33,000. This remarkable outcome showcases the advanced capabilities of AI in navigating complex and often challenging financial negotiation processes within the healthcare sector. The application highlights AI's potential to serve as a powerful tool for consumer advocacy, automating the intricate steps involved in disputing and subsequently lowering medical expenses. This specific instance illustrates how AI-driven systems can effectively analyze detailed billing statements, identify potential overcharges or opportunities for reduction, and formulate persuasive arguments for negotiation. Such technologies are poised to revolutionize how individuals manage their healthcare finances, offering a new frontier in cost containment and promoting greater transparency and fairness in medical billing practices. The success of this AI-led negotiation also points towards a future where automated personal financial agents could routinely handle complex administrative tasks, providing substantial financial relief and efficiency gains for everyday consumers.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Negotiation</span><span>Healthcare Automation</span><span>Cost Reduction</span><span>AI Agents</span><span>Natural Language Processing</span><span>Financial AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.threads.com/@nthmonkey/post/DQVdAD1gHhw" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>EuroLLM: LLM made in Europe built to support all 24 official EU languages</h2>
                <span class="published-time">Published: 2025-10-28 14:58:04</span>
                
                <p class="summary">EuroLLM introduces a significant initiative to develop a Large Language Model specifically tailored for the European context, designed to support all 24 official languages of the European Union. This project aims to address the linguistic diversity challenges inherent in Europe, fostering inclusivity and accessibility in AI technologies. By focusing on a comprehensive multilingual approach, EuroLLM seeks to overcome limitations often observed in models primarily developed for English and a few other major languages. The model's development in Europe signifies a strategic move towards digital sovereignty and localized AI solutions, ensuring that the nuances and specificities of European cultures and languages are adequately represented. This initiative could pave the way for more effective and culturally relevant AI applications across the EU, ranging from public services to commercial tools, while also promoting European technological independence in the rapidly evolving AI landscape. The emphasis on all official languages makes it a unique and ambitious endeavor in the global LLM ecosystem, promising to enhance communication and innovation within the European Union.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>Multilingual AI</span><span>European Union</span><span>Natural Language Processing</span><span>Digital Sovereignty</span><span>AI Development</span><span>Linguistic Diversity</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://eurollm.io/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Our LLM-controlled office robot can't pass butter</h2>
                <span class="published-time">Published: 2025-10-28 14:13:25</span>
                
                <p class="summary">Andon Labs, an AI evaluation startup, is conducting real-world assessments of Large Language Models (LLMs) in controlling office robots, building on their previous work with LLMs managing vending machines. Their initiative involves deploying LLM-controlled robots in an office environment to evaluate their utility in helpful tasks, alongside a systematic benchmarking process for different LLMs on specific robotic assignments. This research is detailed in their paper, "Butter-Bench," available on arXiv. Additionally, a blog post and leaderboard on the Andon Labs website provide comparative performance results for various LLMs. The project humorously acknowledges current practical limitations, as highlighted by the title "Our LLM-controlled office robot can't pass butter," indicating that significant challenges remain in achieving robust, general-purpose robotic assistance powered by LLMs. The evaluations aim to identify both the potential and existing shortcomings of AI in complex physical interactions.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Robotics</span><span>AI Evaluation</span><span>Benchmarking</span><span>AI Agent</span><span>Robot Control</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://andonlabs.com/evals/butter-bench" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Poker Tournament for LLMs</h2>
                <span class="published-time">Published: 2025-10-28 07:42:18</span>
                
                <p class="summary">PokerBattle.AI has unveiled an innovative 'Poker Tournament for LLMs,' an event specifically crafted to evaluate and benchmark the strategic capabilities and advanced decision-making processes of Large Language Models (LLMs) within the dynamic and imperfect information environment of a poker game. This unique competition is poised to serve as a critical testbed for AI agent development, challenging participants to demonstrate sophisticated reasoning, probabilistic inference, and adaptive strategy against diverse opponents, including handling bluffing and incomplete information. Beyond conventional Natural Language Processing benchmarks, the tournament aims to illuminate LLMs' capacity for complex strategic gameplay and their ability to emulate human-like interaction under competitive pressure. The insights garnered from this initiative are expected to significantly contribute to the advancement of AI agents, particularly in areas requiring nuanced strategic thinking, risk assessment, and adaptability in uncertain, adversarial contexts, thereby pushing the frontiers of artificial intelligence in game theory applications and real-world problem-solving.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>AI Agents</span><span>Game Theory</span><span>Strategic Reasoning</span><span>AI Evaluation</span><span>Benchmarking</span><span>Decision Making</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://pokerbattle.ai/event" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>The next chapter of the Microsoft‚ÄìOpenAI partnership</h2>
                <span class="published-time">Published: 2025-10-28 13:05:40</span>
                
                <p class="summary">The announcement titled 'The next chapter of the Microsoft‚ÄìOpenAI partnership' signals a significant continuation and deepening of the strategic alliance between the two technology giants. This ongoing collaboration is a pivotal force in the artificial intelligence landscape, primarily focused on accelerating breakthroughs in generative AI and large language models. Historically, the partnership has leveraged Microsoft's robust Azure supercomputing infrastructure to power OpenAI's advanced research and model development, including the renowned GPT series. This 'next chapter' is anticipated to involve further integration of OpenAI's cutting-edge AI technologies into Microsoft's expansive suite of products and services, broadening their reach and application across various industries. It likely encompasses continued financial investment from Microsoft, joint efforts in AI ethics and safety, and a shared vision for the future of AI. The alliance aims to push the boundaries of AI capabilities, translating innovative research into practical, scalable solutions while maintaining a competitive edge in the rapidly evolving global AI market, fostering innovation and widespread adoption of AI technologies.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Large Language Models</span><span>AI Partnership</span><span>Cloud Computing</span><span>AI Research</span><span>Technology Integration</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://openai.com/index/next-chapter-of-microsoft-openai-partnership/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Dexto ‚Äì Connect your AI Agents with real-world tools and data</h2>
                <span class="published-time">Published: 2025-10-28 16:07:27</span>
                
                <p class="summary">Dexto, a new runtime and orchestration layer developed by Truffle AI (YC W25), has been launched to enhance the capabilities of AI Agents by seamlessly integrating them with real-world tools, applications, and data sources. This innovative platform allows any existing app or service to be transformed into an intelligent AI assistant, equipped with the ability to reason, think, and act autonomously. The genesis of Dexto stemmed from the team's experience in deploying AI agents for various marketing tasks, including LinkedIn posts, Reddit data retrieval, and ad creative generation. They identified that the primary bottleneck was not the Large Language Models themselves, but rather the cumbersome and repetitive orchestration required to connect LLMs with external tools, manage context and data persistence, implement memory functions, facilitate approval flows, and customize agent behaviors for diverse client needs. Dexto aims to alleviate these significant operational overheads, thereby accelerating the deployment and scalability of sophisticated, tool-augmented AI agents.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Orchestration</span><span>Runtime Environment</span><span>Large Language Models</span><span>Tool Integration</span><span>Context Management</span><span>AI Assistant</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/truffle-ai/dexto" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>AFFiNE.ProWrite, Draw and Plan All at Once</h2>
                <span class="published-time">Published: 2025-10-28T17:27:54Z</span>
                
                <p class="summary">AFFiNE.Pro is a cutting-edge, open-source, local-first, and privacy-focused all-in-one workspace designed to be a comprehensive alternative to popular tools like Notion and Miro. It brilliantly merges document editing and whiteboard functionalities into a single, hyper-fused canvas, empowering users to seamlessly integrate diverse building blocks‚Äîincluding rich text, sticky notes, embedded web pages, multi-view databases, linked pages, and shapes‚Äîonto an edgeless surface. A standout feature is its multimodal AI partner, AFFiNE AI, which significantly boosts productivity by assisting with tasks such as generating professional reports, creating presentable slides from outlines, summarizing articles into structured mind maps, and even prototyping applications directly from prompts. The platform prioritizes user data ownership through its local-first architecture, while simultaneously supporting real-time synchronization and collaboration across various web and cross-platform clients. Furthermore, AFFiNE offers extensive flexibility, allowing users to self-host, fork, and customize the environment, with a vibrant plugin community and third-party blocks developed on Blocksuite.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Open-source</span><span>Collaborative Workspace</span><span>Knowledge Management</span><span>AI Assistant</span><span>Canvas Editor</span><span>Self-hosting</span><span>Productivity Tool</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Multimodal</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/toeverything/AFFiNE" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent Lightning‚ö°</h2>
                <span class="published-time">Published: 2025-10-28T06:58:41Z</span>
                
                <p class="summary">Agent Lightning‚ö° is an innovative training framework designed to optimize AI agents with minimal code alterations, supporting integration with any agent framework, including LangChain, OpenAI Agent SDK, AutoGen, and CrewAI. It offers selective optimization within multi-agent systems and incorporates advanced algorithms such as Reinforcement Learning, Automatic Prompt Optimization, and Supervised Fine-tuning. The framework's architecture features a lightweight tracer that captures events like prompts and tool calls, feeding them into a central LightningStore. This store harmonizes tasks, resources, and traces, enabling chosen algorithms to learn from these interactions and refine resources like prompt templates or policy weights. The Trainer component orchestrates data flow and updates the inference engine, ensuring continuous improvement from initial rollout without requiring agent rewrites or framework lock-in. This versatile solution is applicable across diverse scenarios, from enhancing SQL self-correction capabilities to training agents for complex game environments.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Reinforcement Learning</span><span>Prompt Optimization</span><span>Multi-Agent Systems</span><span>Machine Learning</span><span>Agent Frameworks</span><span>Training Framework</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/microsoft/agent-lightning" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>ReCode: Unify Plan and Action for Universal Granularity Control</h2>
                <span class="published-time">Published: 2025-10-27T17:35:15.000Z</span>
                
                <p class="summary">Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>ReCode</span><span>AI Agent</span><span>Large Language Models</span><span>Recursive Code Generation</span><span>Granularity Control</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.23564" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>A Survey of Data Agents: Emerging Paradigm or Overstated Hype?</h2>
                <span class="published-time">Published: 2025-10-27T17:54:07.000Z</span>
                
                <p class="summary">The rapid advancement of large language models (LLMs) has spurred the emergence of data agents--autonomous systems designed to orchestrate Data + AI ecosystems for tackling complex data-related tasks. However, the term "data agent" currently suffers from terminological ambiguity and inconsistent adoption, conflating simple query responders with sophisticated autonomous architectures. This terminological ambiguity fosters mismatched user expectations, accountability challenges, and barriers to industry growth. Inspired by the SAE J3016 standard for driving automation, this survey introduces the first systematic hierarchical taxonomy for data agents, comprising six levels that delineate and trace progressive shifts in autonomy, from manual operations (L0) to a vision of generative, fully autonomous data agents (L5), thereby clarifying capability boundaries and responsibility allocation. Through this lens, we offer a structured review of existing research arranged by increasing autonomy, encompassing specialized data agents for data management, preparation, and analysis, alongside emerging efforts toward versatile, comprehensive systems with enhanced autonomy. We further analyze critical evolutionary leaps and technical gaps for advancing data agents, especially the ongoing L2-to-L3 transition, where data agents evolve from procedural execution to autonomous orchestration. Finally, we conclude with a forward-looking roadmap, envisioning the advent of proactive, generative data agents.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Data Agents</span><span>Large Language Models</span><span>Autonomy</span><span>Taxonomy</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.23587" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Open Multimodal Retrieval-Augmented Factual Image Generation</h2>
                <span class="published-time">Published: 2025-10-26T04:13:31.000Z</span>
                
                <p class="summary">Large Multimodal Models (LMMs) have achieved remarkable progress ingenerating photorealistic and prompt-aligned images, but they often produceoutputs that contradict verifiable knowledge, especially when prompts involvefine-grained attributes or time-sensitive events. Conventionalretrieval-augmented approaches attempt to address this issue by introducingexternal information, yet they are fundamentally incapable of groundinggeneration in accurate and evolving knowledge due to their reliance on staticsources and shallow evidence integration. To bridge this gap, we introduceORIG, an agentic open multimodal retrieval-augmented framework for FactualImage Generation (FIG), a new task that requires both visual realism andfactual grounding. ORIG iteratively retrieves and filters multimodal evidencefrom the web and incrementally integrates the refined knowledge into enrichedprompts to guide generation. To support systematic evaluation, we buildFIG-Eval, a benchmark spanning ten categories across perceptual, compositional,and temporal dimensions. Experiments demonstrate that ORIG substantiallyimproves factual consistency and overall image quality over strong baselines,highlighting the potential of open multimodal retrieval for factual imagegeneration.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Factual Image Generation</span><span>Large Multimodal Models</span><span>Retrieval-Augmented Generation</span><span>Open Multimodal Retrieval</span><span>AI Agent</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.22521" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation</h2>
                <span class="published-time">Published: 2025-10-27T14:47:30.000Z</span>
                
                <p class="summary">The application of Reinforcement Learning with Verifiable Rewards (RLVR) to mathematical and coding domains has demonstrated significant improvements in the reasoning and problem-solving abilities of Large Language Models. Despite its success in single generation problem solving, the reinforcement learning fine-tuning process may harm the model's exploration ability, as reflected in decreased diversity of generations and a resulting degradation of performance during Best-of-N sampling for large N values. In this work, we focus on optimizing the max@k metric, a continuous generalization of pass@k. We derive an unbiased on-policy gradient estimate for direct optimization of this metric. Furthermore, we extend our derivations to the off-policy updates, a common element in modern RLVR algorithms, that allows better sample efficiency. Empirically, we show that our objective effectively optimizes max@k metric in off-policy scenarios, aligning the model with the Best-of-N inference strategy.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reinforcement Learning</span><span>Large Language Models</span><span>Best-of-N Sampling</span><span>max@k Optimization</span><span>Off-policy Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.23393" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LongCat-Video Technical Report</h2>
                <span class="published-time">Published: 2025-10-25T07:41:02.000Z</span>
                
                <p class="summary">Video generation is a critical pathway toward world models, with efficientlong video inference as a key capability. Toward this end, we introduceLongCat-Video, a foundational video generation model with 13.6B parameters,delivering strong performance across multiple video generation tasks. Itparticularly excels in efficient and high-quality long video generation,representing our first step toward world models. Key features include: Unifiedarchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)framework, LongCat-Video supports Text-to-Video, Image-to-Video, andVideo-Continuation tasks with a single model; Long video generation:Pretraining on Video-Continuation tasks enables LongCat-Video to maintain highquality and temporal coherence in the generation of minutes-long videos;Efficient inference: LongCat-Video generates 720p, 30fps videos within minutesby employing a coarse-to-fine generation strategy along both the temporal andspatial axes. Block Sparse Attention further enhances efficiency, particularlyat high resolutions; Strong performance with multi-reward RLHF: Multi-rewardRLHF training enables LongCat-Video to achieve performance on par with thelatest closed-source and leading open-source models. Code and model weights arepublicly available to accelerate progress in the field.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video generation</span><span>LongCat-Video</span><span>Diffusion Transformer</span><span>Long video inference</span><span>World models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Video Understanding</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.22200" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling</h2>
                <span class="published-time">Published: 2025-10-27T17:59:51.000Z</span>
                
                <p class="summary">Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>3D/4D Generation</span><span>Subject-driven Generation</span><span>Personalization</span><span>Texture Infilling</span><span>Video Tracking</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Generative AI</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.23605" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>