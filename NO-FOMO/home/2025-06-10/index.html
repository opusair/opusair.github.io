<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI 日报 - 2025-06-10</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI 日报</h1>
            <p class="date">2025-06-10</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">🏠 返回主页</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">📅 最新日报</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">👤 关于我们</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Twitter</h2>

            <article class="item-card">
                <h2>LangChainAI_Uber利用LangGraph构建AI开发代理实现代码修复</h2>
                <span class="published-time">Published: 2025-06-10T17:41:32.000Z</span>
                <img src="screenshot/twitter/LangChainAI_1932493346498543898.png" alt="LangChainAI_Uber利用LangGraph构建AI开发代理实现代码修复">
                <p class="summary">Uber成功利用LangGraph构建了AI开发代理，这些代理每天能生成数千个代码修复方案，为5000名开发者节省了超过21,000小时的工作时间。该方案有效服务于处理数亿行代码的庞大开发组织，展示了AI在提升软件开发效率方面的巨大潜力。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>LangGraph</span><span>AI开发代理</span><span>代码修复</span><span>Uber</span><span>效率提升</span><span>软件开发</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>智能体</span><span>技术动态</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/LangChainAI/status/1932493346498543898" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>juliewdesign__探索Google Veo 3视频生成：文本到视频的角色一致性挑战与实践</h2>
                <span class="published-time">Published: 2025-06-10T13:49:41.000Z</span>
                <img src="screenshot/twitter/juliewdesign__1932608957945950407.png" alt="juliewdesign__探索Google Veo 3视频生成：文本到视频的角色一致性挑战与实践">
                <p class="summary">Julie W. Design分享了其在使用Google Veo 3进行文本到视频（T2V）生成时，如何实现角色和情绪一致性的实践经验。她指出，此前在文本到图像（如Midjourney）中花费大量时间优化提示词以保持角色连贯性，现在正将类似方法应用于T2V，旨在仅通过文本提示词最大限度地提升视觉连续性。此举旨在探索生成式AI在视频内容创作中保持视觉统一性的潜力。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>Google Veo 3</span><span>文本到视频</span><span>生成式AI</span><span>视觉一致性</span><span>提示工程</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>计算机视觉</span><span>技术动态</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/juliewdesign_/status/1932608957945950407" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>qx_dong_发布强化预训练RPT：将下一词预测重构为推理任务</h2>
                <span class="published-time">Published: 2025-06-10T02:49:51.000Z</span>
                <img src="screenshot/twitter/qx_dong_1932454923322450186.png" alt="qx_dong_发布强化预训练RPT：将下一词预测重构为推理任务">
                <p class="summary">qx_dong团队近日发布了名为“强化预训练”（Reinforcement Pre-Training, RPT）的新方法。该方法创新性地将传统的下一词预测任务重构为一项推理任务，并结合RLVR技术实现通用目的推理。RPT支持在网络语料库上进行可扩展的强化学习，旨在提升预训练模型的性能，并优化计算资源分配，使其能更高效地聚焦于特定token的处理，从而有望在语言模型推理能力上取得显著进展。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>强化预训练</span><span>RPT</span><span>下一词预测</span><span>推理任务</span><span>强化学习</span><span>大模型预训练</span></div>
                    <div class="area"><span class="label">区域：</span><span>自然语言处理</span><span>大模型</span><span>研究进展</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/qx_dong/status/1932454923322450186" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>jerryjliu0_LlamaIndex代理转MCP服务器实现智能工具部署</h2>
                <span class="published-time">Published: 2025-06-10T16:18:44.000Z</span>
                <img src="screenshot/twitter/jerryjliu0_1932473411152064742.png" alt="jerryjliu0_LlamaIndex代理转MCP服务器实现智能工具部署">
                <p class="summary">Jerry Liu转发LlamaIndex推文，展示如何将LlamaIndex代理转换为MCP服务器，从而实现更高级的智能工具部署。推文强调了将现有工作流升级为可被Claude、Cursor等MCP客户端调用的代理工具的重要性。通过一个演示，详细介绍了如何将用于从复杂多基金PDF中提取结构化数据的FidelityFundExtraction工作流部署为MCP服务器，并成功通过Claude进行调用。此举简化了代理工具的开发与集成，标志着代理UI演进的关键一步。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>LlamaIndex</span><span>MCP</span><span>智能体</span><span>代理工具</span><span>数据提取</span><span>Claude</span></div>
                    <div class="area"><span class="label">区域：</span><span>智能体</span><span>技术动态</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/jerryjliu0/status/1932473411152064742" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MistralAI_发布首个推理模型Magistral，聚焦领域、透明与多语言推理</h2>
                <span class="published-time">Published: 2025-06-10T14:15:33.000Z</span>
                <img src="screenshot/twitter/MistralAI_1932445269611688315.png" alt="MistralAI_发布首个推理模型Magistral，聚焦领域、透明与多语言推理">
                <p class="summary">Mistral AI正式发布其首个推理模型Magistral。该模型旨在专注于领域特定、透明化以及多语言推理能力，标志着Mistral AI在AI推理领域迈出重要一步。Magistral的推出有望提升AI在复杂场景下的逻辑分析和决策能力，为特定行业应用提供更高效、可靠的解决方案。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>Mistral AI</span><span>Magistral</span><span>推理模型</span><span>领域特定</span><span>多语言</span><span>人工智能</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>产品发布</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/MistralAI/status/1932445269611688315" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAIDevs_o3-pro模型全面推出及性能揭秘</h2>
                <span class="published-time">Published: 2025-06-10T20:10:36.000Z</span>
                <img src="screenshot/twitter/OpenAIDevs_1932538094168801492.png" alt="OpenAIDevs_o3-pro模型全面推出及性能揭秘">
                <p class="summary">OpenAI宣布其o3-pro模型已面向所有ChatGPT Pro用户及API全面推出。该模型经测试显示，相比o1-pro，其成本更低、速度更快且精度更高。特别是在物理模拟方面，o3-pro是首个能几乎完美处理球体与墙壁间真实碰撞的模型，预示着AI在复杂物理交互领域取得显著进展。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>o3-pro</span><span>OpenAI</span><span>模型发布</span><span>性能提升</span><span>物理模拟</span><span>API</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>产品发布</span><span>技术动态</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/OpenAIDevs/status/1932538094168801492" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>李飞飞团队新作：DiT不训练直接改架构，模型深度减半，质量还提高了</h2>
                <span class="published-time">Published: 2025-06-10T16:06:12.000Z</span>
                <img src="screenshot/wechat/wechat_image_KoghoE6po2gw8HTBxczT9A.png" alt="李飞飞团队新作：DiT不训练直接改架构，模型深度减半，质量还提高了">
                <p class="summary">李飞飞团队提出“嫁接”技术，旨在通过编辑预训练Diffusion Transformer（DiT）模型架构，在不从头训练的情况下探索新的高效设计。该方法包含激活蒸馏和轻量级调优两阶段，能有效替换模型算子或重构架构。研究表明，嫁接技术可在小计算预算下构建高质量混合架构，如将DiT-XL/2模型深度减半，质量仍有提升；应用于文本到图像模型时，可实现1.43倍加速且生成质量损失极小。此创新为大模型架构探索提供了低成本、高效率的新范式。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>嫁接</span><span>DiffusionTransformer</span><span>模型架构</span><span>预训练模型</span><span>生成模型</span><span>激活蒸馏</span><span>文本到图像</span><span>计算预算</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>深度学习</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/KoghoE6po2gw8HTBxczT9A" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LLM 自回归做理解，MaskGIT 方案做生成！VILA‑U：一个模型搞定图像理解、视频理解和生成，简洁而强大</h2>
                <span class="published-time">Published: 2025-06-10T16:06:12.000Z</span>
                <img src="screenshot/wechat/wechat_image_XvNpHmrvzYV40CgGD_MVBQ.png" alt="LLM 自回归做理解，MaskGIT 方案做生成！VILA‑U：一个模型搞定图像理解、视频理解和生成，简洁而强大">
                <p class="summary">VILA-U是一款由清华大学、MIT及NVIDIA团队提出的生成理解统一模型，旨在通过单一架构实现图像文本理解、视频文本理解、图像生成及视频生成。该模型核心在于其“统一基础视觉塔”架构，该视觉编码器通过结合图像重建损失与图文对比损失进行训练，并采用统一的Next-Token Prediction训练模式，有效解决了传统方法中理解与生成任务分离的复杂性。VILA-U创新性地利用CLIP预训练权重初始化视觉编码器，克服了高级语义与低级外观特征学习的冲突，使其在视觉理解和生成任务上均展现出竞争力，尤其在离散视觉token下仍能接近领先的VLM性能，并在生成质量上可与部分扩散模型媲美，为多模态AI发展提供了简洁而强大的新范式。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>VILA-U</span><span>统一模型</span><span>多模态</span><span>图像生成</span><span>视频理解</span><span>自回归</span><span>视觉塔</span><span>Next-TokenPrediction</span></div>
                    <div class="area"><span class="label">区域：</span><span>多模态</span><span>生成式AI</span><span>视频理解</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/XvNpHmrvzYV40CgGD_MVBQ" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>北大伯克利联手“拷问”大模型：最强Agent也才40分！新基准专治“不听话”的AI分析师</h2>
                <span class="published-time">Published: 2025-06-10T05:16:42.000Z</span>
                <img src="screenshot/wechat/wechat_image_0DVNQbifWQ5nWHKPNSwwHA.png" alt="北大伯克利联手“拷问”大模型：最强Agent也才40分！新基准专治“不听话”的AI分析师">
                <p class="summary">北京大学与加州大学伯克利分校联合推出全新基准IDA-Bench，旨在模拟真实世界中迭代、交互式的数据分析场景，以评估大模型智能体（Agent）的实际能力。测试结果显示，即便顶尖模型如Claude-3.7和Gemini-2.5 Pro，在面对多轮动态指令时，任务成功率最高仅为40%。研究发现，现有Agent在遵循指令和自主推理之间难以平衡，常表现出“过度自信”或“过度谨慎”的“性格缺陷”，导致任务失败或产生“幻觉”。这表明当前大模型智能体在理解、遵循和交互能力上仍存在显著不足，距离成为可靠的数据分析助手尚有较大差距。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>IDA-Bench</span><span>大模型</span><span>智能体</span><span>数据分析</span><span>评估基准</span><span>多轮交互</span><span>指令遵循</span><span>幻觉</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>智能体</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/0DVNQbifWQ5nWHKPNSwwHA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SOTA级视频编辑新方法：无需训练一句话编辑视频，背景保持100%</h2>
                <span class="published-time">Published: 2025-06-10T05:16:42.000Z</span>
                <img src="screenshot/wechat/wechat_image_Dsy3bQacYXLRqAGvAhrCcw.png" alt="SOTA级视频编辑新方法：无需训练一句话编辑视频，背景保持100%">
                <p class="summary">西湖大学AGILab提出SOTA级视频编辑新方法FlowDirector，旨在解决传统反演-去噪范式导致的视频运动不连贯、意外变化等问题。FlowDirector摒弃反演和训练，通过构建“源视频→目标视频”的直接编辑路径，并引入空间感知流矫正（SAFC）以限制编辑区域，以及差分平均引导（DAG）来抑制原始信息干扰，确保背景保真度与编辑精度。该方法在定性与定量实验中均表现出色，尤其在对象形变、文本一致性及运动流畅度方面超越现有SOTA方法，实现了高质量、高效率的视频编辑，为AI视频创作带来新突破。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>FlowDirector</span><span>视频编辑</span><span>无需训练</span><span>反演去噪</span><span>空间感知流矫正</span><span>差分平均引导</span><span>SOTA</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>计算机视觉</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/Dsy3bQacYXLRqAGvAhrCcw" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>让AI自己设计芯片！中国科学院发布「启蒙」，芯片全流程自动设计</h2>
                <span class="published-time">Published: 2025-06-10T04:06:16.000Z</span>
                <img src="screenshot/wechat/wechat_image_cNGAFt0FxbI0_WPwYZgEmA.png" alt="让AI自己设计芯片！中国科学院发布「启蒙」，芯片全流程自动设计">
                <p class="summary">中国科学院计算技术研究所联合软件研究所，发布了基于大模型等AI技术的处理器芯片及基础软件全自动设计系统“启蒙”。该系统旨在变革传统芯片设计范式，解决其耗时、高成本及软件适配难题。“启蒙”能够全自动完成芯片软硬件设计各环节，包括RISC-V CPU设计、操作系统配置、程序转译及高性能库生成，性能已达到或超越人类专家水平。该系统通过处理器芯片领域大模型、芯片与软件智能体及反馈式推理机制，实现从功能需求到芯片软硬件的端到端自动化设计，有望大幅提升设计效率，缩短周期，并支持快速定制化，推动芯片产业智能化发展。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>启蒙</span><span>芯片设计</span><span>大模型</span><span>自动化</span><span>处理器</span><span>智能体</span><span>RISC-V</span><span>软硬件协同</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/cNGAFt0FxbI0_WPwYZgEmA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>一块4090搞定实时视频生成！Adobe黑科技来了</h2>
                <span class="published-time">Published: 2025-06-10T03:59:40.000Z</span>
                <img src="screenshot/wechat/wechat_image_G--WIqw9wMlkMkIVgFJXug.png" alt="一块4090搞定实时视频生成！Adobe黑科技来了">
                <p class="summary">Adobe联合德克萨斯大学奥斯汀分校提出“Self Forcing”算法，旨在解决自回归视频生成中的暴露偏差问题，实现实时、高质量视频生成。该算法通过在训练期间显式展开自回归生成过程，弥合训练与测试分布差距，并引入滚动KV缓存机制，显著提升效率。实验证明，Self Forcing模型能在单块H100 GPU上以17 FPS、亚秒级延迟生成480p准高清视频，且生成质量优于现有模型，为游戏直播、虚拟世界模拟等实时交互应用带来突破性进展，有望击穿实时渲染门槛。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>SelfForcing</span><span>视频生成</span><span>自回归模型</span><span>实时渲染</span><span>暴露偏差</span><span>扩散模型</span><span>KV缓存</span><span>Adobe</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>深度学习</span><span>计算机视觉</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/G--WIqw9wMlkMkIVgFJXug" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>Prompt Engineering Guide</h2>
                <span class="published-time">Published: 2025-06-09T14:29:31+00:00</span>
                <img src="screenshot/github/Prompt-Engineering-Guide.png" alt="Prompt Engineering Guide">
                <p class="summary">Prompt Engineering Guide是一个全面的资源库，旨在帮助开发者和研究人员掌握提示工程技术，以高效利用大型语言模型（LLMs）。它涵盖了从基础概念、高级技巧到实际应用、风险分析等多个方面，并提供了最新的论文、学习指南、讲座和工具，是LLM应用开发的宝贵参考。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>提示工程</span><span>大型语言模型</span><span>LLM应用开发</span><span>自然语言处理</span><span>生成式AI</span><span>检索增强生成</span><span>链式思考</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/dair-ai/Prompt-Engineering-Guide" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>开源大模型食用指南</h2>
                <span class="published-time">Published: 2025-06-09T14:27:09Z</span>
                <img src="https://contrib.rocks/image?repo=datawhalechina/self-llm" alt="开源大模型食用指南">
                <p class="summary">本项目是面向国内初学者的开源大模型Linux平台教程，提供环境配置、本地部署、高效微调等全流程指导。涵盖主流LLM如LLaMA、ChatGLM、InternLM的部署与应用，以及全量/高效微调方法。旨在简化开源大模型的使用，助力更多学习者融入大模型世界。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>开源大模型</span><span>环境配置</span><span>本地部署</span><span>模型微调</span><span>大语言模型</span><span>LangChain</span><span>Linux</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/datawhalechina/self-llm" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Model Context Protocol servers</h2>
                <span class="published-time">Published: 2025-06-10T22:14:54Z</span>
                <img src="screenshot/github/servers.png" alt="Model Context Protocol servers">
                <p class="summary">该GitHub仓库提供了模型上下文协议（MCP）的参考实现，旨在展示MCP如何为大型语言模型（LLM）提供安全、受控的工具和数据源访问。它包含多种服务器示例，并引用了社区构建的服务器及相关资源，支持TypeScript和Python SDK。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>模型上下文协议</span><span>大语言模型</span><span>AI智能体</span><span>服务器</span><span>SDK</span><span>工具集成</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/modelcontextprotocol/servers" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GenAI Agents: Comprehensive Repository for Development and Implementation 🚀</h2>
                <span class="published-time">Published: 2025-05-14T19:56:00Z</span>
                <img src="https://github.com/NirDiamant/GenAI_Agents/raw/main/images/substack_image.png" alt="GenAI Agents: Comprehensive Repository for Development and Implementation 🚀">
                <p class="summary">该GitHub仓库是GenAI智能体开发与实现的综合资源库，涵盖从基础对话机器人到复杂多智能体系统的教程与实现。它提供了丰富的实践案例，并利用LangChain、LangGraph等主流框架，旨在促进GenAI智能体技术的学习、实验与创新。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>生成式AI</span><span>智能体</span><span>LangChain</span><span>LangGraph</span><span>多智能体系统</span><span>检索增强生成</span><span>提示工程</span><span>AI开发</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>生成式AI</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/NirDiamant/GenAI_Agents" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>GUI-Reflection：赋能多模态GUI模型以自我反思能力</h2>
                <span class="published-time">Published: 2025-06-09T17:59:57.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08012.png" alt="GUI-Reflection：赋能多模态GUI模型以自我反思能力">
                <p class="summary">多模态大语言模型（MLLMs）在革新图形用户界面（GUI）自动化方面展现出巨大潜力。然而，现有GUI模型主要依赖于从几乎无错误的离线轨迹中学习，因此缺乏反思和错误恢复能力。为弥补这一不足，我们提出了GUI-Reflection，一个新颖的框架，通过专门的训练阶段（GUI特定预训练、离线监督微调（SFT）和在线反思调优）将自我反思和错误纠正能力明确地整合到端到端多模态GUI模型中。GUI-Reflection通过全自动数据生成和学习过程实现自我反思行为的涌现，无需任何人工标注。具体而言，1）我们首先提出了可扩展的数据管道，以从现有成功轨迹中自动构建反思和错误纠正数据。鉴于现有GUI模型主要侧重于基础和UI理解能力，我们提出了GUI-Reflection任务套件，以明确学习和评估面向反思的能力。2）此外，我们为移动设备上的GUI模型在线训练和数据收集构建了一个多样化且高效的环境。3）我们还提出了一种利用所提环境的迭代在线反思调优算法，使模型能够持续增强其反思和错误纠正能力。我们的框架赋予GUI智能体自我反思和纠正能力，为更鲁棒、适应性更强、更智能的GUI自动化铺平了道路，所有数据、模型、环境和工具都将公开发布。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>GUI自动化</span><span>多模态大语言模型</span><span>自我反思</span><span>错误纠正</span><span>GUI-Reflection</span></div>
                    <div class="area"><span class="label">区域：</span><span>多模态</span><span>大模型</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08012" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Dreamland：基于模拟器和生成模型的受控世界创建</h2>
                <span class="published-time">Published: 2025-06-09T17:59:52.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08006.png" alt="Dreamland：基于模拟器和生成模型的受控世界创建">
                <p class="summary">大规模视频生成模型能够合成多样化且逼真的视觉内容，用于动态世界创建，但它们通常缺乏元素级的可控性，这阻碍了它们在场景编辑和具身AI智能体训练中的应用。我们提出了Dreamland，一个混合世界生成框架，它结合了基于物理的模拟器的精细控制能力和大规模预训练生成模型的光真实感内容输出。具体而言，我们设计了一种分层世界抽象，将像素级和对象级的语义与几何信息编码为中间表示，以连接模拟器和生成模型。这种方法增强了可控性，通过与真实世界分布的早期对齐最小化了适应成本，并支持现有和未来预训练生成模型的即插即用。我们进一步构建了一个D3Sim数据集，以促进混合生成管道的训练和评估。实验表明，Dreamland在图像质量方面比现有基线提高了50.8%，可控性增强了17.9%，并且在增强具身智能体训练方面具有巨大潜力。代码和数据将公开。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>生成模型</span><span>世界创建</span><span>可控性</span><span>模拟器</span><span>具身智能体</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>计算机视觉</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08006" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>CyberV：视频理解测试时扩展的控制论</h2>
                <span class="published-time">Published: 2025-06-09T17:45:18.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07971.png" alt="CyberV：视频理解测试时扩展的控制论">
                <p class="summary">当前的多模态大语言模型（MLLMs）在理解长视频或复杂视频时可能面临挑战，这主要源于其前馈处理的特性，导致测试时计算需求高、鲁棒性不足以及准确性有限。对于参数量较少的模型，这些局限性可能更为严重。为解决这些问题，我们提出了一种受控制论原理启发的创新框架，将视频MLLMs重新设计为能够在推理过程中进行自我监控、自我校正和动态资源分配的自适应系统。我们的方法CyberV引入了一个控制论循环，该循环由一个MLLM推理系统、一个传感器和一个控制器组成。具体而言，传感器监控MLLM的前向处理过程并收集中间解释（例如注意力漂移），然后控制器决定何时以及如何触发自我校正并生成反馈以指导下一轮处理。这种测试时自适应扩展框架无需重新训练或额外组件即可增强冻结的MLLMs。实验表明，CyberV带来了显著的改进：在VideoMMMU数据集上，CyberV将Qwen2.5-VL-7B的性能提升了8.3%，将InternVL3-8B提升了5.5%，超越了具有竞争力的专有模型GPT-4o。当应用于Qwen2.5-VL-72B时，它带来了10.0%的提升，甚至达到了与人类专家相当的性能。此外，我们的方法在VideoMME和WorldSense等通用基准测试中也表现出持续的提升，突显了其在使MLLMs更鲁棒、更准确地进行动态视频理解方面的有效性和泛化能力。代码已在https://github.com/marinero4972/CyberV发布。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>CyberV</span><span>视频理解</span><span>多模态大语言模型</span><span>控制论</span><span>测试时自适应</span></div>
                    <div class="area"><span class="label">区域：</span><span>视频理解</span><span>多模态</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07971" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PolyVivid：基于跨模态交互与增强的生动多主体视频生成</h2>
                <span class="published-time">Published: 2025-06-09T15:11:09.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07848.png" alt="PolyVivid：基于跨模态交互与增强的生动多主体视频生成">
                <p class="summary">尽管视频生成领域取得了最新进展，但现有模型仍缺乏细粒度控制能力，尤其是在多主体定制方面，难以保持一致的身份和交互。在本文中，我们提出了 PolyVivid，一个多主体视频定制框架，能够实现灵活且身份一致的生成。为了在主体图像和文本实体之间建立精确的对应关系，我们设计了一个基于 VLLM 的文本-图像融合模块，将视觉身份嵌入到文本空间中以实现精确的接地。为了进一步增强身份保留和主体交互，我们提出了一个基于 3D-RoPE 的增强模块，该模块支持文本和图像嵌入之间的结构化双向融合。此外，我们开发了一个注意力继承的身份注入模块，以有效地将融合后的身份特征注入到视频生成过程中，从而减轻身份漂移。最后，我们构建了一个基于 MLLM 的数据管道，该管道结合了基于 MLLM 的接地、分割和基于团的主体整合策略，以生成高质量的多主体数据，有效增强主体区分度并减少下游视频生成中的歧义。大量实验表明，PolyVivid 在身份保真度、视频真实感和主体对齐方面取得了卓越性能，超越了现有的开源和商业基线。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>多主体视频生成</span><span>跨模态交互</span><span>身份保持</span><span>大模型</span><span>生成式AI</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>多模态</span><span>计算机视觉</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07848" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>强化预训练</h2>
                <span class="published-time">Published: 2025-06-09T17:59:53.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08007.png" alt="强化预训练">
                <p class="summary">在这项工作中，我们引入了强化预训练（RPT）作为大型语言模型和强化学习（RL）的一种新型扩展范式。具体而言，我们将下一词元预测重新定义为一项使用强化学习训练的推理任务，在该任务中，它通过正确预测给定上下文的下一词元而获得可验证的奖励。RPT 提供了一种可扩展的方法，可以利用大量的文本数据进行通用强化学习，而不是依赖于领域特定的标注答案。通过激励下一词元推理能力，RPT 显著提高了预测下一词元的语言建模准确性。此外，RPT 为进一步的强化微调提供了强大的预训练基础。扩展曲线表明，增加训练计算量持续提高了下一词元预测的准确性。这些结果表明 RPT 是一种有效且有前景的扩展范式，可推动语言模型预训练的发展。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>强化预训练</span><span>大型语言模型</span><span>强化学习</span><span>下一词元预测</span><span>预训练</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>自然语言处理</span><span>机器学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08007" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SAFEFLOW：一种用于可信和事务性自主智能体系统的原则性协议</h2>
                <span class="published-time">Published: 2025-06-09T09:04:37.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07564.png" alt="SAFEFLOW：一种用于可信和事务性自主智能体系统的原则性协议">
                <p class="summary">大型语言模型（LLMs）和视觉-语言模型（VLMs）的最新进展，使得能够进行复杂推理和多模态工具使用的强大自主智能体成为可能。尽管其能力日益增强，但当前的智能体框架仍然脆弱，缺乏用于安全信息流、可靠性和多智能体协调的原则性机制。在这项工作中，我们引入了SAFEFLOW，一个用于构建可信LLM/VLM智能体的新型协议级框架。SAFEFLOW强制执行细粒度信息流控制（IFC），精确跟踪智能体、工具、用户和环境之间所有交换数据的来源、完整性和机密性。通过限制LLM推理以遵守这些安全标签，SAFEFLOW防止不受信任或对抗性输入污染高完整性决策。为确保并发多智能体设置中的鲁棒性，SAFEFLOW引入了事务性执行、冲突解决和共享状态上的安全调度，从而保持智能体间的全局一致性。我们进一步引入了包括预写日志、回滚和安全缓存等机制，以进一步增强对运行时错误和策略违规的弹性。为了验证性能，我们构建了SAFEFLOWBENCH，一个旨在评估智能体在对抗性、噪声和并发操作条件下的可靠性的综合基准套件。大量实验表明，使用SAFEFLOW构建的智能体即使在恶劣环境中也能保持出色的任务性能和安全保障，显著优于现有技术。SAFEFLOW和SAFEFLOWBENCH共同为原则性、鲁棒和安全的智能体生态系统奠定了基础，推动了可靠自主性的前沿发展。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>SAFEFLOW</span><span>自主智能体</span><span>信息流控制</span><span>事务性执行</span><span>可信系统</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>智能体</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07564" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>