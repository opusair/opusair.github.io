<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI 日报 - 2025-06-10</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <script src="../../js/analytics.js"></script>
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter, Noto Sans SC', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI 日报</h1>
            <p class="date">2025-06-10</p>
            <p class="theme-info">关于我们: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Twitter</h2>

            <article class="item-card">
                <h2>OpenAI_o3-pro向Pro用户全面推出</h2>
                <span class="published-time">发布时间: 2025-06-10T20:08:49.000Z</span>
                <img src="screenshot/twitter/OpenAI_1932530409684005048.png" alt="OpenAI_o3-pro向Pro用户全面推出">
                <p class="summary">OpenAI宣布其最新模型o3-pro已全面向所有ChatGPT Pro用户及API开发者推出。此次更新旨在提升用户体验和开发效率，进一步拓展AI应用场景。o3-pro的上线预计将为专业用户带来更强大的功能和更稳定的服务，巩固OpenAI在AI领域的领先地位。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>OpenAI</span><span>o3-pro</span><span>ChatGPT</span><span>API</span><span>产品发布</span><span>大模型</span></div>
                    <div class="area"><span class="label">区域：</span><span>产品发布</span><span>技术动态</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/OpenAI/status/1932530409684005048" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ArtificialAnlys_OpenAI大幅下调o3模型价格</h2>
                <span class="published-time">发布时间: 2025-06-10T17:26:32.000Z</span>
                <img src="screenshot/twitter/ArtificialAnlys_1932489580592435301.png" alt="ArtificialAnlys_OpenAI大幅下调o3模型价格">
                <p class="summary">OpenAI宣布将其o3模型的定价大幅下调80%，使其在价格上与Gemini 2.5 Pro和Claude 4 Sonnet具有竞争力，并且比Claude 4 Opus便宜8倍。o3模型现在每百万输入/输出token的价格分别为2美元和8美元（此前为8美元和40美元），同时为缓存的输入token提供75%的折扣，显著提升了其市场竞争力。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>OpenAI</span><span>o3</span><span>价格下调</span><span>大模型</span><span>市场竞争</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>产品发布</span><span>行业资讯</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/ArtificialAnlys/status/1932489580592435301" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MistralAI_发布首个推理模型Magistral</h2>
                <span class="published-time">发布时间: 2025-06-10T14:15:33.000Z</span>
                <img src="screenshot/twitter/MistralAI_1932445269611688315.png" alt="MistralAI_发布首个推理模型Magistral">
                <p class="summary">Mistral AI近日宣布推出其首个推理模型Magistral。该模型专为在特定领域、透明化以及多语言推理方面表现卓越而设计，旨在提供更高效、更可靠的AI推理能力。此次发布标志着Mistral AI在AI模型研发领域的新进展，有望在多个应用场景中发挥重要作用。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>Mistral AI</span><span>Magistral</span><span>推理模型</span><span>多语言推理</span><span>产品发布</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>产品发布</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/MistralAI/status/1932445269611688315" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LangChainAI_Uber利用LangGraph构建AI开发代理实现代码修复</h2>
                <span class="published-time">发布时间: 2025-06-10T17:41:32.000Z</span>
                <img src="screenshot/twitter/LangChainAI_1932493346498543898.png" alt="LangChainAI_Uber利用LangGraph构建AI开发代理实现代码修复">
                <p class="summary">LangChainAI分享了Uber如何利用LangGraph框架构建AI开发代理的案例。这些智能体能够每天生成数千个代码修复方案，为5000名开发者提供服务，处理数亿行代码，并已累计节省超过21000小时的开发时间。这展示了AI在提升软件开发效率方面的巨大潜力。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>LangChain</span><span>LangGraph</span><span>Uber</span><span>AI开发代理</span><span>代码修复</span><span>自动化</span></div>
                    <div class="area"><span class="label">区域：</span><span>智能体</span><span>技术动态</span><span>行业资讯</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/LangChainAI/status/1932493346498543898" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Kling_ai_万鹏飞将在CVPR分享Kling视频生成模型</h2>
                <span class="published-time">发布时间: 2025-06-10T15:48:33.000Z</span>
                <img src="screenshot/twitter/Kling_ai_1932464913018147291.png" alt="Kling_ai_万鹏飞将在CVPR分享Kling视频生成模型">
                <p class="summary">Kling AI宣布其视频生成模型负责人万鹏飞将在IEEE年度计算机视觉顶级会议CVPR上发表演讲。万鹏飞将带来题为“Kling介绍及我们迈向更强大视频生成模型的研究”的主题演讲，深入分享Kling在视频生成技术方面的最新突破和前沿进展。此次活动将于2025年6月11日9:00-17:00 (UTC-5)举行，旨在为学生、研究人员和行业专业人士提供价值。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>Kling AI</span><span>CVPR</span><span>视频生成</span><span>计算机视觉</span><span>万鹏飞</span><span>主题演讲</span></div>
                    <div class="area"><span class="label">区域：</span><span>计算机视觉</span><span>生成式AI</span><span>研究进展</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/Kling_ai/status/1932464913018147291" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>summeryue0_Scale AI发布LLM红队测试与系统安全路线图</h2>
                <span class="published-time">发布时间: 2025-06-10T17:19:19.000Z</span>
                <img src="screenshot/twitter/summeryue0_1932487755034210475.png" alt="summeryue0_Scale AI发布LLM红队测试与系统安全路线图">
                <p class="summary">Scale AI的Summer Yue和Zifan (Sail) Wang共同发布了一份关于大型语言模型（LLM）红队测试的立场文件。该文件总结了迄今为止红队测试LLM的经验教训，探讨了关键问题、缺失环节以及模型安全如何融入更广泛的系统安全和监控体系。报告强调了红队测试前沿模型的研究重点，并提出了实现系统级安全和AI监控的路线图，旨在汇集不同视角，推动AI安全发展。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>红队测试</span><span>大模型安全</span><span>系统安全</span><span>AI监控</span><span>Scale AI</span><span>立场文件</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>研究进展</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/summeryue0/status/1932487755034210475" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>中科院首创“1比特VLA模型” | 内存骤降70%+效果媲美4比特OpenVLA，百元级设备畅跑VLA！</h2>
                <span class="published-time">发布时间: 2025-06-10T23:45:43.000Z</span>
                <img src="screenshot/20250610/wechat/wechat_image_zhCpd8QZTf4JLK6OdufEiQ.png" alt="中科院首创“1比特VLA模型” | 内存骤降70%+效果媲美4比特OpenVLA，百元级设备畅跑VLA！">
                <p class="summary">中科院团队首创“1比特VLA模型”（BitVLA），这是首个用于机器人操纵的1比特视觉语言动作模型，其参数为三元组{-1,0,1}。该模型通过蒸馏感知训练策略，将视觉编码器压缩至1.58位权重，实现了内存占用骤降70%以上，同时在LIBERO基准上性能媲美4比特OpenVLA。BitVLA的创新之处在于其极低的内存消耗，使其能够在百元级边缘设备上高效运行VLA任务，展现了在资源受限环境下部署机器人AI的巨大潜力。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>1比特VLA模型</span><span>机器人操纵</span><span>内存优化</span><span>模型量化</span><span>边缘设备</span><span>蒸馏感知训练</span></div>
                    <div class="area"><span class="label">区域：</span><span>机器人</span><span>多模态</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/zhCpd8QZTf4JLK6OdufEiQ" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>20人团队提前实现DeepSeek构想，AI算力变天？直击大模型算力成本痛点</h2>
                <span class="published-time">发布时间: 2025-06-10T16:06:12.000Z</span>
                <img src="screenshot/20250610/wechat/wechat_image_wx1ujkfAlQfAN0VCOO4bBA.png" alt="20人团队提前实现DeepSeek构想，AI算力变天？直击大模型算力成本痛点">
                <p class="summary">针对AI大模型算力瓶颈，文章探讨了传统GPGPU架构局限性及DeepSeek的硬件构想。重点介绍了玉盘AI团队的SRDA系统级数据流计算架构，其旨在从硬件层面解决内存、带宽、精度、集群扩展及PCIe竞争等核心痛点。SRDA通过数据流驱动、系统级互联、3D堆叠高带宽内存、精简高效及软件定义可重构性等创新设计，有望大幅提升大模型训练与推理性能，降低成本并增强稳定性。文章指出，SRDA理念与DeepSeek研究不谋而合，可能预示着AI专用计算架构的新范式，有望成为通用GPGPU与AI专用架构的分水岭。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>AI算力</span><span>大模型</span><span>数据流架构</span><span>SRDA</span><span>硬件加速</span><span>DeepSeek</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>深度学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/wx1ujkfAlQfAN0VCOO4bBA" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LLM 自回归做理解，MaskGIT 方案做生成！VILA‑U：一个模型搞定图像理解、视频理解和生成，简洁而强大</h2>
                <span class="published-time">发布时间: 2025-06-10T16:06:12.000Z</span>
                <img src="screenshot/20250610/wechat/wechat_image_XvNpHmrvzYV40CgGD_MVBQ.png" alt="LLM 自回归做理解，MaskGIT 方案做生成！VILA‑U：一个模型搞定图像理解、视频理解和生成，简洁而强大">
                <p class="summary">VILA-U是一个创新的统一基础模型，旨在整合视觉理解与生成任务，涵盖图像文本理解、视频文本理解、图像生成及视频生成。该模型核心在于其“统一基础视觉塔”，作为图像分词器，通过结合图像重建损失与图文对比损失进行训练，使其同时具备生成与理解能力。VILA-U采用统一的Next-Token Prediction训练范式，避免了对外部扩散模型的依赖，实现了端到端的自回归框架。实验证明，VILA-U在视觉语言理解和生成任务上均展现出竞争力，有效解决了离散视觉token在理解任务中性能下降的问题，为多模态AI发展提供了简洁而强大的解决方案。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>VILA-U</span><span>统一模型</span><span>视觉理解</span><span>视觉生成</span><span>多模态</span><span>Next-Token Prediction</span></div>
                    <div class="area"><span class="label">区域：</span><span>多模态</span><span>计算机视觉</span><span>生成式AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/XvNpHmrvzYV40CgGD_MVBQ" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>北大伯克利联手“拷问”大模型：最强Agent也才40分！新基准专治“不听话”的AI分析师</h2>
                <span class="published-time">发布时间: 2025-06-10T05:16:42.000Z</span>
                <img src="screenshot/20250610/wechat/wechat_image_0DVNQbifWQ5nWHKPNSwwHA.png" alt="北大伯克利联手“拷问”大模型：最强Agent也才40分！新基准专治“不听话”的AI分析师">
                <p class="summary">北京大学与加州大学伯克利分校联合发布IDA-Bench新基准，旨在模拟真实世界中迭代、探索性的数据分析场景，评估大模型Agent在多轮、动态指令下的表现。测试结果显示，即使是Claude-3.7和Gemini-2.5 Pro等顶尖模型，任务成功率也仅为40%，远低于预期。研究揭示，当前Agent在遵循指令与自主推理间难以平衡，常出现“过度自信”或“过度谨慎”等问题，导致任务失败。这表明，LLM Agent若要成为可靠的数据分析助手，仍需在理解、遵循和交互能力上进行重大改进。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大模型</span><span>智能体</span><span>数据分析</span><span>基准测试</span><span>交互式</span><span>指令遵循</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>智能体</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/0DVNQbifWQ5nWHKPNSwwHA" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>让AI自己设计芯片！中国科学院发布「启蒙」，芯片全流程自动设计</h2>
                <span class="published-time">发布时间: 2025-06-10T04:06:16.000Z</span>
                <img src="screenshot/20250610/wechat/wechat_image_cNGAFt0FxbI0_WPwYZgEmA.png" alt="让AI自己设计芯片！中国科学院发布「启蒙」，芯片全流程自动设计">
                <p class="summary">中国科学院计算技术研究所联合软件研究所，发布了基于大模型等AI技术的处理器芯片和基础软件全自动设计系统「启蒙」。该系统能全自动完成芯片软硬件设计，部分或全面超越人类专家水平，已成功自动设计RISC-V CPU，性能达ARM Cortex A53。「启蒙」采用领域大模型、智能体和应用层三级架构，通过迭代演进解决数据稀缺、正确性与求解规模等挑战，有望大幅提升芯片设计效率，缩短周期，实现快速定制化，改变处理器芯片软硬件设计范式。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>芯片设计</span><span>人工智能</span><span>大模型</span><span>自动化设计</span><span>启蒙系统</span><span>处理器芯片</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/cNGAFt0FxbI0_WPwYZgEmA" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>一块4090搞定实时视频生成！Adobe黑科技来了</h2>
                <span class="published-time">发布时间: 2025-06-10T03:59:40.000Z</span>
                <img src="screenshot/20250610/wechat/wechat_image_G--WIqw9wMlkMkIVgFJXug.png" alt="一块4090搞定实时视频生成！Adobe黑科技来了">
                <p class="summary">Adobe与德克萨斯大学奥斯汀分校联合推出Self Forcing算法，旨在解决自回归视频生成中的暴露偏差与误差累积。该算法受RNN启发，通过训练时显式展开自回归生成，使模型从自身预测错误中学习，并利用整体分布匹配损失监督。研究通过少量步数扩散网络、梯度截断、动态步数采样及梯度流隔离等策略，实现高效训练。同时，引入滚动KV缓存支持无限长视频生成。该技术在单块H100 GPU上实现17 FPS、亚秒级延迟的实时视频生成，RTX 4090亦可达10 FPS，且生成质量优于现有模型，为直播、游戏等交互式视频应用开辟新可能。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>实时视频生成</span><span>Self Forcing</span><span>自回归模型</span><span>暴露偏差</span><span>扩散模型</span><span>Adobe</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>深度学习</span><span>计算机视觉</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/G--WIqw9wMlkMkIVgFJXug" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>🌟 Awesome LLM Apps</h2>
                <span class="published-time">发布时间: 2025-06-06T22:50:54Z</span>
                <img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" alt="🌟 Awesome LLM Apps">
                <p class="summary">该GitHub仓库“Awesome LLM Apps”汇集了大量基于RAG、AI智能体、多智能体团队、MCP及语音智能体等技术构建的LLM应用。它展示了如何利用OpenAI、Anthropic、Google及开源模型（如DeepSeek、Qwen、Llama）解决实际问题，涵盖从代码库到邮件处理等多样化场景。项目旨在提供实用且创新的LLM应用范例，推动大模型技术在不同领域的落地与发展，并鼓励社区贡献，共同构建丰富的开源LLM应用生态。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大语言模型</span><span>AI智能体</span><span>RAG</span><span>多智能体系统</span><span>生成式AI</span><span>应用开发</span><span>机器学习</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/Shubhamsaboo/awesome-llm-apps" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Boltz</h2>
                <span class="published-time">发布时间: 2025-06-11T15:19:15Z</span>
                <img src="https://github.com/jwohlwend/boltz/raw/main/docs/boltz2_title.png" alt="Boltz">
                <p class="summary">Boltz是一个用于生物分子相互作用预测的模型家族，其中Boltz-2是最新一代基础模型，超越了AlphaFold3和Boltz-1，能够联合建模复杂结构和结合亲和力。它首次实现了接近基于物理的自由能微扰（FEP）方法的精度，同时速度提升1000倍，使得早期药物发现中的高通量虚拟筛选成为可能。Boltz模型及其代码在MIT许可下开源，可用于学术和商业用途。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>生物分子相互作用</span><span>结合亲和力</span><span>药物发现</span><span>深度学习</span><span>分子设计</span><span>虚拟筛选</span><span>开源模型</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>机器学习</span><span>深度学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/jwohlwend/boltz" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>✨ YouTube Transcript API ✨</h2>
                <span class="published-time">发布时间: 2025-04-23T07:30:48Z</span>
                <img src="screenshot/github/youtube-transcript-api.png" alt="✨ YouTube Transcript API ✨">
                <p class="summary">YouTube Transcript API是一个Python库，用于获取YouTube视频的字幕和转录文本。它支持自动生成字幕和多语言翻译，无需依赖无头浏览器，显著提升了效率。该API还提供了处理IP封锁的代理功能、年龄限制视频的Cookie认证，并支持多种输出格式（如JSON、SRT），同时提供命令行工具，极大方便了开发者和用户获取和处理YouTube视频内容。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>YouTube字幕</span><span>视频转录</span><span>Python库</span><span>API</span><span>字幕翻译</span><span>代理</span><span>命令行工具</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>自然语言处理</span><span>视频理解</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/jdepoix/youtube-transcript-api" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>开源大模型食用指南</h2>
                <span class="published-time">发布时间: 2025-06-11T14:30:51Z</span>
                <img src="https://contrib.rocks/image?repo=datawhalechina/self-llm" alt="开源大模型食用指南">
                <p class="summary">本项目是面向国内初学者的开源大模型教程，基于Linux平台，提供从环境配置、本地部署到高效微调的全流程指导。它简化了开源大模型的应用流程，涵盖LLaMA、ChatGLM、InternLM等主流模型，并指导命令行调用、在线Demo部署及LangChain集成。旨在帮助学生和研究者掌握开源大模型的“食用”方法，促进其在学习和实践中的普及应用。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大模型</span><span>开源</span><span>环境配置</span><span>模型部署</span><span>模型微调</span><span>自然语言处理</span><span>Linux</span><span>LangChain</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>自然语言处理</span><span>深度学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/datawhalechina/self-llm" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiniCPM</h2>
                <span class="published-time">发布时间: 2025-06-11T06:36:09Z</span>
                <img src="https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm_logo.png" alt="MiniCPM">
                <p class="summary">MiniCPM是一个极致高效的端侧大模型系列，由面壁智能、清华大学和中国人民大学联合开发。它通过创新的模型架构（如InfLLM v2稀疏注意力）、高效学习算法（如BitCPM三值量化）和优化推理系统（CPM.cu），实现了卓越的效率提升。MiniCPM系列模型在保持同等规模最优性能的同时，在典型端侧芯片上实现了5倍以上生成加速，并在工具调用、代码解释器、长文本处理等多个任务上超越同级别甚至更大参数量的模型，为端侧AI应用提供了强大的解决方案。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大模型</span><span>端侧AI</span><span>高效推理</span><span>稀疏注意力</span><span>模型量化</span><span>工具调用</span><span>长文本</span><span>自然语言处理</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>深度学习</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/OpenBMB/MiniCPM" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>强化预训练</h2>
                <span class="published-time">发布时间: 2025-06-09T17:59:53.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08007.png" alt="强化预训练">
                <p class="summary">在这项工作中，我们引入强化预训练（RPT）作为大语言模型和强化学习（RL）的一种新型扩展范式。具体而言，我们将下一个词元预测重构为一项使用强化学习训练的推理任务，通过正确预测给定上下文的下一个词元来获得可验证的奖励。RPT 提供了一种可扩展的方法，可以利用大量的文本数据进行通用强化学习，而无需依赖领域特定的标注答案。通过激励下一个词元推理能力，RPT 显著提高了预测下一个词元的语言模型准确性。此外，RPT 为进一步的强化微调提供了强大的预训练基础。扩展曲线表明，增加训练计算量持续提高了下一个词元预测的准确性。这些结果表明 RPT 是一种有效且有前景的扩展范式，可推动语言模型预训练的发展。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>强化预训练</span><span>大语言模型</span><span>强化学习</span><span>下一个词元预测</span><span>语言模型预训练</span></div>
                    <div class="area"><span class="label">区域：</span><span>机器学习</span><span>大模型</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08007" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiniCPM4：端侧设备上的超高效大语言模型</h2>
                <span class="published-time">发布时间: 2025-06-09T16:16:50.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07900.png" alt="MiniCPM4：端侧设备上的超高效大语言模型">
                <p class="summary">本文介绍了MiniCPM4，这是一种专为端侧设备设计的高效大语言模型（LLM）。我们通过在模型架构、训练数据、训练算法和推理系统这四个关键维度进行系统性创新，实现了这种高效率。具体而言，在模型架构方面，我们提出了InfLLM v2，这是一种可训练的稀疏注意力机制，可加速长上下文处理的预填充和解码阶段。在训练数据方面，我们提出了UltraClean，一种高效准确的预训练数据过滤和生成策略，以及UltraChat v2，一个全面的监督微调数据集。这些数据集使得模型仅使用8万亿训练tokens即可达到令人满意的性能。在训练算法方面，我们提出了ModelTunnel v2用于高效的预训练策略搜索，并通过引入用于负载均衡强化学习的chunk-wise rollout和数据高效的三元LLM BitCPM，改进了现有的后训练方法。在推理系统方面，我们提出了CPM.cu，它集成了稀疏注意力、模型量化和推测采样，以实现高效的预填充和解码。为了满足多样化的设备端需求，MiniCPM4提供了0.5B和8B参数的两个版本。充分的评估结果表明，MiniCPM4在多个基准测试中优于同等规模的开源模型，凸显了其效率和有效性。值得注意的是，MiniCPM4-8B在处理长序列时，相比Qwen3-8B展现出显著的速度提升。通过进一步的适配，MiniCPM4成功地支持了多种应用，包括可信赖的问卷生成和基于模型上下文协议的工具使用，清晰地展示了其广泛的可用性。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>MiniCPM4</span><span>大语言模型</span><span>端侧设备</span><span>超高效</span><span>稀疏注意力</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>自然语言处理</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07900" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>CyberV：视频理解中测试时扩展的控制论</h2>
                <span class="published-time">发布时间: 2025-06-09T17:45:18.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07971.png" alt="CyberV：视频理解中测试时扩展的控制论">
                <p class="summary">当前的多模态大语言模型（MLLM）在理解长视频或复杂视频时可能面临挑战，这主要是由于测试时的计算需求、鲁棒性不足以及准确性有限，其根本原因在于其前馈处理的特性。对于参数量较少的模型，这些限制可能更为严重。为了解决这些问题，我们提出了一种受控制论原理启发的创新框架，将视频MLLM重新设计为能够在推理过程中进行自我监控、自我校正和动态资源分配的自适应系统。我们的方法CyberV引入了一个控制论循环，该循环由一个MLLM推理系统、一个传感器和一个控制器组成。具体而言，传感器监控MLLM的前向处理过程并收集中间解释，例如注意力漂移，然后控制器决定何时以及如何触发自我校正并生成反馈以指导下一轮处理。这种测试时自适应扩展框架无需重新训练或额外组件即可增强冻结的MLLM。实验表明，CyberV带来了显著的改进：在VideoMMMU数据集上，CyberV将Qwen2.5-VL-7B的性能提升了8.3%，将InternVL3-8B提升了5.5%，超越了具有竞争力的专有模型GPT-4o。当应用于Qwen2.5-VL-72B时，它带来了10.0%的提升，甚至达到了与人类专家相当的性能。此外，我们的方法在VideoMME和WorldSense等通用基准测试中也表现出持续的提升，这突显了其在使MLLM更鲁棒、更准确地进行动态视频理解方面的有效性和泛化能力。代码已在https://github.com/marinero4972/CyberV发布。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>视频理解</span><span>多模态大语言模型</span><span>控制论</span><span>测试时自适应</span><span>自校正</span></div>
                    <div class="area"><span class="label">区域：</span><span>视频理解</span><span>多模态</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07971" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PolyVivid：基于跨模态交互与增强的生动多主体视频生成</h2>
                <span class="published-time">发布时间: 2025-06-09T15:11:09.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07848.png" alt="PolyVivid：基于跨模态交互与增强的生动多主体视频生成">
                <p class="summary">尽管视频生成领域取得了最新进展，但现有模型仍缺乏细粒度控制能力，尤其是在多主体定制方面，难以保持一致的身份和交互。本文提出 PolyVivid，一个多主体视频定制框架，能够实现灵活且身份一致的生成。为了在主体图像和文本实体之间建立准确的对应关系，我们设计了一个基于 VLLM 的文本-图像融合模块，将视觉身份嵌入到文本空间中以实现精确的接地。为了进一步增强身份保持和主体交互，我们提出了一个基于 3D-RoPE 的增强模块，该模块支持文本和图像嵌入之间的结构化双向融合。此外，我们开发了一个注意力继承的身份注入模块，以有效地将融合后的身份特征注入到视频生成过程中，从而减轻身份漂移。最后，我们构建了一个基于 MLLM 的数据管道，该管道结合了基于 MLLM 的接地、分割和基于团的主体整合策略，以生成高质量的多主体数据，有效增强主体区分度并减少下游视频生成中的歧义。大量实验表明，PolyVivid 在身份保真度、视频真实感和主体对齐方面取得了卓越性能，优于现有的开源和商业基线。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>多主体视频生成</span><span>跨模态交互</span><span>身份保持</span><span>视频定制</span><span>多模态模型</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>计算机视觉</span><span>多模态</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07848" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GUI-Reflection：赋能多模态GUI模型自反思能力</h2>
                <span class="published-time">发布时间: 2025-06-09T17:59:57.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08012.png" alt="GUI-Reflection：赋能多模态GUI模型自反思能力">
                <p class="summary">多模态大语言模型（MLLMs）在革新图形用户界面（GUI）自动化方面展现出巨大潜力。然而，现有的GUI模型大多依赖于从几乎无错误的离线轨迹中学习，因此缺乏反思和错误恢复能力。为了弥补这一差距，我们提出了GUI-Reflection，这是一个新颖的框架，通过专门的训练阶段（GUI特定预训练、离线监督微调（SFT）和在线反思调优），将自反思和错误纠正能力明确地整合到端到端的多模态GUI模型中。GUI-Reflection通过全自动数据生成和学习过程实现自反思行为的出现，无需任何人工标注。具体而言，1）我们首先提出了可扩展的数据管道，以从现有成功轨迹中自动构建反思和错误纠正数据。虽然现有GUI模型主要关注接地和UI理解能力，但我们提出了GUI-Reflection任务套件，以明确学习和评估面向反思的能力。2）此外，我们为移动设备上的GUI模型的在线训练和数据收集构建了一个多样化且高效的环境。3）我们还提出了一种利用所提环境的迭代在线反思调优算法，使模型能够持续增强其反思和错误纠正能力。我们的框架为GUI智能体配备了自反思和纠正能力，为更鲁棒、适应性更强、更智能的GUI自动化铺平了道路，所有数据、模型、环境和工具都将公开发布。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>GUI自动化</span><span>多模态大语言模型</span><span>自反思</span><span>错误纠正</span><span>智能体</span></div>
                    <div class="area"><span class="label">区域：</span><span>多模态</span><span>大模型</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08012" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SAFEFLOW：一种可信赖且事务性的自主智能体系统原则性协议</h2>
                <span class="published-time">发布时间: 2025-06-09T09:04:37.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07564.png" alt="SAFEFLOW：一种可信赖且事务性的自主智能体系统原则性协议">
                <p class="summary">大型语言模型（LLM）和视觉-语言模型（VLM）的最新进展使得强大的自主智能体能够进行复杂的推理和多模态工具使用。尽管其能力日益增强，但当前的智能体框架仍然脆弱，缺乏安全信息流、可靠性和多智能体协调的原则性机制。在这项工作中，我们引入了SAFEFLOW，一个用于构建可信赖的基于LLM/VLM智能体的新型协议级框架。SAFEFLOW强制执行细粒度信息流控制（IFC），精确跟踪智能体、工具、用户和环境之间所有交换数据的来源、完整性和机密性。通过限制LLM推理以遵守这些安全标签，SAFEFLOW防止不受信任或对抗性输入污染高完整性决策。为确保并发多智能体设置中的鲁棒性，SAFEFLOW引入了事务性执行、冲突解决和共享状态上的安全调度，从而保持智能体间的全局一致性。我们进一步引入了包括预写日志、回滚和安全缓存等机制，以进一步增强对运行时错误和策略违规的弹性。为了验证性能，我们构建了SAFEFLOWBENCH，一个全面的基准测试套件，旨在评估智能体在对抗性、噪声和并发操作条件下的可靠性。大量实验表明，使用SAFEFLOW构建的智能体即使在恶意环境中也能保持出色的任务性能和安全保障，显著优于现有技术。SAFEFLOW和SAFEFLOWBENCH共同为原则性、鲁棒和安全的智能体生态系统奠定了基础，推动了可靠自主性的前沿发展。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>自主智能体</span><span>信息流控制</span><span>事务性执行</span><span>多智能体系统</span><span>可信赖协议</span></div>
                    <div class="area"><span class="label">区域：</span><span>智能体</span><span>大模型</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07564" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            由 AI 助手生成
        </footer>
    </div>
</body>
</html>