<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-06-10</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <script src="../../js/analytics.js"></script>
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2025-06-10</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Twitter</h2>

            <article class="item-card">
                <h2>sama_sama_On 'Gentle Singularity' and the End of AI-Free Writing</h2>
                <span class="published-time">Published: 2025-06-10T21:15:43.000Z</span>
                <img src="../screenshot/twitter/sama_1932547251945288147.png" alt="sama_sama_On 'Gentle Singularity' and the End of AI-Free Writing">
                <p class="summary">Sam Altman released a new post titled "the gentle singularity," remarking that it might be the last piece he writes entirely without AI assistance. He highlighted the perspective that "From a relativistic perspective, the singularity happens bit by bit, and the merge happens slowly," suggesting a future where creative work is deeply integrated with AI. This tweet signifies the advent of a new era of human-AI collaboration.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Sam Altman</span><span>Gentle Singularity</span><span>AI Writing</span><span>Artificial Intelligence</span><span>Future Trends</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Tech News</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/sama/status/1932547251945288147" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAI_OpenAI_o3-pro Fully Rolls Out to Pro Users</h2>
                <span class="published-time">Published: 2025-06-10T20:08:49.000Z</span>
                <img src="../screenshot/twitter/OpenAI_1932530409684005048.png" alt="OpenAI_OpenAI_o3-pro Fully Rolls Out to Pro Users">
                <p class="summary">OpenAI has announced that its latest model, o3-pro, is now fully rolling out to all ChatGPT Pro users and API developers. This update aims to enhance user experience and development efficiency, further expanding AI application scenarios. The launch of o3-pro is expected to bring more powerful features and stable services to professional users, solidifying OpenAI's leading position in the AI field.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>OpenAI</span><span>o3-pro</span><span>ChatGPT</span><span>API</span><span>Product Launch</span><span>Large Language Model</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Product Launch</span><span>Tech News</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/OpenAI/status/1932530409684005048" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>sama_sama_OpenAI Open-Weights Model Release Delayed</h2>
                <span class="published-time">Published: 2025-06-10T22:58:58.000Z</span>
                <img src="../screenshot/twitter/sama_1932573231199707168.png" alt="sama_sama_OpenAI Open-Weights Model Release Delayed">
                <p class="summary">Sam Altman announced a delay in the release of OpenAI's "open-weights model." Originally anticipated for June, the model is now expected to be launched later this summer. Altman stated that the postponement is due to an "unexpected and quite amazing" discovery by their research team, which necessitates additional development time to ensure its full potential is realized. This suggests a significant breakthrough in OpenAI's open-source initiatives, building anticipation for its eventual release and indicating a potentially groundbreaking development in the field.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Open-weights model</span><span>Sam Altman</span><span>OpenAI</span><span>Model release</span><span>Research breakthrough</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Large Language Model</span><span>Tech News</span><span>Research Progress</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/sama/status/1932573231199707168" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>sarahookr_MLStreetTalk Exposes LM Arena Ranking Overfitting</h2>
                <span class="published-time">Published: 2025-06-10T09:49:07.000Z</span>
                <img src="../screenshot/twitter/sarahookr_1932374460528562430.png" alt="sarahookr_MLStreetTalk Exposes LM Arena Ranking Overfitting">
                <p class="summary">Sara Hooker recommends an excellent MLStreetTalk video that exposes how a handful of providers have systematically overfit to the LM Arena evaluation benchmarks, easily distorting the rankings. The 26-minute video details this issue. Hooker urges scientists and the community to strive for better industry standards and demand more fair and transparent evaluation mechanisms.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>LM Arena</span><span>Overfitting</span><span>Model Evaluation</span><span>Ranking</span><span>Machine Learning</span><span>Industry Standards</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Machine Learning</span><span>Large Language Model</span><span>Industry News</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/sarahookr/status/1932374460528562430" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>mahbub_bello_AISheets: Open-Source LLM-Powered AI Spreadsheet App Released</h2>
                <span class="published-time">Published: 2025-06-10T15:32:50.000Z</span>
                <img src="../screenshot/twitter/mahbub_bello_1932460957332054163.png" alt="mahbub_bello_AISheets: Open-Source LLM-Powered AI Spreadsheet App Released">
                <p class="summary">Twitter user mahbub_bello retweeted Thomas Wolf's announcement of AISheets. AISheets is described as their most fun AI experiment to date, integrating thousands of AI models with spreadsheets. It allows users to build, analyze, and automate data using open-source large language models within a slick, fast, and simple application. The app is highlighted as "surprisingly powerful," aiming to revolutionize data handling.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>AISheets</span><span>Open-source LLMs</span><span>Spreadsheets</span><span>Data Automation</span><span>AI Application</span><span>Product Launch</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Artificial Intelligence</span><span>Open Source</span><span>Product Launch</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/mahbub_bello/status/1932460957332054163" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Google_Gemini 2.5 Pro Generates Interactive Fractal Art</h2>
                <span class="published-time">Published: 2025-06-10T20:30:59.000Z</span>
                <img src="../screenshot/twitter/Google_1932535990301655492.png" alt="Google_Gemini 2.5 Pro Generates Interactive Fractal Art">
                <p class="summary">Google has showcased the impressive artistic creation capabilities of its advanced Gemini 2.5 Pro model. Users can now instantly generate interactive fractal art by simply providing text prompts, such as the example: "Make me a beautiful particle based, animated, endless, 3D, symmetrical, fractal art piece inspired by a math formula." This demonstration highlights Gemini 2.5 Pro's significant potential in the field of generative AI and multimodal content creation, effectively transforming complex mathematical aesthetics into captivating visual art forms.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Gemini 2.5 Pro</span><span>Fractal Art</span><span>Generative AI</span><span>Art Creation</span><span>Google</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Generative AI</span><span>Large Language Model</span><span>Tech News</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/Google/status/1932535990301655492" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>CAS Pioneers '1-bit VLA Model': 70%+ Memory Reduction, Performance Comparable to 4-bit OpenVLA, Enabling VLA on Low-Cost Devices</h2>
                <span class="published-time">Published: 2025-06-10T23:45:43.000Z</span>
                <img src="../screenshot/20250610/wechat/wechat_image_zhCpd8QZTf4JLK6OdufEiQ.png" alt="CAS Pioneers '1-bit VLA Model': 70%+ Memory Reduction, Performance Comparable to 4-bit OpenVLA, Enabling VLA on Low-Cost Devices">
                <p class="summary">The Chinese Academy of Sciences (CAS) team has pioneered the "1-bit VLA Model" (BitVLA), marking the first 1-bit Vision-Language-Action model specifically designed for robot manipulation. This innovative model utilizes ternary parameters of {-1, 0, 1} for its core architecture. Through a novel distillation-aware training strategy, BitVLA effectively compresses its visual encoder to just 1.58-bit weights, leading to a remarkable memory reduction of over 70%. Despite this significant compression, the model maintains performance comparable to state-of-the-art 4-bit OpenVLA on the challenging LIBERO benchmark. BitVLA's key advantage is its exceptionally low memory footprint, which enables efficient execution of complex VLA tasks even on low-cost, resource-constrained edge devices. This breakthrough demonstrates immense potential for deploying advanced robotic AI in a wider range of practical applications, making intelligent robotics more accessible and scalable.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>1-bit VLA Model</span><span>Robot Manipulation</span><span>Memory Optimization</span><span>Model Quantization</span><span>Edge Devices</span><span>Distillation-Aware Training</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Robotics</span><span>Multimodal</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/zhCpd8QZTf4JLK6OdufEiQ" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>A 20-Person Team Preemptively Realizes DeepSeek's Vision: A Paradigm Shift in AI Computing Power and Addressing Large Model Compute Cost Challenges</h2>
                <span class="published-time">Published: 2025-06-10T16:06:12.000Z</span>
                <img src="../screenshot/20250610/wechat/wechat_image_wx1ujkfAlQfAN0VCOO4bBA.png" alt="A 20-Person Team Preemptively Realizes DeepSeek's Vision: A Paradigm Shift in AI Computing Power and Addressing Large Model Compute Cost Challenges">
                <p class="summary">Addressing the current computing power bottlenecks in large AI models, this article discusses the limitations of traditional GPGPU architectures and DeepSeek's vision for AI hardware development. It primarily introduces YuPan AI's SRDA system-level dataflow computing architecture, which aims to resolve core hardware pain points such as memory capacity/bandwidth, computational precision, cluster expansion, and PCIe bus contention. SRDA's innovative design, featuring dataflow-driven principles, system-level interconnection, 3D-stacked high-bandwidth memory, streamlined efficiency, and software-defined reconfigurability, is expected to significantly enhance the performance of large model training and inference, reduce costs, and improve stability. The article suggests that SRDA's philosophy aligns with DeepSeek's cutting-edge research, potentially signaling a new paradigm for AI-specific computing architectures. Especially as large model technical requirements converge, SRDA could become a watershed moment, distinguishing between general-purpose GPGPUs and truly AI-dedicated architectures.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>AI Computing Power</span><span>Large Models</span><span>Dataflow Architecture</span><span>SRDA</span><span>Hardware Acceleration</span><span>DeepSeek</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/wx1ujkfAlQfAN0VCOO4bBA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>VILA-U: A Unified Foundation Model Integrating Visual Understanding and Generation</h2>
                <span class="published-time">Published: 2025-06-10T16:06:12.000Z</span>
                <img src="../screenshot/20250610/wechat/wechat_image_XvNpHmrvzYV40CgGD_MVBQ.png" alt="VILA-U: A Unified Foundation Model Integrating Visual Understanding and Generation">
                <p class="summary">VILA-U introduces an innovative unified foundation model designed to integrate visual understanding and generation tasks, encompassing image-text understanding, video-text understanding, image generation, and video generation. Central to this model is its "Unified Foundation Vision Tower," which functions as an image tokenizer. This tower is trained using a combination of image reconstruction loss and image-text contrastive loss, enabling it to excel in both generative and discriminative capabilities. VILA-U adopts a unified Next-Token Prediction training paradigm, eliminating reliance on external diffusion models and establishing an end-to-end autoregressive framework. Experimental results demonstrate VILA-U's competitive performance across visual-language understanding and generation benchmarks. It effectively addresses the performance degradation often associated with discrete visual tokens in understanding tasks, offering a concise yet powerful solution for advancing multimodal AI.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>VILA-U</span><span>Unified Model</span><span>Visual Understanding</span><span>Visual Generation</span><span>Multimodal</span><span>Next-Token Prediction</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Multimodal</span><span>Computer Vision</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/XvNpHmrvzYV40CgGD_MVBQ" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Peking University and UC Berkeley Unveil IDA-Bench: A New Benchmark Exposing LLM Agents' Limitations in Iterative Data Analysis, Top Models Score Only 40%</h2>
                <span class="published-time">Published: 2025-06-10T05:16:42.000Z</span>
                <img src="../screenshot/20250610/wechat/wechat_image_0DVNQbifWQ5nWHKPNSwwHA.png" alt="Peking University and UC Berkeley Unveil IDA-Bench: A New Benchmark Exposing LLM Agents' Limitations in Iterative Data Analysis, Top Models Score Only 40%">
                <p class="summary">Peking University and UC Berkeley have jointly introduced IDA-Bench, a novel benchmark designed to simulate real-world, iterative, and exploratory data analysis scenarios, specifically evaluating large language model agents' performance under multi-turn, dynamic instructions. Unlike traditional single-turn evaluations, IDA-Bench exposes the challenges of continuous interaction. Test results reveal that even top models like Claude-3.7 and Gemini-2.5 Pro achieve only a 40% task success rate, significantly below expectations. The research highlights current agents' profound difficulty in balancing strict instruction adherence with necessary autonomous reasoning, often exhibiting "overconfident" or "overcautious" behaviors that lead to critical task failures. This comprehensive evaluation underscores the critical need for substantial improvements in LLM agents' understanding, instruction following, and interactive capabilities to truly become reliable and effective data analysis assistants in complex, real-world settings.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Large Language Model</span><span>AI Agent</span><span>Data Analysis</span><span>Benchmark</span><span>Iterative Interaction</span><span>Instruction Following</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Large Language Model</span><span>AI Agent</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/0DVNQbifWQ5nWHKPNSwwHA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Let AI Design Chips! Chinese Academy of Sciences Launches 'QiMeng' for Fully Automated Chip Design Flow</h2>
                <span class="published-time">Published: 2025-06-10T04:06:16.000Z</span>
                <img src="../screenshot/20250610/wechat/wechat_image_cNGAFt0FxbI0_WPwYZgEmA.png" alt="Let AI Design Chips! Chinese Academy of Sciences Launches 'QiMeng' for Fully Automated Chip Design Flow">
                <p class="summary">The Institute of Computing Technology, Chinese Academy of Sciences, in collaboration with the Institute of Software, has unveiled 'QiMeng,' a fully automated design system for processor chips and foundational software, powered by large models and other AI technologies. This system can autonomously complete chip hardware and software design, partially or entirely surpassing human expert levels. It has successfully designed RISC-V CPUs automatically, achieving performance comparable to ARM Cortex A53. 'QiMeng' employs a three-tiered architecture comprising domain-specific large models, intelligent agents, and an application layer. It addresses challenges such as data scarcity, correctness, and solution scale through an iterative evolution approach, promising to significantly enhance chip design efficiency, shorten development cycles, enable rapid customization, and fundamentally transform the paradigm of processor chip hardware and software design.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Chip Design</span><span>Artificial Intelligence</span><span>Large Models</span><span>Automated Design</span><span>QiMeng System</span><span>Processor Chip</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/cNGAFt0FxbI0_WPwYZgEmA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Adobe's Self Forcing: Real-time Video Generation on a Single RTX 4090</h2>
                <span class="published-time">Published: 2025-06-10T03:59:40.000Z</span>
                <img src="../screenshot/20250610/wechat/wechat_image_G--WIqw9wMlkMkIVgFJXug.png" alt="Adobe's Self Forcing: Real-time Video Generation on a Single RTX 4090">
                <p class="summary">Adobe, in collaboration with the University of Texas at Austin, has introduced the Self Forcing algorithm, designed to address exposure bias and error accumulation in autoregressive video generation. Inspired by RNN sequence modeling, this algorithm explicitly unfolds the autoregressive generation process during training, enabling the model to learn from its own prediction errors and utilizing holistic distribution-matching losses to supervise the complete sequence. The research employs innovative strategies such as a few-step diffusion backbone, gradient truncation, dynamic step sampling, and gradient flow isolation to achieve efficient training. Furthermore, a rolling KV cache mechanism is introduced to support infinitely long video generation. This technology enables real-time video generation at 17 FPS with sub-second latency on a single H100 GPU, and 10 FPS on an RTX 4090, while delivering superior generation quality compared to existing models. This breakthrough opens new possibilities for interactive video applications like live streaming and gaming.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Real-time Video Generation</span><span>Self Forcing</span><span>Autoregressive Models</span><span>Exposure Bias</span><span>Diffusion Models</span><span>Adobe</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/G--WIqw9wMlkMkIVgFJXug" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>ðŸŒŸ Awesome LLM Apps</h2>
                <span class="published-time">Published: 2025-06-06T22:50:54Z</span>
                <img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" alt="ðŸŒŸ Awesome LLM Apps">
                <p class="summary">The "Awesome LLM Apps" GitHub repository presents a meticulously curated collection of practical large language model (LLM) applications. These applications are ingeniously built utilizing advanced techniques such as Retrieval-Augmented Generation (RAG), sophisticated AI Agents, collaborative Multi-agent Teams, Multi-Context Processing (MCP), and intuitive Voice Agents. The repository showcases the versatile integration of leading LLM providers like OpenAI, Anthropic, and Google, alongside powerful open-source models including DeepSeek, Qwen, and Llama, which can even be run locally. It illustrates how LLMs can address real-world challenges across diverse domains, from analyzing code repositories to managing email inboxes. This project's core objective is to offer tangible, innovative LLM application examples, thereby accelerating the practical deployment and advancement of large model technologies across various industries. Furthermore, it actively encourages community contributions, aiming to cultivate a vibrant and comprehensive open-source ecosystem for LLM-powered solutions.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Large Language Model</span><span>AI Agent</span><span>RAG</span><span>Multi-agent Systems</span><span>Generative AI</span><span>Application Development</span><span>Machine Learning</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/Shubhamsaboo/awesome-llm-apps" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Boltz</h2>
                <span class="published-time">Published: 2025-06-11T15:19:15Z</span>
                <img src="https://github.com/jwohlwend/boltz/raw/main/docs/boltz2_title.png" alt="Boltz">
                <p class="summary">Boltz is a family of models designed for biomolecular interaction prediction, with Boltz-2 being the latest foundational model. It surpasses AlphaFold3 and Boltz-1 by jointly modeling complex structures and binding affinities. Boltz-2 is the first deep learning model to achieve accuracy comparable to physics-based free-energy perturbation (FEP) methods, while being 1000 times faster. This breakthrough makes accurate in silico screening practical for early-stage drug discovery, including hit-discovery and ligand optimization. The Boltz models and code are open-sourced under the MIT license, available for both academic and commercial use.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Biomolecular Interaction</span><span>Binding Affinity</span><span>Drug Discovery</span><span>Deep Learning</span><span>Molecular Design</span><span>In Silico Screening</span><span>Open Source Model</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/jwohlwend/boltz" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>âœ¨ YouTube Transcript API âœ¨</h2>
                <span class="published-time">Published: 2025-04-23T07:30:48Z</span>
                <img src="../screenshot/github/youtube-transcript-api.png" alt="âœ¨ YouTube Transcript API âœ¨">
                <p class="summary">The YouTube Transcript API is a Python library designed to retrieve transcripts and subtitles for YouTube videos. It supports both automatically generated subtitles and multi-language translation, operating efficiently without the need for headless browsers, which significantly improves performance. The API also features proxy support to circumvent IP blocks, cookie authentication for age-restricted content, and offers various output formats like JSON and SRT. Additionally, it includes a command-line interface, greatly simplifying the process for developers and users to access and process YouTube video content.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>YouTube Subtitles</span><span>Video Transcription</span><span>Python Library</span><span>API</span><span>Subtitle Translation</span><span>Proxy</span><span>Command Line Tool</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/jdepoix/youtube-transcript-api" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Open-source Large Language Model Handbook</h2>
                <span class="published-time">Published: 2025-06-11T14:30:51Z</span>
                <img src="https://contrib.rocks/image?repo=datawhalechina/self-llm" alt="Open-source Large Language Model Handbook">
                <p class="summary">This project presents a dedicated open-source large language model (LLM) tutorial, specifically designed for beginners in China and optimized for Linux platforms. It provides comprehensive, full-process guidance encompassing essential skills such as environment configuration, local deployment, and efficient fine-tuning for a wide array of open-source LLMs. By simplifying the complex deployment, usage, and application workflows, the initiative aims to make advanced LLM technologies more accessible to a broader audience of students and researchers. The tutorial covers mainstream models like LLaMA, ChatGLM, and InternLM, offering practical instructions on command-line invocation, setting up online demonstrations, and integrating with frameworks like LangChain. Furthermore, it delves into advanced topics such as distributed full fine-tuning, LoRA, and P-tuning methods. This resource is crucial for fostering the adoption of open-source, free large models, enabling learners to seamlessly incorporate them into their studies and future professional endeavors.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Large Language Models</span><span>Open-source</span><span>Environment Configuration</span><span>Model Deployment</span><span>Model Fine-tuning</span><span>Natural Language Processing</span><span>Linux</span><span>LangChain</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/datawhalechina/self-llm" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiniCPM</h2>
                <span class="published-time">Published: 2025-06-11T06:36:09Z</span>
                <img src="https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm_logo.png" alt="MiniCPM">
                <p class="summary">MiniCPM is an ultra-efficient series of large language models designed for edge devices, co-developed by ModelBest, Tsinghua University, and Renmin University of China. It achieves exceptional efficiency through innovative model architectures like InfLLM v2 sparse attention, efficient learning algorithms such as BitCPM 3-value quantization, and optimized inference systems like CPM.cu. While maintaining state-of-the-art performance for its size, MiniCPM models deliver over 5x generation speedup on typical edge chips. They also surpass similarly sized and even larger models in tasks like tool calling, code interpretation, and long-context processing, offering a powerful solution for edge AI applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Large Language Model</span><span>Edge AI</span><span>Efficient Inference</span><span>Sparse Attention</span><span>Model Quantization</span><span>Tool Calling</span><span>Long Context</span><span>Natural Language Processing</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Large Language Model</span><span>Deep Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/OpenBMB/MiniCPM" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Reinforcement Pre-Training</h2>
                <span class="published-time">Published: 2025-06-09T17:59:53.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08007.png" alt="Reinforcement Pre-Training">
                <p class="summary">In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling
paradigm for large language models and reinforcement learning (RL).
Specifically, we reframe next-token prediction as a reasoning task trained
using RL, where it receives verifiable rewards for correctly predicting the
next token for a given context. RPT offers a scalable method to leverage vast
amounts of text data for general-purpose RL, rather than relying on
domain-specific annotated answers. By incentivizing the capability of
next-token reasoning, RPT significantly improves the language modeling accuracy
of predicting the next tokens. Moreover, RPT provides a strong pre-trained
foundation for further reinforcement fine-tuning. The scaling curves show that
increased training compute consistently improves the next-token prediction
accuracy. The results position RPT as an effective and promising scaling
paradigm to advance language model pre-training.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Reinforcement Pre-Training</span><span>Large Language Models</span><span>Reinforcement Learning</span><span>Next-token prediction</span><span>Language Model Pre-training</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Machine Learning</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08007" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiniCPM4: Ultra-Efficient LLMs on End Devices</h2>
                <span class="published-time">Published: 2025-06-09T16:16:50.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07900.png" alt="MiniCPM4: Ultra-Efficient LLMs on End Devices">
                <p class="summary">This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable sparse attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates sparse attention, model quantization, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Sufficient evaluation results show that MiniCPM4
outperforms open-source models of similar size across multiple benchmarks,
highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B
demonstrates significant speed improvements over Qwen3-8B when processing long
sequences. Through further adaptation, MiniCPM4 successfully powers diverse
applications, including trustworthy survey generation and tool use with model
context protocol, clearly showcasing its broad usability.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>MiniCPM4</span><span>Large Language Models</span><span>End Devices</span><span>Ultra-Efficient</span><span>Sparse Attention</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07900" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>CyberV: Cybernetics for Test-time Scaling in Video Understanding</h2>
                <span class="published-time">Published: 2025-06-09T17:45:18.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07971.png" alt="CyberV: Cybernetics for Test-time Scaling in Video Understanding">
                <p class="summary">Current Multimodal Large Language Models (MLLMs) may struggle with
understanding long or complex videos due to computational demands at test time,
lack of robustness, and limited accuracy, primarily stemming from their
feed-forward processing nature. These limitations could be more severe for
models with fewer parameters. To address these limitations, we propose a novel
framework inspired by cybernetic principles, redesigning video MLLMs as
adaptive systems capable of self-monitoring, self-correction, and dynamic
resource allocation during inference. Our approach, CyberV, introduces a
cybernetic loop consisting of an MLLM Inference System, a Sensor, and a
Controller. Specifically, the sensor monitors forward processes of the MLLM and
collects intermediate interpretations, such as attention drift, then the
controller determines when and how to trigger self-correction and generate
feedback to guide the next round. This test-time adaptive scaling framework
enhances frozen MLLMs without requiring retraining or additional components.
Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B
by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive
proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%
improvement, achieving performance even comparable to human experts.
Furthermore, our method demonstrates consistent gains on general-purpose
benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and
generalization capabilities in making MLLMs more robust and accurate for
dynamic video understanding. The code is released at
https://github.com/marinero4972/CyberV.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Video Understanding</span><span>Multimodal Large Language Models</span><span>Cybernetics</span><span>Test-time Adaptation</span><span>Self-correction</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Video Understanding</span><span>Multimodal</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07971" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal
  Interaction and Enhancement</h2>
                <span class="published-time">Published: 2025-06-09T15:11:09.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07848.png" alt="PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal
  Interaction and Enhancement">
                <p class="summary">Despite recent advances in video generation, existing models still lack
fine-grained controllability, especially for multi-subject customization with
consistent identity and interaction. In this paper, we propose PolyVivid, a
multi-subject video customization framework that enables flexible and
identity-consistent generation. To establish accurate correspondences between
subject images and textual entities, we design a VLLM-based text-image fusion
module that embeds visual identities into the textual space for precise
grounding. To further enhance identity preservation and subject interaction, we
propose a 3D-RoPE-based enhancement module that enables structured
bidirectional fusion between text and image embeddings. Moreover, we develop an
attention-inherited identity injection module to effectively inject fused
identity features into the video generation process, mitigating identity drift.
Finally, we construct an MLLM-based data pipeline that combines MLLM-based
grounding, segmentation, and a clique-based subject consolidation strategy to
produce high-quality multi-subject data, effectively enhancing subject
distinction and reducing ambiguity in downstream video generation. Extensive
experiments demonstrate that PolyVivid achieves superior performance in
identity fidelity, video realism, and subject alignment, outperforming existing
open-source and commercial baselines.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Multi-subject video generation</span><span>Cross-modal interaction</span><span>Identity preservation</span><span>Video customization</span><span>Multimodal models</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Generative AI</span><span>Computer Vision</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07848" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection
  Behavior</h2>
                <span class="published-time">Published: 2025-06-09T17:59:57.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08012.png" alt="GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection
  Behavior">
                <p class="summary">Multimodal Large Language Models (MLLMs) have shown great potential in
revolutionizing Graphical User Interface (GUI) automation. However, existing
GUI models mostly rely on learning from nearly error-free offline trajectories,
thus lacking reflection and error recovery capabilities. To bridge this gap, we
propose GUI-Reflection, a novel framework that explicitly integrates
self-reflection and error correction capabilities into end-to-end multimodal
GUI models throughout dedicated training stages: GUI-specific pre-training,
offline supervised fine-tuning (SFT), and online reflection tuning.
GUI-reflection enables self-reflection behavior emergence with fully automated
data generation and learning processes without requiring any human annotation.
Specifically, 1) we first propose scalable data pipelines to automatically
construct reflection and error correction data from existing successful
trajectories. While existing GUI models mainly focus on grounding and UI
understanding ability, we propose the GUI-Reflection Task Suite to learn and
evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a
diverse and efficient environment for online training and data collection of
GUI models on mobile devices. 3) We also present an iterative online reflection
tuning algorithm leveraging the proposed environment, enabling the model to
continuously enhance its reflection and error correction abilities. Our
framework equips GUI agents with self-reflection and correction capabilities,
paving the way for more robust, adaptable, and intelligent GUI automation, with
all data, models, environments, and tools to be released publicly.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>GUI Automation</span><span>Multimodal Large Language Models</span><span>Self-reflection</span><span>Error Correction</span><span>AI Agent</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>Multimodal</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08012" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SAFEFLOW: A Principled Protocol for Trustworthy and Transactional
  Autonomous Agent Systems</h2>
                <span class="published-time">Published: 2025-06-09T09:04:37.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07564.png" alt="SAFEFLOW: A Principled Protocol for Trustworthy and Transactional
  Autonomous Agent Systems">
                <p class="summary">Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywordsï¼š</span><span>Autonomous Agents</span><span>Information Flow Control</span><span>Transactional Execution</span><span>Multi-Agent Systems</span><span>Trustworthy Protocol</span></div>
                    <div class="area"><span class="label">Areasï¼š</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07564" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>