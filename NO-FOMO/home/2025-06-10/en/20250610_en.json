[
  {
    "id": "twitter_OpenAI_1932530409684005048",
    "source": "Twitter",
    "url": "https://x.com/OpenAI/status/1932530409684005048",
    "title_en": "OpenAI_OpenAI_o3-pro Fully Rolls Out to Pro Users",
    "summary_en": "OpenAI has announced that its latest model, o3-pro, is now fully rolling out to all ChatGPT Pro users and API developers. This update aims to enhance user experience and development efficiency, further expanding AI application scenarios. The launch of o3-pro is expected to bring more powerful features and stable services to professional users, solidifying OpenAI's leading position in the AI field.",
    "keywords_en": [
      "OpenAI",
      "o3-pro",
      "ChatGPT",
      "API",
      "Product Launch",
      "Large Language Model"
    ],
    "area_en": [
      "Product Launch",
      "Tech News",
      "Large Language Model"
    ],
    "published_time": "2025-06-10T20:08:49.000Z",
    "download_time": "2025-06-11 09:42:37",
    "visual_resource": [
      "screenshot/twitter/OpenAI_1932530409684005048.png"
    ],
    "extra_info": "{\"username\": \"OpenAI\", \"tweet_id\": \"1932530409684005048\"}"
  },
  {
    "id": "twitter_ArtificialAnlys_1932489580592435301",
    "source": "Twitter",
    "url": "https://twitter.com/ArtificialAnlys/status/1932489580592435301",
    "title_en": "ArtificialAnlys_OpenAI Significantly Reduces o3 Model Pricing",
    "summary_en": "OpenAI has announced an 80% reduction in the pricing of its o3 model, making it price-competitive with Gemini 2.5 Pro and Claude 4 Sonnet, and eight times cheaper than Claude 4 Opus. The o3 model is now priced at $2/$8 per 1M input/output tokens, down from $8/$40. Additionally, it offers a 75% discount for cached input tokens, significantly enhancing its market competitiveness.",
    "keywords_en": [
      "OpenAI",
      "o3",
      "Pricing Reduction",
      "Large Language Model",
      "Market Competition"
    ],
    "area_en": [
      "Large Language Model",
      "Product Launch",
      "Industry News"
    ],
    "published_time": "2025-06-10T17:26:32.000Z",
    "download_time": "2025-06-11 17:18:01",
    "visual_resource": [
      "screenshot/twitter/ArtificialAnlys_1932489580592435301.png"
    ],
    "extra_info": "{\"username\": \"ArtificialAnlys\", \"tweet_id\": \"1932489580592435301\"}"
  },
  {
    "id": "twitter_MistralAI_1932445269611688315",
    "source": "Twitter",
    "url": "https://twitter.com/MistralAI/status/1932445269611688315",
    "title_en": "MistralAI_Launches First Reasoning Model Magistral",
    "summary_en": "Mistral AI recently announced the launch of Magistral, its first reasoning model. This model is specifically designed to excel in domain-specific, transparent, and multilingual reasoning, aiming to provide more efficient and reliable AI inference capabilities. This release marks a new advancement for Mistral AI in the field of AI model development, expected to play a significant role in various application scenarios.",
    "keywords_en": [
      "Mistral AI",
      "Magistral",
      "Reasoning Model",
      "Multilingual Reasoning",
      "Product Launch"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Product Launch"
    ],
    "published_time": "2025-06-10T14:15:33.000Z",
    "download_time": "2025-06-11 17:18:23",
    "visual_resource": [
      "screenshot/twitter/MistralAI_1932445269611688315.png"
    ],
    "extra_info": "{\"username\": \"MistralAI\", \"tweet_id\": \"1932445269611688315\"}"
  },
  {
    "id": "twitter_LangChainAI_1932493346498543898",
    "source": "Twitter",
    "url": "https://twitter.com/LangChainAI/status/1932493346498543898",
    "title_en": "LangChainAI_Uber Uses LangGraph to Build AI Developer Agents for Code Fixes",
    "summary_en": "LangChainAI shared how Uber leveraged the LangGraph framework to build AI developer agents. These agents are capable of generating thousands of code fixes daily, serving an organization of 5,000 developers working with hundreds of millions of lines of code, and have already saved over 21,000 hours of development time. This case highlights the significant potential of AI in enhancing software development efficiency.",
    "keywords_en": [
      "LangChain",
      "LangGraph",
      "Uber",
      "AI Developer Agents",
      "Code Fixes",
      "Automation"
    ],
    "area_en": [
      "AI Agent",
      "Tech News",
      "Industry News"
    ],
    "published_time": "2025-06-10T17:41:32.000Z",
    "download_time": "2025-06-11 17:18:35",
    "visual_resource": [
      "screenshot/twitter/LangChainAI_1932493346498543898.png"
    ],
    "extra_info": "{\"username\": \"LangChainAI\", \"tweet_id\": \"1932493346498543898\"}"
  },
  {
    "id": "twitter_Kling_ai_1932464913018147291",
    "source": "Twitter",
    "url": "https://twitter.com/Kling_ai/status/1932464913018147291",
    "title_en": "Kling_ai_Pengfei Wan to Share Kling Video Generation Models at CVPR",
    "summary_en": "Kling AI announced that Pengfei Wan, Head of Kling Video Generation Models, will deliver a keynote speech at CVPR, the leading annual computer vision event hosted by IEEE. Wan's presentation, titled \"An Introduction to Kling and Our Research towards More Powerful Video Generation Models,\" will delve into Kling's latest breakthroughs and cutting-edge advancements in video generation technology. The event is scheduled for June 11, 2025, from 9:00-17:00 (UTC-5), aiming to provide exceptional value to students, researchers, and industry professionals.",
    "keywords_en": [
      "Kling AI",
      "CVPR",
      "Video Generation",
      "Computer Vision",
      "Pengfei Wan",
      "Keynote"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Research Progress"
    ],
    "published_time": "2025-06-10T15:48:33.000Z",
    "download_time": "2025-06-11 17:19:04",
    "visual_resource": [
      "screenshot/twitter/Kling_ai_1932464913018147291.png"
    ],
    "extra_info": "{\"username\": \"Kling_ai\", \"tweet_id\": \"1932464913018147291\"}"
  },
  {
    "id": "twitter_summeryue0_1932487755034210475",
    "source": "Twitter",
    "url": "https://twitter.com/summeryue0/status/1932487755034210475",
    "title_en": "summeryue0_Scale AI Releases LLM Red Teaming and System Safety Roadmap",
    "summary_en": "Summer Yue and Zifan (Sail) Wang from Scale AI, representing SEAL and Red Team, have jointly published a significant position paper. This document outlines key learnings from their extensive experience in red teaming Large Language Models (LLMs), addressing what truly matters, what remains to be explored, and how model safety is intrinsically linked to broader system safety and continuous monitoring. The paper emphasizes crucial research priorities for red teaming frontier AI models and proposes a comprehensive roadmap for achieving robust system-level safety and effective AI monitoring, integrating diverse perspectives from practitioners to researchers.",
    "keywords_en": [
      "Red Teaming",
      "LLM Safety",
      "System Safety",
      "AI Monitoring",
      "Scale AI",
      "Position Paper"
    ],
    "area_en": [
      "Large Language Model",
      "Research Progress",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-10T17:19:19.000Z",
    "download_time": "2025-06-11 17:19:59",
    "visual_resource": [
      "screenshot/twitter/summeryue0_1932487755034210475.png"
    ],
    "extra_info": "{\"username\": \"summeryue0\", \"tweet_id\": \"1932487755034210475\"}"
  },
  {
    "id": "zhCpd8QZTf4JLK6OdufEiQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/zhCpd8QZTf4JLK6OdufEiQ",
    "title_en": "CAS Pioneers '1-bit VLA Model': 70%+ Memory Reduction, Performance Comparable to 4-bit OpenVLA, Enabling VLA on Low-Cost Devices",
    "summary_en": "The Chinese Academy of Sciences (CAS) team has pioneered the \"1-bit VLA Model\" (BitVLA), marking the first 1-bit Vision-Language-Action model specifically designed for robot manipulation. This innovative model utilizes ternary parameters of {-1, 0, 1} for its core architecture. Through a novel distillation-aware training strategy, BitVLA effectively compresses its visual encoder to just 1.58-bit weights, leading to a remarkable memory reduction of over 70%. Despite this significant compression, the model maintains performance comparable to state-of-the-art 4-bit OpenVLA on the challenging LIBERO benchmark. BitVLA's key advantage is its exceptionally low memory footprint, which enables efficient execution of complex VLA tasks even on low-cost, resource-constrained edge devices. This breakthrough demonstrates immense potential for deploying advanced robotic AI in a wider range of practical applications, making intelligent robotics more accessible and scalable.",
    "keywords_en": [
      "1-bit VLA Model",
      "Robot Manipulation",
      "Memory Optimization",
      "Model Quantization",
      "Edge Devices",
      "Distillation-Aware Training"
    ],
    "area_en": [
      "Robotics",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-06-10T23:45:43.000Z",
    "download_time": "2025-06-12T01:06:52.454428",
    "visual_resource": [
      "screenshot/20250610/wechat/wechat_image_zhCpd8QZTf4JLK6OdufEiQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "wx1ujkfAlQfAN0VCOO4bBA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/wx1ujkfAlQfAN0VCOO4bBA",
    "title_en": "A 20-Person Team Preemptively Realizes DeepSeek's Vision: A Paradigm Shift in AI Computing Power and Addressing Large Model Compute Cost Challenges",
    "summary_en": "Addressing the current computing power bottlenecks in large AI models, this article discusses the limitations of traditional GPGPU architectures and DeepSeek's vision for AI hardware development. It primarily introduces YuPan AI's SRDA system-level dataflow computing architecture, which aims to resolve core hardware pain points such as memory capacity/bandwidth, computational precision, cluster expansion, and PCIe bus contention. SRDA's innovative design, featuring dataflow-driven principles, system-level interconnection, 3D-stacked high-bandwidth memory, streamlined efficiency, and software-defined reconfigurability, is expected to significantly enhance the performance of large model training and inference, reduce costs, and improve stability. The article suggests that SRDA's philosophy aligns with DeepSeek's cutting-edge research, potentially signaling a new paradigm for AI-specific computing architectures. Especially as large model technical requirements converge, SRDA could become a watershed moment, distinguishing between general-purpose GPGPUs and truly AI-dedicated architectures.",
    "keywords_en": [
      "AI Computing Power",
      "Large Models",
      "Dataflow Architecture",
      "SRDA",
      "Hardware Acceleration",
      "DeepSeek"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Deep Learning"
    ],
    "published_time": "2025-06-10T16:06:12.000Z",
    "download_time": "2025-06-12T01:06:52.344375",
    "visual_resource": [
      "screenshot/20250610/wechat/wechat_image_wx1ujkfAlQfAN0VCOO4bBA.png"
    ],
    "extra_info": null
  },
  {
    "id": "XvNpHmrvzYV40CgGD_MVBQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/XvNpHmrvzYV40CgGD_MVBQ",
    "title_en": "VILA-U: A Unified Foundation Model Integrating Visual Understanding and Generation",
    "summary_en": "VILA-U introduces an innovative unified foundation model designed to integrate visual understanding and generation tasks, encompassing image-text understanding, video-text understanding, image generation, and video generation. Central to this model is its \"Unified Foundation Vision Tower,\" which functions as an image tokenizer. This tower is trained using a combination of image reconstruction loss and image-text contrastive loss, enabling it to excel in both generative and discriminative capabilities. VILA-U adopts a unified Next-Token Prediction training paradigm, eliminating reliance on external diffusion models and establishing an end-to-end autoregressive framework. Experimental results demonstrate VILA-U's competitive performance across visual-language understanding and generation benchmarks. It effectively addresses the performance degradation often associated with discrete visual tokens in understanding tasks, offering a concise yet powerful solution for advancing multimodal AI.",
    "keywords_en": [
      "VILA-U",
      "Unified Model",
      "Visual Understanding",
      "Visual Generation",
      "Multimodal",
      "Next-Token Prediction"
    ],
    "area_en": [
      "Multimodal",
      "Computer Vision",
      "Generative AI"
    ],
    "published_time": "2025-06-10T16:06:12.000Z",
    "download_time": "2025-06-12T01:06:47.228143",
    "visual_resource": [
      "screenshot/20250610/wechat/wechat_image_XvNpHmrvzYV40CgGD_MVBQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "0DVNQbifWQ5nWHKPNSwwHA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/0DVNQbifWQ5nWHKPNSwwHA",
    "title_en": "Peking University and UC Berkeley Unveil IDA-Bench: A New Benchmark Exposing LLM Agents' Limitations in Iterative Data Analysis, Top Models Score Only 40%",
    "summary_en": "Peking University and UC Berkeley have jointly introduced IDA-Bench, a novel benchmark designed to simulate real-world, iterative, and exploratory data analysis scenarios, specifically evaluating large language model agents' performance under multi-turn, dynamic instructions. Unlike traditional single-turn evaluations, IDA-Bench exposes the challenges of continuous interaction. Test results reveal that even top models like Claude-3.7 and Gemini-2.5 Pro achieve only a 40% task success rate, significantly below expectations. The research highlights current agents' profound difficulty in balancing strict instruction adherence with necessary autonomous reasoning, often exhibiting \"overconfident\" or \"overcautious\" behaviors that lead to critical task failures. This comprehensive evaluation underscores the critical need for substantial improvements in LLM agents' understanding, instruction following, and interactive capabilities to truly become reliable and effective data analysis assistants in complex, real-world settings.",
    "keywords_en": [
      "Large Language Model",
      "AI Agent",
      "Data Analysis",
      "Benchmark",
      "Iterative Interaction",
      "Instruction Following"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-10T05:16:42.000Z",
    "download_time": "2025-06-12T01:06:53.904459",
    "visual_resource": [
      "screenshot/20250610/wechat/wechat_image_0DVNQbifWQ5nWHKPNSwwHA.png"
    ],
    "extra_info": null
  },
  {
    "id": "cNGAFt0FxbI0_WPwYZgEmA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/cNGAFt0FxbI0_WPwYZgEmA",
    "title_en": "Let AI Design Chips! Chinese Academy of Sciences Launches 'QiMeng' for Fully Automated Chip Design Flow",
    "summary_en": "The Institute of Computing Technology, Chinese Academy of Sciences, in collaboration with the Institute of Software, has unveiled 'QiMeng,' a fully automated design system for processor chips and foundational software, powered by large models and other AI technologies. This system can autonomously complete chip hardware and software design, partially or entirely surpassing human expert levels. It has successfully designed RISC-V CPUs automatically, achieving performance comparable to ARM Cortex A53. 'QiMeng' employs a three-tiered architecture comprising domain-specific large models, intelligent agents, and an application layer. It addresses challenges such as data scarcity, correctness, and solution scale through an iterative evolution approach, promising to significantly enhance chip design efficiency, shorten development cycles, enable rapid customization, and fundamentally transform the paradigm of processor chip hardware and software design.",
    "keywords_en": [
      "Chip Design",
      "Artificial Intelligence",
      "Large Models",
      "Automated Design",
      "QiMeng System",
      "Processor Chip"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-10T04:06:16.000Z",
    "download_time": "2025-06-12T01:06:57.500291",
    "visual_resource": [
      "screenshot/20250610/wechat/wechat_image_cNGAFt0FxbI0_WPwYZgEmA.png"
    ],
    "extra_info": null
  },
  {
    "id": "G--WIqw9wMlkMkIVgFJXug",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/G--WIqw9wMlkMkIVgFJXug",
    "title_en": "Adobe's Self Forcing: Real-time Video Generation on a Single RTX 4090",
    "summary_en": "Adobe, in collaboration with the University of Texas at Austin, has introduced the Self Forcing algorithm, designed to address exposure bias and error accumulation in autoregressive video generation. Inspired by RNN sequence modeling, this algorithm explicitly unfolds the autoregressive generation process during training, enabling the model to learn from its own prediction errors and utilizing holistic distribution-matching losses to supervise the complete sequence. The research employs innovative strategies such as a few-step diffusion backbone, gradient truncation, dynamic step sampling, and gradient flow isolation to achieve efficient training. Furthermore, a rolling KV cache mechanism is introduced to support infinitely long video generation. This technology enables real-time video generation at 17 FPS with sub-second latency on a single H100 GPU, and 10 FPS on an RTX 4090, while delivering superior generation quality compared to existing models. This breakthrough opens new possibilities for interactive video applications like live streaming and gaming.",
    "keywords_en": [
      "Real-time Video Generation",
      "Self Forcing",
      "Autoregressive Models",
      "Exposure Bias",
      "Diffusion Models",
      "Adobe"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-06-10T03:59:40.000Z",
    "download_time": "2025-06-12T01:06:56.170141",
    "visual_resource": [
      "screenshot/20250610/wechat/wechat_image_G--WIqw9wMlkMkIVgFJXug.png"
    ],
    "extra_info": null
  },
  {
    "id": "awesome-llm-apps",
    "source": "GitHub",
    "url": "https://github.com/Shubhamsaboo/awesome-llm-apps",
    "title_en": "ðŸŒŸ Awesome LLM Apps",
    "summary_en": "The \"Awesome LLM Apps\" GitHub repository presents a meticulously curated collection of practical large language model (LLM) applications. These applications are ingeniously built utilizing advanced techniques such as Retrieval-Augmented Generation (RAG), sophisticated AI Agents, collaborative Multi-agent Teams, Multi-Context Processing (MCP), and intuitive Voice Agents. The repository showcases the versatile integration of leading LLM providers like OpenAI, Anthropic, and Google, alongside powerful open-source models including DeepSeek, Qwen, and Llama, which can even be run locally. It illustrates how LLMs can address real-world challenges across diverse domains, from analyzing code repositories to managing email inboxes. This project's core objective is to offer tangible, innovative LLM application examples, thereby accelerating the practical deployment and advancement of large model technologies across various industries. Furthermore, it actively encourages community contributions, aiming to cultivate a vibrant and comprehensive open-source ecosystem for LLM-powered solutions.",
    "keywords_en": [
      "Large Language Model",
      "AI Agent",
      "RAG",
      "Multi-agent Systems",
      "Generative AI",
      "Application Development",
      "Machine Learning"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-06T22:50:54Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png",
      "https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&type=Date",
      "https://trendshift.io/api/badge/repositories/9876"
    ],
    "extra_info": null
  },
  {
    "id": "boltz",
    "source": "GitHub",
    "url": "https://github.com/jwohlwend/boltz",
    "title_en": "Boltz",
    "summary_en": "Boltz is a family of models designed for biomolecular interaction prediction, with Boltz-2 being the latest foundational model. It surpasses AlphaFold3 and Boltz-1 by jointly modeling complex structures and binding affinities. Boltz-2 is the first deep learning model to achieve accuracy comparable to physics-based free-energy perturbation (FEP) methods, while being 1000 times faster. This breakthrough makes accurate in silico screening practical for early-stage drug discovery, including hit-discovery and ligand optimization. The Boltz models and code are open-sourced under the MIT license, available for both academic and commercial use.",
    "keywords_en": [
      "Biomolecular Interaction",
      "Binding Affinity",
      "Drug Discovery",
      "Deep Learning",
      "Molecular Design",
      "In Silico Screening",
      "Open Source Model"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-06-11T15:19:15Z",
    "download_time": "2024-05-20 10:00:00",
    "visual_resource": [
      "https://github.com/jwohlwend/boltz/raw/main/docs/boltz2_title.png",
      "https://github.com/jwohlwend/boltz/raw/main/docs/boltz1_pred_figure.png",
      "https://github.com/jwohlwend/boltz/raw/main/docs/plot_test_boltz2.png"
    ],
    "extra_info": null
  },
  {
    "id": "youtube-transcript-api",
    "source": "GitHub",
    "url": "https://github.com/jdepoix/youtube-transcript-api",
    "title_en": "âœ¨ YouTube Transcript API âœ¨",
    "summary_en": "The YouTube Transcript API is a Python library designed to retrieve transcripts and subtitles for YouTube videos. It supports both automatically generated subtitles and multi-language translation, operating efficiently without the need for headless browsers, which significantly improves performance. The API also features proxy support to circumvent IP blocks, cookie authentication for age-restricted content, and offers various output formats like JSON and SRT. Additionally, it includes a command-line interface, greatly simplifying the process for developers and users to access and process YouTube video content.",
    "keywords_en": [
      "YouTube Subtitles",
      "Video Transcription",
      "Python Library",
      "API",
      "Subtitle Translation",
      "Proxy",
      "Command Line Tool"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Video Understanding"
    ],
    "published_time": "2025-04-23T07:30:48Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "screenshot/github/youtube-transcript-api.png"
    ],
    "extra_info": null
  },
  {
    "id": "self-llm",
    "source": "GitHub",
    "url": "https://github.com/datawhalechina/self-llm",
    "title_en": "Open-source Large Language Model Handbook",
    "summary_en": "This project presents a dedicated open-source large language model (LLM) tutorial, specifically designed for beginners in China and optimized for Linux platforms. It provides comprehensive, full-process guidance encompassing essential skills such as environment configuration, local deployment, and efficient fine-tuning for a wide array of open-source LLMs. By simplifying the complex deployment, usage, and application workflows, the initiative aims to make advanced LLM technologies more accessible to a broader audience of students and researchers. The tutorial covers mainstream models like LLaMA, ChatGLM, and InternLM, offering practical instructions on command-line invocation, setting up online demonstrations, and integrating with frameworks like LangChain. Furthermore, it delves into advanced topics such as distributed full fine-tuning, LoRA, and P-tuning methods. This resource is crucial for fostering the adoption of open-source, free large models, enabling learners to seamlessly incorporate them into their studies and future professional endeavors.",
    "keywords_en": [
      "Large Language Models",
      "Open-source",
      "Environment Configuration",
      "Model Deployment",
      "Model Fine-tuning",
      "Natural Language Processing",
      "Linux",
      "LangChain"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-06-11T14:30:51Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://contrib.rocks/image?repo=datawhalechina/self-llm"
    ],
    "extra_info": null
  },
  {
    "id": "MiniCPM",
    "source": "GitHub",
    "url": "https://github.com/OpenBMB/MiniCPM",
    "title_en": "MiniCPM",
    "summary_en": "MiniCPM is an ultra-efficient series of large language models designed for edge devices, co-developed by ModelBest, Tsinghua University, and Renmin University of China. It achieves exceptional efficiency through innovative model architectures like InfLLM v2 sparse attention, efficient learning algorithms such as BitCPM 3-value quantization, and optimized inference systems like CPM.cu. While maintaining state-of-the-art performance for its size, MiniCPM models deliver over 5x generation speedup on typical edge chips. They also surpass similarly sized and even larger models in tasks like tool calling, code interpretation, and long-context processing, offering a powerful solution for edge AI applications.",
    "keywords_en": [
      "Large Language Model",
      "Edge AI",
      "Efficient Inference",
      "Sparse Attention",
      "Model Quantization",
      "Tool Calling",
      "Long Context",
      "Natural Language Processing"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-11T06:36:09Z",
    "download_time": "2024-07-10 07:00:00",
    "visual_resource": [
      "https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm_logo.png",
      "https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm4/efficiency.png",
      "https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm4/benchmark.png"
    ],
    "extra_info": null
  },
  {
    "id": "2506.08007",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.08007",
    "title_en": "Reinforcement Pre-Training",
    "summary_en": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\nparadigm for large language models and reinforcement learning (RL).\nSpecifically, we reframe next-token prediction as a reasoning task trained\nusing RL, where it receives verifiable rewards for correctly predicting the\nnext token for a given context. RPT offers a scalable method to leverage vast\namounts of text data for general-purpose RL, rather than relying on\ndomain-specific annotated answers. By incentivizing the capability of\nnext-token reasoning, RPT significantly improves the language modeling accuracy\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\nfoundation for further reinforcement fine-tuning. The scaling curves show that\nincreased training compute consistently improves the next-token prediction\naccuracy. The results position RPT as an effective and promising scaling\nparadigm to advance language model pre-training.",
    "keywords_en": [
      "Reinforcement Pre-Training",
      "Large Language Models",
      "Reinforcement Learning",
      "Next-token prediction",
      "Language Model Pre-training"
    ],
    "area_en": [
      "Machine Learning",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-09T17:59:53.000Z",
    "download_time": "2025-06-11 10:07:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08007.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.08007\", \"arxiv_url\": \"https://arxiv.org/abs/2506.08007\"}"
  },
  {
    "id": "2506.07900",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.07900",
    "title_en": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
    "summary_en": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.",
    "keywords_en": [
      "MiniCPM4",
      "Large Language Models",
      "End Devices",
      "Ultra-Efficient",
      "Sparse Attention"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-09T16:16:50.000Z",
    "download_time": "2025-06-11 10:07:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07900.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.07900\", \"arxiv_url\": \"https://arxiv.org/abs/2506.07900\"}"
  },
  {
    "id": "2506.07971",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.07971",
    "title_en": "CyberV: Cybernetics for Test-time Scaling in Video Understanding",
    "summary_en": "Current Multimodal Large Language Models (MLLMs) may struggle with\nunderstanding long or complex videos due to computational demands at test time,\nlack of robustness, and limited accuracy, primarily stemming from their\nfeed-forward processing nature. These limitations could be more severe for\nmodels with fewer parameters. To address these limitations, we propose a novel\nframework inspired by cybernetic principles, redesigning video MLLMs as\nadaptive systems capable of self-monitoring, self-correction, and dynamic\nresource allocation during inference. Our approach, CyberV, introduces a\ncybernetic loop consisting of an MLLM Inference System, a Sensor, and a\nController. Specifically, the sensor monitors forward processes of the MLLM and\ncollects intermediate interpretations, such as attention drift, then the\ncontroller determines when and how to trigger self-correction and generate\nfeedback to guide the next round. This test-time adaptive scaling framework\nenhances frozen MLLMs without requiring retraining or additional components.\nExperiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B\nby 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive\nproprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%\nimprovement, achieving performance even comparable to human experts.\nFurthermore, our method demonstrates consistent gains on general-purpose\nbenchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and\ngeneralization capabilities in making MLLMs more robust and accurate for\ndynamic video understanding. The code is released at\nhttps://github.com/marinero4972/CyberV.",
    "keywords_en": [
      "Video Understanding",
      "Multimodal Large Language Models",
      "Cybernetics",
      "Test-time Adaptation",
      "Self-correction"
    ],
    "area_en": [
      "Video Understanding",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-06-09T17:45:18.000Z",
    "download_time": "2025-06-11 10:07:38",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07971.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.07971\", \"arxiv_url\": \"https://arxiv.org/abs/2506.07971\"}"
  },
  {
    "id": "2506.07848",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.07848",
    "title_en": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\n  Interaction and Enhancement",
    "summary_en": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.",
    "keywords_en": [
      "Multi-subject video generation",
      "Cross-modal interaction",
      "Identity preservation",
      "Video customization",
      "Multimodal models"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Multimodal"
    ],
    "published_time": "2025-06-09T15:11:09.000Z",
    "download_time": "2025-06-11 10:07:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07848.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.07848\", \"arxiv_url\": \"https://arxiv.org/abs/2506.07848\"}"
  },
  {
    "id": "2506.08012",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.08012",
    "title_en": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior",
    "summary_en": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.",
    "keywords_en": [
      "GUI Automation",
      "Multimodal Large Language Models",
      "Self-reflection",
      "Error Correction",
      "AI Agent"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-09T17:59:57.000Z",
    "download_time": "2025-06-11 10:07:27",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08012.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.08012\", \"arxiv_url\": \"https://arxiv.org/abs/2506.08012\"}"
  },
  {
    "id": "2506.07564",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.07564",
    "title_en": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
    "summary_en": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
    "keywords_en": [
      "Autonomous Agents",
      "Information Flow Control",
      "Transactional Execution",
      "Multi-Agent Systems",
      "Trustworthy Protocol"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-09T09:04:37.000Z",
    "download_time": "2025-06-11 10:07:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07564.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.07564\", \"arxiv_url\": \"https://arxiv.org/abs/2506.07564\"}"
  }
]