[
  {
    "id": "twitter_sama_1932547251945288147",
    "source": "Twitter",
    "url": "https://x.com/sama/status/1932547251945288147",
    "title_en": "sama_sama_On 'Gentle Singularity' and the End of AI-Free Writing",
    "summary_en": "Sam Altman released a new post titled \"the gentle singularity,\" remarking that it might be the last piece he writes entirely without AI assistance. He highlighted the perspective that \"From a relativistic perspective, the singularity happens bit by bit, and the merge happens slowly,\" suggesting a future where creative work is deeply integrated with AI. This tweet signifies the advent of a new era of human-AI collaboration.",
    "keywords_en": [
      "Sam Altman",
      "Gentle Singularity",
      "AI Writing",
      "Artificial Intelligence",
      "Future Trends"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-06-10T21:15:43.000Z",
    "download_time": "2025-06-11 09:42:04",
    "visual_resource": [
      "screenshot/twitter/sama_1932547251945288147.png"
    ],
    "extra_info": "{\"username\": \"sama\", \"tweet_id\": \"1932547251945288147\"}"
  },
  {
    "id": "twitter_OpenAI_1932530409684005048",
    "source": "Twitter",
    "url": "https://x.com/OpenAI/status/1932530409684005048",
    "title_en": "OpenAI_OpenAI_o3-pro Fully Rolls Out to Pro Users",
    "summary_en": "OpenAI has announced that its latest model, o3-pro, is now fully rolling out to all ChatGPT Pro users and API developers. This update aims to enhance user experience and development efficiency, further expanding AI application scenarios. The launch of o3-pro is expected to bring more powerful features and stable services to professional users, solidifying OpenAI's leading position in the AI field.",
    "keywords_en": [
      "OpenAI",
      "o3-pro",
      "ChatGPT",
      "API",
      "Product Launch",
      "Large Language Model"
    ],
    "area_en": [
      "Product Launch",
      "Tech News",
      "Large Language Model"
    ],
    "published_time": "2025-06-10T20:08:49.000Z",
    "download_time": "2025-06-11 09:42:37",
    "visual_resource": [
      "screenshot/twitter/OpenAI_1932530409684005048.png"
    ],
    "extra_info": "{\"username\": \"OpenAI\", \"tweet_id\": \"1932530409684005048\"}"
  },
  {
    "id": "twitter_sama_1932573231199707168",
    "source": "Twitter",
    "url": "https://x.com/sama/status/1932573231199707168",
    "title_en": "sama_sama_OpenAI Open-Weights Model Release Delayed",
    "summary_en": "Sam Altman announced a delay in the release of OpenAI's \"open-weights model.\" Originally anticipated for June, the model is now expected to be launched later this summer. Altman stated that the postponement is due to an \"unexpected and quite amazing\" discovery by their research team, which necessitates additional development time to ensure its full potential is realized. This suggests a significant breakthrough in OpenAI's open-source initiatives, building anticipation for its eventual release and indicating a potentially groundbreaking development in the field.",
    "keywords_en": [
      "Open-weights model",
      "Sam Altman",
      "OpenAI",
      "Model release",
      "Research breakthrough",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Large Language Model",
      "Tech News",
      "Research Progress"
    ],
    "published_time": "2025-06-10T22:58:58.000Z",
    "download_time": "2025-06-11 09:42:05",
    "visual_resource": [
      "screenshot/twitter/sama_1932573231199707168.png"
    ],
    "extra_info": "{\"username\": \"sama\", \"tweet_id\": \"1932573231199707168\"}"
  },
  {
    "id": "zhCpd8QZTf4JLK6OdufEiQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/zhCpd8QZTf4JLK6OdufEiQ",
    "title_en": "CAS Pioneers '1-bit VLA Model': 70%+ Memory Reduction, Performance Comparable to 4-bit OpenVLA, Enabling VLA on Low-Cost Devices",
    "summary_en": "The Chinese Academy of Sciences (CAS) team has pioneered the \"1-bit VLA Model\" (BitVLA), marking the first 1-bit Vision-Language-Action model specifically designed for robot manipulation. This innovative model utilizes ternary parameters of {-1, 0, 1} for its core architecture. Through a novel distillation-aware training strategy, BitVLA effectively compresses its visual encoder to just 1.58-bit weights, leading to a remarkable memory reduction of over 70%. Despite this significant compression, the model maintains performance comparable to state-of-the-art 4-bit OpenVLA on the challenging LIBERO benchmark. BitVLA's key advantage is its exceptionally low memory footprint, which enables efficient execution of complex VLA tasks even on low-cost, resource-constrained edge devices. This breakthrough demonstrates immense potential for deploying advanced robotic AI in a wider range of practical applications, making intelligent robotics more accessible and scalable.",
    "keywords_en": [
      "1-bit VLA Model",
      "Robot Manipulation",
      "Memory Optimization",
      "Model Quantization",
      "Edge Devices",
      "Distillation-Aware Training"
    ],
    "area_en": [
      "Robotics",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-06-10T23:45:43.000Z",
    "download_time": "2025-06-11T16:53:00.775368",
    "visual_resource": [
      "screenshot/wechat/wechat_image_zhCpd8QZTf4JLK6OdufEiQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "wx1ujkfAlQfAN0VCOO4bBA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/wx1ujkfAlQfAN0VCOO4bBA",
    "title_en": "A 20-Person Team Preemptively Realizes DeepSeek's Vision: A Paradigm Shift in AI Computing Power and Addressing Large Model Compute Cost Challenges",
    "summary_en": "Addressing the current computing power bottlenecks in large AI models, this article discusses the limitations of traditional GPGPU architectures and DeepSeek's vision for AI hardware development. It primarily introduces YuPan AI's SRDA system-level dataflow computing architecture, which aims to resolve core hardware pain points such as memory capacity/bandwidth, computational precision, cluster expansion, and PCIe bus contention. SRDA's innovative design, featuring dataflow-driven principles, system-level interconnection, 3D-stacked high-bandwidth memory, streamlined efficiency, and software-defined reconfigurability, is expected to significantly enhance the performance of large model training and inference, reduce costs, and improve stability. The article suggests that SRDA's philosophy aligns with DeepSeek's cutting-edge research, potentially signaling a new paradigm for AI-specific computing architectures. Especially as large model technical requirements converge, SRDA could become a watershed moment, distinguishing between general-purpose GPGPUs and truly AI-dedicated architectures.",
    "keywords_en": [
      "AI Computing Power",
      "Large Models",
      "Dataflow Architecture",
      "SRDA",
      "Hardware Acceleration",
      "DeepSeek"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Deep Learning"
    ],
    "published_time": "2025-06-10T16:06:12.000Z",
    "download_time": "2025-06-11T16:53:02.543764",
    "visual_resource": [
      "screenshot/wechat/wechat_image_wx1ujkfAlQfAN0VCOO4bBA.png"
    ],
    "extra_info": null
  },
  {
    "id": "XvNpHmrvzYV40CgGD_MVBQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/XvNpHmrvzYV40CgGD_MVBQ",
    "title_en": "VILA-U: A Unified Foundation Model Integrating Visual Understanding and Generation",
    "summary_en": "VILA-U introduces an innovative unified foundation model designed to integrate visual understanding and generation tasks, encompassing image-text understanding, video-text understanding, image generation, and video generation. Central to this model is its \"Unified Foundation Vision Tower,\" which functions as an image tokenizer. This tower is trained using a combination of image reconstruction loss and image-text contrastive loss, enabling it to excel in both generative and discriminative capabilities. VILA-U adopts a unified Next-Token Prediction training paradigm, eliminating reliance on external diffusion models and establishing an end-to-end autoregressive framework. Experimental results demonstrate VILA-U's competitive performance across visual-language understanding and generation benchmarks. It effectively addresses the performance degradation often associated with discrete visual tokens in understanding tasks, offering a concise yet powerful solution for advancing multimodal AI.",
    "keywords_en": [
      "VILA-U",
      "Unified Model",
      "Visual Understanding",
      "Visual Generation",
      "Multimodal",
      "Next-Token Prediction"
    ],
    "area_en": [
      "Multimodal",
      "Computer Vision",
      "Generative AI"
    ],
    "published_time": "2025-06-10T16:06:12.000Z",
    "download_time": "2025-06-11T16:52:55.281123",
    "visual_resource": [
      "screenshot/wechat/wechat_image_XvNpHmrvzYV40CgGD_MVBQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "0DVNQbifWQ5nWHKPNSwwHA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/0DVNQbifWQ5nWHKPNSwwHA",
    "title_en": "Peking University and UC Berkeley Unveil IDA-Bench: A New Benchmark Exposing LLM Agents' Limitations in Iterative Data Analysis, Top Models Score Only 40%",
    "summary_en": "Peking University and UC Berkeley have jointly introduced IDA-Bench, a novel benchmark designed to simulate real-world, iterative, and exploratory data analysis scenarios, specifically evaluating large language model agents' performance under multi-turn, dynamic instructions. Unlike traditional single-turn evaluations, IDA-Bench exposes the challenges of continuous interaction. Test results reveal that even top models like Claude-3.7 and Gemini-2.5 Pro achieve only a 40% task success rate, significantly below expectations. The research highlights current agents' profound difficulty in balancing strict instruction adherence with necessary autonomous reasoning, often exhibiting \"overconfident\" or \"overcautious\" behaviors that lead to critical task failures. This comprehensive evaluation underscores the critical need for substantial improvements in LLM agents' understanding, instruction following, and interactive capabilities to truly become reliable and effective data analysis assistants in complex, real-world settings.",
    "keywords_en": [
      "Large Language Model",
      "AI Agent",
      "Data Analysis",
      "Benchmark",
      "Iterative Interaction",
      "Instruction Following"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-10T05:16:42.000Z",
    "download_time": "2025-06-11T16:53:17.855293",
    "visual_resource": [
      "screenshot/wechat/wechat_image_0DVNQbifWQ5nWHKPNSwwHA.png"
    ],
    "extra_info": null
  },
  {
    "id": "cNGAFt0FxbI0_WPwYZgEmA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/cNGAFt0FxbI0_WPwYZgEmA",
    "title_en": "Let AI Design Chips! Chinese Academy of Sciences Launches 'QiMeng' for Fully Automated Chip Design Flow",
    "summary_en": "The Institute of Computing Technology, Chinese Academy of Sciences, in collaboration with the Institute of Software, has unveiled 'QiMeng,' a fully automated design system for processor chips and foundational software, powered by large models and other AI technologies. This system can autonomously complete chip hardware and software design, partially or entirely surpassing human expert levels. It has successfully designed RISC-V CPUs automatically, achieving performance comparable to ARM Cortex A53. 'QiMeng' employs a three-tiered architecture comprising domain-specific large models, intelligent agents, and an application layer. It addresses challenges such as data scarcity, correctness, and solution scale through an iterative evolution approach, promising to significantly enhance chip design efficiency, shorten development cycles, enable rapid customization, and fundamentally transform the paradigm of processor chip hardware and software design.",
    "keywords_en": [
      "Chip Design",
      "Artificial Intelligence",
      "Large Models",
      "Automated Design",
      "QiMeng System",
      "Processor Chip"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-10T04:06:16.000Z",
    "download_time": "2025-06-11T16:53:22.122462",
    "visual_resource": [
      "screenshot/wechat/wechat_image_cNGAFt0FxbI0_WPwYZgEmA.png"
    ],
    "extra_info": null
  },
  {
    "id": "_5M7uc86kCTWxqSUSPkIfg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/_5M7uc86kCTWxqSUSPkIfg",
    "title_en": "Are Large Language Models 'Observing the World from a Cave'? RL Expert Warns of LLM's Fatal Flaws",
    "summary_en": "University of California, Berkeley reinforcement learning expert Sergey Levine argues that current Large Language Models (LLMs) do not learn directly from the world but rather indirectly \"scan\" the \"projections\" of human thought processes from internet text, akin to observers in Plato's Cave. He contends that LLMs' success stems from \"reverse engineering\" human cognitive processes, not from genuinely understanding the world or learning from direct experience. Levine questions the limitations of LLMs in physical world comprehension and autonomous skill acquisition, proposing that future AI development must explore new methods for acquiring representations directly from physical experience. This approach is crucial for achieving truly flexible and adaptive intelligence, moving beyond merely replicating the \"shadows\" of human minds. He contrasts this with the relative lack of success in video models, despite video containing richer real-world information. The core issue, according to Levine, is that LLMs are mimicking human intelligence by observing its output, not by developing their own understanding of reality. This perspective challenges the current trajectory of AGI research, advocating for AI systems that can learn and adapt from their own interactions with the physical world, rather than relying on human-mediated data.",
    "keywords_en": [
      "Large Language Models",
      "Reinforcement Learning",
      "Plato's Cave",
      "Artificial Intelligence",
      "Cognitive Limitations",
      "Sergey Levine"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "published_time": "2025-06-10T03:59:40.000Z",
    "download_time": "2025-06-11T16:53:18.908745",
    "visual_resource": [
      "screenshot/wechat/wechat_image__5M7uc86kCTWxqSUSPkIfg.png"
    ],
    "extra_info": null
  },
  {
    "id": "BlackFriday-GPTs-Prompts",
    "source": "GitHub",
    "url": "https://github.com/friuns2/BlackFriday-GPTs-Prompts",
    "title_en": "GPTs Prompts And Jailbreaks",
    "summary_en": "This GitHub repository compiles a vast collection of GPTs prompts and jailbreak instructions, spanning various domains such as programming, marketing, academia, and gaming. It aims to provide users with diverse AI interaction examples, highlighting the fully autonomous AI software engineer AiDark.net and the GPTOS Android application that supports these prompts. This resource offers practical tools for AI application development and prompt engineering practices.",
    "keywords_en": [
      "GPTs",
      "Prompt Engineering",
      "Jailbreak Instructions",
      "Large Language Model Applications",
      "AI Software Engineer",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2024-11-08T11:03:14Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "screenshot/github/BlackFriday-GPTs-Prompts.png"
    ],
    "extra_info": null
  },
  {
    "id": "Prompt-Engineering-Guide",
    "source": "GitHub",
    "url": "https://github.com/dair-ai/Prompt-Engineering-Guide",
    "title_en": "Prompt Engineering Guide",
    "summary_en": "The Prompt Engineering Guide is a comprehensive resource dedicated to prompt engineering, designed to help developers and researchers efficiently utilize large language models (LLMs). This guide compiles the latest research papers, learning tutorials, lectures, and tools, covering a wide range of topics from fundamental concepts to advanced techniques (e.g., Chain-of-Thought, Retrieval Augmented Generation) and practical applications (e.g., question answering, code generation). It also offers related courses and consulting services, serving as an authoritative resource for learning and practicing LLM prompt engineering.",
    "keywords_en": [
      "Prompt Engineering",
      "Large Language Models",
      "Natural Language Processing",
      "LLM Applications",
      "Chain-of-Thought",
      "Retrieval Augmented Generation",
      "AI Education"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-09T14:29:31Z",
    "download_time": "2024-07-30 10:30:00",
    "visual_resource": [
      "screenshot/github/Prompt-Engineering-Guide.png"
    ],
    "extra_info": null
  },
  {
    "id": "self-llm",
    "source": "GitHub",
    "url": "https://github.com/datawhalechina/self-llm",
    "title_en": "Open-source Large Language Model Tutorial",
    "summary_en": "This project is an open-source large language model tutorial specifically designed for beginners in China, based on the Linux platform. It provides comprehensive guidance covering environment configuration, local model deployment, and efficient fine-tuning. The tutorial includes mainstream models like LLaMA, ChatGLM, and InternLM, and teaches application methods such as command-line invocation, Web Demo deployment, and LangChain integration. Its goal is to lower the barrier to using open-source LLMs, empowering more students and researchers, and promoting the widespread application of open-source large models in daily learning and life.",
    "keywords_en": [
      "Open-source Large Language Models",
      "Environment Configuration",
      "Model Deployment",
      "Efficient Fine-tuning",
      "LLM Applications",
      "Linux Platform"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-09T14:27:09Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "screenshot/github/self-llm.png"
    ],
    "extra_info": null
  },
  {
    "id": "servers",
    "source": "GitHub",
    "url": "https://github.com/modelcontextprotocol/servers",
    "title_en": "Model Context Protocol servers",
    "summary_en": "This GitHub repository serves as a collection of reference implementations for Model Context Protocol (MCP) servers, designed to provide Large Language Models (LLMs) with secure, controlled access to tools and data sources. It demonstrates MCP's versatility and extensibility, featuring reference servers built with TypeScript and Python SDKs, alongside a vast array of third-party and community-developed integration servers. These integrations span various domains including cloud services, databases, APIs, and blockchain, significantly expanding the application capabilities of LLMs.",
    "keywords_en": [
      "Model Context Protocol",
      "Large Language Model",
      "AI Agent",
      "API Integration",
      "Reference Implementation",
      "Tool Protocol",
      "SDK",
      "Server"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-10T22:14:54Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "screenshot/github/servers.png"
    ],
    "extra_info": null
  },
  {
    "id": "GenAI_Agents",
    "source": "GitHub",
    "url": "https://github.com/NirDiamant/GenAI_Agents",
    "title_en": "GenAI Agents: Comprehensive Repository for Development and Implementation 🚀",
    "summary_en": "This GitHub repository serves as a comprehensive resource for the development and implementation of Generative AI agents, offering tutorials and code examples ranging from basic conversational bots to complex multi-agent systems. Leveraging mainstream frameworks like LangChain and LangGraph, it provides a wealth of practical cases, aiming to foster learning, experimentation, and innovation in the field of AI agents, while also encouraging community contributions.",
    "keywords_en": [
      "Generative AI",
      "AI Agents",
      "LangChain",
      "LangGraph",
      "Multi-Agent Systems",
      "RAG",
      "Prompt Engineering",
      "AI Development"
    ],
    "area_en": [
      "Generative AI",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-05-14T19:56:00Z",
    "download_time": "2024-05-15 07:00:00",
    "visual_resource": [
      "https://github.com/NirDiamant/GenAI_Agents/blob/main/images/substack_image.png",
      "https://github.com/NirDiamant/GenAI_Agents/blob/main/images/subscribe-button.svg"
    ],
    "extra_info": null
  },
  {
    "id": "2506.08012",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.08012",
    "title_en": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior",
    "summary_en": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.",
    "keywords_en": [
      "GUI Automation",
      "Multimodal Large Language Models",
      "Self-reflection",
      "Error Correction",
      "AI Agent"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-09T17:59:57.000Z",
    "download_time": "2025-06-11 01:29:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08012.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.08012\", \"arxiv_url\": \"https://arxiv.org/abs/2506.08012\"}"
  },
  {
    "id": "2506.04651",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.04651",
    "title_en": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning",
    "summary_en": "Recent advances in LLMs have enabled their use as autonomous agents across a\nrange of tasks, yet they continue to struggle with formulating and adhering to\ncoherent long-term strategies. In this paper, we investigate whether LLM agents\ncan self-improve when placed in environments that explicitly challenge their\nstrategic planning abilities. Using the board game Settlers of Catan, accessed\nthrough the open-source Catanatron framework, we benchmark a progression of\nLLM-based agents, from a simple game-playing agent to systems capable of\nautonomously rewriting their own prompts and their player agent's code. We\nintroduce a multi-agent architecture in which specialized roles (Analyzer,\nResearcher, Coder, and Player) collaborate to iteratively analyze gameplay,\nresearch new strategies, and modify the agent's logic or prompt. By comparing\nmanually crafted agents to those evolved entirely by LLMs, we evaluate how\neffectively these systems can diagnose failure and adapt over time. Our results\nshow that self-evolving agents, particularly when powered by models like Claude\n3.7 and GPT-4o, outperform static baselines by autonomously adopting their\nstrategies, passing along sample behavior to game-playing agents, and\ndemonstrating adaptive reasoning over multiple iterations.",
    "keywords_en": [
      "LLM agents",
      "Self-evolving",
      "Strategic planning",
      "Multi-agent",
      "Adaptive reasoning"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-05T05:45:24.000Z",
    "download_time": "2025-06-11 01:29:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04651.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.04651\", \"arxiv_url\": \"https://arxiv.org/abs/2506.04651\"}"
  },
  {
    "id": "2506.07564",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.07564",
    "title_en": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
    "summary_en": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
    "keywords_en": [
      "Autonomous Agents",
      "Information Flow Control",
      "Transactional Execution",
      "Multi-Agent Systems",
      "Trustworthy Protocol"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-09T09:04:37.000Z",
    "download_time": "2025-06-11 01:29:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07564.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.07564\", \"arxiv_url\": \"https://arxiv.org/abs/2506.07564\"}"
  },
  {
    "id": "2506.03231",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.03231",
    "title_en": "NetPress: Dynamically Generated LLM Benchmarks for Network Applications",
    "summary_en": "Despite growing interest in domain-specific benchmarking of large language\nmodels (LLMs) and agents, current evaluations remain limited to static,\nsmall-scale datasets, especially in high-stakes tasks like network operations\nthat demand reliability for deployments. We present NetPress, an automated\nbenchmark generation framework for evaluating LLM agents in network\napplications. NetPress introduces a unified abstraction with state and action,\nenabling dynamic generation of diverse query sets along with corresponding\nground truths. At runtime, users can specify benchmark configurations to\ngenerate millions of queries on the fly. In addition to dynamic benchmark\nconstruction, NetPress integrates with network emulators to provide realistic\nenvironment feedback, supporting comprehensive evaluation across correctness,\nsafety, and latency. We instantiate NetPress on three representative\napplications, revealing interesting fine-grained differences in agent behavior\nthat static, correctness-only benchmarks often miss. NetPress moves LLM\nevaluation toward realistic, scalable testing in infrastructure-centric\ndomains, helping close the gap between benchmark performance and real-world\ndeployment readiness. Code is available at\nhttps://github.com/Froot-NetSys/NetPress.",
    "keywords_en": [
      "Large Language Models",
      "Benchmarking",
      "Network Applications",
      "AI Agents",
      "Dynamic Generation"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-03T14:04:22.000Z",
    "download_time": "2025-06-11 01:29:47",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03231.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.03231\", \"arxiv_url\": \"https://arxiv.org/abs/2506.03231\"}"
  },
  {
    "id": "2506.06205",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.06205",
    "title_en": "Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\n  Learning",
    "summary_en": "Modern robot navigation systems encounter difficulties in diverse and complex\nindoor environments. Traditional approaches rely on multiple modules with small\nmodels or rule-based systems and thus lack adaptability to new environments. To\naddress this, we developed Astra, a comprehensive dual-model architecture,\nAstra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a\nmultimodal LLM, processes vision and language inputs to perform self and goal\nlocalization using a hybrid topological-semantic graph as the global map, and\noutperforms traditional visual place recognition methods. Astra-Local, a\nmultitask network, handles local path planning and odometry estimation. Its 4D\nspatial-temporal encoder, trained through self-supervised learning, generates\nrobust 4D features for downstream tasks. The planning head utilizes flow\nmatching and a novel masked ESDF loss to minimize collision risks for\ngenerating local trajectories, and the odometry head integrates multi-sensor\ninputs via a transformer encoder to predict the relative pose of the robot.\nDeployed on real in-house mobile robots, Astra achieves high end-to-end mission\nsuccess rate across diverse indoor environments.",
    "keywords_en": [
      "Mobile Robots",
      "Robot Navigation",
      "Multimodal Learning",
      "Large Language Model",
      "Path Planning"
    ],
    "area_en": [
      "Robotics",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-06-06T16:08:47.000Z",
    "download_time": "2025-06-11 01:29:38",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06205.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.06205\", \"arxiv_url\": \"https://arxiv.org/abs/2506.06205\"}"
  },
  {
    "id": "2506.07982",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.07982",
    "title_en": "τ^2-Bench: Evaluating Conversational Agents in a Dual-Control\n  Environment",
    "summary_en": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce tau^2-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, tau^2-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.",
    "keywords_en": [
      "conversational agents",
      "dual-control",
      "benchmark",
      "user simulator",
      "Dec-POMDP"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-09T17:52:18.000Z",
    "download_time": "2025-06-11 01:29:37",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07982.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.07982\", \"arxiv_url\": \"https://arxiv.org/abs/2506.07982\"}"
  }
]