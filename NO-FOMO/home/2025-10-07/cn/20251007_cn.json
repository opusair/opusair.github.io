[
  {
    "id": "hackernews_45502541",
    "source": "Hacker News",
    "url": "https://www.qualcomm.com/news/releases/2025/10/qualcomm-to-acquire-arduino-accelerating-developers--access-to-i",
    "title": "Qualcomm to acquire Arduino",
    "summary": "Qualcomm's acquisition of Arduino has been announced, signaling a notable shift in the embedded systems and IoT development landscape. While Arduino is slated to maintain its distinct brand identity and core mission, this integration with Qualcomm's extensive technological portfolio is set to significantly enhance developer access to advanced capabilities. This strategic move aims to merge Arduino's widely recognized developer ecosystem with Qualcomm's expertise in chipsets and connectivity solutions. A key outcome anticipated from this merger includes the potential introduction of innovative hardware, such as the 'Arduino Uno Q,' designed to facilitate the execution of artificial intelligence and large language model code. Additionally, these new platforms are expected to feature robust signal processing capabilities and support sophisticated operating systems like Linux and Zephyr OS, thereby offering a more powerful and versatile environment for embedded development and edge computing.",
    "keywords": [
      "Qualcomm",
      "Arduino",
      "Embedded Systems",
      "IoT",
      "Artificial Intelligence",
      "Large Language Models",
      "Linux",
      "Zephyr OS"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Others"
    ],
    "published_time": "2025-10-07 13:00:08",
    "download_time": "2025-10-07 20:01:29",
    "extra_info": "{\"score\": 795, \"by\": \"janjongboom\", \"descendants\": 349, \"story_id\": 45502541}"
  },
  {
    "id": "hackernews_45506268",
    "source": "Hacker News",
    "url": "https://arxiv.org/abs/2510.04871",
    "title": "Less Is More: Recursive Reasoning with Tiny Networks",
    "summary": "A recent research paper, 'Less Is More: Recursive Reasoning with Tiny Networks,' introduces a novel paradigm for achieving robust reasoning capabilities in artificial intelligence systems using significantly smaller neural network architectures. This study challenges the prevailing trend of developing ever-larger models by demonstrating that computational efficiency and effective problem-solving can be realized through innovative algorithmic designs, specifically recursive reasoning. The core idea involves equipping tiny networks with mechanisms to iteratively process information and refine their internal states, allowing for deep understanding and complex inference without a massive parameter count. This recursive approach enables these compact models to perform tasks that typically demand much larger computational resources. The findings highlight a critical direction for sustainable AI development, offering a path to deploy sophisticated AI solutions in resource-constrained environments, such as edge devices or applications requiring real-time performance. This research contributes significantly to the fields of efficient AI, neural architecture design, and the broader pursuit of intelligent systems that balance performance with computational footprint.",
    "keywords": [
      "Recursive Reasoning",
      "Tiny Networks",
      "Neural Networks",
      "Model Efficiency",
      "AI Research",
      "Deep Learning",
      "Resource-Efficient AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-10-07 17:42:06",
    "download_time": "2025-10-07 20:01:27",
    "extra_info": "{\"score\": 64, \"by\": \"guybedo\", \"descendants\": 12, \"story_id\": 45506268}"
  },
  {
    "id": "hackernews_45504388",
    "source": "Hacker News",
    "url": "https://github.com/llama-farm/llamafarm",
    "title": "Launch HN: LlamaFarm (YC W22) – Open-source framework for distributed AI",
    "summary": "LlamaFarm, founded by Rob, Matt, and Rachel (YC W22), has launched an open-source AI framework designed for distributed AI. Their core philosophy posits that the future of AI lies in specialized models deployed widely and continually fine-tuned with real-world data, rather than monolithic cloud-based models. The team identified a critical challenge in AI development: the failure of AI demos to transition into robust production systems, often due to issues like RAG degradation and model obsolescence with live data. LlamaFarm's innovative solution is 'declarative AI-as-code,' which streamlines the AI lifecycle. This approach uses a single YAML configuration to define and manage models, policies, data, evaluation metrics, and deployment, aiming to overcome the inherent fragility of current AI deployment practices and ensure models remain effective in real-world scenarios.",
    "keywords": [
      "Distributed AI",
      "Open-source AI",
      "AI Framework",
      "Model Fine-tuning",
      "Declarative AI",
      "AI Deployment",
      "RAG",
      "YAML configuration"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-07 15:30:20",
    "download_time": "2025-10-07 20:01:35",
    "extra_info": "{\"score\": 41, \"by\": \"mhamann\", \"descendants\": 26, \"story_id\": 45504388}"
  },
  {
    "id": "hackernews_45507264",
    "source": "Hacker News",
    "url": "https://photowand.ai/packs",
    "title": "Photographers are losing their jobs faster than software engineers",
    "summary": "A recent observation highlights a significant trend in the modern labor market, suggesting that photographers are experiencing job displacement at a faster rate than software engineers. This phenomenon is largely attributed to the rapid advancements in artificial intelligence and automation technologies, which are increasingly capable of performing tasks traditionally requiring human creativity and skill in photography. AI-powered tools, such as those for image generation, editing, and enhancement, are streamlining workflows and, in some cases, replacing the need for human photographers. In contrast, while software engineering is also evolving with AI assistance, the complexity and dynamic nature of software development tasks often require a deeper level of human problem-solving, abstract thinking, and collaborative innovation that AI has yet to fully replicate. This trend underscores a shifting landscape where creative professions, once considered less susceptible to automation, are now facing substantial technological disruption, prompting a reevaluation of future workforce skills and the economic impacts of AI across various sectors.",
    "keywords": [
      "Artificial Intelligence",
      "Automation",
      "Generative AI",
      "Job Displacement",
      "Labor Market Impact",
      "Creative Automation",
      "AI in Photography"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Computer Vision"
    ],
    "published_time": "2025-10-07 18:57:17",
    "download_time": "2025-10-07 20:01:49",
    "extra_info": "{\"score\": 6, \"by\": \"fengjiabo2400\", \"descendants\": 11, \"story_id\": 45507264}"
  },
  {
    "id": "hackernews_45504127",
    "source": "Hacker News",
    "url": "https://news.ycombinator.com/item?id=45504127",
    "title": "Show HN: MARS – Personal AI robot for builders (< $2k)",
    "summary": "Innate co-founders Axel and Vignesh have introduced MARS, a personal AI robot specifically designed for builders, priced under $2,000. MARS is positioned as a general-purpose robotic platform featuring an open onboard agentic operating system, which is built on the ROS2 framework. This design aims to provide a versatile and customizable tool, empowering users to develop and integrate various applications. The team has released several resources, including an overview video, a control demonstration, examples of autonomous use-cases, and a quickstart guide, to facilitate understanding and adoption. The open architecture and agentic OS underscore a commitment to making advanced robotics and AI more accessible, enabling developers and hobbyists to explore complex automation and intelligent functionalities within an affordable and adaptable hardware ecosystem.",
    "keywords": [
      "Personal AI robot",
      "Agentic OS",
      "ROS2",
      "Robotics",
      "Open Source AI"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Robotics"
    ],
    "published_time": "2025-10-07 15:11:52",
    "download_time": "2025-10-07 20:01:45",
    "extra_info": "{\"score\": 30, \"by\": \"apeytavin\", \"descendants\": 23, \"story_id\": 45504127}"
  },
  {
    "id": "hackernews_45500485",
    "source": "Hacker News",
    "url": "https://www.theguardian.com/australia-news/2025/oct/06/deloitte-to-pay-money-back-to-albanese-government-after-using-ai-in-440000-report",
    "title": "Deloitte to refund the Australian government after using AI in $440k report",
    "summary": "Deloitte Australia has agreed to refund the federal government $440,000 after it was revealed that artificial intelligence tools were used in a major report without adequate disclosure or oversight. The incident raises significant questions regarding the ethical deployment of AI within professional consulting services and the expectations of clients, particularly government bodies, concerning the provenance and integrity of deliverables. While the specific nature of the AI's application in the report was not detailed, the requirement for a refund underscores growing scrutiny over the quality and originality of work produced with AI assistance. This development highlights the imperative for consulting firms to establish clear policies on AI integration, ensuring transparency with clients and maintaining rigorous human-centric quality controls to avoid misrepresentation and preserve professional trust. The case serves as a precedent, emphasizing that the mere use of AI, if improperly managed or disclosed, can lead to substantial financial and reputational repercussions for major firms. It points to an increasing demand for accountability in the rapidly evolving landscape of AI-augmented services.",
    "keywords": [
      "AI ethics",
      "AI governance",
      "Professional Services",
      "Consulting",
      "Transparency",
      "Accountability",
      "Artificial Intelligence"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-07 07:51:57",
    "download_time": "2025-10-07 20:02:23",
    "extra_info": "{\"score\": 397, \"by\": \"fforflo\", \"descendants\": 201, \"story_id\": 45500485}"
  },
  {
    "id": "zen-mcp-server",
    "source": "GitHub",
    "url": "https://github.com/BeehiveInnovations/zen-mcp-server",
    "title": "Zen MCP: Many Workflows. One Context.",
    "summary": "Zen MCP is a Model Context Protocol server designed to empower AI development by orchestrating multiple AI models and CLIs within a single, continuous workflow. It integrates popular developer CLIs such as Claude Code, Gemini CLI, and Codex CLI with various AI providers including OpenAI, Gemini, Anthropic, and local Ollama models. A core feature is the new `clink` tool, enabling direct connection and interaction between external AI CLIs and supporting isolated \"subagents\" for specialized tasks like code reviews or bug hunting, preserving main context. Zen MCP emphasizes conversation continuity, allowing models to discuss ideas, exchange reasoning, and conduct collaborative debates, with context seamlessly flowing across tools and models, even after context resets. It offers specialized tools for code analysis, planning, and development, facilitating complex workflows from multi-model code review to implementation and pre-commit validation. This platform transforms a single CLI into an AI dev team, enabling strategic application of diverse model strengths and capabilities.",
    "keywords": [
      "AI Orchestration",
      "Multi-Model AI",
      "AI Agent",
      "CLI Integration",
      "Code Review",
      "Context Management",
      "Large Language Models",
      "Development Workflow"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-10-07T14:51:19Z",
    "download_time": "2024-05-16 12:35:00",
    "extra_info": null
  },
  {
    "id": "sim",
    "source": "GitHub",
    "url": "https://github.com/simstudioai/sim",
    "title": "Build and deploy AI agent workflows in minutes.",
    "summary": "Sim is an open-source platform specifically engineered for the rapid construction and deployment of AI agent workflows. It provides flexible deployment models, allowing users to choose between a convenient cloud-hosted service or robust self-hosted options via NPM, Docker Compose, or manual setup. A notable feature is its compatibility with local AI models through Ollama, granting users full control and reducing reliance on external APIs. The platform is architected on a modern tech stack, including Next.js, Bun, and PostgreSQL with the pgvector extension, which is crucial for enabling advanced AI functionalities like knowledge bases and semantic search through vector embeddings. Further enhancing its capabilities are Drizzle ORM, Better Auth for authentication, ReactFlow for an intuitive workflow editor, and Socket.io for real-time interactions. The integration of Trigger.dev for background jobs and E2B for remote code execution positions Sim as a comprehensive and powerful environment for developers to design, test, and operationalize sophisticated AI applications efficiently. This makes Sim a compelling choice for organizations and individuals aiming to harness the power of AI agents with complete control over their infrastructure.",
    "keywords": [
      "AI Agent",
      "Workflow Automation",
      "Self-hosting",
      "LLM",
      "Vector Embeddings",
      "PostgreSQL",
      "ReactFlow",
      "Next.js"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-10-06T20:22:22Z",
    "download_time": "2024-05-18 12:00:00",
    "extra_info": null
  },
  {
    "id": "cua",
    "source": "GitHub",
    "url": "https://github.com/trycua/cua",
    "title": "cua",
    "summary": "Cua (pronounced 'koo-ah') serves as a Docker-like platform for Computer-Use Agents, enabling AI agents to control full operating systems within virtualized containers, deployable both locally and in the cloud. The platform offers a robust Computer SDK for automating Windows, Linux, and macOS virtual machines via a consistent, pyautogui-like API, facilitating local or cloud-based VM creation and management. Additionally, the Agent SDK provides a unified schema for running computer-use models, simplifying benchmarking against datasets like OSWorld-Verified and SheetBench-V2 with a single line of code. It supports combining UI grounding models with various Large Language Models and integrates a diverse Model Zoo, including models from Anthropic, OpenAI, Hugging Face, and Microsoft. Cua aims to streamline the development, deployment, and evaluation of AI agents capable of interacting with and controlling operating systems, offering solutions for automated task execution, research, and benchmarking in the evolving field of AI interaction.",
    "keywords": [
      "Computer-Use Agents",
      "AI Agents",
      "Virtual Machines",
      "Operating System Automation",
      "Python SDK",
      "LLM Integration",
      "UI Grounding",
      "Benchmarking"
    ],
    "area": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-10-06T17:07:33Z",
    "download_time": "2024-07-30 15:10:00",
    "extra_info": null
  },
  {
    "id": "2510.05096",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.05096",
    "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
    "summary": "Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce PaperTalker, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.",
    "keywords": [
      "Video generation",
      "Academic presentations",
      "Multi-modal information",
      "AI agent",
      "Research communication"
    ],
    "area": [
      "Generative AI",
      "Multimodal",
      "AI Agent"
    ],
    "published_time": "2025-10-06T17:58:02.000Z",
    "download_time": "2025-10-07 13:02:41",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.05096\", \"arxiv_url\": \"https://arxiv.org/abs/2510.05096\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05096.png\", \"original_title\": \"Paper2Video: Automatic Video Generation from Scientific Papers\"}"
  },
  {
    "id": "2510.05091",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.05091",
    "title": "Factuality Matters: When Image Generation and Editing Meet Structured Visuals",
    "summary": "While modern visual generation models excel at creating aesthetically pleasing natural images, they struggle with producing or editing structured visuals like charts, diagrams, and mathematical figures, which demand composition planning, text rendering, and multimodal reasoning for factual fidelity. To address this, we present the first comprehensive, systematic investigation of this domain, encompassing data construction, model training, and an evaluation benchmark. First, we construct a large-scale dataset of 1.3 million high-quality structured image pairs derived from executable drawing programs and augmented with chain-of-thought reasoning annotations. Building on it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a lightweight connector for enhanced multimodal understanding. A three-stage training curriculum enables progressive feature alignment, knowledge infusion, and reasoning-augmented generation, further boosted by an external reasoner at inference time. Finally, we introduce StructBench, a novel benchmark for generation and editing with over 1,700 challenging instances, and an accompanying evaluation metric, StructScore, which employs a multi-round Q&A protocol to assess fine-grained factual accuracy. Evaluations of 15 models reveal that even leading closed-source systems remain far from satisfactory. Our model attains strong editing performance, and inference-time reasoning yields consistent gains across diverse architectures. By releasing the dataset, model, and benchmark, we aim to advance unified multimodal foundations for structured visuals.",
    "keywords": [
      "Structured Visuals",
      "Image Generation and Editing",
      "Multimodal Reasoning",
      "Factual Fidelity",
      "Generative AI"
    ],
    "area": [
      "Generative AI",
      "Multimodal",
      "Computer Vision"
    ],
    "published_time": "2025-10-06T17:56:55.000Z",
    "download_time": "2025-10-07 13:02:42",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.05091\", \"arxiv_url\": \"https://arxiv.org/abs/2510.05091\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05091.png\", \"original_title\": \"Factuality Matters: When Image Generation and Editing Meet Structured\n  Visuals\"}"
  },
  {
    "id": "2510.05040",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.05040",
    "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts",
    "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.",
    "keywords": [
      "Diffusion LLMs",
      "Test-Time Scaling",
      "Hidden Semi-Autoregressive Experts",
      "Inference Method",
      "Reasoning Benchmarks"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-10-06T17:16:41.000Z",
    "download_time": "2025-10-07 13:02:45",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.05040\", \"arxiv_url\": \"https://arxiv.org/abs/2510.05040\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05040.png\", \"original_title\": \"Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive\n  Experts\"}"
  },
  {
    "id": "2510.04618",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.04618",
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
    "summary": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.",
    "keywords": [
      "Large Language Models",
      "AI Agent",
      "Context Adaptation",
      "Self-Improving LLMs",
      "Agentic Context Engineering"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-06T09:30:18.000Z",
    "download_time": "2025-10-07 13:02:43",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.04618\", \"arxiv_url\": \"https://arxiv.org/abs/2510.04618\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04618.png\", \"original_title\": \"Agentic Context Engineering: Evolving Contexts for Self-Improving\n  Language Models\"}"
  },
  {
    "id": "2510.03561",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.03561",
    "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models",
    "summary": "The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity (O(L^2)) with respect to sequence length L. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic (O(N^2 cdot T)) to linear (O(N cdot T)) with respect to the number of interactions N. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size.",
    "keywords": [
      "Reactive Transformer",
      "Event-Driven",
      "Stateful Processing",
      "Large Language Models",
      "Conversational AI"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-10-03T23:18:07.000Z",
    "download_time": "2025-10-07 13:02:41",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.03561\", \"arxiv_url\": \"https://arxiv.org/abs/2510.03561\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03561.png\", \"original_title\": \"Reactive Transformer (RxT) -- Stateful Real-Time Processing for\n  Event-Driven Reactive Language Models\"}"
  },
  {
    "id": "2510.01586",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.01586",
    "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning",
    "summary": "LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.",
    "keywords": [
      "Multi-Agent Reinforcement Learning",
      "Adversarial Co-Evolution",
      "AI Safety",
      "LLM Agents",
      "Jailbreak Attacks"
    ],
    "area": [
      "AI Agent",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-10-02T02:06:30.000Z",
    "download_time": "2025-10-07 13:02:48",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.01586\", \"arxiv_url\": \"https://arxiv.org/abs/2510.01586\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01586.png\", \"original_title\": \"AdvEvo-MARL: Shaping Internalized Safety through Adversarial\n  Co-Evolution in Multi-Agent Reinforcement Learning\"}"
  }
]