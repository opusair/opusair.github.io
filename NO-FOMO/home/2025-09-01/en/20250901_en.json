[
  {
    "id": "twitter_roo_code_1962571908224110673",
    "source": "Twitter",
    "url": "https://twitter.com/roo_code/status/1962571908224110673",
    "title_en": "roo_code_Grok Code Achieves 90% on Evals, Half Cost, Free Trial",
    "summary_en": "Roo Code announced that XAI's Grok Code achieved an impressive 90% score on Roo Code evaluations, demonstrating top-tier code generation performance at half the cost of its peers. Grok Code is currently available for a free trial on the Roo Code Cloud platform until September 10. This performance highlights Grok's significant advantages in terms of speed and cost-effectiveness, positioning it as a strong new addition in the code development and evaluation domain.",
    "keywords_en": [
      "Grok Code",
      "Roo Code",
      "Code Evaluation",
      "AI Model",
      "Cost-effectiveness",
      "Free Trial"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Product Launch"
    ],
    "published_time": "2025-09-01T17:43:00.000Z",
    "download_time": "2025-09-02 05:43:05",
    "visual_resource": [
      "screenshot/twitter/roo_code_1962571908224110673.png"
    ],
    "extra_info": "{\"username\": \"roo_code\", \"tweet_id\": \"1962571908224110673\"}"
  },
  {
    "id": "twitter_QuixiAI_1962600301309108304",
    "source": "Twitter",
    "url": "https://twitter.com/QuixiAI/status/1962600301309108304",
    "title_en": "QuixiAI_Evaluating Grok-code-fast-1's Performance in Code Porting",
    "summary_en": "Eric Hartford of QuixiAI tested xAI's grok-code-fast-1 model for porting a Python project (tinygrad) to C. He found the model performed poorly when handling large files, failing to meet expectations and even corrupting test code. Hartford noted that grok-code-fast-1 needs improvement in agentic capabilities and large file editing, suggesting that splitting files might enhance its processing ability in the future. Previously, he had success using Claude and GPT models for similar tasks.",
    "keywords_en": [
      "grok-code-fast-1",
      "Code Porting",
      "Large Language Model",
      "AI Agent",
      "Code Generation",
      "Performance Evaluation"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Tech News"
    ],
    "published_time": "2025-09-01T19:35:49.000Z",
    "download_time": "2025-09-02 05:43:05",
    "visual_resource": [
      "screenshot/twitter/QuixiAI_1962600301309108304.png"
    ],
    "extra_info": "{\"username\": \"QuixiAI\", \"tweet_id\": \"1962600301309108304\"}"
  },
  {
    "id": "twitter_Tim_Dettmers_1962603940291260533",
    "source": "Twitter",
    "url": "https://twitter.com/Tim_Dettmers/status/1962603940291260533",
    "title_en": "Tim_Dettmers_GLM-4.5 Speed and Performance Review",
    "summary_en": "Tim Dettmers tested GLM 4.5, finding it approximately 3x faster than Claude Code + Opus 4.1 and 5x faster than GPT-5-high, while maintaining performance comparable to closed-source models. He noted that GLM-4.5's speed significantly boosts productivity. Z.ai also announced a GLM Coding Plan to make GLM-4.5 more accessible, priced at one-seventh of the original Claude Code.",
    "keywords_en": [
      "GLM 4.5",
      "Large Language Model",
      "Performance",
      "Speed",
      "AI Tools",
      "Z.ai"
    ],
    "area_en": [
      "Large Language Model",
      "Tech News",
      "Product Launch"
    ],
    "published_time": "2025-09-01T19:50:17.000Z",
    "download_time": "2025-09-02 05:43:02",
    "visual_resource": [
      "screenshot/twitter/Tim_Dettmers_1962603940291260533.png"
    ],
    "extra_info": "{\"username\": \"Tim_Dettmers\", \"tweet_id\": \"1962603940291260533\"}"
  },
  {
    "id": "twitter_ClementDelangue_1962526559115358645",
    "source": "Twitter",
    "url": "https://twitter.com/ClementDelangue/status/1962526559115358645",
    "title_en": "ClementDelangue_Apple Releases FastVLM and MobileCLIP2, Advancing AI Vision Language Models",
    "summary_en": "Clement Delangue highlights Apple's ongoing advancements in AI, noting the recent release of FastVLM and MobileCLIP2 models on Hugging Face. These models are up to 85 times faster and 3.4 times smaller than previous work, significantly accelerating the development of real-time Vision Language Model (VLM) applications, including local live video captioning directly in the browser.",
    "keywords_en": [
      "Apple",
      "Artificial Intelligence",
      "Vision Language Model",
      "FastVLM",
      "MobileCLIP2",
      "Hugging Face"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Computer Vision",
      "Multimodal"
    ],
    "published_time": "2025-09-01T14:42:48.000Z",
    "download_time": "2025-09-02 05:43:23",
    "visual_resource": [
      "screenshot/twitter/ClementDelangue_1962526559115358645.png"
    ],
    "extra_info": "{\"username\": \"ClementDelangue\", \"tweet_id\": \"1962526559115358645\"}"
  },
  {
    "id": "twitter_vllm_project_1962509793345859666",
    "source": "Twitter",
    "url": "https://twitter.com/vllm_project/status/1962509793345859666",
    "title_en": "vllm_project_vLLM Announces Support for Kwai Keye-VL-1.5 Multimodal Model",
    "summary_en": "The vLLM project announced that its inference framework now supports the Kwai Keye-VL-1.5 multimodal model. This model significantly enhances video and image comprehension capabilities, strengthens reasoning performance, and features an extended 128K context length. These improvements enable richer conversations and more complex task processing. Users can experience this new functionality by upgrading to vLLM's nightly build.",
    "keywords_en": [
      "vLLM",
      "Keye-VL-1.5",
      "Multimodal Model",
      "Video Understanding",
      "Image Comprehension",
      "Reasoning"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-09-01T13:36:11.000Z",
    "download_time": "2025-09-02 05:43:24",
    "visual_resource": [
      "screenshot/twitter/vllm_project_1962509793345859666.png"
    ],
    "extra_info": "{\"username\": \"vllm_project\", \"tweet_id\": \"1962509793345859666\"}"
  },
  {
    "id": "twitter_gordic_aleksa_1962545137613173124",
    "source": "Twitter",
    "url": "https://twitter.com/gordic_aleksa/status/1962545137613173124",
    "title_en": "gordic_aleksa_Inside vLLM: Anatomy of a High-Throughput LLM Inference System",
    "summary_en": "Aleksa GordiÄ‡ has published an in-depth blog post titled \"Inside vLLM: Anatomy of a High-Throughput LLM Inference System,\" detailing the workings of LLM inference engines, particularly vLLM. The article covers fundamental inference engine processes, advanced optimization techniques (e.g., chunked prefill, prefix caching, guided decoding, speculative decoding), multi-GPU/multi-node scaling, and performance measurement. This post aims to provide high-quality, deep technical insights, offering significant value for understanding LLM inference systems.",
    "keywords_en": [
      "vLLM",
      "LLM Inference",
      "High-Throughput",
      "Deep Learning",
      "Performance Optimization",
      "Distributed Systems"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Tech News"
    ],
    "published_time": "2025-09-01T15:56:37.000Z",
    "download_time": "2025-09-02 05:43:28",
    "visual_resource": [
      "screenshot/twitter/gordic_aleksa_1962545137613173124.png"
    ],
    "extra_info": "{\"username\": \"gordic_aleksa\", \"tweet_id\": \"1962545137613173124\"}"
  },
  {
    "id": "RjzobuTbfHTMrClypip0kA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/RjzobuTbfHTMrClypip0kA",
    "title_en": "DeepSeek Discloses Full Training Details of V3/R1 Models, Revealing Significant Information",
    "summary_en": "DeepSeek has responded to new regulations by announcing that all its AI-generated content will be clearly marked as \"AI-generated,\" prohibiting tampering with these identifiers and the dissemination of false information. Concurrently, DeepSeek has for the first time publicly disclosed the detailed training methodologies for its V3/R1 large models, encompassing pre-training and optimization training phases. The company emphasizes the critical role of high-quality, large-scale data and outlines its robust data governance processes, including filtering harmful content and mitigating biases. Furthermore, DeepSeek elaborated on its model inference mechanisms, open-source strategy, and approaches to address \"hallucination\" issues. The company also detailed multiple stringent measures concerning privacy protection, data security, and content safety, such as granting users rights to information, choice, and control. This comprehensive disclosure underscores DeepSeek's commitment to ethical AI development and practical safety implementations.",
    "keywords_en": [
      "DeepSeek",
      "Large Language Model",
      "AI-Generated Content Identification",
      "Model Training",
      "Data Governance",
      "Safety Alignment"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-09-01T16:00:30.000Z",
    "download_time": "2025-09-02T13:46:30.899344",
    "visual_resource": [
      "screenshot/wechat/wechat_image_RjzobuTbfHTMrClypip0kA.png"
    ],
    "extra_info": null
  },
  {
    "id": "XvKyamMpYSUb2Rwbv88kgg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/XvKyamMpYSUb2Rwbv88kgg",
    "title_en": "ICCV 2025 | DescriptiveEdit: Nanjing University and vivo Introduce a New Paradigm for Semantic Image Editing with Descriptive Instructions",
    "summary_en": "Nanjing University and vivo have jointly released DescriptiveEdit, a novel framework addressing the limitations of traditional instruction-based image editing, particularly concerning expressiveness, generalization, and fine-grained control. This innovative approach redefines the editing pipeline as \"instruction â†’ editing description â†’ edited image,\" or directly utilizes user-provided descriptions. Key technical advancements include the introduction of an Attention Bridge for efficient reference image control and an adaptive attention fusion mechanism with a learnable linear layer, which optimally balances generation quality with reference fidelity. DescriptiveEdit demonstrates superior performance in both instruction adherence and image consistency. Furthermore, its seamless compatibility with existing community extensions establishes a new, extensible, and plug-and-play paradigm for semantic image editing, significantly enhancing the practical utility and flexibility of image manipulation.",
    "keywords_en": [
      "Semantic Image Editing",
      "DescriptiveEdit",
      "Diffusion Models",
      "Attention Mechanism",
      "Image Generation"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-09-01T16:00:30.000Z",
    "download_time": "2025-09-02T13:46:46.527676",
    "visual_resource": [
      "screenshot/wechat/wechat_image_XvKyamMpYSUb2Rwbv88kgg.png"
    ],
    "extra_info": null
  },
  {
    "id": "jflP4kxlmKArK6vavyGEog",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/jflP4kxlmKArK6vavyGEog",
    "title_en": "Apple Releases MobileCLIP2: The Strongest Mobile CLIP, Open-Sourcing Data Generation Code!",
    "summary_en": "Apple has unveiled MobileCLIP2, marking a significant advancement in lightweight multimodal models specifically designed for mobile devices. This new iteration achieves state-of-the-art (SOTA) performance in zero-shot classification and various downstream tasks by leveraging a meticulously optimized training methodology. Key innovations include the development of the DFNDR enhanced training scheme, which refines the selection and utilization of datasets, improves the efficacy of teacher models, and enhances synthetic caption generation for robust data augmentation. Furthermore, MobileCLIP2 introduces a novel S3/S4 architecture, featuring a 5-stage image encoder, specifically engineered for superior efficiency and scalability. These comprehensive advancements enable MobileCLIP2 to deliver significantly improved performance with remarkably low latency, thereby expanding the possibilities for sophisticated multimodal applications on resource-constrained mobile platforms. The research also notably includes the open-sourcing of its data generation code, fostering further development and accessibility within the AI community.",
    "keywords_en": [
      "MobileCLIP2",
      "Multimodal",
      "Mobile AI",
      "Zero-shot Learning",
      "Knowledge Distillation",
      "Data Augmentation"
    ],
    "area_en": [
      "Multimodal",
      "Computer Vision",
      "Large Language Model"
    ],
    "published_time": "2025-09-01T14:01:34.000Z",
    "download_time": "2025-09-02T13:46:38.279375",
    "visual_resource": [
      "screenshot/wechat/wechat_image_jflP4kxlmKArK6vavyGEog.png"
    ],
    "extra_info": null
  },
  {
    "id": "nTpq87KiuKpAYFd5NO7hNA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/nTpq87KiuKpAYFd5NO7hNA",
    "title_en": "PhD Application | Cutting-edge Research Project Jointly Supervised by INSAIT and Google: Egocentric Vision Research Based on Multimodal Large Models",
    "summary_en": "INSAIT is globally recruiting PhD candidates for a cutting-edge research project, jointly funded and supervised by Google, focusing on advancing Egocentric Vision research using Multimodal Large Language Models (MLLMs). This initiative aims to develop AI systems capable of real-time perception, memory, and reasoning, leading to adaptive, context-aware agents that can more naturally understand and interact with first-person video content generated by increasingly prevalent smart glasses and AR/VR headsets. The project addresses the growing importance of comprehending egocentric video data as wearable devices proliferate. Candidates will receive joint guidance from leading experts at INSAIT and Google Zurich, including prominent figures in computer vision. This collaborative effort seeks to push the boundaries of AI's ability to interpret human-perspective data, tackling new challenges posed by the widespread adoption of wearable technology.",
    "keywords_en": [
      "Multimodal Large Models",
      "Egocentric Vision",
      "Artificial Intelligence",
      "AI Agent",
      "Computer Vision"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "Computer Vision"
    ],
    "published_time": "2025-09-01T14:01:34.000Z",
    "download_time": "2025-09-02T13:46:42.057803",
    "visual_resource": [
      "screenshot/wechat/wechat_image_nTpq87KiuKpAYFd5NO7hNA.png"
    ],
    "extra_info": null
  },
  {
    "id": "-8a4Xjoi4nZB2EOk8KqxCA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/-8a4Xjoi4nZB2EOk8KqxCA",
    "title_en": "ICCV 2025 | GUAVAâ€”Single-Image Creation of Drivable Upper-Body 3D Avatars! Real-time, Efficient, Capturing Delicate Facial Expressions and Gestures",
    "summary_en": "The latest research introduces GUAVA, the first framework for creating drivable upper-body 3D Gaussian avatars from a single image, addressing limitations of existing methods in real-time performance, efficiency, and expression capture. This innovative framework incorporates the Expressive Human Model (EHM) and inverse texture mapping techniques, utilizing a dual-branch model to achieve reconstruction in seconds and real-time animation rendering. Experimental results demonstrate that GUAVA significantly outperforms current 2D and 3D methods in rendering quality and efficiency, excelling particularly in identity consistency, facial expression, and gesture capture. This breakthrough offers an efficient and realistic virtual avatar solution for applications in film, gaming, and virtual conferencing, pushing the boundaries of single-image 3D avatar generation.",
    "keywords_en": [
      "3D Avatar",
      "Single-Image Reconstruction",
      "Gaussian Splatting",
      "Real-time Rendering",
      "Facial Expressions",
      "Computer Vision"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-09-01T14:01:34.000Z",
    "download_time": "2025-09-02T13:46:39.734931",
    "visual_resource": [
      "screenshot/wechat/wechat_image_-8a4Xjoi4nZB2EOk8KqxCA.png"
    ],
    "extra_info": null
  },
  {
    "id": "myqiJWSctD8QAt4BEWQdaA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/myqiJWSctD8QAt4BEWQdaA",
    "title_en": "Meituan's First Open-Source Large Model, Longcat-Flash-Chat, Matches DeepSeek-V3.1",
    "summary_en": "Meituan has officially open-sourced its inaugural large model, Longcat-Flash-Chat, a 560B Mixture-of-Experts (MoE) architecture. This model has achieved remarkable performance, notably matching or even exceeding leading models like DeepSeek-V3.1 and Claude4 Sonnet in critical benchmarks such as agent tool invocation, instruction adherence, and programming proficiency. Longcat-Flash-Chat integrates advanced technical innovations, including \"Zero-computation Experts\" and a Shortcut-connected MoE design, which collectively boost training and inference throughput while maintaining a comparatively smaller parameter count. This significant open-source release underscores Meituan's substantial advancements in large model technology. It also marks a pivotal moment in the company's strategic evolution, transitioning from its traditional image as a \"delivery company\" to a formidable technology-driven enterprise. Meituan's sustained commitment to research and development, coupled with a well-defined AI strategy, aims to seamlessly integrate cutting-edge AI capabilities into diverse business applications and scenarios.",
    "keywords_en": [
      "Large Language Model",
      "Open Source",
      "Meituan",
      "Longcat",
      "MoE",
      "AI Strategy"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "published_time": "2025-09-01T04:36:57.000Z",
    "download_time": "2025-09-02T13:46:48.414390",
    "visual_resource": [
      "screenshot/wechat/wechat_image_myqiJWSctD8QAt4BEWQdaA.png"
    ],
    "extra_info": null
  },
  {
    "id": "koog",
    "source": "GitHub",
    "url": "https://github.com/JetBrains/koog",
    "title_en": "Koog",
    "summary_en": "Koog is a robust Kotlin-based framework for developing and deploying AI agents, emphasizing a pure, idiomatic Kotlin implementation. It empowers developers to create sophisticated agents capable of interacting with external tools, orchestrating complex workflows, and engaging in dynamic user communications. Core functionalities include seamless MCP integration, advanced embedding capabilities for knowledge retrieval, flexible custom tool creation, and intelligent history compression for token optimization. The framework also offers a powerful streaming API, persistent agent memory, and comprehensive tracing for debugging. With its scalable, modular, and multiplatform architecture (JVM, JS, WasmJS, iOS), Koog supports a broad spectrum of LLM providers such as Google, OpenAI, and Anthropic, making it ideal for applications ranging from basic chatbots to complex enterprise solutions.",
    "keywords_en": [
      "Kotlin",
      "AI Agent",
      "Large Language Model",
      "Multiplatform",
      "Agentic Framework",
      "Workflow",
      "Embedding",
      "Tracing"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-01T15:26:27Z",
    "download_time": "2024-05-15 12:30:00",
    "visual_resource": [
      "screenshot/github/koog.png"
    ],
    "extra_info": null
  },
  {
    "id": "activepieces",
    "source": "GitHub",
    "url": "https://github.com/activepieces/activepieces",
    "title_en": "Welcome to Activepieces",
    "summary_en": "Activepieces is an open-source AI automation platform designed as a replacement for Zapier, offering robust workflow automation capabilities. Built with TypeScript, it boasts high extensibility through a type-safe \"pieces\" framework. The platform integrates over 280 open-source components, with 60% contributed by the community. It features native AI capabilities, allowing users to experiment with various AI providers or build custom AI agents. Activepieces provides an intuitive no-code builder while also supporting code-based development. It includes enterprise-ready features such as self-hosting, data security control, full versioning, multi-language support, and human-in-the-loop processes. It is an AI-first automation solution suitable for both technical and non-technical users.",
    "keywords_en": [
      "AI Automation",
      "Workflow Automation",
      "Open Source",
      "No-code",
      "TypeScript",
      "Integration Platform",
      "AI Agent",
      "Self-hosted"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Others"
    ],
    "published_time": "2025-09-02T01:40:43Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://github.com/activepieces/activepieces/raw/main/docs/resources/templates.gif",
      "https://github.com/activepieces/activepieces/raw/main/docs/resources/create-action.png"
    ],
    "extra_info": null
  },
  {
    "id": "Fooocus",
    "source": "GitHub",
    "url": "https://github.com/lllyasviel/Fooocus",
    "title_en": "Fooocus",
    "summary_en": "Fooocus is an open-source, offline image generation software built on Gradio, designed to simplify the creative process by allowing users to focus solely on prompts for high-quality image output. Based on the Stable Diffusion XL architecture, it offers a Midjourney-like experience without the need for extensive parameter tuning. Key features include a GPT-2 based prompt processing engine, advanced sampling techniques, and custom inpaint/image prompt algorithms. With a minimal requirement of 4GB Nvidia GPU memory and a streamlined installation, Fooocus aims to provide an efficient and high-quality AI painting solution.",
    "keywords_en": [
      "Image Generation",
      "Stable Diffusion XL",
      "AI Painting",
      "Open Source",
      "Deep Learning",
      "Gradio"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-01-24T10:55:35Z",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "screenshot/github/Fooocus.png"
    ],
    "extra_info": null
  },
  {
    "id": "humanlayer",
    "source": "GitHub",
    "url": "https://github.com/humanlayer/humanlayer",
    "title_en": "HumanLayer",
    "summary_en": "HumanLayer is a crucial toolkit addressing the inherent safety and reliability concerns of Large Language Models (LLMs) when interacting with high-stakes functions. It introduces deterministic human oversight mechanisms, ensuring that critical operations are always subject to human approval, even if the LLM generates errors or hallucinates. This capability is vital for deploying AI agents in sensitive environments where \"90% accuracy\" is insufficient. HumanLayer facilitates the creation of next-generation autonomous agents that can safely perform high-value tasks like updating private data or communicating on behalf of a company. By embedding a human-in-the-loop directly into the tool/function, HumanLayer automates complex workflows while maintaining essential human control, thereby significantly enhancing the trustworthiness and practical utility of advanced AI applications in real-world, high-impact scenarios.",
    "keywords_en": [
      "Large Language Model",
      "AI Agent",
      "Human-in-the-Loop",
      "Function Calling",
      "Safety",
      "Automation",
      "High-Stakes Operations"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-01T19:50:19Z",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "https://github.com/humanlayer/humanlayer/raw/main/docs/images/wordmark-light.svg",
      "https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8rEqjGZs_e6dibWeaqaQg.png",
      "https://github.com/humanlayer/humanlayer/raw/main/docs/images/gen-2-gen-3-agents.png"
    ],
    "extra_info": null
  },
  {
    "id": "chatterbox",
    "source": "GitHub",
    "url": "https://github.com/resemble-ai/chatterbox",
    "title_en": "Chatterbox TTS",
    "summary_en": "Chatterbox, Resemble AI's inaugural production-grade open-source Text-to-Speech (TTS) model, is released under an MIT license. This advanced model has demonstrated superior performance in side-by-side evaluations against prominent closed-source systems such as ElevenLabs. A groundbreaking feature is its unique emotion exaggeration control, allowing users to infuse voices with distinct expressiveness. Architected with a 0.5B Llama backbone and rigorously trained on 0.5 million hours of cleaned data, Chatterbox delivers state-of-the-art zeroshot TTS capabilities, ensuring ultra-stable and alignment-informed inference. Furthermore, it integrates built-in PerTh watermarking for responsible AI usage. Chatterbox is highly versatile, suitable for a wide array of applications including memes, videos, games, and AI agents, effectively bringing digital content to life with high-fidelity, controllable speech synthesis.",
    "keywords_en": [
      "Text-to-Speech",
      "Emotion Control",
      "Zeroshot TTS",
      "Llama Model",
      "Audio Watermarking",
      "Speech Synthesis"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-08-01T10:22:29Z",
    "download_time": "2024-07-30 15:30:00",
    "visual_resource": [
      "screenshot/github/chatterbox.png"
    ],
    "extra_info": null
  },
  {
    "id": "MiniCPM-V",
    "source": "GitHub",
    "url": "https://github.com/OpenBMB/MiniCPM-V",
    "title_en": "MiniCPM-V",
    "summary_en": "The MiniCPM-V and MiniCPM-o series represent a significant advancement in efficient end-side multimodal large language models (MLLMs), specifically optimized for powerful single-image, multi-image, and high-frame-rate video understanding on mobile devices. MiniCPM-V 4.5, with its 8 billion parameters, demonstrates performance that surpasses leading proprietary models like GPT-4o, Gemini-2.0 Pro, and Qwen2.5-VL 72B in vision-language tasks. Key innovations include a 96x video token compression rate for efficient long video understanding, controllable hybrid fast/deep thinking modes, and robust OCR capabilities for complex document parsing. MiniCPM-o 2.6 extends this by integrating audio input and high-quality speech output, facilitating end-to-end real-time voice conversations with configurable voices and enabling multimodal live streaming on edge devices such as iPads. This series prioritizes both strong performance and efficient deployment, establishing itself as a cutting-edge open-source solution for on-device multimodal AI applications.",
    "keywords_en": [
      "Multimodal Large Language Model",
      "Video Understanding",
      "Speech Recognition",
      "Edge Deployment",
      "OCR",
      "Real-time Streaming",
      "Artificial Intelligence",
      "Deep Learning"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Video Understanding"
    ],
    "published_time": "2025-09-02T04:16:14Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://github.com/OpenBMB/MiniCPM-V/raw/main/assets/minicpm_v_and_minicpm_o_title.png",
      "https://github.com/OpenBMB/MiniCPM-V/raw/main/assets/minicpm-v-4dot5-framework.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.21113",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.21113",
    "title_en": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning",
    "summary_en": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking\ncapabilities have demonstrated remarkable performance on complex reasoning\nproblems. However, this thinking process is redundant for simple problems\nsolvable without complex reasoning. To address this inefficiency, we propose\nR-4B, an auto-thinking MLLM, which can adaptively decide when to think based on\nproblem complexity. The central idea of R-4B is to empower the model with both\nthinking and non-thinking capabilities using bi-mode annealing, and apply\nBi-mode Policy Optimization~(BPO) to improve the model's accuracy in\ndetermining whether to activate the thinking process. Specifically, we first\ntrain the model on a carefully curated dataset spanning various topics, which\ncontains samples from both thinking and non-thinking modes. Then it undergoes a\nsecond phase of training under an improved GRPO framework, where the policy\nmodel is forced to generate responses from both modes for each input query.\nExperimental results show that R-4B achieves state-of-the-art performance\nacross 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks\nand achieves performance comparable to larger models such as\nKimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower\ncomputational cost.",
    "keywords_en": [
      "Multimodal Large Language Models",
      "Auto-Thinking",
      "Bi-Mode Annealing",
      "Reinforce Learning",
      "Policy Optimization"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-08-28T17:48:19.000Z",
    "download_time": "2025-09-01 22:45:01",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21113.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.21113\", \"arxiv_url\": \"https://arxiv.org/abs/2508.21113\"}"
  },
  {
    "id": "2508.21148",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.21148",
    "title_en": "A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers",
    "summary_en": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is\nrepresented, integrated, and applied in scientific research, yet their progress\nis shaped by the complex nature of scientific data. This survey presents a\ncomprehensive, data-centric synthesis that reframes the development of Sci-LLMs\nas a co-evolution between models and their underlying data substrate. We\nformulate a unified taxonomy of scientific data and a hierarchical model of\nscientific knowledge, emphasizing the multimodal, cross-scale, and\ndomain-specific challenges that differentiate scientific corpora from general\nnatural language processing datasets. We systematically review recent Sci-LLMs,\nfrom general-purpose foundations to specialized models across diverse\nscientific disciplines, alongside an extensive analysis of over 270\npre-/post-training datasets, showing why Sci-LLMs pose distinct demands --\nheterogeneous, multi-scale, uncertainty-laden corpora that require\nrepresentations preserving domain invariance and enabling cross-modal\nreasoning. On evaluation, we examine over 190 benchmark datasets and trace a\nshift from static exams toward process- and discovery-oriented assessments with\nadvanced evaluation protocols. These data-centric analyses highlight persistent\nissues in scientific data development and discuss emerging solutions involving\nsemi-automated annotation pipelines and expert validation. Finally, we outline\na paradigm shift toward closed-loop systems where autonomous agents based on\nSci-LLMs actively experiment, validate, and contribute to a living, evolving\nknowledge base. Collectively, this work provides a roadmap for building\ntrustworthy, continually evolving artificial intelligence (AI) systems that\nfunction as a true partner in accelerating scientific discovery.",
    "keywords_en": [
      "Scientific Large Language Models",
      "Scientific Data",
      "AI Agent",
      "Multimodal",
      "Knowledge Discovery"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-28T18:30:52.000Z",
    "download_time": "2025-09-01 22:45:04",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21148.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.21148\", \"arxiv_url\": \"https://arxiv.org/abs/2508.21148\"}"
  },
  {
    "id": "2508.20470",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.20470",
    "title_en": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
    "summary_en": "Scaling laws have validated the success and promise of large-data-trained\nmodels in creative generation across text, image, and video domains. However,\nthis paradigm faces data scarcity in the 3D domain, as there is far less of it\navailable on the internet compared to the aforementioned modalities.\nFortunately, there exist adequate videos that inherently contain commonsense\npriors, offering an alternative supervisory signal to mitigate the\ngeneralization bottleneck caused by limited native 3D data. On the one hand,\nvideos capturing multiple views of an object or scene provide a spatial\nconsistency prior for 3D generation. On the other hand, the rich semantic\ninformation contained within the videos enables the generated content to be\nmore faithful to the text prompts and semantically plausible. This paper\nexplores how to apply the video modality in 3D asset generation, spanning\ndatasets to models. We introduce Droplet3D-4M, the first large-scale video\ndataset with multi-view level annotations, and train Droplet3D, a generative\nmodel supporting both image and dense text input. Extensive experiments\nvalidate the effectiveness of our approach, demonstrating its ability to\nproduce spatially consistent and semantically plausible content. Moreover, in\ncontrast to the prevailing 3D solutions, our approach exhibits the potential\nfor extension to scene-level applications. This indicates that the commonsense\npriors from the videos significantly facilitate 3D creation. We have\nopen-sourced all resources including the dataset, code, technical framework,\nand model weights: https://dropletx.github.io/.",
    "keywords_en": [
      "3D Generation",
      "Video Priors",
      "Commonsense Priors",
      "Multi-view Dataset",
      "Generative Model"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Video Understanding"
    ],
    "published_time": "2025-08-28T06:39:41.000Z",
    "download_time": "2025-09-01 22:45:03",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20470.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.20470\", \"arxiv_url\": \"https://arxiv.org/abs/2508.20470\"}"
  },
  {
    "id": "2508.18106",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.18106",
    "title_en": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
    "summary_en": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.",
    "keywords_en": [
      "AI-generated code",
      "security evaluation",
      "repository-level benchmark",
      "Large Language Models",
      "code security"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-08-25T15:11:11.000Z",
    "download_time": "2025-09-01 22:45:01",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18106.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.18106\", \"arxiv_url\": \"https://arxiv.org/abs/2508.18106\"}"
  },
  {
    "id": "2508.21365",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.21365",
    "title_en": "Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models",
    "summary_en": "Large language models (LLMs) excel at complex reasoning tasks such as\nmathematics and coding, yet they frequently struggle with simple interactive\ntasks that young children perform effortlessly. This discrepancy highlights a\ncritical gap between declarative knowledge (knowing about something) and\nprocedural knowledge (knowing how to do something). Although traditional\nreinforcement learning (RL) agents can acquire procedural knowledge through\nenvironmental interaction, they often operate as black boxes and require\nsubstantial training data. In contrast, LLMs possess extensive world knowledge\nand reasoning capabilities, but are unable to effectively convert this static\nknowledge into dynamic decision-making in interactive settings. To address this\nchallenge, we propose Think in Games (TiG), a novel framework that empowers\nLLMs to develop procedural understanding through direct interaction with game\nenvironments, while retaining their inherent reasoning and explanatory\nabilities. Specifically, TiG reformulates RL-based decision-making as a\nlanguage modeling task: LLMs generate language-guided policies, which are\nrefined iteratively through online reinforcement learning based on\nenvironmental feedback. Our experimental results show that TiG successfully\nbridges the gap between declarative and procedural knowledge, achieving\ncompetitive performance with dramatically lower data and computational demands\ncompared to conventional RL methods. Moreover, TiG provides step-by-step\nnatural language explanations for its decisions, greatly improving transparency\nand interpretability in complex interactive tasks.",
    "keywords_en": [
      "Large Language Models",
      "Reinforcement Learning",
      "Think in Games",
      "Procedural Knowledge",
      "Language Modeling"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-29T07:13:39.000Z",
    "download_time": "2025-09-01 22:45:05",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21365.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.21365\", \"arxiv_url\": \"https://arxiv.org/abs/2508.21365\"}"
  },
  {
    "id": "2508.21767",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.21767",
    "title_en": "UItron: Foundational GUI Agent with Advanced Perception and Planning",
    "summary_en": "GUI agent aims to enable automated operations on Mobile/PC devices, which is\nan important task toward achieving artificial general intelligence. The rapid\nadvancement of VLMs accelerates the development of GUI agents, owing to their\npowerful capabilities in visual understanding and task planning. However,\nbuilding a GUI agent remains a challenging task due to the scarcity of\noperation trajectories, the availability of interactive infrastructure, and the\nlimitation of initial capabilities in foundation models. In this work, we\nintroduce UItron, an open-source foundational model for automatic GUI agents,\nfeaturing advanced GUI perception, grounding, and planning capabilities. UItron\nhighlights the necessity of systemic data engineering and interactive\ninfrastructure as foundational components for advancing GUI agent development.\nIt not only systematically studies a series of data engineering strategies to\nenhance training effects, but also establishes an interactive environment\nconnecting both Mobile and PC devices. In training, UItron adopts supervised\nfinetuning over perception and planning tasks in various GUI scenarios, and\nthen develop a curriculum reinforcement learning framework to enable complex\nreasoning and exploration for online environments. As a result, UItron achieves\nsuperior performance in benchmarks of GUI perception, grounding, and planning.\nIn particular, UItron highlights the interaction proficiency with top-tier\nChinese mobile APPs, as we identified a general lack of Chinese capabilities\neven in state-of-the-art solutions. To this end, we manually collect over one\nmillion steps of operation trajectories across the top 100 most popular apps,\nand build the offline and online agent evaluation environments. Experimental\nresults demonstrate that UItron achieves significant progress in Chinese app\nscenarios, propelling GUI agents one step closer to real-world application.",
    "keywords_en": [
      "GUI Agent",
      "Foundational Model",
      "GUI Perception",
      "Task Planning",
      "Visual Language Models"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Deep Learning"
    ],
    "published_time": "2025-08-29T16:40:57.000Z",
    "download_time": "2025-09-01 22:45:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21767.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.21767\", \"arxiv_url\": \"https://arxiv.org/abs/2508.21767\"}"
  }
]