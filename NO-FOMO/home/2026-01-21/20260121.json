[
  {
    "id": "hackernews_46710108",
    "source": "Hacker News",
    "url": "https://news.ycombinator.com/item?id=46710108",
    "title": "How are you automating your coding work?",
    "summary": "A recent Hacker News discussion thread invites software developers to share their strategies and innovative methods for automating various aspects of their coding work. The inquiry is specifically framed within the context of an observed rise in \"vibe coding,\" which can be interpreted as a more intuitive, perhaps less structured, or highly personalized approach to programming. The post aims to uncover creative solutions that extend beyond conventional automation techniques, seeking insights into how developers are leveraging tools, scripts, and potentially AI-driven systems to streamline repetitive tasks. This includes automating code generation, testing, deployment, refactoring, debugging, and documentation. The overarching goal is to enhance developer efficiency, reduce manual effort, and allow programmers to focus on more complex or creative problem-solving, thereby optimizing the modern software development workflow in an era increasingly characterized by intelligent assistance and productivity-focused practices.",
    "keywords": [
      "Coding Automation",
      "Developer Productivity",
      "Software Development",
      "Workflow Automation",
      "Programming Tools",
      "AI in Programming"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Others"
    ],
    "published_time": "2026-01-21 19:13:44",
    "download_time": "2026-01-21 20:00:44",
    "extra_info": "{\"score\": 18, \"by\": \"manthangupta109\", \"descendants\": 13, \"story_id\": 46710108}"
  },
  {
    "id": "hackernews_46708990",
    "source": "Hacker News",
    "url": "https://jobswithgpt.com/company-profiles/",
    "title": "Show HN: Company hiring trends and insights from job postings",
    "summary": "The Hacker News submission highlights a new platform, \"jobswithgpt.com,\" developed to offer unique insights into companies through the systematic analysis of their job postings. The core objective of this tool is to furnish users with a concise yet informative snapshot of an organization, proving especially valuable for activities like interview preparation, market research, or simply gaining a deeper understanding of a company's strategic direction and growth areas. By meticulously processing and interpreting data extracted from numerous job advertisements, the platform aims to reveal evolving hiring trends, preferred skill sets, and technological emphases within various companies. The developer acknowledges that the project is still a Work In Progress, with continuous efforts focused on refining data quality and ensuring the accuracy of its presented insights. Illustrative examples provided include detailed company profiles for prominent entities such as OpenAI and Advanced Micro Devices Inc., demonstrating the practical application of the tool in transforming raw job market data into actionable business intelligence for job seekers and industry observers.",
    "keywords": [
      "Job Market Analysis",
      "Hiring Trends",
      "Company Insights",
      "Recruitment Data",
      "Business Intelligence",
      "AI Tools"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2026-01-21 17:52:17",
    "download_time": "2026-01-21 20:00:50",
    "extra_info": "{\"score\": 12, \"by\": \"sp1982\", \"descendants\": 1, \"story_id\": 46708990}"
  },
  {
    "id": "hackernews_46707572",
    "source": "Hacker News",
    "url": "https://www.anthropic.com/news/claude-new-constitution",
    "title": "Claude's New Constitution",
    "summary": "Anthropic has introduced \"Claude's New Constitution,\" marking a significant stride in the development of safe and ethically aligned artificial intelligence. This constitution refers to Anthropic's innovative Constitutional AI approach, a training methodology designed to align large language models with human values without extensive human feedback. Instead, the AI critiques and revises its own responses based on a codified set of principles derived from ethical guidelines. This iterative process allows Claude to self-correct and refine its behavior, minimizing the generation of harmful, biased, or unhelpful content. The framework aims to instill a robust understanding of ethical considerations directly into the AI's operational structure, offering a scalable solution for AI alignment. This initiative reinforces Anthropic's commitment to responsible AI development, providing a blueprint for building more trustworthy and beneficial general-purpose AI systems.",
    "keywords": [
      "Constitutional AI",
      "AI Safety",
      "Large Language Model",
      "AI Alignment",
      "Ethical AI",
      "Machine Learning",
      "Generative AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2026-01-21 16:04:49",
    "download_time": "2026-01-21 20:00:32",
    "extra_info": "{\"score\": 105, \"by\": \"meetpateltech\", \"descendants\": 54, \"story_id\": 46707572}"
  },
  {
    "id": "hackernews_46706796",
    "source": "Hacker News",
    "url": "https://github.com/borenstein/yolo-cage",
    "title": "Show HN: yolo-cage â€“ AI coding agents that can't exfiltrate secrets",
    "summary": "A new open-source tool named \"yolo-cage\" has been introduced on Hacker News, designed to securely sandbox AI coding agents, such as Claude, to prevent sensitive data exfiltration and control Git access. The developer created this solution to address \"decision fatigue\" stemming from managing multiple parallel AI agents, which frequently demand permission prompts during complex development tasks like building financial analysis tools. The yolo-cage enables a \"YOLO mode\" for agents, where their operations are contained within a secure environment, thus mitigating potential harm. This approach aims to streamline development workflows by reducing the need for continuous permission reviews, allowing developers to consolidate agent output assessments into a single review process. The tool is shared to solicit community feedback on its threat model and overall utility in enhancing AI agent safety and efficiency.",
    "keywords": [
      "AI Agents",
      "Sandboxing",
      "Security",
      "Data Exfiltration",
      "Git Access",
      "Developer Tools",
      "Secure Coding",
      "Claude"
    ],
    "area": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2026-01-21 15:13:29",
    "download_time": "2026-01-21 20:00:55",
    "extra_info": "{\"score\": 34, \"by\": \"borenstein\", \"descendants\": 53, \"story_id\": 46706796}"
  },
  {
    "id": "hackernews_46700594",
    "source": "Hacker News",
    "url": "https://github.com/anthropics/original_performance_takehome",
    "title": "Anthropic's original take home assignment open sourced",
    "summary": "Anthropic, a leading artificial intelligence research company, has recently open-sourced its original take-home assignment, providing an unprecedented insight into the rigorous evaluation process for potential hires. This development is particularly significant for aspiring AI professionals and software engineers seeking roles within top-tier AI organizations. The publicly available assignment offers a practical framework for understanding the types of technical challenges and problem-solving skills highly valued by companies at the forefront of AI innovation. It likely encompasses a blend of theoretical AI/ML concepts and their practical implementation, demanding robust coding abilities and analytical thinking. This strategic release not only enriches the open-source community with high-quality educational material but also fosters greater transparency in Anthropic's recruitment methodology. Consequently, it could serve as a valuable preparatory tool for candidates globally and potentially influence industry standards for technical assessments in the competitive AI landscape, ensuring candidates are well-equipped to contribute to cutting-edge AI research and development.",
    "keywords": [
      "Anthropic",
      "Open Source",
      "Take-home assignment",
      "AI Hiring",
      "Machine Learning Engineering",
      "Technical Assessment",
      "AI Research",
      "Software Development"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2026-01-21 02:54:32",
    "download_time": "2026-01-21 20:00:49",
    "extra_info": "{\"score\": 584, \"by\": \"myahio\", \"descendants\": 309, \"story_id\": 46700594}"
  },
  {
    "id": "2601.12993",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.12993",
    "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
    "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.",
    "keywords": [
      "Being-H0.5",
      "Human-Centric Robot Learning",
      "Cross-Embodiment Generalization",
      "Vision-Language-Action Models",
      "Embodied Pre-training"
    ],
    "area": [
      "Robotics",
      "Machine Learning",
      "Multimodal"
    ],
    "published_time": "2026-01-19T12:20:38.000Z",
    "download_time": "2026-01-21 12:01:39",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.12993\", \"arxiv_url\": \"https://arxiv.org/abs/2601.12993\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12993.png\", \"original_title\": \"Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization\"}"
  },
  {
    "id": "2601.14192",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.14192",
    "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
    "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.",
    "keywords": [
      "AI Agents",
      "Efficiency",
      "Memory",
      "Tool Learning",
      "Planning"
    ],
    "area": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2026-01-20T17:51:56.000Z",
    "download_time": "2026-01-21 12:01:36",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.14192\", \"arxiv_url\": \"https://arxiv.org/abs/2601.14192\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14192.png\", \"original_title\": \"Toward Efficient Agents: Memory, Tool learning, and Planning\"}"
  },
  {
    "id": "2601.14250",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.14250",
    "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
    "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
    "keywords": [
      "Spatio-temporal Video Transfer",
      "Video Generation",
      "Multimodal Alignment",
      "Video Customization",
      "Appearance Consistency"
    ],
    "area": [
      "Computer Vision",
      "Generative AI",
      "Multimodal"
    ],
    "published_time": "2026-01-20T18:58:11.000Z",
    "download_time": "2026-01-21 12:01:39",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.14250\", \"arxiv_url\": \"https://arxiv.org/abs/2601.14250\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14250.png\", \"original_title\": \"OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer\"}"
  },
  {
    "id": "2601.13836",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.13836",
    "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
    "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).",
    "keywords": [
      "Multimodal LLMs",
      "Future Forecasting",
      "Omni-Modal Context",
      "Audio-Visual Cues",
      "Benchmark"
    ],
    "area": [
      "Multimodal",
      "Large Language Model",
      "Video Understanding"
    ],
    "published_time": "2026-01-20T10:47:20.000Z",
    "download_time": "2026-01-21 12:01:36",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.13836\", \"arxiv_url\": \"https://arxiv.org/abs/2601.13836\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13836.png\", \"original_title\": \"FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs\"}"
  },
  {
    "id": "2601.14209",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.14209",
    "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
    "summary": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.",
    "keywords": [
      "Large Language Models",
      "Reinforcement Learning",
      "Credit Assignment",
      "Intervention Training",
      "LLM Reasoning"
    ],
    "area": [
      "Large Language Model",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2026-01-20T18:15:38.000Z",
    "download_time": "2026-01-21 12:01:38",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.14209\", \"arxiv_url\": \"https://arxiv.org/abs/2601.14209\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14209.png\", \"original_title\": \"InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\"}"
  },
  {
    "id": "2601.13591",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.13591",
    "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems",
    "summary": "Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.",
    "keywords": [
      "Data Science Agents",
      "LLMs",
      "Evaluation",
      "Multimodal",
      "Benchmark"
    ],
    "area": [
      "AI Agent",
      "Large Language Model",
      "Multimodal"
    ],
    "published_time": "2026-01-20T04:44:36.000Z",
    "download_time": "2026-01-21 12:01:37",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.13591\", \"arxiv_url\": \"https://arxiv.org/abs/2601.13591\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13591.png\", \"original_title\": \"DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems\"}"
  }
]