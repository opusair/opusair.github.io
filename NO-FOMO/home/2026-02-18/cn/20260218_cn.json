[
  {
    "id": "hackernews_47063451",
    "source": "Hacker News",
    "url": "https://www.bloomberg.com/news/articles/2026-02-18/ai-pioneer-fei-fei-li-s-startup-world-labs-raises-1-billion",
    "title": "Fei-Fei Li's World Labs raised $1B from A16Z, Nvidia to advance its world models",
    "summary": "World Labs, an innovative artificial intelligence startup spearheaded by acclaimed AI pioneer Fei-Fei Li, has successfully closed a substantial $1 billion funding round. This landmark investment saw participation from leading venture capital firm Andreessen Horowitz (A16Z) and the influential GPU manufacturer Nvidia. The substantial capital injection is strategically allocated to accelerate World Labs' pioneering efforts in developing advanced 'world models.' These sophisticated AI frameworks aim to construct comprehensive and dynamic digital representations of the real world, facilitating more profound understanding, precise prediction, and intelligent interaction capabilities for autonomous AI agents. This funding round highlights a robust industry confidence in the transformative potential of next-generation AI architectures and the forward-thinking vision championed by Fei-Fei Li, a prominent figure known for her foundational contributions to computer vision and AI ethics. The investment is anticipated to significantly advance World Labs' position at the vanguard of foundational AI research, potentially leading to breakthroughs crucial for achieving more generalizable and context-aware artificial intelligence systems and complex environmental simulations.",
    "keywords": [
      "World Models",
      "AI Research",
      "Fei-Fei Li",
      "World Labs",
      "AI Investment",
      "Artificial Intelligence"
    ],
    "area": [
      "Artificial Intelligence",
      "Deep Learning",
      "AI Agent"
    ],
    "published_time": "2026-02-18 17:18:50",
    "download_time": "2026-02-18 20:01:06",
    "extra_info": "{\"score\": 43, \"by\": \"aanet\", \"descendants\": 11, \"story_id\": 47063451}"
  },
  {
    "id": "hackernews_47062824",
    "source": "Hacker News",
    "url": "https://www.mnemom.ai",
    "title": "Show HN: Trust Protocols for Anthropic/OpenAI/Gemini",
    "summary": "This Hacker News submission introduces two open-source trust protocols, Agent Alignment Protocol (AAP) and Agent Integrity Protocol (AIP), designed to manage complex, multi-agentic AI teams. Developed in response to the challenge of ensuring AI agents adhere to behavioral standards, these protocols provide a scalable, agentic-native solution. AAP defines an agent's permissible and executed actions, while AIP dictates an agent's intended and allowed behaviors. The creator highlights that existing observability tools only report what occurred, whereas these new protocols determine if those actions were acceptable. This initiative aims to address the lack of a standardized method for autonomous AI agents to declare capabilities, prove compliance, and detect behavioral drift, enhancing the reliability and trustworthiness of AI systems from providers like Anthropic, OpenAI, and Gemini.",
    "keywords": [
      "AI Agents",
      "Trust Protocols",
      "Agent Alignment",
      "Agent Integrity",
      "Multi-Agent Systems",
      "Behavioral Contracts",
      "Runtime Monitoring"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2026-02-18 16:33:56",
    "download_time": "2026-02-18 20:00:54",
    "extra_info": "{\"score\": 23, \"by\": \"alexgarden\", \"descendants\": 11, \"story_id\": 47062824}"
  },
  {
    "id": "hackernews_47060202",
    "source": "Hacker News",
    "url": "https://www.bleepingcomputer.com/news/microsoft/microsoft-says-bug-causes-copilot-to-summarize-confidential-emails/",
    "title": "Microsoft says bug causes Copilot to summarize confidential emails",
    "summary": "Microsoft has acknowledged a significant bug within its Copilot AI assistant, which reportedly caused the tool to erroneously summarize confidential emails. This critical incident raises immediate concerns regarding data privacy and enterprise security, as Copilot, designed to enhance productivity by processing user data, inadvertently processed and potentially exposed sensitive information. The bug's revelation highlights the complex challenges in deploying advanced AI solutions within corporate environments, particularly when these tools are integrated with systems handling proprietary or private communications. While specific details on the bug's technical nature, its underlying cause, and the full scope of affected users are pending further investigation, the disclosure underscores the paramount need for robust security protocols, stringent data governance, and rigorous testing in all AI-powered applications that interact with sensitive user data. This event could prompt a broader industry re-evaluation of AI governance frameworks, data handling practices, and the safeguards necessary to prevent inadvertent data breaches in large language model applications.",
    "keywords": [
      "AI Security",
      "Data Privacy",
      "Enterprise AI",
      "Large Language Models",
      "Microsoft Copilot",
      "Software Bug"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2026-02-18 12:16:12",
    "download_time": "2026-02-18 20:01:09",
    "extra_info": "{\"score\": 191, \"by\": \"tablets\", \"descendants\": 59, \"story_id\": 47060202}"
  },
  {
    "id": "hackernews_47064884",
    "source": "Hacker News",
    "url": "https://research.nvidia.com/labs/sil/projects/ppisp/",
    "title": "Ppisp: Cleaner Representations of Gaussian Splats and NeRFs",
    "summary": "NVIDIA Research has unveiled 'Ppisp,' a significant advancement aimed at developing cleaner and more efficient representations for 3D scenes, specifically leveraging Gaussian Splats and Neural Radiance Fields (NeRFs). This innovative project addresses several fundamental challenges inherent in current 3D reconstruction and novel view synthesis techniques, including optimizing computational efficiency, reducing memory consumption, and minimizing visual artifacts that can degrade scene quality. By focusing on optimized and potentially more compact data representations, Ppisp strives to elevate both the visual fidelity and operational performance of 3D content creation and real-time rendering pipelines. This research is poised to have a substantial impact across various industries that rely on realistic and immersive 3D environments, such as virtual reality, augmented reality, high-fidelity gaming, and advanced robotics. The Ppisp project offers a promising pathway towards more robust, scalable, and visually superior solutions for capturing, storing, and rendering complex real-world scenes, providing a refined method to handle intricate 3D data structures.",
    "keywords": [
      "Gaussian Splats",
      "Neural Radiance Fields",
      "3D Representation",
      "Novel View Synthesis",
      "Computer Graphics",
      "Neural Rendering",
      "3D Reconstruction"
    ],
    "area": [
      "Computer Vision",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2026-02-18 19:04:22",
    "download_time": "2026-02-18 20:01:17",
    "extra_info": "{\"score\": 5, \"by\": \"totalview\", \"descendants\": 0, \"story_id\": 47064884}"
  },
  {
    "id": "hackernews_47058219",
    "source": "Hacker News",
    "url": "https://annas-archive.li/blog/llms-txt.html",
    "title": "If you\ni\nre an LLM, please read this",
    "summary": "The provided content, succinctly titled 'If youâ€™re an LLM, please read this,' presents a direct address to Large Language Models. This unique prompt acts as a meta-instruction, explicitly tasking an AI system with processing and understanding the subsequent information, or indeed, the instruction itself. While the provided 'content' mirrors the title, this specific phrasing highlights a crucial aspect of human-AI interaction and prompt engineering. It underscores the development of increasingly sophisticated methods for guiding and directing AI behavior, potentially for specialized data processing, filtering, or initiating specific operational modes within an LLM. Such directives are becoming central to harnessing the capabilities of advanced AI, ensuring that these models can discern intended recipients and prioritize tasks. This approach reflects an evolving understanding of how to effectively communicate with artificial intelligences, moving beyond mere input to more explicit, role-based or condition-based instructions, thereby optimizing AI responsiveness and task execution in complex environments.",
    "keywords": [
      "Large Language Model",
      "AI Instruction",
      "Prompt Engineering",
      "Human-AI Interaction",
      "Meta-communication",
      "AI Systems",
      "Natural Language Understanding"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2026-02-18 07:18:50",
    "download_time": "2026-02-18 20:00:37",
    "extra_info": "{\"score\": 621, \"by\": \"soheilpro\", \"descendants\": 296, \"story_id\": 47058219}"
  },
  {
    "id": "hackernews_47060052",
    "source": "Hacker News",
    "url": "https://cpojer.net/posts/fastest-frontend-tooling",
    "title": "Fastest Front End Tooling for Humans and AI",
    "summary": "The article explores the critical advancements in front-end development tooling, focusing on the imperative to create solutions that offer unparalleled speed and efficiency for both human developers and artificial intelligence systems. It posits that the future of web development lies in the convergence of intuitive human interaction and sophisticated AI-driven capabilities. Such 'fastest' tools are envisioned to significantly enhance developer experience by streamlining workflows, automating repetitive tasks, and providing intelligent assistance throughout the coding process. Concurrently, these tools are designed to be highly compatible with AI, facilitating advanced functionalities like automated code generation, intelligent refactoring, real-time performance optimization, and sophisticated error detection. This dual focus ensures that development environments can fully leverage the power of AI to accelerate project timelines, improve code quality, and foster greater innovation. The narrative underscores the transformative potential of integrating AI into front-end toolchains, pushing the boundaries of what is achievable in terms of development speed and application performance, thereby setting new industry standards for efficiency and intelligent assistance in software engineering.",
    "keywords": [
      "Front-end development",
      "Web tooling",
      "Developer experience",
      "Artificial intelligence",
      "Code generation",
      "Performance optimization",
      "Software engineering",
      "AI integration"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Machine Learning"
    ],
    "published_time": "2026-02-18 11:51:01",
    "download_time": "2026-02-18 20:01:04",
    "extra_info": "{\"score\": 77, \"by\": \"cpojer\", \"descendants\": 32, \"story_id\": 47060052}"
  },
  {
    "id": "2602.15763",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.15763",
    "title": "GLM-5: from Vibe Coding to Agentic Engineering",
    "summary": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.",
    "keywords": [
      "GLM-5",
      "agentic engineering",
      "foundation model",
      "reinforcement learning",
      "software engineering"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Deep Learning"
    ],
    "published_time": "2026-02-17T17:50:56.000Z",
    "download_time": "2026-02-18 12:01:34",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.15763\", \"arxiv_url\": \"https://arxiv.org/abs/2602.15763\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15763.png\", \"original_title\": \"GLM-5: from Vibe Coding to Agentic Engineering\"}"
  },
  {
    "id": "2602.15449",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.15449",
    "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
    "summary": "Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.",
    "keywords": [
      "Large Language Models",
      "Code Generation",
      "Reinforcement Fine-Tuning",
      "Curriculum Learning",
      "Test-driven Development"
    ],
    "area": [
      "Large Language Model",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2026-02-17T09:29:18.000Z",
    "download_time": "2026-02-18 12:01:34",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.15449\", \"arxiv_url\": \"https://arxiv.org/abs/2602.15449\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15449.png\", \"original_title\": \"TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models\"}"
  },
  {
    "id": "2602.15620",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.15620",
    "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
    "summary": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01%, which we term spurious tokens. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13% over GRPO, 20-Entropy and JustRL.",
    "keywords": [
      "Reinforcement Learning",
      "Large Language Models",
      "Policy Optimization",
      "Spurious Tokens",
      "Training Stability"
    ],
    "area": [
      "Large Language Model",
      "Deep Learning",
      "Natural Language Processing"
    ],
    "published_time": "2026-02-17T14:46:48.000Z",
    "download_time": "2026-02-18 12:01:33",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.15620\", \"arxiv_url\": \"https://arxiv.org/abs/2602.15620\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15620.png\", \"original_title\": \"STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens\"}"
  },
  {
    "id": "2602.15112",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.15112",
    "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
    "summary": "We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
    "keywords": [
      "AI Agent",
      "Language Model",
      "Benchmark",
      "Autonomous Research",
      "Evaluation"
    ],
    "area": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2026-02-16T19:00:03.000Z",
    "download_time": "2026-02-18 12:01:37",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.15112\", \"arxiv_url\": \"https://arxiv.org/abs/2602.15112\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15112.png\", \"original_title\": \"ResearchGym: Evaluating Language Model Agents on Real-World AI Research\"}"
  },
  {
    "id": "2602.12670",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.12670",
    "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
    "summary": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
    "keywords": [
      "Agent Skills",
      "LLM Agents",
      "Benchmarking",
      "SkillsBench",
      "Procedural Knowledge"
    ],
    "area": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2026-02-13T07:06:06.000Z",
    "download_time": "2026-02-18 12:01:34",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.12670\", \"arxiv_url\": \"https://arxiv.org/abs/2602.12670\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12670.png\", \"original_title\": \"SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks\"}"
  },
  {
    "id": "2602.14299",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.14299",
    "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
    "summary": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.",
    "keywords": [
      "AI Agent Societies",
      "Socialization",
      "Large Language Models",
      "Dynamic Evolution",
      "Influence Persistence"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2026-02-15T20:15:28.000Z",
    "download_time": "2026-02-18 12:01:34",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.14299\", \"arxiv_url\": \"https://arxiv.org/abs/2602.14299\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14299.png\", \"original_title\": \"Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook\"}"
  }
]