[
  {
    "id": "twitter_GoogleDeepMind_1963635422698856705",
    "source": "Twitter",
    "url": "https://x.com/GoogleDeepMind/status/1963635422698856705",
    "title_en": "GoogleDeepMind_发布轻量级端侧AI嵌入模型EmbeddingGemma",
    "summary_en": "Google DeepMind发布了其最新且领先的开放式嵌入模型EmbeddingGemma，专为端侧AI设计。该模型仅3.08亿参数，却能提供最先进的性能，同时体积小巧高效，可在无网络连接的情况下运行，实现随时随地部署。",
    "keywords_en": [
      "EmbeddingGemma",
      "嵌入模型",
      "端侧AI",
      "离线运行",
      "谷歌DeepMind",
      "开源模型"
    ],
    "area_en": [
      "人工智能",
      "产品发布",
      "开源项目"
    ],
    "published_time": "2025-09-04T16:09:02.000Z",
    "download_time": "2025-09-05 05:31:24",
    "visual_resource": [
      "screenshot/twitter/GoogleDeepMind_1963635422698856705.png"
    ],
    "extra_info": "{\"username\": \"GoogleDeepMind\", \"tweet_id\": \"1963635422698856705\"}"
  },
  {
    "id": "twitter_OpenAI_1963697012014215181",
    "source": "Twitter",
    "url": "https://twitter.com/OpenAI/status/1963697012014215181",
    "title_en": "OpenAI_ChatGPT Adds Conversation Branching Feature",
    "summary_en": "OpenAI has announced a highly anticipated new feature for ChatGPT: conversation branching. This allows users to explore different conversational paths from a specific point without losing the original thread. It provides greater flexibility for pursuing multiple branches or conducting short side-prompt conversations without muddying the main context. The feature is now available to logged-in web users.",
    "keywords_en": [
      "OpenAI",
      "ChatGPT",
      "Conversation Branching",
      "New Feature",
      "AI Tool"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Product Launch",
      "Large Language Model"
    ],
    "published_time": "2025-09-04T20:13:46.000Z",
    "download_time": "2025-09-05 05:41:12",
    "visual_resource": [
      "screenshot/twitter/OpenAI_1963697012014215181.png"
    ],
    "extra_info": "{\"username\": \"OpenAI\", \"tweet_id\": \"1963697012014215181\"}"
  },
  {
    "id": "twitter_GroqInc_1963635205899710798",
    "source": "Twitter",
    "url": "https://twitter.com/GroqInc/status/1963635205899710798",
    "title_en": "GroqInc_Groq's First Agentic System Compound Now Generally Available",
    "summary_en": "Groq Inc. has officially announced that its groundbreaking first agentic system, Compound, is now generally available (GA) for all users on GroqCloud. This robust system has undergone extensive real-world validation, being battle-tested by over 100,000 developers and processing more than 5 million requests, proving its exceptional readiness for production at scale. This significant milestone underscores Groq's commitment to delivering advanced, scalable AI solutions and marks a pivotal moment in the broader commercialization of intelligent agent technology.",
    "keywords_en": [
      "Groq",
      "Agentic System",
      "Compound",
      "GroqCloud",
      "General Availability",
      "AI Production"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Product Launch"
    ],
    "published_time": "2025-09-04T16:08:10.000Z",
    "download_time": "2025-09-05 05:41:34",
    "visual_resource": [
      "screenshot/twitter/GroqInc_1963635205899710798.png"
    ],
    "extra_info": "{\"username\": \"GroqInc\", \"tweet_id\": \"1963635205899710798\"}"
  },
  {
    "id": "twitter_GordonWetzstein_1963583050744250879",
    "source": "Twitter",
    "url": "https://twitter.com/GordonWetzstein/status/1963583050744250879",
    "title_en": "GordonWetzstein_Breakthrough in Long Video Generation: Introducing Mixture of Contexts",
    "summary_en": "Gordon Wetzstein's team introduces the \"Mixture of Contexts\" model, designed to address the challenges of content drift and historical context forgetting in long-duration video generation. This innovative model can directly produce minute-long videos in a single pass, eliminating the need for post-processing, stitching, or editing. This development represents a significant advancement in video generation technology, enabling seamless and coherent video creation.",
    "keywords_en": [
      "Video Generation",
      "Mixture of Contexts",
      "Long Video",
      "AI Model",
      "Drift-free"
    ],
    "area_en": [
      "Generative AI",
      "Video Understanding",
      "Research Progress"
    ],
    "published_time": "2025-09-04T12:40:55.000Z",
    "download_time": "2025-09-05 05:41:12",
    "visual_resource": [
      "screenshot/twitter/GordonWetzstein_1963583050744250879.png"
    ],
    "extra_info": "{\"username\": \"GordonWetzstein\", \"tweet_id\": \"1963583050744250879\"}"
  },
  {
    "id": "twitter_ZhihuFrontier_1963532501336695282",
    "source": "Twitter",
    "url": "https://twitter.com/ZhihuFrontier/status/1963532501336695282",
    "title_en": "ZhihuFrontier_Efficient RL Training: Slime Framework Weight Sync Optimization",
    "summary_en": "Zhihu Frontier highly recommends a blog post detailing efficient RL training, specifically optimizing weight synchronization within the slime framework. The post explains its server-based architecture and key achievements, including slashing Qwen3-30B-A3B weight update time from 60s to 7s, and completing GLM4.5-355B-A32B FP8 blockwise quantization and parameter updates in 100s. The team continues to optimize aspects like async collect/send, async loading, and zero redundancy layout to further advance RL infrastructure.",
    "keywords_en": [
      "Reinforcement Learning",
      "AI Infrastructure",
      "Large Language Models",
      "Slime Framework",
      "Weight Synchronization",
      "Efficient Training"
    ],
    "area_en": [
      "Machine Learning",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-09-04T09:20:03.000Z",
    "download_time": "2025-09-05 05:41:39",
    "visual_resource": [
      "screenshot/twitter/ZhihuFrontier_1963532501336695282.png"
    ],
    "extra_info": "{\"username\": \"ZhihuFrontier\", \"tweet_id\": \"1963532501336695282\"}"
  },
  {
    "id": "twitter_Gradio_1963636954999754955",
    "source": "Twitter",
    "url": "https://twitter.com/Gradio/status/1963636954999754955",
    "title_en": "Gradio_Gradio One-Command Deployment of MCP Servers to Google Cloud",
    "summary_en": "Gradio has announced a new feature enabling users to deploy powerful MCP servers to Google Cloud with just a single command. This functionality includes a built-in queue for scaling up to production workloads, significantly simplifying the server deployment process and enhancing efficiency and scalability.",
    "keywords_en": [
      "Gradio",
      "Cloud Deployment",
      "Google Cloud",
      "Servers",
      "Scalability",
      "Product Launch"
    ],
    "area_en": [
      "Product Launch",
      "Tech News",
      "Open Source"
    ],
    "published_time": "2025-09-04T16:15:07.000Z",
    "download_time": "2025-09-05 05:41:29",
    "visual_resource": [
      "screenshot/twitter/Gradio_1963636954999754955.png"
    ],
    "extra_info": "{\"username\": \"Gradio\", \"tweet_id\": \"1963636954999754955\"}"
  },
  {
    "id": "1cMniJgVQQDGaDdaDASFTg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/1cMniJgVQQDGaDdaDASFTg",
    "title_en": "Tsinghua IDEA's GUAVA Framework Generates Upper-Body 3D Avatars from Single Image in 0.1s, Accepted by ICCV 2025",
    "summary_en": "Tsinghua University and IDEA Research Institute have jointly introduced the GUAVA framework, capable of rapidly generating realistic upper-body 3D avatars from a single image in just 0.1 seconds. This innovative framework has been accepted by ICCV 2025. GUAVA overcomes the limitations of traditional methods that rely on multi-view videos or extensive single-person training, effectively addressing challenges in identity consistency and real-time performance. It innovatively employs 3D Gaussian representation and introduces the Expressive Human Model (EHM) to capture intricate facial expressions. Combined with inverse texture mapping technology, GUAVA significantly enhances rendering quality and efficiency. Experimental results demonstrate that GUAVA surpasses existing 2D and 3D methods in terms of rendering quality, identity preservation, and real-time animation capabilities (achieving 50 FPS). This provides an efficient and high-quality solution for virtual content creation in fields such as film and gaming.",
    "keywords_en": [
      "GUAVA",
      "3D Avatar",
      "Single Image Generation",
      "Real-time Rendering",
      "3D Gaussian"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-09-04T16:01:27.000Z",
    "download_time": "2025-09-05T13:42:34.559774",
    "visual_resource": [
      "screenshot/wechat/wechat_image_1cMniJgVQQDGaDdaDASFTg.png"
    ],
    "extra_info": null
  },
  {
    "id": "58lCMBPOF3JEWOnkOhGPHQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/58lCMBPOF3JEWOnkOhGPHQ",
    "title_en": "Minute-Level Long Video Generation Ushers in a \"Memory Revolution,\" with 7x Cost Reduction and 2.2x End-to-End Generation Speed Improvement! | Stanford & ByteDance",
    "summary_en": "This paper introduces Mixture of Contexts (MoC), an innovative adaptive, content-aligned context mixing layer designed for long video generation. MoC replaces the dense attention mechanism in Diffusion Transformers (DiT) to mitigate the quadratic computational cost associated with standard self-attention. By implementing a learnable routing strategy, which includes top-k selection and content-aware block partitioning, MoC achieves a remarkable reduction in FLOPs by over 7 times and boosts end-to-end generation speed by 2.2 times for extensive video sequences (e.g., 180k tokens). This enables minute-level video generation at a computational cost comparable to that of short videos. The method demonstrates improved performance, particularly in motion diversity, while preserving overall quality. MoC's ability to learn long-term dependencies directly from data facilitates scalable and controllable long video generation, marking a significant step towards emergent long-term memory capabilities in video synthesis.",
    "keywords_en": [
      "Long Video Generation",
      "Mixture of Contexts",
      "Sparse Attention",
      "Diffusion Models",
      "Computational Efficiency",
      "Memory Revolution"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-09-04T16:01:27.000Z",
    "download_time": "2025-09-05T13:42:23.518181",
    "visual_resource": [
      "screenshot/wechat/wechat_image_58lCMBPOF3JEWOnkOhGPHQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "gnWF2S6JhwhQGWIcDbNuWQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/gnWF2S6JhwhQGWIcDbNuWQ",
    "title_en": "Empowering Embodied Agents with Spatial Cognition: Tsinghua and Beihang Universities Introduce Brain-Inspired Framework for Navigation, Reasoning, and Complex Tasks",
    "summary_en": "Tsinghua University and Beihang University have jointly introduced the BSC-Nav framework, aiming to equip embodied agents with brain-inspired spatial cognition, addressing the current bottleneck of AI lacking long-term, structured memory in complex physical environments. Inspired by biological brain spatial memory mechanisms, BSC-Nav integrates Landmark Memory, Cognitive Map, and Working Memory modules with multimodal large models to construct a dynamic and continuously updated spatial memory system. Experimental results demonstrate that BSC-Nav significantly outperforms existing methods across various navigation tasks. It also exhibits robust generalization capabilities in real-world settings, enabling complex instruction understanding, active embodied question answering, and multi-step mobile manipulation tasks such as preparing breakfast. This research underscores that memory, rather than mere computational power, is crucial for the evolution of embodied intelligence from reactive behavior to proactive cognition, offering a new paradigm for the development of general artificial intelligence.",
    "keywords_en": [
      "Embodied AI",
      "Spatial Cognition",
      "Brain-Inspired AI",
      "Multimodal Large Models",
      "Navigation",
      "Memory"
    ],
    "area_en": [
      "AI Agent",
      "Robotics",
      "Multimodal"
    ],
    "published_time": "2025-09-04T16:01:27.000Z",
    "download_time": "2025-09-05T13:42:26.003281",
    "visual_resource": [
      "screenshot/wechat/wechat_image_gnWF2S6JhwhQGWIcDbNuWQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "QRegQQtn3U6cl9oYjBSy9w",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/QRegQQtn3U6cl9oYjBSy9w",
    "title_en": "Small Language Models are the Future of Agentic AI",
    "summary_en": "NVIDIA's latest research indicates that most Large Language Model (LLM) applications struggle with profitability and exhibit computational resource mismatch in Agentic AI systems. Their paper, \"Small Language Models are the Future of Agentic AI,\" proposes that Small Language Models (SLMs), defined as having fewer than 10 billion parameters, are better suited as the future core of Agentic AI due to their cost-effectiveness, efficiency, and adaptability to repetitive, convergent tasks. The study reveals that SLMs are 10-30 times cheaper than LLMs in inference efficiency and are easier to fine-tune and deploy on edge devices. NVIDIA's experiments demonstrate that SLMs can reliably handle 40%-70% of query tasks within Agentic AI. The paper also outlines a six-step conversion algorithm for migrating agent systems from LLMs to SLMs, emphasizing SLMs' modularity, flexibility, and cost advantages in Agentic AI. Several high-performing SLMs are recommended.",
    "keywords_en": [
      "Small Language Models",
      "AI Agent",
      "Large Language Models",
      "NVIDIA",
      "Cost-effectiveness"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-04T10:07:01.000Z",
    "download_time": "2025-09-05T13:42:23.932602",
    "visual_resource": [
      "screenshot/wechat/wechat_image_QRegQQtn3U6cl9oYjBSy9w.png"
    ],
    "extra_info": null
  },
  {
    "id": "qKNwEHyzF3eIE1x9A8_bvA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/qKNwEHyzF3eIE1x9A8_bvA",
    "title_en": "Core Knowledge Deficits in Multi-Modal Language Models",
    "summary_en": "A high-scoring ICML 2025 paper reveals significant core knowledge deficits in Multi-Modal Language Models (MLLMs), particularly in \"child-level\" foundational cognitive tasks such as object permanence and conservation. The research, utilizing an innovative evaluation system called CoreCognition and an \"intervention test\" method named Concept Hacking, systematically demonstrates that mainstream MLLMs widely lack fundamental core cognitive abilities. Crucially, this capability cannot be naturally acquired through simply scaling up model size; in some cases, larger models even exhibit worse performance. This finding poses a fundamental challenge to the current AI development paradigm, which primarily relies on scale expansion, indicating that models have not established a robust cognitive system where advanced reasoning is built upon solid foundational understanding. Future directions suggest explicitly injecting common sense knowledge, such as physics and spatial reasoning, during pre-training, or introducing cognitively-guided training mechanisms to address this core deficiency and advance AI towards general intelligence.",
    "keywords_en": [
      "Multi-Modal Large Models",
      "Core Knowledge",
      "Cognitive Deficits",
      "Model Evaluation",
      "Scaling Laws"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-04T09:26:10.000Z",
    "download_time": "2025-09-05T13:42:24.944174",
    "visual_resource": [
      "screenshot/wechat/wechat_image_qKNwEHyzF3eIE1x9A8_bvA.png"
    ],
    "extra_info": null
  },
  {
    "id": "iJcKc4r62mnQV1UXU_QTxw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/iJcKc4r62mnQV1UXU_QTxw",
    "title_en": "Figure Humanoid Robot Finally Learns to Load Dishwashers",
    "summary_en": "Figure's humanoid robot, Figure 02, has successfully learned to load dishwashers, building upon its previous capabilities in package sorting and towel folding. This new skill was achieved using the same versatile Helix architecture, an end-to-end \"vision-language-action\" (VLA) model, without requiring new algorithms or specialized engineering. Instead, the robot acquired complex abilities such as separating stacked dishes, delicately handling glassware, adapting to cluttered initial states, and recovering from grasping errors, simply by incorporating new data. This demonstrates the remarkable generality and scalability of the Helix architecture, highlighting its potential to progressively learn a wide range of new tasks through data-driven training within a single system. This achievement marks a significant stride for Figure towards realizing scalable humanoid intelligence, showcasing a unified model's ability to perform diverse household chores with human-like dexterity and adaptability.",
    "keywords_en": [
      "Humanoid Robot",
      "Figure 02",
      "Helix Architecture",
      "Dishwasher Loading",
      "General Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Robotics",
      "Large Language Model"
    ],
    "published_time": "2025-09-04T04:38:55.000Z",
    "download_time": "2025-09-05T13:42:38.778713",
    "visual_resource": [
      "screenshot/wechat/wechat_image_iJcKc4r62mnQV1UXU_QTxw.png"
    ],
    "extra_info": null
  },
  {
    "id": "bytebot",
    "source": "GitHub",
    "url": "https://github.com/bytebot-ai/bytebot",
    "title_en": "Bytebot: Open-Source AI Desktop Agent",
    "summary_en": "Bytebot is an open-source AI desktop agent designed to empower AI with its own complete virtual computer, allowing it to interact with a desktop environment just like a human. This innovative agent can seamlessly utilize any application, manage files, securely log into websites, process diverse documents, and execute intricate multi-step workflows across various programs. Its architecture integrates a virtual desktop, an intelligent AI agent, a user-friendly task interface, and robust APIs. Bytebot supports leading AI providers such as Anthropic, OpenAI, and Google Gemini, and is easily deployable via Docker containers, offering unparalleled data privacy, full control, and customization for users. It is ideally suited for automating business processes, streamlining development and testing, and enhancing research and analysis tasks.",
    "keywords_en": [
      "AI Desktop Agent",
      "Virtual Desktop",
      "Task Automation",
      "AI Agent",
      "Multi-Application Workflow",
      "Document Processing",
      "Docker",
      "Open Source"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-09-05T00:24:02Z",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "https://github.com/bytebot-ai/bytebot/raw/main/docs/images/bytebot-logo.png"
    ],
    "extra_info": null
  },
  {
    "id": "crewAI",
    "source": "GitHub",
    "url": "https://github.com/crewAIInc/crewAI",
    "title_en": "Fast and Flexible Multi-Agent Automation Framework",
    "summary_en": "CrewAI is a lean, lightning-fast Python framework built for multi-AI agent orchestration, entirely independent of other frameworks like LangChain. It empowers developers to create highly autonomous yet precisely controllable AI agents through its core mechanisms: CrewAI Crews, which optimize for collaborative intelligence and autonomy, and CrewAI Flows, enabling granular, event-driven control. The framework supports enterprise-grade applications with features like real-time tracing, observability, a unified control plane, and advanced security. Designed to transform complex business processes into efficient, intelligent automations, CrewAI is backed by a robust community of over 100,000 certified developers.",
    "keywords_en": [
      "Multi-Agent",
      "AI Agent",
      "Automation Framework",
      "Agent Orchestration",
      "Python",
      "Enterprise AI",
      "Observability",
      "Collaborative Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Generative AI"
    ],
    "published_time": "2025-09-04T19:32:47Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://github.com/crewAIInc/crewAI/raw/main/docs/images/crewai_logo.png",
      "https://github.com/crewAIInc/crewAI/raw/main/docs/images/asset.png",
      "https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "tensorzero",
    "source": "GitHub",
    "url": "https://github.com/tensorzero/tensorzero",
    "title_en": "TensorZero",
    "summary_en": "TensorZero is an open-source stack engineered for industrial-grade LLM applications. It features a unified LLM gateway, enabling low-latency access to major LLM providers. The platform integrates comprehensive observability, optimization, evaluation, and experimentation capabilities. It supports collecting production metrics, incorporating human feedback, fine-tuning models, advanced prompt engineering, and A/B testing. Designed for high performance and scalability, TensorZero empowers developers to build, deploy, and continuously optimize robust LLM applications, ensuring reliability and efficiency in production environments.",
    "keywords_en": [
      "LLM Applications",
      "LLM Gateway",
      "Observability",
      "Model Optimization",
      "Prompt Engineering",
      "A/B Testing",
      "Open Source",
      "LLMOps"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "published_time": "2025-09-04T21:57:36Z",
    "download_time": "2024-07-08 07:30:00",
    "visual_resource": [
      "screenshot/github/tensorzero.png"
    ],
    "extra_info": null
  },
  {
    "id": "elysia",
    "source": "GitHub",
    "url": "https://github.com/weaviate/elysia",
    "title_en": "Elysia: Agentic Framework Powered by Decision Trees",
    "summary_en": "Elysia is an agentic framework powered by decision trees, designed to dynamically utilize tools based on environmental context. It supports both custom tools and pre-built integrations for efficient data retrieval from Weaviate databases. The platform offers a Python API and a web application interface, allowing users to easily configure API keys and models. Elysia is suitable for developing AI applications such as intelligent search, data analysis, and customized workflows.",
    "keywords_en": [
      "Agentic Framework",
      "Decision Trees",
      "Tool Calling",
      "Weaviate",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-09-03T10:58:24Z",
    "download_time": "2024-05-16 08:00:00",
    "visual_resource": [
      "screenshot/github/elysia.png"
    ],
    "extra_info": null
  },
  {
    "id": "ML-From-Scratch",
    "source": "GitHub",
    "url": "https://github.com/eriklindernoren/ML-From-Scratch",
    "title_en": "Machine Learning From Scratch",
    "summary_en": "This GitHub repository offers Python implementations of fundamental machine learning models and algorithms from scratch. Its primary goal is to transparently and accessibly illustrate their inner workings, rather than focusing on computational efficiency. The project spans various machine learning paradigms, including supervised learning, unsupervised learning, reinforcement learning, and deep learning, featuring classic algorithms like linear regression, decision trees, K-Means, GANs, and DQNs. With numerous practical examples such as polynomial regression, CNN-based image classification, DBSCAN clustering, and generative adversarial networks for handwritten digit generation, this repository serves as an excellent practical platform for learners to deeply understand core machine learning concepts.",
    "keywords_en": [
      "Machine Learning",
      "Deep Learning",
      "Supervised Learning",
      "Unsupervised Learning",
      "Reinforcement Learning",
      "Neural Networks",
      "Algorithms",
      "Python"
    ],
    "area_en": [
      "Machine Learning",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2019-10-18T21:42:16Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "http://eriklindernoren.se/images/p_reg.gif",
      "http://eriklindernoren.se/images/mlfs_cnn1.png",
      "http://eriklindernoren.se/images/mlfs_dbscan.png"
    ],
    "extra_info": null
  },
  {
    "id": "2509.00375",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.00375",
    "title_en": "Open Data Synthesis For Deep Research",
    "summary_en": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.",
    "keywords_en": [
      "Large Language Models",
      "Deep Research",
      "Data Synthesis",
      "Hierarchical Constraint Satisfaction Problems",
      "InfoSeek"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "AI Agent"
    ],
    "published_time": "2025-08-30T06:02:56.000Z",
    "download_time": "2025-09-04 22:38:35",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00375.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.00375\", \"arxiv_url\": \"https://arxiv.org/abs/2509.00375\"}"
  },
  {
    "id": "2509.01106",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.01106",
    "title_en": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
    "summary_en": "We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.",
    "keywords_en": [
      "Robotics",
      "Unified Model",
      "Human-Robot Interaction",
      "Embodied AI",
      "Chain-of-Thought"
    ],
    "area_en": [
      "Robotics",
      "Multimodal",
      "AI Agent"
    ],
    "published_time": "2025-09-01T03:53:47.000Z",
    "download_time": "2025-09-04 22:38:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01106.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.01106\", \"arxiv_url\": \"https://arxiv.org/abs/2509.01106\"}"
  },
  {
    "id": "2509.03405",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.03405",
    "title_en": "LMEnt: A Suite for Analyzing Knowledge in Language Models from\n  Pretraining Data to Representations",
    "summary_en": "Language models (LMs) increasingly drive real-world applications that require\nworld knowledge. However, the internal processes through which models turn data\ninto representations of knowledge and beliefs about the world, are poorly\nunderstood. Insights into these processes could pave the way for developing LMs\nwith knowledge representations that are more consistent, robust, and complete.\nTo facilitate studying these questions, we present LMEnt, a suite for analyzing\nknowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a\nknowledge-rich pretraining corpus, fully annotated with entity mentions, based\non Wikipedia, (2) an entity-based retrieval method over pretraining data that\noutperforms previous approaches by as much as 80.4%, and (3) 12 pretrained\nmodels with up to 1B parameters and 4K intermediate checkpoints, with\ncomparable performance to popular open-sourced models on knowledge benchmarks.\nTogether, these resources provide a controlled environment for analyzing\nconnections between entity mentions in pretraining and downstream performance,\nand the effects of causal interventions in pretraining data. We show the\nutility of LMEnt by studying knowledge acquisition across checkpoints, finding\nthat fact frequency is key, but does not fully explain learning trends. We\nrelease LMEnt to support studies of knowledge in LMs, including knowledge\nrepresentations, plasticity, editing, attribution, and learning dynamics.",
    "keywords_en": [
      "Language Models",
      "Knowledge Acquisition",
      "Pretraining",
      "Entity Mentions",
      "Knowledge Representation"
    ],
    "area_en": [
      "Natural Language Processing",
      "Deep Learning",
      "Large Language Model"
    ],
    "published_time": "2025-09-03T15:31:18.000Z",
    "download_time": "2025-09-04 22:38:32",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03405.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.03405\", \"arxiv_url\": \"https://arxiv.org/abs/2509.03405\"}"
  },
  {
    "id": "2509.03403",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.03403",
    "title_en": "Beyond Correctness: Harmonizing Process and Outcome Rewards through RL\n  Training",
    "summary_en": "Reinforcement learning with verifiable rewards (RLVR) has emerged to be a\npredominant paradigm for mathematical reasoning tasks, offering stable\nimprovements in reasoning ability. However, Outcome Reward Models (ORMs) in\nRLVR are too coarse-grained to distinguish flawed reasoning within correct\nanswers or valid reasoning within incorrect answers. This lack of granularity\nintroduces noisy and misleading gradients significantly and hinders further\nprogress in reasoning process quality. While Process Reward Models (PRMs) offer\nfine-grained guidance for intermediate steps, they frequently suffer from\ninaccuracies and are susceptible to reward hacking.\n  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an\neffective data process curation method that harmonizes noisy, fine-grained\nprocess rewards with accurate, coarse-grained outcome rewards. Rather than\nnaively blending PRM and ORM in the objective function\n(arXiv:archive/2506.18896), PROF leverages their complementary strengths\nthrough consistency-driven sample selection. Our approach retains correct\nresponses with higher averaged process values and incorrect responses with\nlower averaged process values, while maintaining positive/negative training\nsample balance. Extensive experiments demonstrate that our method not only\nconsistently improves the final accuracy over 4% compared to the blending\napproaches, but also strengthens the quality of intermediate reasoning steps.\nCodes and training recipes are available at https://github.com/Chenluye99/PROF.",
    "keywords_en": [
      "Reinforcement Learning",
      "Process Reward Models",
      "Outcome Reward Models",
      "Mathematical Reasoning",
      "Data Curation"
    ],
    "area_en": [
      "Machine Learning",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-03T15:28:51.000Z",
    "download_time": "2025-09-04 22:38:32",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03403.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.03403\", \"arxiv_url\": \"https://arxiv.org/abs/2509.03403\"}"
  },
  {
    "id": "2509.02722",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.02722",
    "title_en": "Planning with Reasoning using Vision Language World Model",
    "summary_en": "Effective planning requires strong world models, but high-level world models\nthat can understand and reason about actions with semantic and temporal\nabstraction remain largely underdeveloped. We introduce the Vision Language\nWorld Model (VLWM), a foundation model trained for language-based world\nmodeling on natural videos. Given visual observations, the VLWM first infers\nthe overall goal achievements then predicts a trajectory composed of\ninterleaved actions and world state changes. Those targets are extracted by\niterative LLM Self-Refine conditioned on compressed future observations\nrepresented by Tree of Captions. The VLWM learns both an action policy and a\ndynamics model, which respectively facilitates reactive system-1 plan decoding\nand reflective system-2 planning via cost minimization. The cost evaluates the\nsemantic distance between the hypothetical future states given by VLWM\nroll-outs and the expected goal state, and is measured by a critic model that\nwe trained in a self-supervised manner. The VLWM achieves state-of-the-art\nVisual Planning for Assistance (VPA) performance on both benchmark evaluations\nand our proposed PlannerArena human evaluations, where system-2 improves the\nElo score by +27% upon system-1. The VLWM models also outperforms strong VLM\nbaselines on RoboVQA and WorldPrediction benchmark.",
    "keywords_en": [
      "Vision Language World Model",
      "Reasoning Planning",
      "World Model",
      "Foundation Model",
      "Visual Planning"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Computer Vision"
    ],
    "published_time": "2025-09-02T18:18:57.000Z",
    "download_time": "2025-09-04 22:38:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02722.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.02722\", \"arxiv_url\": \"https://arxiv.org/abs/2509.02722\"}"
  },
  {
    "id": "2509.01977",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.01977",
    "title_en": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement",
    "summary_en": "Multi-subject personalized generation presents unique challenges in\nmaintaining identity fidelity and semantic coherence when synthesizing images\nconditioned on multiple reference subjects. Existing methods often suffer from\nidentity blending and attribute leakage due to inadequate modeling of how\ndifferent subjects should interact within shared representation spaces. We\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\ngeneration through explicit semantic correspondence and orthogonal feature\ndisentanglement. Our key insight is that multi-subject generation requires\nprecise semantic alignment at the representation level - knowing exactly which\nregions in the generated image should attend to which parts of each reference.\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\nproviding fine-grained semantic correspondences between multiple reference\nsubjects and target images, previously unavailable in this domain. Building on\nthis foundation, we propose the semantic correspondence attention loss to\nenforce precise point-to-point semantic alignment, ensuring high consistency\nfrom each reference to its designated regions. Furthermore, we develop the\nmulti-reference disentanglement loss to push different subjects into orthogonal\nattention subspaces, preventing feature interference while preserving\nindividual identity characteristics. Extensive experiments demonstrate that\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\nmulti-subject synthesis applications.",
    "keywords_en": [
      "Multi-subject generation",
      "Personalized generation",
      "Semantic correspondence",
      "Feature disentanglement",
      "Identity fidelity"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-09-02T05:40:07.000Z",
    "download_time": "2025-09-04 22:38:33",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01977.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.01977\", \"arxiv_url\": \"https://arxiv.org/abs/2509.01977\"}"
  }
]