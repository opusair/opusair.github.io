[
  {
    "id": "hackernews_46719229",
    "source": "Hacker News",
    "url": "https://qwen.ai/blog?id=qwen3tts-0115",
    "title": "Qwen3-TTS Family Is Now Open Sourced: Voice Design, Clone, and Generation",
    "summary": "The Qwen3-TTS family, a sophisticated suite of text-to-speech models, has been officially open-sourced, marking a significant advancement in synthetic voice technology. This release empowers developers and researchers with advanced capabilities for voice design, voice cloning, and high-fidelity voice generation. The voice design feature allows for the creation of unique vocal characteristics, offering extensive customization for various applications. Furthermore, the robust voice cloning functionality enables users to replicate specific voices with remarkable accuracy from minimal audio input, opening new possibilities for personalized digital experiences. The core voice generation component excels at converting text into natural and expressive speech, suitable for a wide array of uses, from content creation and virtual assistants to accessibility tools. By making the Qwen3-TTS family open source, its creators aim to foster collaborative innovation and accelerate the development of next-generation AI-driven audio solutions across diverse industries. This move underscores a commitment to advancing generative AI in the audio domain, providing a powerful toolkit for speech synthesis.",
    "keywords": [
      "Qwen3-TTS",
      "Text-to-Speech",
      "Voice Synthesis",
      "Voice Cloning",
      "Voice Generation",
      "Open Source",
      "Generative AI",
      "Audio AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Natural Language Processing"
    ],
    "published_time": "2026-01-22 13:51:25",
    "download_time": "2026-01-22 20:00:35",
    "extra_info": "{\"score\": 291, \"by\": \"Palmik\", \"descendants\": 86, \"story_id\": 46719229}"
  },
  {
    "id": "hackernews_46720395",
    "source": "Hacker News",
    "url": "https://gptzero.me/news/neurips/",
    "title": "GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers",
    "summary": "A significant report from GPTZero reveals the detection of 100 instances of \"hallucinations\" within papers accepted for the prestigious NeurIPS 2025 conference. This finding raises considerable concerns regarding the integrity and factual accuracy of scientific research submissions, particularly in an era where AI-generated content is becoming increasingly prevalent. The identification of such a large number of anomalies by an AI detection tool like GPTZero underscores the critical need for robust verification processes in academic publishing. Hallucinations, typically defined as plausible but factually incorrect outputs generated by AI models, can undermine the credibility of research findings and impede scientific progress. This discovery prompts a re-evaluation of current peer-review mechanisms and the role of AI detection technologies in maintaining high standards for scholarly work. The incident highlights the growing challenges faced by top-tier conferences like NeurIPS in ensuring the originality and veracity of contributions, calling for enhanced vigilance against AI-induced inaccuracies in research papers. It also emphasizes the evolving landscape of academic publishing, where the intersection of AI generation and human review necessitates new protocols and tools to uphold scientific rigor.",
    "keywords": [
      "AI Content Detection",
      "Hallucinations",
      "NeurIPS",
      "Academic Integrity",
      "AI Research",
      "Machine Learning Ethics"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2026-01-22 15:20:48",
    "download_time": "2026-01-22 20:00:38",
    "extra_info": "{\"score\": 476, \"by\": \"segmenta\", \"descendants\": 256, \"story_id\": 46720395}"
  },
  {
    "id": "hackernews_46721474",
    "source": "Hacker News",
    "url": "https://github.com/browseros-ai/BrowserOS",
    "title": "Show HN: BrowserOS – \"Claude Cowork\" in the browser",
    "summary": "BrowserOS, an open-source and privacy-first AI browser developed by Nithin and Nikhil (YC S24), has launched with a core architectural innovation: running AI agents entirely on the client side. This allows users to leverage local Large Language Models (LLMs) or bring their own API keys, ensuring that sensitive company and user data remains securely on the local machine. This client-side processing approach fundamentally differentiates BrowserOS from server-side agent loops found in competitors like ChatGPT Atlas and Perplexity Comet, addressing significant privacy concerns. The latest feature release introduces comprehensive filesystem access, enabling the browser agent to read and write files and execute shell commands, akin to advanced functionalities seen in platforms suchs as Claude Cowork. This capability, unexpectedly derived from their initial privacy-focused design, positions BrowserOS as a compelling alternative for users prioritizing data security and local control in their AI-powered browsing experiences.",
    "keywords": [
      "BrowserOS",
      "Open-source",
      "Client-side AI",
      "Privacy-first AI",
      "Local LLMs",
      "AI Agent",
      "Filesystem Access"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2026-01-22 16:30:58",
    "download_time": "2026-01-22 20:00:53",
    "extra_info": "{\"score\": 16, \"by\": \"felarof\", \"descendants\": 8, \"story_id\": 46721474}"
  },
  {
    "id": "hackernews_46721388",
    "source": "Hacker News",
    "url": "https://www.stealingisntinnovation.com",
    "title": "Stealing Isn't Innovation – America's creative community message against AI",
    "summary": "The campaign \"Stealing Isn't Innovation\" represents a significant pushback from America's creative community against the perceived appropriation of copyrighted works by artificial intelligence technologies. This initiative highlights widespread concerns among artists, writers, musicians, and other creators regarding the ethical implications and legal ramifications of using their intellectual property to train AI models without explicit consent or fair compensation. The core message emphasizes that while AI offers new tools and possibilities, its development should not come at the expense of creators' rights, nor should it be seen as innovative to merely reproduce or derive from existing works without proper licensing. The campaign advocates for robust legal frameworks and industry standards that protect intellectual property in the age of generative AI, ensuring that innovation fosters creativity rather than undermining it by devaluing human artistry. It calls for transparency in AI training data sourcing and equitable compensation models for creators whose works contribute to AI development.",
    "keywords": [
      "Artificial Intelligence Ethics",
      "Intellectual Property",
      "Copyright Law",
      "Generative AI",
      "Content Creation",
      "AI Regulation",
      "Creative Rights",
      "Data Sourcing"
    ],
    "area": [
      "Artificial Intelligence",
      "Generative AI",
      "Others"
    ],
    "published_time": "2026-01-22 16:24:57",
    "download_time": "2026-01-22 20:01:08",
    "extra_info": "{\"score\": 7, \"by\": \"giuliomagnifico\", \"descendants\": 3, \"story_id\": 46721388}"
  },
  {
    "id": "hackernews_46723384",
    "source": "Hacker News",
    "url": "https://hugodaniel.com/posts/claude-code-banned-me/",
    "title": "I was banned from Claude for scaffolding a Claude.md file",
    "summary": "A user reported an unexpected ban from Anthropic's Claude AI service, attributing the suspension to the creation of a file named 'Claude.md' during a code scaffolding process. This incident, documented in a personal blog post, highlights potential issues with automated content moderation or policy enforcement within AI platforms. The user's account suggests that the mere presence or creation of a file directly referencing the AI's own name, even in a development context, triggered a severe account action without prior warning. This raises significant concerns about the sensitivity and specificity of AI content filtering mechanisms and the implications for developers or users who might innocently reference the AI service in their work, potentially limiting their creative freedom or workflow. The event prompts a discussion on the boundaries of acceptable use, the transparency of reasons for account suspensions, and the need for clearer communication from AI service providers in the evolving landscape of human-AI interaction and platform governance.",
    "keywords": [
      "AI Moderation",
      "Large Language Model",
      "Content Policy",
      "User Suspension",
      "AI Ethics",
      "Platform Governance"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2026-01-22 18:38:27",
    "download_time": "2026-01-22 20:00:39",
    "extra_info": "{\"score\": 123, \"by\": \"hugodan\", \"descendants\": 87, \"story_id\": 46723384}"
  },
  {
    "id": "hackernews_46721933",
    "source": "Hacker News",
    "url": "https://constellation-io.com/",
    "title": "Launch HN: Constellation Space (YC W26) – AI for satellite mission assurance",
    "summary": "Constellation Space, a YC W26 company, announced the launch of its AI system focused on satellite mission assurance. Founded by engineers with extensive backgrounds at SpaceX, Blue Origin, and NASA, the team developed a predictive artificial intelligence solution designed to identify and forecast satellite link failures before they occur. Their collective experience, including managing constellation health for Starlink and deep space communications, consistently revealed a critical operational challenge: data loss often happens because degrading radio frequency (RF) links are only detected reactively. Constellation Space's innovative AI system directly addresses this by analyzing dozens of complex, interacting variables that influence satellite RF link performance. This sophisticated analysis enables real-time predictions of link stability for upcoming operational windows, thereby ensuring proactive intervention. The technology promises to significantly enhance the reliability and operational efficiency of modern satellite constellations and critical space missions by preventing costly data loss and ensuring continuous communication.",
    "keywords": [
      "Satellite Mission Assurance",
      "AI System",
      "Link Failure Prediction",
      "Satellite Operations",
      "RF Links",
      "Predictive Analytics",
      "Space Technology",
      "Constellation Management"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Others"
    ],
    "published_time": "2026-01-22 17:03:21",
    "download_time": "2026-01-22 20:00:47",
    "extra_info": "{\"score\": 19, \"by\": \"kmajid\", \"descendants\": 3, \"story_id\": 46721933}"
  },
  {
    "id": "2601.12538",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.12538",
    "title": "Agentic Reasoning for Large Language Models",
    "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
    "keywords": [
      "Agentic Reasoning",
      "Large Language Models",
      "Autonomous Agents",
      "Multi-agent Systems",
      "Reinforcement Learning"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2026-01-18T18:58:23.000Z",
    "download_time": "2026-01-22 12:01:18",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.12538\", \"arxiv_url\": \"https://arxiv.org/abs/2601.12538\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png\", \"original_title\": \"Agentic Reasoning for Large Language Models\"}"
  },
  {
    "id": "2601.15282",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.15282",
    "title": "Rethinking Video Generation Model for the Embodied World",
    "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
    "keywords": [
      "Video Generation Models",
      "Embodied Intelligence",
      "Robotics Benchmarks",
      "Robot Datasets",
      "Physical Realism"
    ],
    "area": [
      "Generative AI",
      "Robotics",
      "Computer Vision"
    ],
    "published_time": "2026-01-21T18:59:18.000Z",
    "download_time": "2026-01-22 12:01:18",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.15282\", \"arxiv_url\": \"https://arxiv.org/abs/2601.15282\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15282.png\", \"original_title\": \"Rethinking Video Generation Model for the Embodied World\"}"
  },
  {
    "id": "2601.14027",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.14027",
    "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
    "summary": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.",
    "keywords": [
      "AI Agent",
      "Formal Mathematics",
      "Theorem Proving",
      "Mathematical Reasoning",
      "Lean"
    ],
    "area": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2026-01-20T14:51:45.000Z",
    "download_time": "2026-01-22 12:01:17",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.14027\", \"arxiv_url\": \"https://arxiv.org/abs/2601.14027\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14027.png\", \"original_title\": \"Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics\"}"
  },
  {
    "id": "2601.07853",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.07853",
    "title": "FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments",
    "summary": "Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.",
    "keywords": [
      "Financial Agents",
      "LLMs Security",
      "Execution-Grounded Benchmark",
      "FinVault",
      "Prompt Injection"
    ],
    "area": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2026-01-09T03:25:45.000Z",
    "download_time": "2026-01-22 12:01:18",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.07853\", \"arxiv_url\": \"https://arxiv.org/abs/2601.07853\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07853.png\", \"original_title\": \"FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments\"}"
  },
  {
    "id": "2601.14681",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.14681",
    "title": "FARE: Fast-Slow Agentic Robotic Exploration",
    "summary": "This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale 200mtimes130m building environment.",
    "keywords": [
      "Robotic Exploration",
      "Large Language Model",
      "Reinforcement Learning",
      "AI Agent",
      "Autonomous Systems"
    ],
    "area": [
      "Robotics",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2026-01-21T05:56:24.000Z",
    "download_time": "2026-01-22 12:01:19",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.14681\", \"arxiv_url\": \"https://arxiv.org/abs/2601.14681\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14681.png\", \"original_title\": \"FARE: Fast-Slow Agentic Robotic Exploration\"}"
  },
  {
    "id": "2601.15059",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.15059",
    "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems",
    "summary": "Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis. We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity. We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime. We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum. We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.",
    "keywords": [
      "responsibility vacuum",
      "scaled agent systems",
      "organizational failure",
      "CI/CD pipelines",
      "automated validation"
    ],
    "area": [
      "AI Agent",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "published_time": "2026-01-21T15:05:27.000Z",
    "download_time": "2026-01-22 12:01:20",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.15059\", \"arxiv_url\": \"https://arxiv.org/abs/2601.15059\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15059.png\", \"original_title\": \"The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems\"}"
  }
]