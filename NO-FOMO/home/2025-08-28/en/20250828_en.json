[
  {
    "id": "twitter_GeminiApp_1961192420496085127",
    "source": "Twitter",
    "url": "https://x.com/GeminiApp/status/1961192420496085127",
    "title_en": "GeminiApp_Gemini应用新增图像生成与编辑功能",
    "summary_en": "Google Gemini应用近期推出图像生成与编辑功能更新，用户社区正积极探索并展示其创意用法。此次更新引入了名为“nanobanana”的图像处理能力，旨在提升Gemini应用在视觉内容创作方面的用户体验，进一步拓展其多模态交互潜力。",
    "keywords_en": [
      "Gemini",
      "图像生成",
      "图像编辑",
      "多模态",
      "应用更新",
      "nanobanana"
    ],
    "area_en": [
      "人工智能",
      "生成式AI",
      "产品发布"
    ],
    "published_time": "2025-08-28T22:21:24.000Z",
    "download_time": "2025-08-29 01:57:45",
    "visual_resource": [
      "screenshot/twitter/GeminiApp_1961192420496085127.png"
    ],
    "extra_info": "{\"username\": \"GeminiApp\", \"tweet_id\": \"1961192420496085127\"}"
  },
  {
    "id": "twitter_AndrewYNg_1961118026398617648",
    "source": "Twitter",
    "url": "https://x.com/AndrewYNg/status/1961118026398617648",
    "title_en": "AndrewYNg_吴恩达：并行智能体是扩展AI能力的新方向",
    "summary_en": "吴恩达指出，并行智能体正成为扩展AI能力的重要新方向。传统AI性能提升依赖更多数据和计算，而测试时计算虽能提高性能但耗时。随着LLM每token成本下降，并行化智能体工作流成为可能，能显著缩短用户等待时间。推文列举了并行研究、并行编程框架及后台任务处理等应用案例，并提及“CodeMonkeys”和“智能体混合架构”等研究，强调并行智能体在提升AI效率和用户体验方面的潜力。",
    "keywords_en": [
      "并行智能体",
      "AI扩展",
      "大模型",
      "智能体工作流",
      "计算效率",
      "吴恩达"
    ],
    "area_en": [
      "智能体",
      "大模型",
      "研究进展"
    ],
    "published_time": "2025-08-28T17:25:47.000Z",
    "download_time": "2025-08-29 01:52:33",
    "visual_resource": [
      "screenshot/twitter/AndrewYNg_1961118026398617648.png"
    ],
    "extra_info": "{\"username\": \"AndrewYNg\", \"tweet_id\": \"1961118026398617648\"}"
  },
  {
    "id": "twitter_premium_1952590396670046264",
    "source": "Twitter",
    "url": "https://x.com/premium/status/1952590396670046264/analytics",
    "title_en": "premium_谷歌将Gemini集成至Chrome浏览器，浏览体验将迎来变革",
    "summary_en": "谷歌已悄然将旗下AI模型Gemini集成至Chrome浏览器，此举预示着未来的网页浏览体验将发生颠覆性变革。此次集成预计将带来一系列创新功能，显著提升用户在信息获取、内容交互及个性化服务方面的效率与便捷性。推文指出，至少有八项值得关注的新特性将随之推出，预示着AI在浏览器应用中的深度融合。",
    "keywords_en": [
      "谷歌",
      "Gemini",
      "Chrome浏览器",
      "人工智能",
      "浏览体验",
      "产品集成"
    ],
    "area_en": [
      "人工智能",
      "产品发布",
      "技术动态"
    ],
    "published_time": "2025-08-28T13:33:06.000Z",
    "download_time": "2025-08-29 01:48:36",
    "visual_resource": [
      "screenshot/twitter/premium_1952590396670046264.png"
    ],
    "extra_info": "{\"username\": \"premium\", \"tweet_id\": \"1952590396670046264\"}"
  },
  {
    "id": "twitter_fchollet_1961140723392332276",
    "source": "Twitter",
    "url": "https://x.com/fchollet/status/1961140723392332276",
    "title_en": "fchollet_模型可解释性：规模而非方法是关键",
    "summary_en": "弗朗索瓦·肖莱指出，模型可解释性并非取决于所使用的机器学习方法（如神经网络或符号代码），而是纯粹由模型规模和复杂性决定。他强调，任何基底的模型在足够小时都可解释，但复杂代码或图形模型的行为难以解释。肖莱认为，“必须使用可解释方法”的说法是站不住脚的，因为它意味着将自己限制在玩具模型中，阻碍了复杂AI系统的发展。",
    "keywords_en": [
      "模型可解释性",
      "机器学习",
      "模型复杂度",
      "神经网络",
      "AI伦理"
    ],
    "area_en": [
      "机器学习",
      "研究进展",
      "人工智能"
    ],
    "published_time": "2025-08-28T18:55:59.000Z",
    "download_time": "2025-08-29 01:53:34",
    "visual_resource": [
      "screenshot/twitter/fchollet_1961140723392332276.png"
    ],
    "extra_info": "{\"username\": \"fchollet\", \"tweet_id\": \"1961140723392332276\"}"
  },
  {
    "id": "twitter_MilesCcc_1961105018272268426",
    "source": "Twitter",
    "url": "https://x.com/MilesCcc/status/1961105018272268426",
    "title_en": "MilesCcc_Selected for TIME's 2025 AI 100 List, Aiding Isomorphic Labs Drug Design",
    "summary_en": "Miles Congreve announced his humble selection for TIME's 2025 AI 100 list. He highlighted his three-year dedication to helping shape Isomorphic Labs' drug design engine and building the team leveraging it for drug discovery. This recognition not only acknowledges his personal achievements but also underscores the growing influence of AI in accelerating drug development.",
    "keywords_en": [
      "TIME 100 AI",
      "Artificial Intelligence",
      "Drug Design",
      "Drug Discovery",
      "Isomorphic Labs"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Industry News",
      "Research Progress"
    ],
    "published_time": "2025-08-28T16:34:06.000Z",
    "download_time": "2025-08-29 02:04:51",
    "visual_resource": [
      "screenshot/twitter/MilesCcc_1961105018272268426.png"
    ],
    "extra_info": "{\"username\": \"MilesCcc\", \"tweet_id\": \"1961105018272268426\"}"
  },
  {
    "id": "twitter_sama_1961096744533647501",
    "source": "Twitter",
    "url": "https://x.com/sama/status/1961096744533647501",
    "title_en": "sama_OpenAI发布Codex新功能",
    "summary_en": "OpenAI开发者宣布推出Codex系列新功能，旨在提升其作为编程协作工具的效率。这些更新包括全新的IDE扩展、支持云端与本地环境间任务无缝迁移、GitHub代码审查集成以及改进的Codex命令行工具。新功能由GPT-5驱动，并通过ChatGPT计划提供。Sam Altman表示用户对新功能反响积极。",
    "keywords_en": [
      "Codex",
      "OpenAI",
      "GPT-5",
      "编程工具",
      "IDE扩展",
      "代码审查"
    ],
    "area_en": [
      "产品发布",
      "大模型",
      "技术动态"
    ],
    "published_time": "2025-08-28T16:01:13.000Z",
    "download_time": "2025-08-29 01:52:00",
    "visual_resource": [
      "screenshot/twitter/sama_1961096744533647501.png"
    ],
    "extra_info": "{\"username\": \"sama\", \"tweet_id\": \"1961096744533647501\"}"
  },
  {
    "id": "E9DcldQZUPyIUUeqzi-STw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/E9DcldQZUPyIUUeqzi-STw",
    "title_en": "VoxHammer: Training-Free Localized 3D Model Editing for Enhanced Efficiency",
    "summary_en": "VoxHammer presents a novel, training-free 3D editing methodology designed for precise and coherent localized modifications directly within the 3D latent space. This innovative technique functions by predicting the inverse trajectory of a given 3D model. During the subsequent denoising and editing phases, it strategically replaces the denoised features within the preserved regions with corresponding inverse delay and cached key-value tokens. This meticulous approach guarantees exceptional consistency between the newly edited sections and the original, untouched areas. To rigorously assess its capabilities, the research team developed Edit3D Bench, a human-annotated dataset. Extensive experimental evaluations conclusively demonstrate that VoxHammer significantly surpasses existing methods in achieving superior 3D consistency and overall quality, thereby offering an exceptionally efficient and flexible solution for localized editing of 3D assets and substantially enhancing productivity in 3D content creation workflows.",
    "keywords_en": [
      "VoxHammer",
      "3D Editing",
      "Training-Free",
      "Localized Modification",
      "Latent Space",
      "Model Consistency"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-28T23:50:40.000Z",
    "download_time": "2025-08-29T16:19:07.992032",
    "visual_resource": [
      "screenshot/wechat/wechat_image_E9DcldQZUPyIUUeqzi-STw.png"
    ],
    "extra_info": null
  },
  {
    "id": "nz9KZ7oN8P5-ZNdDnt8GBg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/nz9KZ7oN8P5-ZNdDnt8GBg",
    "title_en": "Interview with LimX Dynamics' Zhang Wei: Building Robots is Easy, The Key is Practical Application",
    "summary_en": "LimX Dynamics founder Zhang Wei states that while humanoid robot hardware manufacturing is relatively easy, the core challenge lies in the AI-powered \"cerebellum\" for motion control, an area where LimX Dynamics claims global leadership. The company aims to build a robotics platform, providing foundational body and motion control technologies, and plans to develop a \"Windows\" operating system for humanoid robots. This initiative seeks to lower development barriers, making robots user-friendly and easily programmable. Zhang Wei emphasizes that innovation should be company-driven rather than solely academic, viewing humanoid robots as the \"iPhone of robots\" with significant long-term value. LimX Dynamics empowers developers by offering competitively priced full-sized humanoid robots like LimX Oli and open APIs/SDKs. Their business model prioritizes user value and ecosystem development over mere sales volume, with short-term applications focused on the B2B market.",
    "keywords_en": [
      "LimX Dynamics",
      "Humanoid Robots",
      "Embodied AI",
      "Motion Control",
      "Robotics Platform",
      "AI Cerebellum"
    ],
    "area_en": [
      "Robotics",
      "Embodied AI",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-28T16:01:34.000Z",
    "download_time": "2025-08-29T16:19:09.351565",
    "visual_resource": [
      "screenshot/wechat/wechat_image_nz9KZ7oN8P5-ZNdDnt8GBg.png"
    ],
    "extra_info": null
  },
  {
    "id": "ykA5AKduOu5o4gNKdmqslw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ykA5AKduOu5o4gNKdmqslw",
    "title_en": "HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation",
    "summary_en": "Tencent has open-sourced HunyuanVideo-Foley, an innovative end-to-end Text-Video-Audio (TV2A) generation framework, specifically engineered to automatically produce high-fidelity, cinema-quality sound effects for videos. This groundbreaking framework effectively tackles the complex challenges of multimodal alignment and realistic audio generation. It achieves this through the development of a novel, high-quality, and large-scale 100,000-hour TV2A dataset, coupled with the introduction of a sophisticated hybrid architecture, Multimodal Diffusion Transformer (MMDiT), and a unique Representation Alignment (REPA) loss strategy. Comprehensive evaluations demonstrate that HunyuanVideo-Foley significantly outperforms existing State-of-the-Art (SOTA) models across various critical metrics, notably excelling in video-semantic alignment, temporal synchronization, and overall audio quality. This technological advancement represents a substantial leap forward for automated video foley, promising to greatly enhance immersive content creation and user experiences.",
    "keywords_en": [
      "HunyuanVideo-Foley",
      "Text-Video-Audio Generation",
      "Sound Effect Generation",
      "Multimodal Diffusion Model",
      "Dataset Construction",
      "REPA Strategy"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "Generative AI"
    ],
    "published_time": "2025-08-28T16:01:34.000Z",
    "download_time": "2025-08-29T16:19:09.711705",
    "visual_resource": [
      "screenshot/wechat/wechat_image_ykA5AKduOu5o4gNKdmqslw.png"
    ],
    "extra_info": null
  },
  {
    "id": "NChnvM0uijUhDMiu_tWVRg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/NChnvM0uijUhDMiu_tWVRg",
    "title_en": "Your Doubts Are Valid: LLMs as Judges Are Neither Valid Nor Reliable, a Paper Finally Criticizes LLJs",
    "summary_en": "A recent paper critically examines the prevalent \"Large Language Models as Judges\" (LLJs) paradigm in AI, asserting its lack of both reliability and validity across performance evaluation, data construction, and model enhancement. The research challenges core assumptions that LLMs serve as effective proxies for human judgment, capable evaluators, scalable tools, and cost-effective solutions. It exposes inherent flaws such as instruction non-adherence, untrustworthy explanations, biases, fragility, and lack of domain expertise. Furthermore, the article warns against issues like data contamination, narcissistic bias, superficial safety alignment, and hidden costs including economic, environmental, and social bias amplification. The study advocates for a return to scientific rigor in AI evaluation, proposing task-specific LLJ applications, improved assessment practices, and the establishment of independent third-party oversight to address the current crisis in AI evaluation methodology.",
    "keywords_en": [
      "LLM as Judge",
      "Model Evaluation",
      "Reliability",
      "Validity",
      "Bias",
      "Data Contamination"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-28T09:54:13.000Z",
    "download_time": "2025-08-29T16:19:08.806175",
    "visual_resource": [
      "screenshot/wechat/wechat_image_NChnvM0uijUhDMiu_tWVRg.png"
    ],
    "extra_info": null
  },
  {
    "id": "szX0WG9a3pmYnIPSOTjanw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/szX0WG9a3pmYnIPSOTjanw",
    "title_en": "Baidu Smart Cloud Launches AI Search MCP Service: Agents Directly Access Real-time Information!",
    "summary_en": "Baidu Smart Cloud's Qianfan platform has launched its AI Search MCP Service, making Baidu's powerful AI search capabilities available as components. This strategic move addresses a critical bottleneck for AI Agents: the inability to access real-time information and mitigate model hallucinations, enabling Agents to directly link to up-to-date data and enhance output authority. Concurrently, Qianfan 4.0, positioned as a comprehensive enterprise-grade AI platform, introduces significant upgrades. These include multimodal RAG for deep internal data analysis, graph-enhanced RAG for improved retrieval, and a flexible multi-agent orchestration framework. The platform also boasts enhanced model services, integrating over 150 cutting-edge models, offering advanced features like Function Calling, fine-grained thinking control, and RFT fine-tuning tools. This initiative aims to provide robust infrastructure for enterprises to build intelligent Agents that are both \"aware of the external world\" and \"knowledgeable internally,\" shifting the focus from mere model competition to a broader platform and infrastructure battle. The Qianfan platform currently supports over 460,000 enterprises and has fostered over 1.3 million AI Agents, accelerating the deployment of AI applications.",
    "keywords_en": [
      "Baidu Smart Cloud",
      "Qianfan Platform",
      "AI Search",
      "Agent",
      "MCP Service",
      "Enterprise AI Platform"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-28T07:26:57.000Z",
    "download_time": "2025-08-29T16:19:21.537261",
    "visual_resource": [
      "screenshot/wechat/wechat_image_szX0WG9a3pmYnIPSOTjanw.png"
    ],
    "extra_info": null
  },
  {
    "id": "k1rVhAoQxD5phvGq9YWmRA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/k1rVhAoQxD5phvGq9YWmRA",
    "title_en": "NVIDIA Advances FP4 Precision for Pre-training, Offering Faster and Cheaper AI, Following DeepSeek's FP8 Discussion",
    "summary_en": "DeepSeek recently unveiled its FP8 quantization strategy, designed for domestic chips, sparking industry-wide interest in large model quantization and the integration of domestic hardware and software. Shortly thereafter, NVIDIA announced the extension of its NVFP4 (4-bit floating point) precision to the pre-training phase of large language models. NVIDIA claims NVFP4 can achieve 16-bit precision at 4-bit speed and efficiency, significantly boosting training throughput and reducing costs. The article delves into the technical intricacies of NVFP4, including micro-block scaling, high-precision block encoding, and tensor distribution reshaping. It highlights how these innovations address the challenges of low-precision training, such as dynamic range and gradient fluctuations. Validation experiments on a 12-billion-parameter Hybrid Mamba-Transformer model, trained on 10 trillion tokens, demonstrated that NVFP4 maintains accuracy and stability comparable to FP8 across various downstream tasks. This development signifies a pivotal shift in AI training, moving beyond mere computational power scaling towards more efficient, low-precision optimization. It sets a new benchmark for the scalability of 'AI factories' and the development of cutting-edge models, promising a future of faster, more sustainable, and more accessible AI innovation.",
    "keywords_en": [
      "FP4",
      "FP8",
      "Large Model Pre-training",
      "Quantization",
      "NVIDIA"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-08-28T00:02:46.000Z",
    "download_time": "2025-08-29T16:19:39.273503",
    "visual_resource": [
      "screenshot/wechat/wechat_image_k1rVhAoQxD5phvGq9YWmRA.png"
    ],
    "extra_info": null
  },
  {
    "id": "system_prompts_leaks",
    "source": "GitHub",
    "url": "https://github.com/asgeirtj/system_prompts_leaks",
    "title_en": "System Prompts Leaks",
    "summary_en": "The GitHub repository \"System Prompts Leaks\" is a significant open-source project dedicated to the systematic collection and public dissemination of system message instructions employed by various publicly deployed AI chatbots. This initiative provides an invaluable and unique resource for researchers, AI ethicists, and developers who are keen to delve into the intricate internal workings, operational logic, and behavioral patterns of large language models and advanced conversational AI systems. By meticulously compiling these often-undisclosed system prompts, the project plays a pivotal role in advancing the understanding of AI transparency, facilitating the identification of inherent biases, and enabling a deeper analysis of the complex processes that govern AI response generation. Ultimately, this repository aims to significantly enhance the explainability, robustness, and safety of contemporary AI systems, thereby fostering a more transparent, responsible, and informed development ecosystem for artificial intelligence technologies across various domains.",
    "keywords_en": [
      "System Prompts",
      "Chatbots",
      "Large Language Models",
      "AI Ethics",
      "Prompt Engineering",
      "Data Collection"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-27T17:43:45Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://api.star-history.com/svg?repos=asgeirtj/system_prompts_leaks&type=Date"
    ],
    "extra_info": null
  },
  {
    "id": "WrenAI",
    "source": "GitHub",
    "url": "https://github.com/Canner/WrenAI",
    "title_en": "Wren AI - Open-Source GenBI Agent",
    "summary_en": "Wren AI is an open-source GenBI agent that revolutionizes business intelligence by enabling users to query any database with natural language, instantly generating accurate SQL, insightful charts, and AI-driven summaries. It features robust Text-to-SQL and Text-to-Charts capabilities, underpinned by a sophisticated semantic layer that encodes schema, metrics, and joins to ensure precise and governed LLM outputs. The platform boasts broad compatibility with numerous databases and integrates seamlessly with a wide array of large language models. Furthermore, Wren AI offers API embedding, allowing developers to build custom agents, SaaS features, and chatbots. This powerful tool dramatically simplifies data interaction, eliminates the need for extensive SQL knowledge, and delivers decision-ready context, making advanced business intelligence accessible and efficient for enterprises.",
    "keywords_en": [
      "GenBI",
      "AI Agent",
      "Natural Language Query",
      "Text-to-SQL",
      "Semantic Layer",
      "Business Intelligence",
      "Large Language Model"
    ],
    "area_en": [
      "Generative AI",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-28T11:57:35Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/Canner/WrenAI/main/misc/workflow.png",
      "https://raw.githubusercontent.com/Canner/WrenAI/main/misc/how_wrenai_works.png"
    ],
    "extra_info": null
  },
  {
    "id": "chroma",
    "source": "GitHub",
    "url": "https://github.com/chroma-core/chroma",
    "title_en": "Chroma - the open-source embedding database",
    "summary_en": "Chroma is an open-source embedding database designed to provide fast memory management capabilities for Python and JavaScript Large Language Model (LLM) applications. It offers a simple API, supports integration with popular frameworks like LangChain and LlamaIndex, and boasts rich features such as querying and filtering. Chroma supports various embedding models and can be used for scenarios like \"chatting with your data,\" enabling natural language queries of documents and subsequent analysis with LLMs, making it an efficient solution for developing intelligent applications.",
    "keywords_en": [
      "Embedding Database",
      "Vector Database",
      "Large Language Model",
      "Memory Management",
      "Open Source",
      "Natural Language Processing",
      "AI Applications"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-08-28T21:52:15Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png"
    ],
    "extra_info": null
  },
  {
    "id": "audiblez",
    "source": "GitHub",
    "url": "https://github.com/santinic/audiblez",
    "title_en": "Audiblez: Generate audiobooks from e-books",
    "summary_en": "Audiblez is an open-source tool designed to efficiently convert e-books, specifically in EPUB format, into high-quality M4B audiobooks. It utilizes the Kokoro-82M text-to-speech model, known for its natural-sounding output and compact size, supporting a wide array of languages including English, Spanish, French, and Chinese. The tool offers both a command-line interface and a user-friendly graphical interface, catering to different user preferences. A key feature is its support for CUDA acceleration, which dramatically speeds up the conversion process; for example, a 160,000-character book can be converted in about 5 minutes on a GPU. Users can also fine-tune the audiobook experience by adjusting playback speed and selecting from a diverse range of voices. This makes Audiblez a robust and versatile solution for anyone looking to create personalized audiobooks from their digital library.",
    "keywords_en": [
      "Audiobook Generation",
      "E-book Conversion",
      "Text-to-Speech",
      "Speech Synthesis",
      "Multilingual Support",
      "GPU Acceleration",
      "Open Source Tool",
      "EPUB"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-03-02T18:28:03Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://github.com/santinic/audiblez/raw/main/imgs/mac.png"
    ],
    "extra_info": null
  },
  {
    "id": "nn-zero-to-hero",
    "source": "GitHub",
    "url": "https://github.com/karpathy/nn-zero-to-hero",
    "title_en": "Neural Networks: Zero to Hero",
    "summary_en": "This GitHub repository serves as supplementary material for the \"Neural Networks: Zero to Hero\" course. Through a series of YouTube videos and accompanying Jupyter Notebooks, the course progresses from fundamental concepts like backpropagation and multilayer perceptrons to advanced topics such as language models, batch normalization, convolutional neural networks, and culminates in the implementation of GPT and its tokenizer. The curriculum emphasizes hands-on coding to build and train neural networks, aiming to equip learners with a systematic understanding of core deep learning technologies and enhance practical skills. It is particularly beneficial for developers and researchers seeking to deeply comprehend the operational principles of modern large language models.",
    "keywords_en": [
      "Neural Networks",
      "Deep Learning",
      "Backpropagation",
      "Language Model",
      "GPT",
      "Transformer",
      "Tokenizer",
      "Jupyter Notebook"
    ],
    "area_en": [
      "Deep Learning",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2024-02-20T17:19:51Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "screenshot/github/nn-zero-to-hero.png"
    ],
    "extra_info": null
  },
  {
    "id": "SurfSense",
    "source": "GitHub",
    "url": "https://github.com/MODSetter/SurfSense",
    "title_en": "SurfSense",
    "summary_en": "SurfSense is a highly customizable AI research agent designed to integrate personal knowledge bases with external information sources. It supports uploading various file formats, including documents, images, and audio/video, with over 50 extensions. It offers powerful search and content interaction capabilities, and can rapidly generate podcasts. The project utilizes advanced RAG techniques, supporting numerous LLMs and embedding models, and is self-hostable. Its core value lies in seamlessly connecting personal data with external sources like search engines, Slack, and GitHub, providing a private and efficient AI research experience, with support for local LLM deployment ensuring privacy.",
    "keywords_en": [
      "AI Research Agent",
      "Personal Knowledge Base",
      "Retrieval Augmented Generation",
      "Podcast Generation",
      "Self-Hostable",
      "Multimodal",
      "Large Language Model",
      "File Processing"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-28T02:20:00Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/SurfSense.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.19652",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.19652",
    "title_en": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
    "summary_en": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying\nthings that are not actually in the image, and language shortcuts, where they\nskip the visual part and just rely on text priors. These issues arise because\nmost post-training methods for VLMs rely on simple verifiable answer matching\nand supervise only final outputs, leaving intermediate visual reasoning without\nexplicit guidance. As a result, VLMs receive sparse visual signals and often\nlearn to prioritize language-based reasoning over visual perception. To\nmitigate this, some existing methods add visual supervision using human\nannotations or distilled labels from external large models. However, human\nannotations are labor-intensive and costly, and because external signals cannot\nadapt to the evolving policy, they cause distributional shifts that can lead to\nreward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method\nthat improves visual reasoning without relying on external visual supervisions\nvia reinforcement learning. Vision-SR1 decomposes VLM reasoning into two\nstages: visual perception and language reasoning. The model is first prompted\nto produce self-contained visual perceptions that are sufficient to answer the\nquestion without referring back the input image. To validate this\nself-containment, the same VLM model is then re-prompted to perform language\nreasoning using only the generated perception as input to compute reward. This\nself-reward is combined with supervision on final outputs, providing a balanced\ntraining signal that strengthens both visual perception and language reasoning.\nOur experiments demonstrate that Vision-SR1 improves visual reasoning,\nmitigates visual hallucinations, and reduces reliance on language shortcuts\nacross diverse vision-language tasks.",
    "keywords_en": [
      "Vision-Language Models",
      "Self-Rewarding",
      "Reasoning Decomposition",
      "Visual Hallucinations",
      "Reinforcement Learning"
    ],
    "area_en": [
      "Multimodal",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-08-27T08:01:03.000Z",
    "download_time": "2025-08-28 19:05:35",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19652.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.19652\", \"arxiv_url\": \"https://arxiv.org/abs/2508.19652\"}"
  },
  {
    "id": "2508.19320",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.19320",
    "title_en": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time\n  Autoregressive Video Generation",
    "summary_en": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with high latency, heavy\ncomputational cost, and limited controllability. In this work, we introduce an\nautoregressive video generation framework that enables interactive multimodal\ncontrol and low-latency extrapolation in a streaming manner. With minimal\nmodifications to a standard large language model (LLM), our framework accepts\nmultimodal condition encodings including audio, pose, and text, and outputs\nspatially and semantically coherent representations to guide the denoising\nprocess of a diffusion head. To support this, we construct a large-scale\ndialogue dataset of approximately 20,000 hours from multiple sources, providing\nrich conversational scenarios for training. We further introduce a deep\ncompression autoencoder with up to 64times reduction ratio, which\neffectively alleviates the long-horizon inference burden of the autoregressive\nmodel. Extensive experiments on duplex conversation, multilingual human\nsynthesis, and interactive world model highlight the advantages of our approach\nin low latency, high efficiency, and fine-grained multimodal controllability.",
    "keywords_en": [
      "digital human synthesis",
      "multimodal interaction",
      "real-time video generation",
      "autoregressive model",
      "large language model"
    ],
    "area_en": [
      "Multimodal",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-08-26T14:00:16.000Z",
    "download_time": "2025-08-28 19:05:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19320.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.19320\", \"arxiv_url\": \"https://arxiv.org/abs/2508.19320\"}"
  },
  {
    "id": "2508.20096",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.20096",
    "title_en": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer\n  Use Agent with Decoupled Reinforcement Learning",
    "summary_en": "Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.",
    "keywords_en": [
      "Agent",
      "Decoupled Reinforcement Learning",
      "Compositional Framework",
      "Scientific Computing",
      "Planning and Execution"
    ],
    "area_en": [
      "AI Agent",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-08-27T17:59:50.000Z",
    "download_time": "2025-08-28 19:05:36",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20096.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.20096\", \"arxiv_url\": \"https://arxiv.org/abs/2508.20096\"}"
  },
  {
    "id": "2508.19982",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.19982",
    "title_en": "Diffusion Language Models Know the Answer Before Decoding",
    "summary_en": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.",
    "keywords_en": [
      "Diffusion Language Models",
      "Fast Decoding",
      "Early Convergence",
      "Inference Acceleration",
      "Prophet"
    ],
    "area_en": [
      "Deep Learning",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-08-27T15:40:25.000Z",
    "download_time": "2025-08-28 19:05:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19982.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.19982\", \"arxiv_url\": \"https://arxiv.org/abs/2508.19982\"}"
  },
  {
    "id": "2508.20088",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.20088",
    "title_en": "AudioStory: Generating Long-Form Narrative Audio with Large Language\n  Models",
    "summary_en": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short\naudio clips but struggle with long-form narrative audio, which requires\ntemporal coherence and compositional reasoning. To address this gap, we propose\nAudioStory, a unified framework that integrates large language models (LLMs)\nwith TTA systems to generate structured, long-form audio narratives. AudioStory\npossesses strong instruction-following reasoning generation capabilities. It\nemploys LLMs to decompose complex narrative queries into temporally ordered\nsub-tasks with contextual cues, enabling coherent scene transitions and\nemotional tone consistency. AudioStory has two appealing features: (1)\nDecoupled bridging mechanism: AudioStory disentangles LLM-diffuser\ncollaboration into two specialized components, i.e., a bridging query for\nintra-event semantic alignment and a residual query for cross-event coherence\npreservation. (2) End-to-end training: By unifying instruction comprehension\nand audio generation within a single end-to-end framework, AudioStory\neliminates the need for modular training pipelines while enhancing synergy\nbetween components. Furthermore, we establish a benchmark AudioStory-10K,\nencompassing diverse domains such as animated soundscapes and natural sound\nnarratives. Extensive experiments show the superiority of AudioStory on both\nsingle-audio generation and narrative audio generation, surpassing prior TTA\nbaselines in both instruction-following ability and audio fidelity. Our code is\navailable at https://github.com/TencentARC/AudioStory",
    "keywords_en": [
      "Long-form narrative audio",
      "Large Language Models",
      "Text-to-audio generation",
      "Instruction-following",
      "End-to-end training"
    ],
    "area_en": [
      "Large Language Model",
      "Generative AI",
      "Multimodal"
    ],
    "published_time": "2025-08-27T17:55:38.000Z",
    "download_time": "2025-08-28 19:05:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20088.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.20088\", \"arxiv_url\": \"https://arxiv.org/abs/2508.20088\"}"
  },
  {
    "id": "2508.19493",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.19493",
    "title_en": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered\n  Smartphone Agents",
    "summary_en": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
    "keywords_en": [
      "Smartphone Agents",
      "Privacy Awareness",
      "Multimodal Large Language Models",
      "Benchmarking",
      "Personal Information Protection"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "AI Agent"
    ],
    "published_time": "2025-08-27T00:41:28.000Z",
    "download_time": "2025-08-28 19:05:35",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19493.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.19493\", \"arxiv_url\": \"https://arxiv.org/abs/2508.19493\"}"
  }
]