<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-02</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2025-12-02</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Anthropic acquires Bun</h2>
                <span class="published-time">Published: 2025-12-02 18:05:44</span>
                
                <p class="summary">Anthropic, a prominent artificial intelligence research company recognized for developing the Claude series of large language models, has formally announced its acquisition of Bun, the high-performance all-in-one JavaScript runtime, bundler, transpiler, and package manager. This strategic integration is poised to significantly bolster Anthropic's internal developer tooling and infrastructure, enhancing the efficiency and speed at which its AI applications are developed, deployed, and managed. Bun's core strengths lie in its exceptional performance and streamlined developer experience within the JavaScript ecosystem, attributes that are increasingly crucial for building robust and scalable AI systems. The acquisition suggests Anthropic's commitment to leveraging cutting-edge web technologies to optimize its AI research and production workflows. By integrating Bun's capabilities, Anthropic aims to accelerate development cycles, improve resource utilization, and foster a more agile environment for its engineers, ultimately supporting the creation of more sophisticated and performant AI solutions and potentially impacting future developer-facing AI tools.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Anthropic</span><span>Bun</span><span>Acquisition</span><span>JavaScript Runtime</span><span>AI Infrastructure</span><span>Developer Tools</span><span>Large Language Models</span><span>Web Technologies</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://bun.com/blog/bun-joins-anthropic" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Claude 4.5 Opus' Soul Document</h2>
                <span class="published-time">Published: 2025-12-02 19:05:54</span>
                
                <p class="summary">The enigmatic title "Claude 4.5 Opus' Soul Document" suggests a conceptual or actual foundational text outlining the core principles, operational philosophy, or inherent 'identity' of the advanced large language model, Claude 4.5 Opus. While the specific content remains undefined, the notion of a "soul document" for an AI highlights a critical shift towards understanding and articulating the internal mechanisms and ethical frameworks governing highly capable AI systems. This could encompass details regarding its alignment objectives, safety protocols, interpretability features, and the underlying design philosophy that shapes its interactions. Such a document would be invaluable for fostering greater transparency, accountability, and trust among users and developers, providing insight into the AI's intrinsic values and operational guidelines. The emergence of such a concept underscores the increasing focus within AI research on responsible development, ethical considerations, and the long-term governance of advanced AI, potentially setting a precedent for how future intelligent systems communicate their core programming and intended societal roles.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Claude 4.5 Opus</span><span>Large Language Model</span><span>AI Ethics</span><span>AI Alignment</span><span>AI Governance</span><span>Transparency</span><span>Interpretability</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://simonwillison.net/2025/Dec/2/claude-soul-document/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Amazon launches Trainium3</h2>
                <span class="published-time">Published: 2025-12-02 19:04:31</span>
                
                <p class="summary">Amazon has officially announced the launch of Trainium3, its latest generation of custom-designed artificial intelligence chip, signaling a significant advancement in its proprietary hardware capabilities for demanding AI workloads. Trainium3 is engineered to provide high-performance and cost-effective solutions for training complex machine learning models, particularly deep learning applications, within Amazon Web Services (AWS) environments. This new chip aims to bolster AWS's position as a leading provider of cloud-based AI infrastructure, offering customers an alternative to general-purpose GPUs. Notably, the release is accompanied by discussions of an "Nvidia-friendly roadmap," suggesting potential interoperability or strategic alignment that could enhance the appeal of Amazon's AI accelerators in an ecosystem traditionally dominated by Nvidia. The introduction of Trainium3 underscores Amazon's continued investment in developing specialized silicon to optimize efficiency and performance for its growing AI service offerings, catering to the escalating demand for scalable AI training resources across various industries.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Amazon</span><span>Trainium3</span><span>AI Chip</span><span>Machine Learning</span><span>Deep Learning</span><span>AI Hardware</span><span>Cloud Computing</span><span>Nvidia</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAI declares 'code red' as Google catches up in AI race</h2>
                <span class="published-time">Published: 2025-12-02 15:00:16</span>
                
                <p class="summary">OpenAI has reportedly declared an internal "code red," signaling a significant state of alarm triggered by Google's accelerated progress in the competitive artificial intelligence landscape. This urgent declaration underscores the escalating rivalry between the two technology powerhouses, both intensely focused on pioneering advancements in large language models (LLMs) and broader generative AI capabilities. The "code red" status suggests that OpenAI is facing considerable pressure to rapidly re-evaluate and adapt its strategic direction, potentially accelerating its research and development efforts and re-prioritizing its product pipeline to maintain its perceived lead. This intense competition highlights the volatile and fast-moving nature of the AI sector, where technological breakthroughs and swift market deployment are crucial for securing a dominant position. The situation reflects a high-stakes technological race, with both companies investing heavily to define the future trajectory of artificial intelligence innovation and its commercial applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>OpenAI</span><span>Google AI</span><span>AI Race</span><span>Artificial Intelligence</span><span>AI Competition</span><span>Large Language Models</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.theverge.com/news/836212/openai-code-red-chatgpt" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Mistral 3 family of models released</h2>
                <span class="published-time">Published: 2025-12-02 15:01:53</span>
                
                <p class="summary">Mistral AI has officially announced the launch of its new Mistral 3 family of models, representing a notable leap forward in artificial intelligence research and development. This anticipated release is poised to introduce a suite of advanced models designed to offer significant improvements in various performance metrics, including enhanced reasoning, greater efficiency, and expanded capabilities in understanding and generating complex language. The Mistral 3 family aims to elevate the standard for large language models, likely featuring innovations in model architecture and training methodologies that contribute to more robust and versatile applications. The announcement from Mistral AI, a prominent player in the generative AI space, underscores its ongoing dedication to delivering cutting-edge solutions for both developers and enterprises. These new models are expected to empower a wide range of use cases, from sophisticated content creation and advanced code generation to intricate data analysis and intelligent conversational agents, thereby fostering new opportunities across various industries.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Mistral AI</span><span>Large Language Models</span><span>Generative AI</span><span>AI Models</span><span>Model Release</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>AI Research</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mistral.ai/news/mistral-3" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Apple Releases Open Weights Video Model</h2>
                <span class="published-time">Published: 2025-12-02 05:10:01</span>
                
                <p class="summary">Apple's recent announcement marks a significant development in the artificial intelligence domain with the release of an open-weights video model. This move by a major tech company like Apple, traditionally known for its closed ecosystem, signals a strategic shift towards greater transparency and collaboration within the AI research community. An open-weights model means that researchers and developers can access, modify, and build upon the foundational components of Apple's video processing technology, potentially accelerating innovation in areas such as video understanding, content generation, and machine learning applications. This initiative is expected to foster broader adoption and contribute to the advancement of next-generation AI capabilities, particularly in computer vision and multimodal AI frameworks. The availability of such a model could democratize access to advanced video AI tools, facilitating academic research and commercial development, and is anticipated to spark new research directions in scalable and efficient video analysis systems.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Apple AI</span><span>Open Weights</span><span>Video Understanding</span><span>Computer Vision</span><span>Machine Learning</span><span>AI Research</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://starflow-v.github.io" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>TrendRadar: 30-Second Hot News Assistant</h2>
                <span class="published-time">Published: 2025-11-29T11:59:33Z</span>
                
                <p class="summary">TrendRadar is a lightweight and easily deployable hot news assistant designed to combat information overload by delivering personalized news. It aggregates real-time hot topics from over 11 mainstream platforms including Zhihu, Douyin, Weibo, and financial news sources. The system offers three intelligent push strategies—daily summary, current ranking, and incremental monitoring—tailored for diverse user needs from corporate managers to investors, ensuring zero repetition for critical updates. Users can define precise content filters using keywords with advanced syntax, and the platform provides hot topic trend analysis, tracking evolution, heat changes, and cross-platform comparisons. A personalized algorithm sorts news by ranking, frequency, and overall hotness. Furthermore, TrendRadar integrates AI intelligent analysis via the Model Context Protocol (MCP), allowing natural language queries for deep data insights, trend tracking, and sentiment analysis. It supports multi-channel real-time notifications (WeChat, Feishu, Telegram, Email, etc.) and multi-terminal adaptation, including GitHub Pages for web reports and Docker for containerized deployment, making it accessible with zero technical threshold.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>News Aggregation</span><span>Real-time Notifications</span><span>AI Analysis</span><span>Content Filtering</span><span>Hot Topic Monitoring</span><span>Data Trend Analysis</span><span>GitHub Actions</span><span>Docker Deployment</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/sansan0/TrendRadar" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent Development Kit (ADK) for Go</h2>
                <span class="published-time">Published: 2025-12-02T16:28:10Z</span>
                
                <p class="summary">The Agent Development Kit (ADK) for Go is an open-source, code-first toolkit designed to streamline the building, evaluation, and deployment of sophisticated AI agents. Applying robust software development principles, ADK offers a flexible and modular framework for creating and orchestrating agent workflows, from simple tasks to complex multi-agent systems. While optimized for Google's Gemini, it maintains model and deployment agnosticism, ensuring compatibility across various AI models and cloud environments. This Go version specifically targets developers building cloud-native agent applications, leveraging Go's inherent strengths in concurrency and performance. Key features include idiomatic Go design, a rich ecosystem for integrating pre-built or custom tools, code-first development for ultimate flexibility and testability, and strong support for containerization and deployment in cloud-native platforms like Google Cloud Run. ADK empowers developers with fine-grained control over agent logic and orchestration, making it ideal for scalable and high-performance AI solutions.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Agent</span><span>Go Programming</span><span>Agent Development Kit</span><span>Cloud-Native</span><span>Modular AI</span><span>Software Development</span><span>Orchestration</span><span>Gemini</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/google/adk-go" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence</h2>
                <span class="published-time">Published: 2025-11-23T17:09:34.000Z</span>
                
                <p class="summary">Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Code Foundation Models</span><span>Large Language Models</span><span>Automated Software Development</span><span>AI Agents</span><span>Code Intelligence</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>AI Agent</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.18538" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>How Far Are We from Genuinely Useful Deep Research Agents?</h2>
                <span class="published-time">Published: 2025-12-01T17:58:59.000Z</span>
                
                <p class="summary">Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Deep Research Agents</span><span>Report Synthesis</span><span>Benchmarks</span><span>Failure Taxonomy</span><span>Information Retrieval</span></div>
                    <div class="area"><span class="label">Areas：</span><span>AI Agent</span><span>Natural Language Processing</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.01948" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Stabilizing Reinforcement Learning with LLMs: Formulation and Practices</h2>
                <span class="published-time">Published: 2025-12-01T07:45:39.000Z</span>
                
                <p class="summary">This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Reinforcement Learning</span><span>Large Language Models</span><span>Policy Gradient</span><span>Training Stability</span><span>Mixture-of-Experts</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.01374" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</h2>
                <span class="published-time">Published: 2025-11-25T18:59:46.000Z</span>
                
                <p class="summary">Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce infty-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish infty-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that infty-RoPE consistently surpasses previous autoregressive models in overall VBench scores.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Infinite Video Generation</span><span>Autoregressive Diffusion Models</span><span>Rotary Positional Embedding</span><span>Action Control</span><span>Cinematic Transitions</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.20649" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights</h2>
                <span class="published-time">Published: 2025-12-01T15:52:31.000Z</span>
                
                <p class="summary">Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Multimodal Models</span><span>Causal Event Progression</span><span>Text-to-Multi-Image Generation</span><span>World Knowledge</span><span>Spatiotemporal Consistency</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Multimodal</span><span>Generative AI</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.01816" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs</h2>
                <span class="published-time">Published: 2025-11-30T04:32:43.000Z</span>
                
                <p class="summary">In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Long-context Reasoning</span><span>Large Language Models</span><span>Speculative Context Sparsity</span><span>Retrieval Algorithms</span><span>System Co-design</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.00722" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>