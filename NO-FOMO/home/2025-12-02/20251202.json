[
  {
    "id": "hackernews_46124267",
    "source": "Hacker News",
    "url": "https://bun.com/blog/bun-joins-anthropic",
    "title": "Anthropic acquires Bun",
    "summary": "Anthropic, a prominent artificial intelligence research company recognized for developing the Claude series of large language models, has formally announced its acquisition of Bun, the high-performance all-in-one JavaScript runtime, bundler, transpiler, and package manager. This strategic integration is poised to significantly bolster Anthropic's internal developer tooling and infrastructure, enhancing the efficiency and speed at which its AI applications are developed, deployed, and managed. Bun's core strengths lie in its exceptional performance and streamlined developer experience within the JavaScript ecosystem, attributes that are increasingly crucial for building robust and scalable AI systems. The acquisition suggests Anthropic's commitment to leveraging cutting-edge web technologies to optimize its AI research and production workflows. By integrating Bun's capabilities, Anthropic aims to accelerate development cycles, improve resource utilization, and foster a more agile environment for its engineers, ultimately supporting the creation of more sophisticated and performant AI solutions and potentially impacting future developer-facing AI tools.",
    "keywords": [
      "Anthropic",
      "Bun",
      "Acquisition",
      "JavaScript Runtime",
      "AI Infrastructure",
      "Developer Tools",
      "Large Language Models",
      "Web Technologies"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Machine Learning"
    ],
    "published_time": "2025-12-02 18:05:44",
    "download_time": "2025-12-02 20:02:24",
    "extra_info": "{\"score\": 683, \"by\": \"ryanvogel\", \"descendants\": 314, \"story_id\": 46124267}"
  },
  {
    "id": "hackernews_46125184",
    "source": "Hacker News",
    "url": "https://simonwillison.net/2025/Dec/2/claude-soul-document/",
    "title": "Claude 4.5 Opus' Soul Document",
    "summary": "The enigmatic title \"Claude 4.5 Opus' Soul Document\" suggests a conceptual or actual foundational text outlining the core principles, operational philosophy, or inherent 'identity' of the advanced large language model, Claude 4.5 Opus. While the specific content remains undefined, the notion of a \"soul document\" for an AI highlights a critical shift towards understanding and articulating the internal mechanisms and ethical frameworks governing highly capable AI systems. This could encompass details regarding its alignment objectives, safety protocols, interpretability features, and the underlying design philosophy that shapes its interactions. Such a document would be invaluable for fostering greater transparency, accountability, and trust among users and developers, providing insight into the AI's intrinsic values and operational guidelines. The emergence of such a concept underscores the increasing focus within AI research on responsible development, ethical considerations, and the long-term governance of advanced AI, potentially setting a precedent for how future intelligent systems communicate their core programming and intended societal roles.",
    "keywords": [
      "Claude 4.5 Opus",
      "Large Language Model",
      "AI Ethics",
      "AI Alignment",
      "AI Governance",
      "Transparency",
      "Interpretability"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-12-02 19:05:54",
    "download_time": "2025-12-02 20:02:17",
    "extra_info": "{\"score\": 77, \"by\": \"the-needful\", \"descendants\": 33, \"story_id\": 46125184}"
  },
  {
    "id": "hackernews_46125155",
    "source": "Hacker News",
    "url": "https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/",
    "title": "Amazon launches Trainium3",
    "summary": "Amazon has officially announced the launch of Trainium3, its latest generation of custom-designed artificial intelligence chip, signaling a significant advancement in its proprietary hardware capabilities for demanding AI workloads. Trainium3 is engineered to provide high-performance and cost-effective solutions for training complex machine learning models, particularly deep learning applications, within Amazon Web Services (AWS) environments. This new chip aims to bolster AWS's position as a leading provider of cloud-based AI infrastructure, offering customers an alternative to general-purpose GPUs. Notably, the release is accompanied by discussions of an \"Nvidia-friendly roadmap,\" suggesting potential interoperability or strategic alignment that could enhance the appeal of Amazon's AI accelerators in an ecosystem traditionally dominated by Nvidia. The introduction of Trainium3 underscores Amazon's continued investment in developing specialized silicon to optimize efficiency and performance for its growing AI service offerings, catering to the escalating demand for scalable AI training resources across various industries.",
    "keywords": [
      "Amazon",
      "Trainium3",
      "AI Chip",
      "Machine Learning",
      "Deep Learning",
      "AI Hardware",
      "Cloud Computing",
      "Nvidia"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-12-02 19:04:31",
    "download_time": "2025-12-02 20:02:15",
    "extra_info": "{\"score\": 39, \"by\": \"thnaks\", \"descendants\": 13, \"story_id\": 46125155}"
  },
  {
    "id": "hackernews_46121870",
    "source": "Hacker News",
    "url": "https://www.theverge.com/news/836212/openai-code-red-chatgpt",
    "title": "OpenAI declares 'code red' as Google catches up in AI race",
    "summary": "OpenAI has reportedly declared an internal \"code red,\" signaling a significant state of alarm triggered by Google's accelerated progress in the competitive artificial intelligence landscape. This urgent declaration underscores the escalating rivalry between the two technology powerhouses, both intensely focused on pioneering advancements in large language models (LLMs) and broader generative AI capabilities. The \"code red\" status suggests that OpenAI is facing considerable pressure to rapidly re-evaluate and adapt its strategic direction, potentially accelerating its research and development efforts and re-prioritizing its product pipeline to maintain its perceived lead. This intense competition highlights the volatile and fast-moving nature of the AI sector, where technological breakthroughs and swift market deployment are crucial for securing a dominant position. The situation reflects a high-stakes technological race, with both companies investing heavily to define the future trajectory of artificial intelligence innovation and its commercial applications.",
    "keywords": [
      "OpenAI",
      "Google AI",
      "AI Race",
      "Artificial Intelligence",
      "AI Competition",
      "Large Language Models",
      "Generative AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-12-02 15:00:16",
    "download_time": "2025-12-02 20:02:20",
    "extra_info": "{\"score\": 151, \"by\": \"goplayoutside\", \"descendants\": 156, \"story_id\": 46121870}"
  },
  {
    "id": "hackernews_46121889",
    "source": "Hacker News",
    "url": "https://mistral.ai/news/mistral-3",
    "title": "Mistral 3 family of models released",
    "summary": "Mistral AI has officially announced the launch of its new Mistral 3 family of models, representing a notable leap forward in artificial intelligence research and development. This anticipated release is poised to introduce a suite of advanced models designed to offer significant improvements in various performance metrics, including enhanced reasoning, greater efficiency, and expanded capabilities in understanding and generating complex language. The Mistral 3 family aims to elevate the standard for large language models, likely featuring innovations in model architecture and training methodologies that contribute to more robust and versatile applications. The announcement from Mistral AI, a prominent player in the generative AI space, underscores its ongoing dedication to delivering cutting-edge solutions for both developers and enterprises. These new models are expected to empower a wide range of use cases, from sophisticated content creation and advanced code generation to intricate data analysis and intelligent conversational agents, thereby fostering new opportunities across various industries.",
    "keywords": [
      "Mistral AI",
      "Large Language Models",
      "Generative AI",
      "AI Models",
      "Model Release",
      "Artificial Intelligence",
      "Natural Language Processing",
      "AI Research"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-12-02 15:01:53",
    "download_time": "2025-12-02 20:02:18",
    "extra_info": "{\"score\": 514, \"by\": \"pember\", \"descendants\": 164, \"story_id\": 46121889}"
  },
  {
    "id": "hackernews_46117802",
    "source": "Hacker News",
    "url": "https://starflow-v.github.io",
    "title": "Apple Releases Open Weights Video Model",
    "summary": "Apple's recent announcement marks a significant development in the artificial intelligence domain with the release of an open-weights video model. This move by a major tech company like Apple, traditionally known for its closed ecosystem, signals a strategic shift towards greater transparency and collaboration within the AI research community. An open-weights model means that researchers and developers can access, modify, and build upon the foundational components of Apple's video processing technology, potentially accelerating innovation in areas such as video understanding, content generation, and machine learning applications. This initiative is expected to foster broader adoption and contribute to the advancement of next-generation AI capabilities, particularly in computer vision and multimodal AI frameworks. The availability of such a model could democratize access to advanced video AI tools, facilitating academic research and commercial development, and is anticipated to spark new research directions in scalable and efficient video analysis systems.",
    "keywords": [
      "Apple AI",
      "Open Weights",
      "Video Understanding",
      "Computer Vision",
      "Machine Learning",
      "AI Research",
      "Generative AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Video Understanding"
    ],
    "published_time": "2025-12-02 05:10:01",
    "download_time": "2025-12-02 20:02:31",
    "extra_info": "{\"score\": 401, \"by\": \"vessenes\", \"descendants\": 137, \"story_id\": 46117802}"
  },
  {
    "id": "TrendRadar",
    "source": "GitHub",
    "url": "https://github.com/sansan0/TrendRadar",
    "title": "TrendRadar: 30-Second Hot News Assistant",
    "summary": "TrendRadar is a lightweight and easily deployable hot news assistant designed to combat information overload by delivering personalized news. It aggregates real-time hot topics from over 11 mainstream platforms including Zhihu, Douyin, Weibo, and financial news sources. The system offers three intelligent push strategies—daily summary, current ranking, and incremental monitoring—tailored for diverse user needs from corporate managers to investors, ensuring zero repetition for critical updates. Users can define precise content filters using keywords with advanced syntax, and the platform provides hot topic trend analysis, tracking evolution, heat changes, and cross-platform comparisons. A personalized algorithm sorts news by ranking, frequency, and overall hotness. Furthermore, TrendRadar integrates AI intelligent analysis via the Model Context Protocol (MCP), allowing natural language queries for deep data insights, trend tracking, and sentiment analysis. It supports multi-channel real-time notifications (WeChat, Feishu, Telegram, Email, etc.) and multi-terminal adaptation, including GitHub Pages for web reports and Docker for containerized deployment, making it accessible with zero technical threshold.",
    "keywords": [
      "News Aggregation",
      "Real-time Notifications",
      "AI Analysis",
      "Content Filtering",
      "Hot Topic Monitoring",
      "Data Trend Analysis",
      "GitHub Actions",
      "Docker Deployment"
    ],
    "area": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "AI Agent"
    ],
    "published_time": "2025-11-29T11:59:33Z",
    "download_time": "2024-02-20 12:44:48",
    "extra_info": null
  },
  {
    "id": "adk-go",
    "source": "GitHub",
    "url": "https://github.com/google/adk-go",
    "title": "Agent Development Kit (ADK) for Go",
    "summary": "The Agent Development Kit (ADK) for Go is an open-source, code-first toolkit designed to streamline the building, evaluation, and deployment of sophisticated AI agents. Applying robust software development principles, ADK offers a flexible and modular framework for creating and orchestrating agent workflows, from simple tasks to complex multi-agent systems. While optimized for Google's Gemini, it maintains model and deployment agnosticism, ensuring compatibility across various AI models and cloud environments. This Go version specifically targets developers building cloud-native agent applications, leveraging Go's inherent strengths in concurrency and performance. Key features include idiomatic Go design, a rich ecosystem for integrating pre-built or custom tools, code-first development for ultimate flexibility and testability, and strong support for containerization and deployment in cloud-native platforms like Google Cloud Run. ADK empowers developers with fine-grained control over agent logic and orchestration, making it ideal for scalable and high-performance AI solutions.",
    "keywords": [
      "AI Agent",
      "Go Programming",
      "Agent Development Kit",
      "Cloud-Native",
      "Modular AI",
      "Software Development",
      "Orchestration",
      "Gemini"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-12-02T16:28:10Z",
    "download_time": "2024-05-15 10:49:00",
    "extra_info": null
  },
  {
    "id": "2511.18538",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2511.18538",
    "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
    "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.",
    "keywords": [
      "Code Foundation Models",
      "Large Language Models",
      "Automated Software Development",
      "AI Agents",
      "Code Intelligence"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-11-23T17:09:34.000Z",
    "download_time": "2025-12-02 12:03:17",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2511.18538\", \"arxiv_url\": \"https://arxiv.org/abs/2511.18538\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png\", \"original_title\": \"From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence\"}"
  },
  {
    "id": "2512.01948",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.01948",
    "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
    "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
    "keywords": [
      "Deep Research Agents",
      "Report Synthesis",
      "Benchmarks",
      "Failure Taxonomy",
      "Information Retrieval"
    ],
    "area": [
      "AI Agent",
      "Natural Language Processing",
      "Large Language Model"
    ],
    "published_time": "2025-12-01T17:58:59.000Z",
    "download_time": "2025-12-02 12:03:16",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.01948\", \"arxiv_url\": \"https://arxiv.org/abs/2512.01948\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01948.png\", \"original_title\": \"How Far Are We from Genuinely Useful Deep Research Agents?\"}"
  },
  {
    "id": "2512.01374",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.01374",
    "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
    "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.",
    "keywords": [
      "Reinforcement Learning",
      "Large Language Models",
      "Policy Gradient",
      "Training Stability",
      "Mixture-of-Experts"
    ],
    "area": [
      "Large Language Model",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-12-01T07:45:39.000Z",
    "download_time": "2025-12-02 12:03:15",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.01374\", \"arxiv_url\": \"https://arxiv.org/abs/2512.01374\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01374.png\", \"original_title\": \"Stabilizing Reinforcement Learning with LLMs: Formulation and Practices\"}"
  },
  {
    "id": "2511.20649",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2511.20649",
    "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
    "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce infty-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish infty-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that infty-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
    "keywords": [
      "Infinite Video Generation",
      "Autoregressive Diffusion Models",
      "Rotary Positional Embedding",
      "Action Control",
      "Cinematic Transitions"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-11-25T18:59:46.000Z",
    "download_time": "2025-12-02 12:03:16",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2511.20649\", \"arxiv_url\": \"https://arxiv.org/abs/2511.20649\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20649.png\", \"original_title\": \"Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout\"}"
  },
  {
    "id": "2512.01816",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.01816",
    "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights",
    "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.",
    "keywords": [
      "Multimodal Models",
      "Causal Event Progression",
      "Text-to-Multi-Image Generation",
      "World Knowledge",
      "Spatiotemporal Consistency"
    ],
    "area": [
      "Multimodal",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-12-01T15:52:31.000Z",
    "download_time": "2025-12-02 12:03:17",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.01816\", \"arxiv_url\": \"https://arxiv.org/abs/2512.01816\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01816.png\", \"original_title\": \"Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights\"}"
  },
  {
    "id": "2512.00722",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.00722",
    "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
    "summary": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
    "keywords": [
      "Long-context Reasoning",
      "Large Language Models",
      "Speculative Context Sparsity",
      "Retrieval Algorithms",
      "System Co-design"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-11-30T04:32:43.000Z",
    "download_time": "2025-12-02 12:03:17",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.00722\", \"arxiv_url\": \"https://arxiv.org/abs/2512.00722\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00722.png\", \"original_title\": \"SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs\"}"
  }
]