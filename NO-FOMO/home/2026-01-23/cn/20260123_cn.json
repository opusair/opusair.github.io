[
  {
    "id": "hackernews_46733301",
    "source": "Hacker News",
    "url": "https://huggingface.co/blog/waypoint-1",
    "title": "Waypoint-1: Real-Time Interactive Video Diffusion from Overworld",
    "summary": "Waypoint-1 heralds a breakthrough in generative AI with its real-time interactive video diffusion capabilities, directly generating dynamic video content from an \"Overworld\" environment. This innovative system allows for immediate user interaction, enabling creators and users to influence video generation in real-time and observe high-fidelity visual outputs instantaneously. The core technology leverages advanced video diffusion models to synthesize continuous and visually consistent video sequences, pushing the boundaries of what is possible in interactive content creation. Potential applications span diverse fields, including immersive virtual reality experiences, dynamic game environments, and next-generation interactive media platforms. By significantly minimizing the computational latency often associated with complex video generation, Waypoint-1 promises to unlock new avenues for spontaneous digital storytelling, rapid prototyping in virtual worlds, and more engaging user interactions, marking a pivotal advancement in the intersection of AI and digital media.",
    "keywords": [
      "Video Diffusion",
      "Real-Time Video Generation",
      "Interactive AI",
      "Generative AI",
      "Deep Learning",
      "Overworld",
      "Content Creation",
      "Artificial Intelligence"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2026-01-23 15:03:01",
    "download_time": "2026-01-23 20:00:46",
    "extra_info": "{\"score\": 16, \"by\": \"avaer\", \"descendants\": 3, \"story_id\": 46733301}"
  },
  {
    "id": "hackernews_46736091",
    "source": "Hacker News",
    "url": "https://github.com/anthropics/claude-code/issues/18866",
    "title": "Claude.ai silently failing since Jan 14, no official acknowledgment",
    "summary": "Reports indicate that Claude.ai, a prominent large language model service, has been experiencing widespread \"silent failures\" since January 14, causing significant disruption for its users. The issue, detailed in a GitHub issue (#18866) on Anthropics' repository, describes instances where the AI fails to generate responses without providing explicit error messages, leading to user confusion and productivity loss. Despite the persistent nature of these technical problems and user complaints across various platforms, Anthropics, the developer behind Claude.ai, has not yet issued any official statement or acknowledgment regarding the service outage or degradation. This lack of communication has sparked concern within the AI community regarding service reliability, user support, and transparency from major AI providers. The ongoing silent failures underscore the critical need for robust error handling and clear communication channels in the operation of advanced AI systems to maintain user trust and ensure consistent performance.",
    "keywords": [
      "Claude.ai",
      "Large Language Model",
      "AI service",
      "service failure",
      "technical issue",
      "Anthropics",
      "AI reliability"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2026-01-23 18:42:38",
    "download_time": "2026-01-23 20:00:30",
    "extra_info": "{\"score\": 94, \"by\": \"nurimamedov\", \"descendants\": 54, \"story_id\": 46736091}"
  },
  {
    "id": "hackernews_46733905",
    "source": "Hacker News",
    "url": "https://teemux.com/",
    "title": "Show HN: Teemux – Zero-config log multiplexer with built-in MCP server",
    "summary": "Teemux is presented as a novel zero-configuration command-line interface (CLI) program engineered to streamline log management for software developers, especially those integrating AI coding agents into their workflows. The project directly tackles the prevalent issue of difficult and inefficient sharing of development environment logs with AI agents. Functioning as a robust log multiplexer, Teemux aggregates diverse log streams, presenting them to the developer through an intuitive user interface while simultaneously exposing them to AI coding agents via a built-in MCP (Message Communication Protocol) server. A standout technical innovation lies in its zero-config operational model and integrated leader nomination system. This design allows multiple Teemux instances to seamlessly coalesce; one instance automatically assumes the role of running the web and MCP servers, with subsequent instances joining to merge logs. Should the primary instance fail, a new leader is autonomously nominated, ensuring high availability and operational continuity. This distributed architecture facilitates the effortless scaling of log-sharing nodes without requiring a centralized aggregator, significantly enhancing flexibility and efficiency for modern, AI-augmented development environments.",
    "keywords": [
      "Teemux",
      "log multiplexer",
      "MCP server",
      "AI agents",
      "CLI tool",
      "zero-config",
      "leader election",
      "development environment"
    ],
    "area": [
      "AI Agent",
      "Artificial Intelligence",
      "Others"
    ],
    "published_time": "2026-01-23 15:49:35",
    "download_time": "2026-01-23 20:00:58",
    "extra_info": "{\"score\": 7, \"by\": \"gajus\", \"descendants\": 5, \"story_id\": 46733905}"
  },
  {
    "id": "hackernews_46731612",
    "source": "Hacker News",
    "url": "https://rubinmuseum.org/presence-in-death/",
    "title": "Presence in Death",
    "summary": "The concept of \"Presence in Death\" delves into the profound intersection of artificial intelligence and the enduring human desire for legacy and continued connection. This topic explores the potential for advanced AI technologies to facilitate digital forms of presence following an individual's physical demise. It examines how AI models, including sophisticated natural language processing and generative AI, could be employed to create and maintain interactive virtual representations, preserving aspects of a person's personality, memories, and communication patterns. Furthermore, the discussion extends to the philosophical questions surrounding the authenticity of such digital entities and their role in grief processing. Discussions often revolve around the technical challenges of data collection and synthesis, ensuring the ethical deployment of these technologies, and mitigating algorithmic bias in recreating human characteristics. The narrative highlights the opportunities for innovative applications in digital archiving, personalized legacy creation, and even therapeutic support, while also scrutinizing the societal implications of blurring the lines between life and post-life existence through artificial intelligence.",
    "keywords": [
      "Digital Legacy",
      "AI Ethics",
      "Simulated Consciousness",
      "Post-Mortem AI",
      "Generative AI",
      "Human-AI Interaction",
      "Virtual Presence",
      "Data Preservation"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Generative AI"
    ],
    "published_time": "2026-01-23 12:16:54",
    "download_time": "2026-01-23 20:01:12",
    "extra_info": "{\"score\": 60, \"by\": \"tock\", \"descendants\": 23, \"story_id\": 46731612}"
  },
  {
    "id": "hackernews_46730504",
    "source": "Hacker News",
    "url": "https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md",
    "title": "AI Usage Policy",
    "summary": "This AI Usage Policy outlines comprehensive guidelines and principles for the responsible, ethical, and secure integration of Artificial Intelligence tools within an organizational environment. It is designed to ensure that the adoption and application of AI technologies by employees align seamlessly with established company values, pertinent legal requirements, and industry best practices concerning data privacy, security, and intellectual property. The policy typically addresses critical areas such as the appropriate use of AI tools, strict measures for safeguarding confidential and proprietary information from inadvertent exposure to third-party AI systems, and clear directives regarding the ownership, attribution, and commercial use of AI-generated content. Furthermore, it emphasizes the paramount importance of actively identifying and mitigating potential biases in AI outputs, promoting transparency in all AI-assisted processes, and upholding individual accountability for decisions and actions informed by AI. Strict adherence to these outlined guidelines is considered essential for fostering innovation and efficiency while effectively managing the inherent risks associated with rapidly evolving AI technologies, thereby ensuring fair, secure, and legally compliant usage across all operational facets of the organization.",
    "keywords": [
      "AI Governance",
      "Data Privacy",
      "Ethical AI",
      "Intellectual Property Rights",
      "Organizational Policy",
      "AI Risk Management",
      "Data Security"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Others"
    ],
    "published_time": "2026-01-23 09:50:26",
    "download_time": "2026-01-23 20:01:03",
    "extra_info": "{\"score\": 447, \"by\": \"mefengl\", \"descendants\": 234, \"story_id\": 46730504}"
  },
  {
    "id": "hackernews_46729368",
    "source": "Hacker News",
    "url": "https://dbushell.com/2026/01/22/proton-spam/",
    "title": "Proton Spam and the AI Consent Problem",
    "summary": "This article critically examines the ethical implications arising from the deployment of Artificial Intelligence in services like email spam detection, with a specific focus on platforms such as Proton Mail. It highlights the \"AI Consent Problem,\" which refers to the complex challenges of obtaining and managing user consent when AI systems are utilized to process and filter personal communications. The discussion likely delves into scenarios where AI-driven spam filters, while beneficial for security, might inadvertently infringe upon user privacy if explicit and informed consent protocols are not meticulously implemented. The piece underscores the growing imperative for greater transparency from technology providers regarding AI's role in data handling and decision-making. It suggests that without robust frameworks for user consent and data governance, the integration of AI into essential communication services could lead to significant trust issues and potential ethical breaches, advocating for a balance between security benefits and individual privacy rights.",
    "keywords": [
      "AI Ethics",
      "Data Privacy",
      "Spam Detection",
      "User Consent",
      "Artificial Intelligence",
      "Email Security",
      "Privacy-preserving AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2026-01-23 07:01:29",
    "download_time": "2026-01-23 20:00:46",
    "extra_info": "{\"score\": 420, \"by\": \"dbushell\", \"descendants\": 263, \"story_id\": 46729368}"
  },
  {
    "id": "2601.15876",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.15876",
    "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
    "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.",
    "keywords": [
      "Computer Use Agents",
      "Multimodal AI",
      "Agentic Model",
      "Evolutionary Learning",
      "Synthetic Experience"
    ],
    "area": [
      "AI Agent",
      "Multimodal",
      "Artificial Intelligence"
    ],
    "published_time": "2026-01-22T11:36:43.000Z",
    "download_time": "2026-01-23 12:01:34",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.15876\", \"arxiv_url\": \"https://arxiv.org/abs/2601.15876\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15876.png\", \"original_title\": \"EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience\"}"
  },
  {
    "id": "2601.15165",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.15165",
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
    "keywords": [
      "Diffusion Language Models",
      "Arbitrary Order Generation",
      "Reasoning Potential",
      "Reinforcement Learning",
      "Policy Optimization"
    ],
    "area": [
      "Large Language Model",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2026-01-21T16:41:58.000Z",
    "download_time": "2026-01-23 12:01:34",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.15165\", \"arxiv_url\": \"https://arxiv.org/abs/2601.15165\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15165.png\", \"original_title\": \"The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models\"}"
  },
  {
    "id": "2601.16206",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.16206",
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
    "keywords": [
      "LLM-in-Sandbox",
      "Large Language Models",
      "Code Sandbox",
      "Agentic AI",
      "General Intelligence"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2026-01-22T18:57:09.000Z",
    "download_time": "2026-01-23 12:01:34",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.16206\", \"arxiv_url\": \"https://arxiv.org/abs/2601.16206\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16206.png\", \"original_title\": \"LLM-in-Sandbox Elicits General Agentic Intelligence\"}"
  },
  {
    "id": "2601.16208",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.16208",
    "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
    "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
    "keywords": [
      "Representation Autoencoders",
      "Text-to-Image Generation",
      "Diffusion Models",
      "Latent Space",
      "Multimodal AI"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Multimodal"
    ],
    "published_time": "2026-01-22T18:58:16.000Z",
    "download_time": "2026-01-23 12:01:35",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.16208\", \"arxiv_url\": \"https://arxiv.org/abs/2601.16208\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16208.png\", \"original_title\": \"Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders\"}"
  },
  {
    "id": "2601.16175",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.16175",
    "title": "Learning to Discover at Test Time",
    "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
    "keywords": [
      "Reinforcement Learning",
      "Test-Time Training",
      "Large Language Models",
      "Scientific Discovery",
      "Continual Learning"
    ],
    "area": [
      "Machine Learning",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2026-01-22T18:24:00.000Z",
    "download_time": "2026-01-23 12:01:33",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.16175\", \"arxiv_url\": \"https://arxiv.org/abs/2601.16175\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16175.png\", \"original_title\": \"Learning to Discover at Test Time\"}"
  },
  {
    "id": "2601.16163",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2601.16163",
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
    "keywords": [
      "Video Models",
      "Robotics",
      "Visuomotor Control",
      "Policy Learning",
      "Diffusion Models"
    ],
    "area": [
      "Robotics",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2026-01-22T18:09:30.000Z",
    "download_time": "2026-01-23 12:01:37",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2601.16163\", \"arxiv_url\": \"https://arxiv.org/abs/2601.16163\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16163.png\", \"original_title\": \"Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning\"}"
  }
]