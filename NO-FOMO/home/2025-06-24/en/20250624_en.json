[
  {
    "id": "twitter_mjbommar_1937562175955980614",
    "source": "Twitter",
    "url": "https://twitter.com/mjbommar/status/1937562175955980614",
    "title_en": "mjbommar_Anthropic Wins Fair Use Summary Judgment, Still Faces Damages Trial",
    "summary_en": "Anthropic has achieved a significant legal victory, with Judge Alsup granting its motion for summary judgment on fair use grounds. This ruling indicates that Anthropic's use of certain materials was deemed permissible under fair use principles at this preliminary stage. However, despite this win, the company will still proceed to trial to determine potential damages related to the use of \"pirated\" material from the internet. This case is closely watched as it has significant implications for AI content generation and the evolving boundaries of copyright law.",
    "keywords_en": [
      "Anthropic",
      "Fair Use",
      "Copyright",
      "Lawsuit",
      "AI",
      "Damages"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Industry News",
      "Others"
    ],
    "published_time": "2025-06-24T11:58:06.000Z",
    "download_time": "2025-06-25 09:14:06",
    "visual_resource": [
      "screenshot/twitter/mjbommar_1937562175955980614.png"
    ],
    "extra_info": "{\"username\": \"mjbommar\", \"tweet_id\": \"1937562175955980614\"}"
  },
  {
    "id": "twitter_DeepLearningAI_1937314755066171580",
    "source": "Twitter",
    "url": "https://twitter.com/DeepLearningAI/status/1937314755066171580",
    "title_en": "DeepLearningAI_Disney Universal Sue Midjourney for Copyright Infringement",
    "summary_en": "Disney and Universal have filed a lawsuit against image generation company Midjourney, accusing it of training its models on their copyrighted content and reproducing unauthorized images. The studios claim Midjourney generated images of characters like Spider-Man and Iron Man, sometimes without explicit prompts. The suit seeks to halt the distribution of this content and demands damages potentially amounting to $150,000 per image.",
    "keywords_en": [
      "Midjourney",
      "copyright infringement",
      "AI-generated images",
      "lawsuit",
      "Disney",
      "Universal"
    ],
    "area_en": [
      "Generative AI",
      "Industry News",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-24T01:00:05.000Z",
    "download_time": "2025-06-25 09:14:05",
    "visual_resource": [
      "screenshot/twitter/DeepLearningAI_1937314755066171580.png"
    ],
    "extra_info": "{\"username\": \"DeepLearningAI\", \"tweet_id\": \"1937314755066171580\"}"
  },
  {
    "id": "twitter_GoogleDeepMind_1937535740206022773",
    "source": "Twitter",
    "url": "https://twitter.com/GoogleDeepMind/status/1937535740206022773",
    "title_en": "GoogleDeepMind_Launches Gemini Robotics On-Device AI and Open-Source Tools",
    "summary_en": "Google DeepMind announced the launch of Gemini Robotics On-Device AI, aiming to bring AI capabilities to local robotic devices. This release includes an on-device VLA that can run on a GPU, and an open-source MuJoCo simulator and benchmark for bimanual dexterity, designed to broaden access to these models for academics and developers.",
    "keywords_en": [
      "Gemini Robotics",
      "On-Device AI",
      "Robotics",
      "MuJoCo",
      "Open Source",
      "VLA"
    ],
    "area_en": [
      "Robotics",
      "Product Launch",
      "Open Source"
    ],
    "published_time": "2025-06-24T14:25:11.000Z",
    "download_time": "2025-06-25 09:14:03",
    "visual_resource": [
      "screenshot/twitter/GoogleDeepMind_1937535740206022773.png"
    ],
    "extra_info": "{\"username\": \"GoogleDeepMind\", \"tweet_id\": \"1937535740206022773\"}"
  },
  {
    "id": "twitter__akhaliq_1937542375179448828",
    "source": "Twitter",
    "url": "https://twitter.com/_akhaliq/status/1937542375179448828",
    "title_en": "_akhaliq_Warp 2.0 Agentic Development Environment",
    "summary_en": "Warp has launched Warp 2.0, an innovative Agentic Development Environment. This platform is touted as the top overall coding agent, achieving first place on Terminal-Bench and 71% on SWE-bench. Warp 2.0 features verified agent multi-threading, enabling simultaneous feature building, debugging, and shipping. It aims to be the first all-in-one platform for agentic development, significantly boosting development efficiency.",
    "keywords_en": [
      "Warp 2.0",
      "Agentic Development",
      "Coding Agent",
      "Development Environment",
      "Multi-threading",
      "Software Development"
    ],
    "area_en": [
      "AI Agent",
      "Product Launch",
      "Tech News"
    ],
    "published_time": "2025-06-24T14:56:16.000Z",
    "download_time": "2025-06-25 09:14:16",
    "visual_resource": [
      "screenshot/twitter/_akhaliq_1937542375179448828.png"
    ],
    "extra_info": "{\"username\": \"_akhaliq\", \"tweet_id\": \"1937542375179448828\"}"
  },
  {
    "id": "twitter_mustafasuleyman_1937553061427445824",
    "source": "Twitter",
    "url": "https://twitter.com/mustafasuleyman/status/1937553061427445824",
    "title_en": "mustafasuleyman_AI Models Evolve from \"Chain of Thought\" to \"Chain of Debate\"",
    "summary_en": "Mustafa Suleyman introduces \"Chain of Debate\" as the next evolution for AI, moving from a single model's \"thinking out loud\" to multiple models discussing, debating, debugging, and deliberating. He emphasizes that this collaborative approach, where \"two heads are better than one,\" will lead to more advanced AI systems, signifying a shift from individual AI intelligence to collective AI intelligence.",
    "keywords_en": [
      "Chain of Debate",
      "Chain of Thought",
      "Multi-model",
      "Artificial Intelligence",
      "LLM",
      "Collaborative AI"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Research Progress"
    ],
    "published_time": "2025-06-24T16:47:02.000Z",
    "download_time": "2025-06-25 09:14:27",
    "visual_resource": [
      "screenshot/twitter/mustafasuleyman_1937553061427445824.png"
    ],
    "extra_info": "{\"username\": \"mustafasuleyman\", \"tweet_id\": \"1937553061427445824\"}"
  },
  {
    "id": "twitter_corbtt_1937594932040204483",
    "source": "Twitter",
    "url": "https://twitter.com/corbtt/status/1937594932040204483",
    "title_en": "corbtt_RL Sample Efficiency Breakthrough: GRPO Enables Qwen2.5-14B to Surpass Gemini Flash",
    "summary_en": "Kyle Corbitt reported exciting evidence of significant sample efficiency in Reinforcement Learning (RL). By applying GRPO to train a modified version of ART-E (agentic RAG task), Qwen2.5-14B was able to exceed Gemini 2.5 Flash performance with just one training scenario, and surpass O3 with 16 scenarios. This outcome greatly exceeded initial expectations for sample efficiency, highlighting RL's potential for highly efficient large model training.",
    "keywords_en": [
      "Reinforcement Learning",
      "Sample Efficiency",
      "GRPO",
      "ART-E",
      "Qwen2.5-14B",
      "Large Language Model"
    ],
    "area_en": [
      "Machine Learning",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-06-24T19:33:25.000Z",
    "download_time": "2025-06-25 09:14:28",
    "visual_resource": [
      "screenshot/twitter/corbtt_1937594932040204483.png"
    ],
    "extra_info": "{\"username\": \"corbtt\", \"tweet_id\": \"1937594932040204483\"}"
  },
  {
    "id": "Kntj7tn_wf--Q3sxwmzYDg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Kntj7tn_wf--Q3sxwmzYDg",
    "title_en": "Cache Me If You Can: How ManyKVsDoYouNeed for Effective Long-Context LMs?",
    "summary_en": "Princeton University's Danqi Chen's team has introduced innovative solutions to address the significant KV cache memory consumption in large language models (LLMs) during long-context inference. They proposed \"KV footprint\" as a unified metric and defined \"critical KV footprint\" to enable fair comparison of optimization methods while ensuring performance remains at least 90% of full attention. Their research identified and improved shortcomings of existing eviction methods, particularly the high peak memory issue of post-fill eviction. Building upon this, the team developed PruLong, an end-to-end optimization approach that intelligently learns which attention heads require full KV cache retention. PruLong effectively saves memory, achieving a 12% smaller KV footprint compared to previous methods, while maintaining strong long-context performance and recall task capabilities. This work offers a crucial pathway to alleviate the memory bottleneck in LLMs, facilitating their application in more demanding scenarios.",
    "keywords_en": [
      "KV Cache",
      "Large Language Models",
      "Memory Optimization",
      "Long Context",
      "PruLong"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-24T16:09:26.000Z",
    "download_time": "2025-06-26T10:37:11.469585",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Kntj7tn_wf--Q3sxwmzYDg.png"
    ],
    "extra_info": null
  },
  {
    "id": "mXXVADVyTpm-Up_oiRIUuw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/mXXVADVyTpm-Up_oiRIUuw",
    "title_en": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation",
    "summary_en": "FilMaster, developed by a Chinese team, is the first end-to-end AI system for automated film generation, capable of producing complete movies from text input and reference images. Its core innovations include the \"Multi-shot Collaborative RAG Cinematic Language Design Module\" and the \"Audience-Centric Film Rhythm Control Module.\" The former leverages a vast library of 440,000 film clips to learn director-level cinematography, ensuring shot coherence and continuity. The latter optimizes audiovisual elements through simulated audience reviews and professional post-production workflows, enhancing emotional resonance and viewer engagement. Experimental results demonstrate FilMaster's significant superiority over existing methods in cinematic language expressiveness and rhythmic appeal, achieving an average 68.44% improvement in user studies and 58.06% in automated evaluations. This system represents a groundbreaking AI tool for professional-grade film production.",
    "keywords_en": [
      "FilMaster",
      "Film Generation",
      "Cinematic Language",
      "Film Rhythm",
      "Generative AI",
      "Large Language Model"
    ],
    "area_en": [
      "Generative AI",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-06-24T16:09:26.000Z",
    "download_time": "2025-06-26T10:37:07.790052",
    "visual_resource": [
      "screenshot/wechat/wechat_image_mXXVADVyTpm-Up_oiRIUuw.png"
    ],
    "extra_info": null
  },
  {
    "id": "ukcoXZ3wEY_gtRa2D3SwIA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ukcoXZ3wEY_gtRa2D3SwIA",
    "title_en": "Synthetic Data Outperforms Human Data: Over 10% Absolute Performance Gain with Efficient LLM Fine-tuning via Task Definition Only",
    "summary_en": "Researchers from Peking University, MIT, and others have introduced the \"Synthetic Data Reinforcement Learning\" (Synthetic Data RL) framework, addressing the challenges of large language models' (LLMs) limitations in specialized domains and the high cost of human-annotated data for fine-tuning. This innovative framework requires only a task definition to generate high-quality, domain-specific synthetic data. It operates in three stages: knowledge-guided synthesis, difficulty adaptation, and high-potential sample selection combined with reinforcement learning. Experimental results demonstrate significant performance improvements, with absolute gains exceeding 10 percentage points across eight benchmarks in fields like mathematics, medicine, and law. The method substantially outperforms traditional supervised fine-tuning and existing synthetic data generation techniques, even matching or surpassing human-annotated data under equivalent data budgets. This framework enables efficient, human-annotation-free model customization, drastically reducing the cost of domain adaptation and laying a solid foundation for scalable AI applications.",
    "keywords_en": [
      "Synthetic Data",
      "Reinforcement Learning",
      "LLM Fine-tuning",
      "Domain Knowledge",
      "Data Efficiency",
      "Task Definition"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-24T16:09:26.000Z",
    "download_time": "2025-06-26T10:37:09.245306",
    "visual_resource": [
      "screenshot/wechat/wechat_image_ukcoXZ3wEY_gtRa2D3SwIA.png"
    ],
    "extra_info": null
  },
  {
    "id": "TQBWalcM4fdB--m2oR8CJQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/TQBWalcM4fdB--m2oR8CJQ",
    "title_en": "7B Small Model Surpasses DeepSeek-R1: Weak Models Can Teach Strong Reasoning LLMs by Imitating Human Teachers | Transformer Author Team",
    "summary_en": "Sakana AI introduces a novel large language model (LLM) training methodology called Reinforcement Learning Teacher (RLT), which redefines the traditional approach where teacher models solve problems from scratch. This innovative method mandates that teacher models, akin to human educators, provide clear, step-by-step explanations based on known solutions, rather than independently deriving answers. Experimental results demonstrate that a 7B small model trained with RLT surpasses the 671B DeepSeek-R1 in imparting reasoning skills and can efficiently train student models several times its own size. This approach significantly enhances training efficiency, drastically reduces costs, and enables smaller models to effectively serve as teachers, offering a more economical and potent pathway for developing LLMs with robust reasoning capabilities.",
    "keywords_en": [
      "Teacher Model",
      "Reasoning Capability",
      "LLM Training",
      "Reinforcement Learning",
      "Small Models",
      "Sakana AI"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-24T13:37:41.000Z",
    "download_time": "2025-06-26T10:37:06.128656",
    "visual_resource": [
      "screenshot/wechat/wechat_image_TQBWalcM4fdB--m2oR8CJQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "ubVieEt3kJ9PNImErFWzJA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ubVieEt3kJ9PNImErFWzJA",
    "title_en": "OmniAvatar Open-Sourced! Alibaba Quake Releases New Audio-Driven Character Model",
    "summary_en": "Alibaba Quake team has open-sourced OmniAvatar, an innovative audio-driven full-body video generation model, addressing key challenges in existing techniques such as limited facial animation, lack of natural synchronization, and difficulties in generating fluid full-body movements. This novel model leverages a pixel-wise multi-level audio embedding strategy and a LoRA-based training method, which collectively enhance lip-sync precision and natural motion expression. OmniAvatar demonstrates superior performance in both facial and half-body video generation compared to current state-of-the-art models. Furthermore, it supports precise control via text prompts, enabling diverse applications including podcasts, human interaction, and singing, and offers the ability to control character emotions. Despite these significant advancements in virtual human video generation, the model still presents limitations regarding long video consistency, complex multi-character text control, and inference speed, which require further optimization.",
    "keywords_en": [
      "OmniAvatar",
      "Audio-driven",
      "Full-body video generation",
      "Virtual human",
      "LoRA",
      "Quark"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-06-24T06:54:35.000Z",
    "download_time": "2025-06-26T10:37:13.919770",
    "visual_resource": [
      "screenshot/wechat/wechat_image_ubVieEt3kJ9PNImErFWzJA.png"
    ],
    "extra_info": null
  },
  {
    "id": "geOn7zyJRQwfJra38EjcxQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/geOn7zyJRQwfJra38EjcxQ",
    "title_en": "LLMs Enter the \"Drag-and-Drop Era\"! Customize Large Models in Seconds with Just a Prompt, Boosting Efficiency by 12,000x",
    "summary_en": "Researchers from the National University of Singapore and other institutions have introduced \"Drag-and-Drop LLMs\" (DnD), a novel prompt-based parameter generator enabling training-free adaptive fine-tuning for large language models. DnD leverages a lightweight text encoder and a cascaded super-convolutional decoder to generate task-specific LoRA weight matrices in mere seconds, solely from unlabeled task prompts, effectively bypassing the traditional gradient descent process. This innovative method boasts a computational overhead 12,000 times lower than conventional full fine-tuning. Furthermore, it demonstrates a remarkable 30% performance improvement over trained LoRA models in zero-shot learning benchmarks and exhibits robust generalization capabilities across diverse domains. DnD offers a highly efficient, flexible, and superior alternative for rapid LLM specialization, significantly accelerating the model customization process.",
    "keywords_en": [
      "Drag-and-Drop LLMs",
      "Parameter Generation",
      "Training-Free Fine-tuning",
      "LoRA Weights",
      "Zero-Shot Learning"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Generative AI"
    ],
    "published_time": "2025-06-24T04:36:19.000Z",
    "download_time": "2025-06-26T10:37:30.931630",
    "visual_resource": [
      "screenshot/wechat/wechat_image_geOn7zyJRQwfJra38EjcxQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "claude-code-router",
    "source": "GitHub",
    "url": "https://github.com/musistudio/claude-code-router",
    "title_en": "Claude Code Router",
    "summary_en": "Claude Code Router is a request routing tool specifically designed for Claude Code, enabling users to flexibly distribute code generation requests to various LLM models with high customization. It achieves model routing through configuration, allowing selection of the optimal model based on task types such as background tasks, reasoning, or long-context processing, thereby effectively reducing costs and enhancing performance. The tool also offers a plugin mechanism for extended functionality and seamless integration with GitHub Actions, providing an efficient and economical solution for AI-driven development workflows.",
    "keywords_en": [
      "Claude Code",
      "Model Routing",
      "Large Language Model",
      "Cost Optimization",
      "AI Programming",
      "GitHub Actions",
      "Plugins"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-25T09:43:47Z",
    "download_time": "2024-07-30 12:00:00",
    "visual_resource": [
      "https://github.com/musistudio/claude-code-router/blob/main/screenshoots/claude-code.png?raw=true",
      "https://github.com/musistudio/claude-code-router/blob/main/screenshoots/contexterror.jpg?raw=true"
    ],
    "extra_info": null
  },
  {
    "id": "leaked-system-prompts",
    "source": "GitHub",
    "url": "https://github.com/jujumilk3/leaked-system-prompts",
    "title_en": "leaked-system-prompts",
    "summary_en": "This GitHub repository is a dedicated collection of leaked system prompts sourced from various widely used Large Language Model (LLM) based services. It functions as a community-driven platform, inviting contributions of such prompts, with a clear protocol for submission: all entries must be accompanied by verifiable sources or reproducible examples to ensure authenticity. The repository explicitly warns against the inclusion of sensitive commercial source code to prevent potential DMCA issues, a critical consideration given its frequent citation in academic research papers. This highlights its significant role as a valuable resource for understanding and analyzing the underlying instructions guiding prominent LLM applications, contributing to the broader study of prompt engineering and AI system behavior.",
    "keywords_en": [
      "Leaked Prompts",
      "System Prompts",
      "Large Language Models",
      "Prompt Engineering",
      "LLM Services",
      "Open Source Project"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-11T11:08:13Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/leaked-system-prompts.png"
    ],
    "extra_info": null
  },
  {
    "id": "ML-From-Scratch",
    "source": "GitHub",
    "url": "https://github.com/eriklindernoren/ML-From-Scratch",
    "title_en": "Machine Learning From Scratch",
    "summary_en": "This GitHub repository offers Python implementations of various fundamental machine learning models and algorithms from scratch. Its primary goal is to transparently and accessibly illustrate their inner workings, rather than focusing on computational optimization. The project spans four major domains: supervised learning, unsupervised learning, reinforcement learning, and deep learning, encompassing classic algorithms such as Decision Trees, Support Vector Machines, K-Means, GANs, and DQNs. Through numerous examples like polynomial regression, CNN image classification, DBSCAN clustering, generative adversarial networks for digit generation, and deep Q-networks, it provides an invaluable resource for learners to deeply understand core machine learning concepts and algorithm implementations.",
    "keywords_en": [
      "Machine Learning",
      "Deep Learning",
      "Supervised Learning",
      "Unsupervised Learning",
      "Reinforcement Learning",
      "Algorithms",
      "Python",
      "From Scratch"
    ],
    "area_en": [
      "Machine Learning",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2019-10-18T21:42:16Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "http://eriklindernoren.se/images/p_reg.gif",
      "http://eriklindernoren.se/images/mlfs_cnn1.png",
      "http://eriklindernoren.se/images/mlfs_dbscan.png"
    ],
    "extra_info": null
  },
  {
    "id": "2506.18871",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.18871",
    "title_en": "OmniGen2: Exploration to Advanced Multimodal Generation",
    "summary_en": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
    "keywords_en": [
      "Multimodal Generation",
      "Text-to-Image",
      "Image Editing",
      "In-context Generation",
      "OmniGen2"
    ],
    "area_en": [
      "Multimodal",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-06-23T17:38:54.000Z",
    "download_time": "2025-06-25 19:37:43",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18871.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.18871\", \"arxiv_url\": \"https://arxiv.org/abs/2506.18871\"}"
  },
  {
    "id": "2506.18841",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.18841",
    "title_en": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
    "summary_en": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
    "keywords_en": [
      "Ultra-long Text Generation",
      "Large Language Models",
      "Reinforcement Learning",
      "Reward Models",
      "Text Generation"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-06-23T16:59:02.000Z",
    "download_time": "2025-06-25 19:37:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18841.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.18841\", \"arxiv_url\": \"https://arxiv.org/abs/2506.18841\"}"
  },
  {
    "id": "2506.15741",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.15741",
    "title_en": "OAgents: An Empirical Study of Building Effective Agents",
    "summary_en": "Recently, Agentic AI has become an increasingly popular research field.\nHowever, we argue that current agent research practices lack standardization\nand scientific rigor, making it hard to conduct fair comparisons among methods.\nAs a result, it is still unclear how different design choices in agent\nframeworks affect effectiveness, and measuring their progress remains\nchallenging. In this work, we conduct a systematic empirical study on GAIA\nbenchmark and BrowseComp to examine the impact of popular design choices in key\nagent components in a fair and rigorous manner. We find that the lack of a\nstandard evaluation protocol makes previous works, even open-sourced ones,\nnon-reproducible, with significant variance between random runs. Therefore, we\nintroduce a more robust evaluation protocol to stabilize comparisons. Our study\nreveals which components and designs are crucial for effective agents, while\nothers are redundant, despite seeming logical. Based on our findings, we build\nand open-source OAgents, a new foundation agent framework that achieves\nstate-of-the-art performance among open-source projects. OAgents offers a\nmodular design for various agent components, promoting future research in\nAgentic AI.",
    "keywords_en": [
      "Agentic AI",
      "Empirical Study",
      "OAgents",
      "Evaluation Protocol",
      "Design Choices"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-06-17T17:59:02.000Z",
    "download_time": "2025-06-25 19:37:42",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15741.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.15741\", \"arxiv_url\": \"https://arxiv.org/abs/2506.15741\"}"
  },
  {
    "id": "2506.18890",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.18890",
    "title_en": "4D-LRM: Large Space-Time Reconstruction Model From and To Any View at\n  Any Time",
    "summary_en": "Can we scale 4D pretraining to learn general space-time representations that\nreconstruct an object from a few views at some times to any view at any time?\nWe provide an affirmative answer with 4D-LRM, the first large-scale 4D\nreconstruction model that takes input from unconstrained views and timestamps\nand renders arbitrary novel view-time combinations. Unlike prior 4D approaches,\ne.g., optimization-based, geometry-based, or generative, that struggle with\nefficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time\nrepresentation and directly predicts per-pixel 4D Gaussian primitives from\nposed image tokens across time, enabling fast, high-quality rendering at, in\nprinciple, infinite frame rate. Our results demonstrate that scaling\nspatiotemporal pretraining enables accurate and efficient 4D reconstruction. We\nshow that 4D-LRM generalizes to novel objects, interpolates across time, and\nhandles diverse camera setups. It reconstructs 24-frame sequences in one\nforward pass with less than 1.5 seconds on a single A100 GPU.",
    "keywords_en": [
      "4D Reconstruction",
      "Space-Time Model",
      "Novel View Synthesis",
      "Gaussian Primitives",
      "Efficient Rendering"
    ],
    "area_en": [
      "Computer Vision",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-06-23T17:57:47.000Z",
    "download_time": "2025-06-25 19:37:50",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18890.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.18890\", \"arxiv_url\": \"https://arxiv.org/abs/2506.18890\"}"
  },
  {
    "id": "2506.18879",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.18879",
    "title_en": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
    "summary_en": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
    "keywords_en": [
      "KV Cache Compression",
      "Vector Quantization",
      "Large Language Models",
      "Memory Optimization",
      "Rotary Position Embedding"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-06-23T17:50:11.000Z",
    "download_time": "2025-06-25 19:37:48",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18879.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.18879\", \"arxiv_url\": \"https://arxiv.org/abs/2506.18879\"}"
  },
  {
    "id": "2506.18900",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.18900",
    "title_en": "Audit & Repair: An Agentic Framework for Consistent Story Visualization\n  in Text-to-Image Diffusion Models",
    "summary_en": "Story visualization has become a popular task where visual scenes are\ngenerated to depict a narrative across multiple panels. A central challenge in\nthis setting is maintaining visual consistency, particularly in how characters\nand objects persist and evolve throughout the story. Despite recent advances in\ndiffusion models, current approaches often fail to preserve key character\nattributes, leading to incoherent narratives. In this work, we propose a\ncollaborative multi-agent framework that autonomously identifies, corrects, and\nrefines inconsistencies across multi-panel story visualizations. The agents\noperate in an iterative loop, enabling fine-grained, panel-level updates\nwithout re-generating entire sequences. Our framework is model-agnostic and\nflexibly integrates with a variety of diffusion models, including rectified\nflow transformers such as Flux and latent diffusion models such as Stable\nDiffusion. Quantitative and qualitative experiments show that our method\noutperforms prior approaches in terms of multi-panel consistency.",
    "keywords_en": [
      "Story Visualization",
      "Diffusion Models",
      "Visual Consistency",
      "Multi-agent Framework",
      "Text-to-Image"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "AI Agent"
    ],
    "published_time": "2025-06-23T17:59:29.000Z",
    "download_time": "2025-06-25 19:37:51",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18900.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.18900\", \"arxiv_url\": \"https://arxiv.org/abs/2506.18900\"}"
  }
]