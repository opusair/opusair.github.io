<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2026-02-04</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2026-02-04</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Claude Code for Infrastructure</h2>
                <span class="published-time">Published: 2026-02-04 18:34:08</span>
                
                <p class="summary">The Hacker News story, "Claude Code for Infrastructure," introduces Fluid.sh, a specialized platform designed to provide developer-friendly infrastructure explicitly for AI-first applications. Although the direct content provided is concise, the title strongly implies an innovative approach: utilizing advanced AI models, specifically Claude, to generate, manage, or optimize infrastructure code. This paradigm aims to significantly streamline the deployment and operational lifecycle of complex AI models by automating the typically manual and intricate processes of infrastructure provisioning and configuration. Fluid.sh positions itself as a crucial enabler for AI engineers, offering scalable, secure, and cost-effective infrastructure solutions, including essential computational resources such as GPUs, CPUs, and storage. The platform's objective is to mitigate the complexities associated with underlying operational challenges, thereby allowing AI developers to allocate more focus towards model development and less on infrastructure overhead. This initiative underscores a broader industry movement towards integrating AI capabilities into infrastructure management to enhance efficiency and accessibility for demanding AI workloads.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Infrastructure</span><span>Infrastructure as Code</span><span>Large Language Models</span><span>Cloud Computing</span><span>AI-first Applications</span><span>DevOps</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.fluid.sh/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Claude Is a Space to Think</h2>
                <span class="published-time">Published: 2026-02-04 12:08:49</span>
                
                <p class="summary">Anthropic's latest announcement positions its AI assistant, Claude, as 'a space to think,' emphasizing its role beyond simple query responses to foster deep engagement and complex cognitive tasks. This initiative highlights Claude's advanced capabilities in sustained reasoning, enabling users to explore intricate problems, develop ideas, and participate in reflective processes. The concept suggests an AI that supports iterative thought, critical analysis, and creative problem-solving, offering a collaborative platform where users can extend their intellectual capacity. By focusing on the quality of interaction and depth of processing, Anthropic aims to differentiate Claude as a sophisticated AI partner. It transcends basic conversational functions, providing a robust platform for intellectual exploration and innovation, making it an invaluable asset for professionals and researchers engaged in analytical and creative workflows.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>AI Assistant</span><span>Conversational AI</span><span>AI Reasoning</span><span>Problem Solving</span><span>Anthropic</span><span>Cognitive AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.anthropic.com/news/claude-is-a-space-to-think" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</h2>
                <span class="published-time">Published: 2026-02-04 14:33:33</span>
                
                <p class="summary">A recent research paper titled 'Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation' introduces an innovative method to enhance the efficiency of attention mechanisms, a fundamental component in many advanced deep learning architectures. The study proposes a technique that ensures a constant computational cost per token, independent of the input sequence length. This is achieved by employing a novel symmetry-aware Taylor approximation. This approach directly addresses a critical limitation of traditional attention models, where computational complexity typically scales quadratically with sequence length, thereby restricting their scalability for processing very long sequences. By providing a more efficient and scalable alternative, this research holds the potential to significantly improve the performance and applicability of deep learning models in scenarios demanding extensive contextual understanding, making advanced AI models more resource-friendly and practical for real-world applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Attention Mechanisms</span><span>Taylor Approximation</span><span>Computational Efficiency</span><span>Deep Learning</span><span>Neural Networks</span><span>Symmetry-Aware Algorithms</span><span>AI Optimization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Deep Learning</span><span>Machine Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://arxiv.org/abs/2602.00294" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Intel will start making GPUs</h2>
                <span class="published-time">Published: 2026-02-04 17:40:37</span>
                
                <p class="summary">Intel, a long-standing titan in the microprocessor industry, has officially declared its intention to commence manufacturing Graphics Processing Units (GPUs). This pivotal strategic decision marks a significant re-entry or deepened commitment by Intel into a hardware segment largely dominated by NVIDIA and AMD. The move is set to intensify competition within the semiconductor landscape, particularly impacting markets crucial for high-performance computing, data centers, and the burgeoning fields of artificial intelligence and machine learning. GPUs are indispensable accelerators for complex computational tasks, including deep learning model training, scientific simulations, and advanced graphics rendering. Intel's venture into this domain suggests a broader ambition to provide comprehensive computing solutions, leveraging its extensive manufacturing capabilities and architectural expertise. Industry observers anticipate this development could stimulate innovation across the board, potentially leading to more competitive pricing and diverse architectural choices for consumers and enterprises reliant on GPU-powered systems. This strategic pivot highlights the increasing importance of integrated and specialized computing hardware in the evolving technological ecosystem.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>GPU</span><span>Intel</span><span>NVIDIA</span><span>Semiconductor Industry</span><span>High-Performance Computing</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Hardware Manufacturing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://techcrunch.com/2026/02/03/intel-will-start-making-gpus-a-market-dominated-by-nvidia/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Voxtral Transcribe 2</h2>
                <span class="published-time">Published: 2026-02-04 15:08:17</span>
                
                <p class="summary">Mistral AI has announced the launch of "Voxtral Transcribe 2," marking an evolution in its sophisticated speech-to-text technology offerings. This release, while concisely presented, signifies a likely advancement in the capabilities of Mistral AI's transcription service. Industry trends for such updates typically include notable improvements in transcription accuracy, particularly in noisy environments or with diverse accents, enhanced processing speed for real-time applications, and expanded support for a wider array of languages and dialects. Leveraging Mistral AI's foundational expertise in advanced AI models, Voxtral Transcribe 2 is expected to integrate cutting-edge deep learning techniques to deliver superior performance in converting spoken language into text. This upgrade is poised to benefit various sectors, from media and customer service to legal and healthcare, by providing highly precise and efficient automated transcription solutions crucial for data analysis, accessibility, and operational efficiency. The second iteration implies a refined and optimized solution, building upon previous successes to meet growing demands for high-quality audio processing.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Speech-to-Text</span><span>AI Transcription</span><span>Natural Language Processing</span><span>Deep Learning</span><span>Audio Processing</span><span>Mistral AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mistral.ai/news/voxtral-transcribe-2" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Ghidra MCP Server ‚Äì 110 tools for AI-assisted reverse engineering</h2>
                <span class="published-time">Published: 2026-02-04 06:51:51</span>
                
                <p class="summary">The Ghidra MCP Server, showcased as a new project, introduces a comprehensive suite of 110 specialized tools aimed at revolutionizing reverse engineering workflows with artificial intelligence assistance. Built to complement or integrate with the established open-source Ghidra reverse engineering framework, this server infrastructure is designed to automate and significantly enhance the analysis of complex binaries. By incorporating advanced AI and machine learning techniques, it promises to elevate the efficiency and precision of critical tasks such as vulnerability identification, intricate software behavior understanding, and sophisticated malware detection. The extensive array of tools within the Multi-Compiler Platform (MCP) Server is expected to span diverse functionalities, ranging from intelligent pattern recognition and anomaly detection within assembly code to automated deobfuscation and refined function signature identification. This robust platform empowers cybersecurity researchers, software analysts, and forensic experts with cutting-edge capabilities, enabling them to more effectively dissect and comprehend elaborate software systems. This project underscores the accelerating integration of AI into cybersecurity, signaling a transformative shift in the methodologies and tools available for addressing contemporary reverse engineering challenges.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Ghidra</span><span>Reverse Engineering</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Cybersecurity</span><span>Binary Analysis</span><span>Program Analysis</span><span>Software Assurance</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/bethington/ghidra-mcp" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs</h2>
                <span class="published-time">Published: 2026-02-02T13:46:56.000Z</span>
                
                <p class="summary">This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Chain-of-Thought</span><span>Large Language Models</span><span>Latent Planning</span><span>Planning Horizon</span><span>Uncertainty Estimation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.02103" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection</h2>
                <span class="published-time">Published: 2026-02-03T07:31:14.000Z</span>
                
                <p class="summary">The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, V to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to times3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Token Sparse Attention</span><span>Long-Context Inference</span><span>Large Language Models</span><span>Attention Mechanism</span><span>Sparsification</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Deep Learning</span><span>Natural Language Processing</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.03216" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training</h2>
                <span class="published-time">Published: 2026-01-31T14:27:46.000Z</span>
                
                <p class="summary">Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model Pre-training</span><span>Data Mixing</span><span>Model Merging</span><span>Optimal Data Ratios</span><span>DeMix</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.00747" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration</h2>
                <span class="published-time">Published: 2026-02-03T17:46:16.000Z</span>
                
                <p class="summary">Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Agentic Orchestration</span><span>Sub-agent Creation</span><span>Language Agents</span><span>Task Automation</span><span>Agent Abstraction</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.03786" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently</h2>
                <span class="published-time">Published: 2026-02-02T13:23:39.000Z</span>
                
                <p class="summary">While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Long-Horizon Agency</span><span>Large Language Models</span><span>Data Synthesis</span><span>Pull Request Sequences</span><span>AI Agent</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.02619" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation</h2>
                <span class="published-time">Published: 2026-02-03T17:59:09.000Z</span>
                
                <p class="summary">Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>3D-Aware Motion Control</span><span>Human Video Generation</span><span>Implicit Representation</span><span>View-Adaptive</span><span>Video Synthesis</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Generative AI</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.03796" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>