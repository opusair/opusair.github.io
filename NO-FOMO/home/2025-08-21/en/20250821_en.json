[
  {
    "id": "twitter_Cohere_Labs_1958576284763611322",
    "source": "Twitter",
    "url": "https://x.com/Cohere_Labs/status/1958576284763611322",
    "title_en": "Cohere_Labs_Launches Command A Reasoning Model",
    "summary_en": "Cohere Labs has launched Command A Reasoning, its most advanced model specifically designed for complex enterprise reasoning tasks such as deep research and data analysis. This new model aims to enhance AI capabilities for businesses. Demonstrating its commitment to the research ecosystem, Cohere Labs is also releasing the model weights, fostering further development and application of AI technology.",
    "keywords_en": [
      "Command A Reasoning",
      "Cohere Labs",
      "Enterprise AI",
      "Reasoning Model",
      "Model Release",
      "Open Source"
    ],
    "area_en": [
      "Large Language Model",
      "Product Launch",
      "Tech News"
    ],
    "published_time": "2025-08-21T17:05:49.000Z",
    "download_time": "2025-08-22 03:56:27",
    "visual_resource": [
      "screenshot/twitter/Cohere_Labs_1958576284763611322.png"
    ],
    "extra_info": "{\"username\": \"Cohere_Labs\", \"tweet_id\": \"1958576284763611322\"}"
  },
  {
    "id": "twitter_Google_1958624466893770808",
    "source": "Twitter",
    "url": "https://x.com/Google/status/1958624466893770808",
    "title_en": "Google_Unveils Pixel 10 Series, Pixel Watch 4, and New AI Products",
    "summary_en": "Google has unveiled its Pixel 10 series, featuring the Tensor G5 chip and Gemini Nano model, enhancing AI capabilities and camera systems with 100x zoom and AI photography assistance. The launch also includes the Pixel Watch 4, offering standalone satellite communication and on-wrist Gemini, alongside Pixel Buds 2a with Active Noise Cancellation. Furthermore, Google introduced AI health coaching, Magic Cue, and Gemini for Home, comprehensively improving user experience across its ecosystem.",
    "keywords_en": [
      "Google",
      "Pixel 10",
      "Pixel Watch 4",
      "Artificial Intelligence",
      "Gemini",
      "Product Launch"
    ],
    "area_en": [
      "Product Launch",
      "Artificial Intelligence",
      "Tech News"
    ],
    "published_time": "2025-08-21T20:17:17.000Z",
    "download_time": "2025-08-22 03:58:45",
    "visual_resource": [
      "screenshot/twitter/Google_1958624466893770808.png"
    ],
    "extra_info": "{\"username\": \"Google\", \"tweet_id\": \"1958624466893770808\"}"
  },
  {
    "id": "twitter_googleaidevs_1958599306472206349",
    "source": "Twitter",
    "url": "https://twitter.com/googleaidevs/status/1958599306472206349",
    "title_en": "googleaidevs_Launches Open-Source AI Video Studio Template",
    "summary_en": "Google AI Developers has released an open-source AI video studio template built with Next.js, integrating Veo 3 and Imagen 4 technologies from the Gemini API. This tool enables users to generate text-to-video and image-to-video content, as well as perform video editing directly within the browser, aiming to streamline the AI video creation process.",
    "keywords_en": [
      "AI Video Studio",
      "Open Source Project",
      "Gemini API",
      "Veo 3",
      "Imagen 4",
      "Video Generation"
    ],
    "area_en": [
      "Generative AI",
      "Open Source",
      "Product Launch"
    ],
    "published_time": "2025-08-21T18:37:18.000Z",
    "download_time": "2025-08-22 06:27:14",
    "visual_resource": [
      "screenshot/twitter/googleaidevs_1958599306472206349.png"
    ],
    "extra_info": "{\"username\": \"googleaidevs\", \"tweet_id\": \"1958599306472206349\"}"
  },
  {
    "id": "twitter_weaviate_io_1958568536420299184",
    "source": "Twitter",
    "url": "https://twitter.com/weaviate_io/status/1958568536420299184",
    "title_en": "weaviate_io_Weaviate Open-Sources Elysia: Explainable Agentic RAG Framework with Real-time Decision Process Visualization",
    "summary_en": "Weaviate has open-sourced Elysia, an innovative agentic RAG AI framework designed for explainability. Unlike traditional black-box AI systems, Elysia utilizes a decision tree architecture that allows real-time visualization of the AI's decision-making process. Its agents evaluate environments, consider actions, and output reasoning, maintaining global context awareness. The framework features advanced error handling, intelligently recognizing mismatches or irrelevant results and retrying with different approaches. Users can observe the entire decision tree traversal and LLM's reasoning within each node, significantly enhancing transparency and debuggability. Elysia is open-source and pip-installable, making it easy to adopt and customize.",
    "keywords_en": [
      "Weaviate",
      "Elysia",
      "AI Agent",
      "RAG",
      "Explainable AI",
      "Open Source"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Open Source"
    ],
    "published_time": "2025-08-21T16:35:02.000Z",
    "download_time": "2025-08-22 06:27:52",
    "visual_resource": [
      "screenshot/twitter/weaviate_io_1958568536420299184.png"
    ],
    "extra_info": "{\"username\": \"weaviate_io\", \"tweet_id\": \"1958568536420299184\"}"
  },
  {
    "id": "twitter_ErnestRyu_1958408925864403068",
    "source": "Twitter",
    "url": "https://twitter.com/ErnestRyu/status/1958408925864403068",
    "title_en": "ErnestRyu_GPT-5-pro Proves New Math in Convex Optimization, Drawing Attention",
    "summary_en": "Sebastien Bubeck claims that GPT-5-pro successfully proved a new and superior mathematical bound within a convex optimization paper, a result he personally verified as correct. Ernest Ryu, a prominent mathematics researcher specializing in convex optimization, expressed significant excitement and admiration for this achievement, acknowledging its direct relevance to his own field. However, he also indicated a nuanced perspective on the implications of such AI capabilities. This development underscores the profound and evolving potential of large language models to contribute to advanced and complex mathematical research.",
    "keywords_en": [
      "GPT-5-pro",
      "Convex Optimization",
      "Mathematical Proof",
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-08-21T06:00:48.000Z",
    "download_time": "2025-08-22 06:27:36",
    "visual_resource": [
      "screenshot/twitter/ErnestRyu_1958408925864403068.png"
    ],
    "extra_info": "{\"username\": \"ErnestRyu\", \"tweet_id\": \"1958408925864403068\"}"
  },
  {
    "id": "Xraa8M4dB9DxtF7_5lIN-A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Xraa8M4dB9DxtF7_5lIN-A",
    "title_en": "AI Agents Enter Investment Sector: QuantumBit Salon to Explore Future of Fintech",
    "summary_en": "This article explores the burgeoning potential of AI Agents in the financial investment sector, highlighting their capacity to potentially revolutionize traditional investment approaches. It also raises critical questions regarding AI Agents' ability to genuinely comprehend market dynamics, accurately predict trends, and effectively replace professional human investment advisors. To delve deeper into these pivotal discussions, QuantumBit AI Salon is set to host an event on August 22nd, featuring Vakee Lai, Founder and CEO of Singaporean fintech company RockFlow. Vakee brings over 12 years of extensive experience in early-stage investment, fintech product design, and quantitative trading across global high-tech and AI domains. His impressive track record includes leading investments in numerous prominent AI companies, achieving significant returns. Vakee will offer a unique integrated perspective, combining product, capital, and technology insights, to analyze the future development and entrepreneurial opportunities for AI Agents within the investment landscape.",
    "keywords_en": [
      "AI Agent",
      "Investment",
      "Fintech",
      "Quantitative Trading",
      "AI Entrepreneurship"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Others"
    ],
    "published_time": "2025-08-21T04:20:33.000Z",
    "download_time": "2025-08-22T14:28:34.584424",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Xraa8M4dB9DxtF7_5lIN-A.png"
    ],
    "extra_info": null
  },
  {
    "id": "NEdGzIbYYa08piUXq1gFHA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/NEdGzIbYYa08piUXq1gFHA",
    "title_en": "GPT-5 Pro Conducts Independent Mathematical Research! Provides More Precise Bounds After Reading Papers, OpenAI President: This is a Sign of Life",
    "summary_en": "OpenAI's GPT-5 Pro has demonstrated a remarkable capability for independent mathematical research. After analyzing a paper on convex optimization, the model successfully refined a boundary problem, providing a more precise threshold and an original proof that surpassed the initial findings of the human-authored paper. While human researchers later updated their work, presenting an even tighter bound, GPT-5 Pro's unique and distinct proof methodology underscores its genuine capacity for autonomous exploration and discovery, rather than mere replication. OpenAI President Brockman lauded this achievement as a \"sign of life,\" emphasizing the profound implications of AI's advancement in autonomous reasoning and scientific discovery. This breakthrough has ignited widespread discussion and excitement across the tech community, highlighting AI's rapidly evolving role in tackling complex scientific challenges, particularly those demanding abstract mathematical reasoning, novel problem-solving approaches, and the generation of rigorous proofs. It signifies a pivotal step towards more self-sufficient AI systems in research.",
    "keywords_en": [
      "GPT-5 Pro",
      "Mathematical Research",
      "Convex Optimization",
      "Autonomous Exploration",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-08-21T04:20:33.000Z",
    "download_time": "2025-08-22T14:28:34.683407",
    "visual_resource": [
      "screenshot/wechat/wechat_image_NEdGzIbYYa08piUXq1gFHA.png"
    ],
    "extra_info": null
  },
  {
    "id": "ke03ETrnm2B-wFtG5HWlnQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ke03ETrnm2B-wFtG5HWlnQ",
    "title_en": "ByteDance Open-Sources First Large Model Seed-OSS, Surpassing Qwen",
    "summary_en": "ByteDance's Seed team has officially open-sourced its inaugural large language model, Seed-OSS-36B, a 36-billion-parameter model. This new offering exhibits robust capabilities in long-context processing, complex reasoning, AI agent functionality, and general-purpose tasks, with notable optimizations for international applications. Remarkably, despite being trained on a relatively modest 12T of data, Seed-OSS has achieved outstanding performance across numerous mainstream benchmarks. Specifically, its Base-woSyn version has surpassed models like Qwen3-30B, and the Instruct version has demonstrated superior results compared to similar offerings from OpenAI and Google. A distinctive feature of Seed-OSS is its innovative \"thinking budget\" mechanism, which empowers users to precisely control the model's reasoning depth and computational expenditure. Released under the permissive Apache-2.0 license for commercial use, Seed-OSS signifies ByteDance's strategic and impactful contribution to the global open-source large model ecosystem.",
    "keywords_en": [
      "Seed-OSS",
      "ByteDance",
      "Large Language Model",
      "Open Source",
      "AI Agent",
      "Thinking Budget"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "AI Agent"
    ],
    "published_time": "2025-08-21T00:52:20.000Z",
    "download_time": "2025-08-22T14:28:34.574725",
    "visual_resource": [
      "screenshot/wechat/wechat_image_ke03ETrnm2B-wFtG5HWlnQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "airi",
    "source": "GitHub",
    "url": "https://github.com/moeru-ai/airi",
    "title_en": "Project AIRI",
    "summary_en": "Project AIRI aims to recreate AI virtual characters like Neuro-sama, building a real-time interactive digital companion that can seamlessly integrate into users' daily lives. Leveraging advanced Web technologies such as WebGPU, WebAudio, Web Workers, and WebAssembly, the project supports both VRM and Live2D models, ensuring broad compatibility. It operates efficiently across modern browsers, mobile devices (via PWA), and desktop environments, with robust native acceleration capabilities through NVIDIA CUDA and Apple Metal, thanks to projects like Candle. AIRI is designed to integrate with a wide array of large language model APIs, including OpenRouter, OpenAI, and Anthropic Claude, facilitating sophisticated interactions. Its core functionalities include enabling AI characters to play games like Minecraft and Factorio, engage in real-time voice conversations, and provide a truly immersive virtual interaction experience. This project is dedicated to empowering users with their own personalized digital life, offering unparalleled flexibility and control over their AI companions.",
    "keywords_en": [
      "AI VTuber",
      "Digital Companion",
      "Large Language Model",
      "Web Technologies",
      "Real-time Interaction",
      "Virtual Human",
      "Game Integration",
      "AI Agent"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-21T14:14:16Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/moeru-ai/airi/main/docs/content/public/banner-light-1280x640.avif",
      "https://raw.githubusercontent.com/moeru-ai/airi/main/docs/content/public/readme-image-pc-preview.avif"
    ],
    "extra_info": null
  },
  {
    "id": "sim",
    "source": "GitHub",
    "url": "https://github.com/simstudioai/sim",
    "title_en": "Sim: Build and deploy AI agent workflows in minutes.",
    "summary_en": "Sim is a platform designed for rapidly building and deploying AI agent workflows. It supports both cloud-hosted and various self-hosted deployment options, including NPM package, Docker Compose, Dev Containers, and manual setup. The project leverages a modern tech stack including Next.js, Bun, and PostgreSQL with pgvector extension, offering features like AI embeddings, knowledge bases, and semantic search. Sim aims to simplify AI application development, supporting integration with local large language models (e.g., Ollama), providing developers with a flexible and efficient solution for constructing AI agents.",
    "keywords_en": [
      "AI Agent",
      "Workflow",
      "Self-hosted",
      "Docker",
      "PostgreSQL",
      "Vector Database",
      "Large Language Model",
      "Ollama"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-22T04:44:28Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/public/static/demo.gif"
    ],
    "extra_info": null
  },
  {
    "id": "firecrawl",
    "source": "GitHub",
    "url": "https://github.com/firecrawl/firecrawl",
    "title_en": "ðŸ”¥ Firecrawl",
    "summary_en": "Firecrawl is an API service designed to empower AI applications with clean data from any website. It offers advanced web scraping, crawling, and data extraction capabilities, converting URL content into LLM-ready Markdown or structured data, and supporting all accessible subpages without requiring a sitemap. Its technical features include handling proxies, anti-bot mechanisms, dynamic content rendering, outputting various LLM-ready formats (e.g., Markdown, HTML, screenshots, structured data), and providing customizability for crawl depth, media parsing, and page interaction actions. Firecrawl aims to deliver reliable data acquisition solutions and supports batch processing, serving as a powerful tool for building the data foundation of AI applications.",
    "keywords_en": [
      "Web Scraping",
      "Data Extraction",
      "Web Crawling",
      "API Service",
      "LLM Data",
      "AI Applications",
      "Structured Data"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-21T16:17:13Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/firecrawl/firecrawl/main/img/firecrawl_logo.png",
      "https://raw.githubusercontent.com/firecrawl/firecrawl/main/img/open-source-cloud.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.14460",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.14460",
    "title_en": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference\n  Optimization",
    "summary_en": "We present DuPO, a dual learning-based preference optimization framework that\ngenerates annotation-free feedback via a generalized duality. DuPO addresses\ntwo key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s\nreliance on costly labels and applicability restricted to verifiable tasks, and\ntraditional dual learning's restriction to strictly dual task pairs (e.g.,\ntranslation and back-translation). Specifically, DuPO decomposes a primal\ntask's input into known and unknown components, then constructs its dual task\nto reconstruct the unknown part using the primal output and known information\n(e.g., reversing math solutions to recover hidden variables), broadening\napplicability to non-invertible tasks. The quality of this reconstruction\nserves as a self-supervised reward to optimize the primal task, synergizing\nwith LLMs' ability to instantiate both tasks via a single model. Empirically,\nDuPO achieves substantial gains across diverse tasks: it enhances the average\ntranslation quality by 2.13 COMET over 756 directions, boosts the mathematical\nreasoning accuracy by an average of 6.4 points on three challenge benchmarks,\nand enhances performance by 9.3 points as an inference-time reranker (trading\ncomputation for accuracy). These results position DuPO as a scalable, general,\nand annotation-free paradigm for LLM optimization.",
    "keywords_en": [
      "LLM",
      "Dual Preference Optimization",
      "Self-verification",
      "Dual Learning",
      "Annotation-free"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-20T06:31:18.000Z",
    "download_time": "2025-08-21 21:11:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14460.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.14460\", \"arxiv_url\": \"https://arxiv.org/abs/2508.14460\"}"
  },
  {
    "id": "2508.11987",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.11987",
    "title_en": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
    "summary_en": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\nFutureX, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.",
    "keywords_en": [
      "LLM Agents",
      "Future Prediction",
      "Live Benchmark",
      "Evaluation",
      "Dynamic Environment"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-16T08:54:08.000Z",
    "download_time": "2025-08-21 21:11:24",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11987.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.11987\", \"arxiv_url\": \"https://arxiv.org/abs/2508.11987\"}"
  },
  {
    "id": "2508.14879",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.14879",
    "title_en": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
    "summary_en": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.",
    "keywords_en": [
      "Point Clouds",
      "3D Reconstruction",
      "Large Language Models",
      "Code Generation",
      "Multimodal"
    ],
    "area_en": [
      "Large Language Model",
      "Multimodal",
      "Generative AI"
    ],
    "published_time": "2025-08-20T17:50:15.000Z",
    "download_time": "2025-08-21 21:11:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14879.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.14879\", \"arxiv_url\": \"https://arxiv.org/abs/2508.14879\"}"
  },
  {
    "id": "2508.14811",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.14811",
    "title_en": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From\n  Sparse Inputs without Per-Scene Optimization",
    "summary_en": "We introduce Tinker, a versatile framework for high-fidelity 3D editing that\noperates in both one-shot and few-shot regimes without any per-scene\nfinetuning. Unlike prior techniques that demand extensive per-scene\noptimization to ensure multi-view consistency or to produce dozens of\nconsistent edited input views, Tinker delivers robust, multi-view consistent\nedits from as few as one or two images. This capability stems from repurposing\npretrained diffusion models, which unlocks their latent 3D awareness. To drive\nresearch in this space, we curate the first large-scale multi-view editing\ndataset and data pipeline, spanning diverse scenes and styles. Building on this\ndataset, we develop our framework capable of generating multi-view consistent\nedited views without per-scene training, which consists of two novel\ncomponents: (1) Referring multi-view editor: Enables precise, reference-driven\nedits that remain coherent across all viewpoints. (2) Any-view-to-video\nsynthesizer: Leverages spatial-temporal priors from video diffusion to perform\nhigh-quality scene completion and novel-view generation even from sparse\ninputs. Through extensive experiments, Tinker significantly reduces the barrier\nto generalizable 3D content creation, achieving state-of-the-art performance on\nediting, novel-view synthesis, and rendering enhancement tasks. We believe that\nTinker represents a key step towards truly scalable, zero-shot 3D editing.\nProject webpage: https://aim-uofa.github.io/Tinker",
    "keywords_en": [
      "3D editing",
      "Diffusion models",
      "Multi-view consistency",
      "Sparse inputs",
      "Novel-view synthesis"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-08-20T16:02:59.000Z",
    "download_time": "2025-08-21 21:11:27",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14811.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.14811\", \"arxiv_url\": \"https://arxiv.org/abs/2508.14811\"}"
  },
  {
    "id": "2508.14704",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.14704",
    "title_en": "MCP-Universe: Benchmarking Large Language Models with Real-World Model\n  Context Protocol Servers",
    "summary_en": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem.",
    "keywords_en": [
      "Large Language Models",
      "Model Context Protocol",
      "Benchmarking",
      "AI Agents",
      "Evaluation Framework"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-20T13:28:58.000Z",
    "download_time": "2025-08-21 21:11:27",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14704.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.14704\", \"arxiv_url\": \"https://arxiv.org/abs/2508.14704\"}"
  },
  {
    "id": "2508.14111",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.14111",
    "title_en": "From AI for Science to Agentic Science: A Survey on Autonomous\n  Scientific Discovery",
    "summary_en": "Artificial intelligence (AI) is reshaping scientific discovery, evolving from\nspecialized computational tools into autonomous research partners. We position\nAgentic Science as a pivotal stage within the broader AI for Science paradigm,\nwhere AI systems progress from partial assistance to full scientific agency.\nEnabled by large language models (LLMs), multimodal systems, and integrated\nresearch platforms, agentic AI shows capabilities in hypothesis generation,\nexperimental design, execution, analysis, and iterative refinement -- behaviors\nonce regarded as uniquely human. This survey provides a domain-oriented review\nof autonomous scientific discovery across life sciences, chemistry, materials\nscience, and physics. We unify three previously fragmented perspectives --\nprocess-oriented, autonomy-oriented, and mechanism-oriented -- through a\ncomprehensive framework that connects foundational capabilities, core\nprocesses, and domain-specific realizations. Building on this framework, we (i)\ntrace the evolution of AI for Science, (ii) identify five core capabilities\nunderpinning scientific agency, (iii) model discovery as a dynamic four-stage\nworkflow, (iv) review applications across the above domains, and (v) synthesize\nkey challenges and future opportunities. This work establishes a\ndomain-oriented synthesis of autonomous scientific discovery and positions\nAgentic Science as a structured paradigm for advancing AI-driven research.",
    "keywords_en": [
      "Agentic Science",
      "Autonomous Scientific Discovery",
      "AI for Science",
      "Large Language Models",
      "Scientific Agency"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-18T05:25:54.000Z",
    "download_time": "2025-08-21 21:11:25",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14111.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.14111\", \"arxiv_url\": \"https://arxiv.org/abs/2508.14111\"}"
  }
]