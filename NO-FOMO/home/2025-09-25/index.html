<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-09-25</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="." class="active">‰∏≠Êñá</a>
                <a href="en/" class="">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-09-25</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† ËøîÂõû‰∏ªÈ°µ</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ ÊúÄÊñ∞Êó•Êä•</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ ÂÖ≥‰∫éÊàë‰ª¨</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Ollama Web Search</h2>
                <span class="published-time">Published: 2025-09-25 19:21:52</span>
                
                <p class="summary">Ollama has announced a significant enhancement to its platform with the introduction of a built-in web search capability, designed to empower locally run large language models (LLMs). This innovative feature allows users to provide their local LLMs with real-time access to information directly from the internet, effectively overcoming the inherent knowledge cut-off limitations of models trained on static datasets. By integrating web search, Ollama facilitates advanced retrieval-augmented generation (RAG), enabling LLMs to produce more accurate, contextually relevant, and up-to-date responses. This development is pivotal for creating more dynamic and capable AI agents that can operate with improved autonomy and access current information without relying exclusively on their pre-trained knowledge base. The update is anticipated to substantially broaden the practical applications of local LLM deployments, from enhancing research and development workflows to improving personalized information retrieval and fostering more sophisticated AI experiences in edge computing environments.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Ollama</span><span>Large Language Model</span><span>Web Search</span><span>Retrieval Augmented Generation</span><span>Local AI</span><span>AI Agent</span><span>Edge Computing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://ollama.com/blog/web-search" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Improved Gemini 2.5 Flash and Flash-Lite</h2>
                <span class="published-time">Published: 2025-09-25 17:20:56</span>
                
                <p class="summary">Google has announced significant advancements to its Gemini 2.5 Flash and Flash-Lite models, introducing enhanced versions designed to offer superior performance and efficiency for developers. These updated large language models (LLMs) are part of Google's ongoing commitment to delivering cutting-edge AI capabilities. The "Flash" designation typically indicates a focus on high-speed, low-latency applications, making these models ideal for real-time interactions, rapid content generation, and scalable deployment in various AI-powered services. The improvements likely encompass optimizations in processing speed, reduced computational cost, and potentially expanded context windows, allowing for more complex and sophisticated applications. Developers can anticipate a more robust and responsive toolkit for integrating advanced natural language understanding and generation into their projects. This release underscores the continuous evolution of Google's Gemini family, aiming to provide flexible and powerful AI solutions across a spectrum of use cases, from enterprise applications to innovative consumer experiences. The emphasis on "Flash-Lite" further suggests a highly optimized, lightweight variant, perfect for resource-constrained environments or applications requiring extreme efficiency, without sacrificing core capabilities. This strategic update reinforces Google's position in the competitive AI landscape, offering developers versatile and high-performing LLM options.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini</span><span>Large Language Model</span><span>AI Models</span><span>Generative AI</span><span>Machine Learning</span><span>Model Optimization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ChatGPT Pulse</h2>
                <span class="published-time">Published: 2025-09-25 16:59:55</span>
                
                <p class="summary">OpenAI has officially launched 'ChatGPT Pulse', a new development signaling significant advancements for its renowned conversational AI model. While specific technical deep-dives are anticipated, the introduction suggests a focus on elevating ChatGPT's real-time operational understanding and dynamic responsiveness. This initiative likely incorporates enhanced monitoring frameworks and continuous learning methodologies, enabling the model to adapt more swiftly to emerging data trends, user feedback, and nuanced conversational contexts. 'ChatGPT Pulse' could represent a refined mechanism for integrating up-to-the-minute information, improving factual accuracy, and reducing latency in responses, thereby boosting the overall performance and reliability of the large language model. This strategic move underscores OpenAI's commitment to pushing the frontiers of artificial intelligence, ensuring ChatGPT remains at the forefront of generative AI applications and continues to deliver highly relevant and engaging user experiences across diverse interactive platforms. Future announcements are expected to detail the technical underpinnings and practical benefits of this latest evolution.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>ChatGPT</span><span>Large Language Model</span><span>Generative AI</span><span>OpenAI</span><span>AI Development</span><span>Conversational AI</span><span>AI Performance</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://openai.com/index/introducing-chatgpt-pulse/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GDPVal: Measuring the performance of our models on real-world tasks</h2>
                <span class="published-time">Published: 2025-09-25 16:55:48</span>
                
                <p class="summary">OpenAI has introduced GDPVal, a novel framework designed for comprehensively evaluating the practical performance of their AI models in real-world scenarios. This initiative aims to address the limitations of traditional academic benchmarks, which often fail to fully capture a model's effectiveness when deployed in complex, dynamic, and user-centric applications. GDPVal focuses on assessing how well AI systems can execute multi-step tasks, handle ambiguous instructions, and adapt to unforeseen challenges encountered in live environments. By simulating genuine use-cases and measuring outcomes that directly impact real-world applications, GDPVal seeks to provide a more accurate and holistic understanding of model capabilities and shortcomings. This approach is crucial for accelerating the development of more robust, reliable, and practically valuable AI systems, ensuring that advancements in AI research translate effectively into tangible benefits. The framework's emphasis on practical utility marks a significant step towards bridging the gap between theoretical performance and operational excellence in artificial intelligence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Evaluation</span><span>Model Performance</span><span>Real-World AI</span><span>Benchmarks</span><span>OpenAI</span><span>AI Metrics</span><span>Task-Oriented Evaluation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://openai.com/index/gdpval/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Gemini Robotics 1.5 brings AI agents into the physical world</h2>
                <span class="published-time">Published: 2025-09-25 16:00:36</span>
                
                <p class="summary">Google DeepMind's Gemini Robotics 1.5 represents a significant leap forward in integrating advanced AI agents directly into physical robotic systems. This iteration is designed to bridge the chasm between cutting-edge artificial intelligence capabilities and their tangible application in the real world, empowering robots to perform more complex, nuanced, and adaptive tasks within dynamic environments. By leveraging the sophisticated understanding and reasoning power inherent in the Gemini AI model, these robotic systems are expected to exhibit enhanced situational awareness, improved decision-making, and greater autonomy when interacting with their surroundings. The core objective is to equip robots with the intelligence required to execute intricate instructions, learn continuously from new experiences, and operate effectively in unstructured, unpredictable settings, moving beyond conventional pre-programmed actions. This development is crucial for advancing the deployment of highly versatile and intelligent robotic solutions across a diverse range of sectors, from sophisticated industrial automation to responsive service applications, by seamlessly bringing state-of-the-art AI into the physical domain.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Robotics</span><span>AI Agents</span><span>Gemini</span><span>Autonomous Systems</span><span>Physical World Interaction</span><span>Robot Control</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>AI Agent</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Launch HN: Webhound (YC S23) ‚Äì Research agent that builds datasets from the web</h2>
                <span class="published-time">Published: 2025-09-25 14:28:24</span>
                
                <p class="summary">Webhound, an AI agent developed by a YC S23 team, is launched as an innovative solution for automating web-based data collection and dataset generation. The platform enables users to define their specific research objectives through natural language prompts. Following this, the agent autonomously determines the most suitable data structure, identifies pertinent online sources, executes comprehensive searches, meticulously extracts relevant information, and compiles the organized results into an exportable CSV format. This technology directly addresses the significant inefficiencies inherent in traditional manual web research, a laborious process often involving the management of numerous browser tabs and the tedious transcription of information into spreadsheets. A special no-signup version has been made accessible for the Hacker News community, alongside a publicly available demo, underscoring Webhound's potential to dramatically streamline and simplify the creation of structured datasets from a wide array of web content for researchers, analysts, and businesses alike.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agent</span><span>Dataset Generation</span><span>Web Data Extraction</span><span>Natural Language Processing</span><span>Automated Research</span><span>Data Structuring</span><span>Information Retrieval</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://news.ycombinator.com/item?id=45373008" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>HumanLayer</h2>
                <span class="published-time">Published: 2025-09-25T14:58:56Z</span>
                
                <p class="summary">HumanLayer is an open-source project designed to integrate human oversight into high-stakes function calls within AI agentic workflows. It addresses the critical challenge of ensuring reliability and safety when LLMs interact with sensitive real-world systems, where even minor errors can have significant consequences. The platform offers tools like `@require_approval` and `human_as_tool` to deterministically embed a human-in-the-loop, guaranteeing review for operations such as modifying private data or company communications. This approach is crucial for deploying AI agents in scenarios where 90% accuracy is insufficient. HumanLayer aims to enable the next generation of "Autonomous Agents" (Gen 3) to operate effectively in an "outer loop" without constant human initiation, while maintaining essential human review for critical tasks. It supports complex applications like LinkedIn inbox assistants and customer onboarding, facilitating the secure and impactful adoption of AI in automated workflows.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Human-in-the-loop</span><span>Function Calling</span><span>LLMs</span><span>Autonomous Agents</span><span>Agentic Workflows</span><span>Human Oversight</span><span>High-Stakes Functions</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/humanlayer/humanlayer" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AI is the best foreign language teacher in the world today, and Enjoy is the best AI assistant.</h2>
                <span class="published-time">Published: 2025-04-13T01:42:40Z</span>
                
                <p class="summary">The Enjoy project introduces an advanced AI-powered assistant dedicated to foreign language learning, particularly English. Advocating the belief that AI serves as the most effective language teacher, Enjoy provides a robust platform through both web and desktop applications. Its core functionalities are designed to significantly improve spoken English proficiency. Users can engage in extensive audio practice, utilize shadow reading techniques for pronunciation refinement, and receive AI-driven assessments that offer precise feedback. The platform also incorporates an interactive chat feature for conversational practice and deeper engagement. Enjoy is built upon structured learning methodologies, offering comprehensive training tasks, detailed guides on American English phonetics, and self-training frameworks, drawing inspiration from the "One Thousand Hours" and "Everyone Can Use English" (2010) programs. This project aims to empower learners with intelligent tools for guided practice and continuous improvement in their language acquisition journey.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Assistant</span><span>Language Learning</span><span>English Education</span><span>Speech Recognition</span><span>AI Assessment</span><span>Shadow Reading</span><span>Web Application</span><span>Desktop Application</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/ZuodaoTech/everyone-can-use-english" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Video models are zero-shot learners and reasoners</h2>
                <span class="published-time">Published: 2025-09-24T17:17:27.000Z</span>
                
                <p class="summary">The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Zero-shot learning</span><span>Generative video models</span><span>Vision understanding</span><span>Visual reasoning</span><span>Foundation models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Video Understanding</span><span>Computer Vision</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.20328" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SIM-CoT: Supervised Implicit Chain-of-Thought</h2>
                <span class="published-time">Published: 2025-09-24T17:01:32.000Z</span>
                
                <p class="summary">Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Implicit Chain-of-Thought</span><span>Large Language Models</span><span>Step-level supervision</span><span>Latent representations</span><span>Auxiliary decoder</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.20317" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Advancing Speech Understanding in Speech-Aware Language Models with GRPO</h2>
                <span class="published-time">Published: 2025-09-21T09:09:36.000Z</span>
                
                <p class="summary">In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based method for training Speech-Aware Large Language Models (SALLMs) on open-format speech understanding tasks, such as Spoken Question Answering and Automatic Speech Translation. SALLMs have proven highly effective for speech understanding tasks. GRPO has recently gained traction for its efficiency in training LLMs, and prior work has explored its application to SALLMs, primarily in multiple-choice tasks. Building on this, we focus on open-format tasks that better reflect the generative abilities of the models. Our approach leverages GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate empirically that it surpasses standard SFT across several key metrics. Finally, we explore the potential of incorporating off-policy samples within GRPO for these tasks, highlighting avenues for further improvement and further research.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Speech Understanding</span><span>Speech-Aware Language Models</span><span>Group Relative Policy Optimization</span><span>Spoken Question Answering</span><span>Automatic Speech Translation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.16990" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning</h2>
                <span class="published-time">Published: 2025-09-24T17:59:30.000Z</span>
                
                <p class="summary">Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Unified Framework</span><span>Image and Video Editing</span><span>Image and Video Generation</span><span>In-Context Learning</span><span>Multimodal</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.20360" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation</h2>
                <span class="published-time">Published: 2025-09-23T17:05:46.000Z</span>
                
                <p class="summary">We propose Lavida-O, a unified Masked Diffusion Model (MDM) for multimodal understanding and generation. Unlike existing multimodal MDMs such as MMaDa and Muddit which only support simple image-level understanding tasks and low-resolution image generation, Lavida-O presents a single framework that enables image-level understanding, object grounding, image editing, and high-resolution (1024px) text-to-image synthesis. Lavida-O incorporates a novel Elastic Mixture-of-Transformers (Elastic-MoT) architecture that couples a lightweight generation branch with a larger understanding branch, supported by token compression, universal text conditioning and stratified sampling for efficient and high-quality generation. Lavida-O further incorporates planning and iterative self-reflection in image generation and editing tasks, seamlessly boosting generation quality with its understanding capabilities. Lavida-O achieves state-of-the-art performance on a wide range of benchmarks including RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming existing autoregressive models and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while offering considerable speedup at inference. These advances establish Lavida-O as a new paradigm for scalable multimodal reasoning and generation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Masked Diffusion Models</span><span>Multimodal</span><span>Generative AI</span><span>Image Editing</span><span>Object Grounding</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Generative AI</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.19244" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub</h2>
                <span class="published-time">Published: 2025-09-18T08:48:32.000Z</span>
                
                <p class="summary">Large language models (LLMs) are increasingly being integrated into software development processes. The ability to generate code and submit pull requests with minimal human intervention, through the use of autonomous AI agents, is poised to become a standard practice. However, little is known about the practical usefulness of these pull requests and the extent to which their contributions are accepted in real-world projects. In this paper, we empirically study 567 GitHub pull requests (PRs) generated using Claude Code, an agentic coding tool, across 157 diverse open-source projects. Our analysis reveals that developers tend to rely on agents for tasks such as refactoring, documentation, and testing. The results indicate that 83.8% of these agent-assisted PRs are eventually accepted and merged by project maintainers, with 54.9% of the merged PRs are integrated without further modification. The remaining 45.1% require additional changes benefit from human revisions, especially for bug fixes, documentation, and adherence to project-specific standards. These findings suggest that while agent-assisted PRs are largely acceptable, they still benefit from human oversight and refinement.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Agentic Coding</span><span>Large Language Model</span><span>AI Agent</span><span>Pull Requests</span><span>Software Development</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2509.14745" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>