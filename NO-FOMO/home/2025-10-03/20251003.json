[
  {
    "id": "hackernews_45465392",
    "source": "Hacker News",
    "url": "https://buttondown.com/blog/ringo-email-as-an-ai-interface",
    "title": "Email was the user interface for the first AI recommendation engines",
    "summary": "This article highlights the often-overlooked historical role of email as a foundational user interface for early artificial intelligence recommendation engines. Before the proliferation of sophisticated web interfaces and mobile applications, email served as a primary conduit for user interaction with nascent AI systems, particularly in personalized content delivery. A notable example, Ringo, a pioneering music recommendation service from the mid-1990s, leveraged email to gather user preferences and deliver tailored suggestions. Users would send emails with their musical tastes, and the system would respond with recommendations, effectively using email as a text-based conversational interface. This historical context underscores how early AI adoption was often constrained by the prevailing communication technologies, demonstrating an innovative adaptation to existing infrastructure to provide personalized experiences. It emphasizes that the core concept of AI-driven personalization predates modern platforms, with email playing a crucial enabling role in the development and initial deployment of such intelligent systems.",
    "keywords": [
      "AI History",
      "Recommendation Systems",
      "Email Interface",
      "Early AI",
      "User Interface",
      "Personalization"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Others"
    ],
    "published_time": "2025-10-03 17:27:02",
    "download_time": "2025-10-03 20:02:29",
    "extra_info": "{\"score\": 26, \"by\": \"coloneltcb\", \"descendants\": 4, \"story_id\": 45465392}"
  },
  {
    "id": "hackernews_45462297",
    "source": "Hacker News",
    "url": "https://photoblog.nk412.com/Faroe2025/Faroes/n-cPCNFr",
    "title": "The Faroes",
    "summary": "Recent advancements in artificial intelligence research continue to push the boundaries of what's possible, particularly in the integration of diverse data modalities. Scientists are exploring novel neural architectures that can effectively process and synthesize information from disparate sources, leading to more robust and generalized AI models. A key focus remains on developing algorithms that not only achieve high performance but also exhibit explainability and ethical considerations, addressing concerns around bias and transparency. The interdisciplinary nature of modern AI necessitates collaborations across fields such as computer science, cognitive psychology, and neuroscience to unlock new paradigms in machine intelligence. Challenges persist in scaling these sophisticated models to real-world applications while maintaining computational efficiency and ensuring data privacy. Future work aims at creating adaptable AI systems capable of continuous learning and reasoning in dynamic environments, moving closer to truly autonomous and intelligent agents. Furthermore, the push for more sustainable AI development is gaining traction, with researchers investigating methods to reduce the energy consumption of large-scale models. This holistic approach ensures that progress in AI is both innovative and responsible, setting the stage for transformative impacts across various sectors.",
    "keywords": [
      "Artificial Intelligence",
      "Neural Networks",
      "Data Modalities",
      "Explainable AI",
      "Ethical AI",
      "Machine Learning",
      "Autonomous Agents"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "AI Agent"
    ],
    "published_time": "2025-10-03 12:41:03",
    "download_time": "2025-10-03 20:02:49",
    "extra_info": "{\"score\": 113, \"by\": \"speckx\", \"descendants\": 50, \"story_id\": 45462297}"
  },
  {
    "id": "hackernews_45458948",
    "source": "Hacker News",
    "url": "https://github.com/triton-lang/triton/pull/7298",
    "title": "Fp8 runs ~100 tflops faster when the kernel name has \"cutlass\" in it",
    "summary": "A significant performance anomaly has been identified within the Triton deep learning compiler framework, revealing that FP8 (8-bit floating-point) computations can run approximately 100 TFLOPS faster when the kernel's name contains the substring \"cutlass\". This discovery, detailed in a GitHub pull request, points to an unexpected interaction between naming conventions and the underlying compilation or dispatch mechanisms. The 'Cutlass' library is a widely recognized resource for highly optimized GPU linear algebra routines, often serving as a critical backend for high-performance tensor operations in deep learning. The observed performance gain is substantial, indicating a potential latent optimization path or, conversely, an unidentified bug that disproportionately affects performance based on kernel nomenclature. This finding is crucial for developers and researchers working with Triton, particularly those focused on maximizing throughput for AI and machine learning workloads, as it offers an unusual but effective method for immediate performance improvements. Further in-depth analysis is essential to pinpoint the exact root cause of this naming-dependent acceleration and to integrate such optimizations systematically across the framework, ensuring predictable and consistent high performance regardless of kernel naming.",
    "keywords": [
      "FP8",
      "TFLOPS",
      "GPU Performance",
      "Triton",
      "Cutlass",
      "Kernel Optimization",
      "Deep Learning Compilers",
      "High-Performance Computing"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-10-03 04:21:23",
    "download_time": "2025-10-03 20:02:47",
    "extra_info": "{\"score\": 316, \"by\": \"mmastrac\", \"descendants\": 139, \"story_id\": 45458948}"
  },
  {
    "id": "hackernews_45463642",
    "source": "Hacker News",
    "url": "https://www.cnbc.com/2025/10/01/microsoft-wants-to-mainly-use-its-own-ai-chips-in-the-future.html",
    "title": "Microsoft CTO says he wants to swap most AMD and Nvidia GPUs for homemade chips",
    "summary": "Microsoft's Chief Technology Officer has publicly stated the company's ambition to replace a substantial portion of its graphics processing unit (GPU) inventory, currently sourced from industry leaders AMD and Nvidia, with proprietary, homemade artificial intelligence chips. This strategic pivot signals a significant trend among hyperscale cloud providers and AI giants to bring chip design in-house, seeking to gain more control over their hardware infrastructure, enhance performance tailored to specific AI workloads, and potentially achieve cost efficiencies in the long term. By developing custom silicon, Microsoft aims to optimize its extensive cloud services and AI initiatives, including large language models and other machine learning applications. This shift would not only reduce dependence on external suppliers but also enable deeper integration between software and hardware, potentially unlocking new levels of innovation and efficiency for its AI offerings. The move is indicative of a broader industry shift towards vertical integration in the AI stack, which could reshape the competitive landscape for both chip manufacturers and AI service providers.",
    "keywords": [
      "Microsoft",
      "AI Chips",
      "Custom Silicon",
      "GPU",
      "Hardware Development",
      "Semiconductor Industry",
      "Artificial Intelligence",
      "Cloud Computing"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-10-03 14:48:36",
    "download_time": "2025-10-03 20:02:50",
    "extra_info": "{\"score\": 151, \"by\": \"fork-bomber\", \"descendants\": 107, \"story_id\": 45463642}"
  },
  {
    "id": "hackernews_45465964",
    "source": "Hacker News",
    "url": "https://www.wired.com/story/ice-social-media-surveillance-24-7-contract/",
    "title": "ICE Wants to Build Out a 24/7 Social Media Surveillance Team",
    "summary": "The U.S. Immigration and Customs Enforcement (ICE) agency is reportedly pursuing plans to establish a 24/7 social media surveillance team, indicating a significant expansion of its digital monitoring capabilities. This initiative seeks to continuously scan and analyze publicly available information across various social media platforms and other online sources. The proposed system would likely integrate advanced technological solutions, including sophisticated data analytics, artificial intelligence, and natural language processing, to identify trends, track individuals, and gather intelligence for enforcement purposes. This constant digital vigilance raises substantial concerns regarding privacy, civil liberties, and the potential for algorithmic bias in government operations. The move signifies an increasing reliance on automated and persistent online scrutiny as a tool for law enforcement and intelligence gathering, sparking debate about the ethical boundaries of surveillance in a democratic society. It also underscores the evolving technical landscape where agencies are seeking to harness vast quantities of online data for continuous intelligence collection and threat assessment, necessitating a critical examination of oversight mechanisms and data protection protocols.",
    "keywords": [
      "Social Media Surveillance",
      "Data Analytics",
      "Government Monitoring",
      "Natural Language Processing",
      "Artificial Intelligence",
      "Privacy Concerns",
      "Public Information Analysis"
    ],
    "area": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Machine Learning"
    ],
    "published_time": "2025-10-03 18:13:16",
    "download_time": "2025-10-03 20:03:07",
    "extra_info": "{\"score\": 35, \"by\": \"loteck\", \"descendants\": 3, \"story_id\": 45465964}"
  },
  {
    "id": "hackernews_45466244",
    "source": "Hacker News",
    "url": "https://stallman.org/chatgpt.html",
    "title": "Reasons to Not Use ChatGPT",
    "summary": "Richard Stallman, a seminal figure and fervent advocate for the free software movement, articulates compelling reasons to reconsider the adoption and use of ChatGPT, a prominent large language model developed by OpenAI. Although the input content provided is notably brief, Stallman's established philosophical framework strongly indicates that his arguments against such technologies fundamentally revolve around issues of proprietary control, user autonomy, and data privacy. His criticisms typically target systems that preclude users from examining, modifying, and freely distributing the software they utilize, championing principles of transparency and unrestricted access. Applied to ChatGPT, these concerns would encompass the absence of publicly available source code, the inherent proprietary nature of its underlying AI models, and the potential, unstated implications for user data management and surveillance. Stallman's stance encourages individuals and organizations to critically evaluate the ethical and philosophical ramifications inherent in relying on closed-source artificial intelligence systems. He emphasizes that such platforms may not inherently safeguard user freedom or data security, potentially eroding the foundational tenets of digital rights and independent computing.",
    "keywords": [
      "ChatGPT",
      "Large Language Model",
      "Proprietary Software",
      "Free Software",
      "Digital Rights",
      "Data Privacy",
      "Ethical AI",
      "User Freedom"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-03 18:38:40",
    "download_time": "2025-10-03 20:03:03",
    "extra_info": "{\"score\": 9, \"by\": \"sonderotis\", \"descendants\": 8, \"story_id\": 45466244}"
  },
  {
    "id": "TradingAgents-CN",
    "source": "GitHub",
    "url": "https://github.com/hsliuping/TradingAgents-CN",
    "title": "TradingAgents 中文增强版",
    "summary": "TradingAgents 中文增强版 is a multi-agent large language model (LLM) framework specifically designed for financial trading decisions, optimized for Chinese users. It provides comprehensive analysis capabilities for A-share, H-share, and US stock markets, integrating domestic LLMs like Baidu Qianfan (ERNIE), DashScope, and DeepSeek, alongside global models from Google AI and OpenAI. Key features include a multi-agent collaboration architecture with specialized analysts (fundamental, technical, news, social media), researchers, and a trader for intelligent decision-making and risk management. The project boasts a modern, responsive web interface built with Streamlit, offering real-time analysis progress, customizable research depths, multi-LLM provider support, and professional report export in various formats. Recent updates have significantly enhanced its LLM ecosystem, added intelligent news analysis, robust user permission management, and optimized developer experience with a complete toolchain and comprehensive documentation. Docker deployment is recommended for easy setup, offering high-performance data storage with MongoDB and Redis.",
    "keywords": [
      "Large Language Model",
      "AI Agent",
      "Financial Trading",
      "Stock Analysis",
      "Multi-agent System",
      "Docker Deployment",
      "Data Management",
      "Streamlit"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-15T00:28:55Z",
    "download_time": "2024-05-20 17:07:00",
    "extra_info": null
  },
  {
    "id": "tunix",
    "source": "GitHub",
    "url": "https://github.com/google/tunix",
    "title": "Tunix: A JAX-native LLM Post-Training Library",
    "summary": "Tunix, short for Tune-in-JAX, is a JAX-native library engineered to streamline the post-training process for Large Language Models. Leveraging JAX for accelerated computation and seamless integration with Flax NNX, Tunix provides robust support across several critical domains: Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), Preference Fine-Tuning (PFT), and Knowledge Distillation (KD). For SFT, it offers both full weights and Parameter-Efficient Fine-Tuning (PEFT) using LoRA/Q-LoRA. Its RL capabilities include Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and Token-level Group Sequence Policy Optimization (GSPO-token), complemented by Direct Preference Optimization (DPO) for preference alignment. Knowledge Distillation features diverse strategies like logit, attention transfer, and feature projection, facilitating alignment between models of varying architectures. Tunix prioritizes modularity and efficiency, incorporating native support for distributed training with sharding strategies such as DP, FSDP, and TP, optimized for accelerators like TPUs. Currently in early development, it plans to expand with agentic RL training, advanced algorithms, and enhanced scalability. A notable collaboration with GRL integrates Tunix’s TPU optimization into a flexible game RL pipeline for scalable LLM research.",
    "keywords": [
      "JAX",
      "Large Language Model",
      "Fine-Tuning",
      "Reinforcement Learning",
      "Knowledge Distillation",
      "Parameter-Efficient Fine-Tuning",
      "Distributed Training"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-10-03T19:55:09Z",
    "download_time": "2024-05-18 07:05:00",
    "extra_info": null
  },
  {
    "id": "airweave",
    "source": "GitHub",
    "url": "https://github.com/airweave-ai/airweave",
    "title": "Make Any App Searchable for AI Agents",
    "summary": "Airweave is a robust platform designed to enable AI agents to search and interact with data across virtually any application. It achieves this by connecting to a diverse range of sources, including productivity tools, databases, and document stores, transforming their disparate contents into unified, semantically searchable knowledge bases. These knowledge bases are made accessible to AI agents through standardized interfaces such as REST API or MCP, effectively establishing a semantically searchable MCP server. The platform comprehensively manages the entire data pipeline, encompassing authentication, extraction, embedding, and serving of information. Key technical features include extensive data synchronization capabilities for over 25 different sources, intelligent entity extraction and transformation, a multi-tenant architecture with OAuth2, efficient incremental updates via content hashing, advanced semantic search functionalities tailored for agent queries, and robust data versioning. This empowers AI agents to intelligently retrieve and leverage information from various enterprise systems.",
    "keywords": [
      "AI Agents",
      "Semantic Search",
      "Data Synchronization",
      "Knowledge Bases",
      "REST API",
      "Vector Databases",
      "LLM Applications",
      "Data Integration"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-03T18:18:05Z",
    "download_time": "2024-07-30 10:00:00",
    "extra_info": null
  },
  {
    "id": "2510.02283",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.02283",
    "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
    "summary": "Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/",
    "keywords": [
      "Video Generation",
      "Diffusion Models",
      "Long Video Generation",
      "Temporal Consistency",
      "Autoregressive Models"
    ],
    "area": [
      "Deep Learning",
      "Computer Vision",
      "Generative AI"
    ],
    "published_time": "2025-10-02T17:55:42.000Z",
    "download_time": "2025-10-03 13:03:45",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.02283\", \"arxiv_url\": \"https://arxiv.org/abs/2510.02283\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02283.png\", \"original_title\": \"Self-Forcing++: Towards Minute-Scale High-Quality Video Generation\"}"
  },
  {
    "id": "2510.00446",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.00446",
    "title": "LongCodeZip: Compress Long Context for Code Language Models",
    "summary": "Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.",
    "keywords": [
      "Code Language Models",
      "Context Compression",
      "Large Language Models",
      "Context Pruning",
      "Code Generation"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-10-01T02:54:57.000Z",
    "download_time": "2025-10-03 13:03:50",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.00446\", \"arxiv_url\": \"https://arxiv.org/abs/2510.00446\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00446.png\", \"original_title\": \"LongCodeZip: Compress Long Context for Code Language Models\"}"
  },
  {
    "id": "2510.01538",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.01538",
    "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
    "summary": "Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. A general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics and self-planning over the input; Forecaster performs model fitting and validation and, based on the results, adaptively selects the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into a comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into a white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces a clear and rigorous report that makes the forecasting workflow more transparent and interpretable.",
    "keywords": [
      "Time Series Analysis",
      "AI Agent",
      "Large Language Models",
      "Forecasting",
      "Machine Learning"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-10-02T00:18:59.000Z",
    "download_time": "2025-10-03 13:03:54",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.01538\", \"arxiv_url\": \"https://arxiv.org/abs/2510.01538\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01538.png\", \"original_title\": \"TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis\"}"
  },
  {
    "id": "2510.01623",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.01623",
    "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.",
    "keywords": [
      "Vision-Language-Action Models",
      "Reasoning",
      "Reinforcement Learning",
      "Embodied AI",
      "Robotics"
    ],
    "area": [
      "Multimodal",
      "Robotics",
      "AI Agent"
    ],
    "published_time": "2025-10-02T02:54:03.000Z",
    "download_time": "2025-10-03 13:03:49",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.01623\", \"arxiv_url\": \"https://arxiv.org/abs/2510.01623\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01623.png\", \"original_title\": \"VLA-R1: Enhancing Reasoning in Vision-Language-Action Models\"}"
  },
  {
    "id": "2509.22067",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.22067",
    "title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety",
    "summary": "Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a model's hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign features from a sparse autoencoder (SAE), a common source of interpretable directions, increases these rates by a further 2-4%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.",
    "keywords": [
      "Activation steering",
      "LLM safety",
      "Model alignment",
      "Harmful compliance",
      "Jailbreak"
    ],
    "area": [
      "Large Language Model",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-09-26T08:49:47.000Z",
    "download_time": "2025-10-03 13:03:46",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.22067\", \"arxiv_url\": \"https://arxiv.org/abs/2509.22067\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22067.png\", \"original_title\": \"The Rogue Scalpel: Activation Steering Compromises LLM Safety\"}"
  },
  {
    "id": "2510.01265",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.01265",
    "title": "RLP: Reinforcement as a Pretraining Objective",
    "summary": "The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes.",
    "keywords": [
      "Reinforcement Learning",
      "Pretraining",
      "Chain-of-Thought",
      "Large Language Models",
      "Reasoning"
    ],
    "area": [
      "Large Language Model",
      "Deep Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-09-26T17:53:54.000Z",
    "download_time": "2025-10-03 13:03:48",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.01265\", \"arxiv_url\": \"https://arxiv.org/abs/2510.01265\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01265.png\", \"original_title\": \"RLP: Reinforcement as a Pretraining Objective\"}"
  }
]