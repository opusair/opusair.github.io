[
  {
    "id": "twitter_sama_1963366714684707120",
    "source": "Twitter",
    "url": "https://x.com/sama/status/1963366714684707120",
    "title_en": "sama_Sam Altman Notes Surge in LLM-Driven Twitter Accounts",
    "summary_en": "Sam Altman, CEO of OpenAI, tweeted that he previously didn't take the \"dead internet theory\" seriously, but now observes a significant surge in Twitter accounts operated by large language models (LLMs). This phenomenon prompts him to reconsider the theory, suggesting that AI-generated content might be altering the internet's ecosystem and raising new questions about information authenticity and platform content composition.",
    "keywords_en": [
      "Sam Altman",
      "LLM",
      "Twitter",
      "Dead Internet Theory",
      "AI-generated Content"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Industry News"
    ],
    "published_time": "2025-09-03T22:21:17.000Z",
    "download_time": "2025-09-04 03:12:39",
    "visual_resource": [
      "screenshot/twitter/sama_1963366714684707120.png"
    ],
    "extra_info": "{\"username\": \"sama\", \"tweet_id\": \"1963366714684707120\"}"
  },
  {
    "id": "twitter_mathemagic1an_1963273618705482155",
    "source": "Twitter",
    "url": "https://twitter.com/mathemagic1an/status/1963273618705482155",
    "title_en": "mathemagic1an_Agent/Client Protocol (ACP) Unveiled",
    "summary_en": "Jay Hack introduced the Agent/Client Protocol (ACP) from the zeddotdev team, designed to manage interactions between AI agents and Integrated Development Environments (IDEs), similar to an LSP. Supporting Claude Code and Gemini CLI, ACP signals a future trend where human UIs will be decoupled from CLI agent operations via open protocols, advancing AI development paradigms.",
    "keywords_en": [
      "Agent/Client Protocol",
      "AI Agent",
      "IDE",
      "Open Protocol",
      "zeddotdev"
    ],
    "area_en": [
      "AI Agent",
      "Tech News",
      "Open Source"
    ],
    "published_time": "2025-09-03T16:11:21.000Z",
    "download_time": "2025-09-04 07:50:31",
    "visual_resource": [
      "screenshot/twitter/mathemagic1an_1963273618705482155.png"
    ],
    "extra_info": "{\"username\": \"mathemagic1an\", \"tweet_id\": \"1963273618705482155\"}"
  },
  {
    "id": "twitter_LangChainAI_1963285794954907750",
    "source": "Twitter",
    "url": "https://twitter.com/LangChainAI/status/1963285794954907750",
    "title_en": "LangChainAI_LangChain 1.0 Alpha Release Unifies LLM Interfaces",
    "summary_en": "LangChain has officially released the alpha preview of its 1.0 version, introducing significant improvements in standardizing content across various Large Language Model (LLM) providers. This includes enhanced capabilities for reasoning, citations, tool calls, and handling multimodal data. The new release aims to provide a single, consistent interface, thereby eliminating the need for developers to juggle multiple APIs. This standardization greatly simplifies the development workflow and enhances efficiency for building applications powered by LLMs.",
    "keywords_en": [
      "LangChain",
      "Large Language Model",
      "Standardization",
      "Tool Calls",
      "Multimodal",
      "Product Launch"
    ],
    "area_en": [
      "Product Launch",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-09-03T16:59:44.000Z",
    "download_time": "2025-09-04 07:50:35",
    "visual_resource": [
      "screenshot/twitter/LangChainAI_1963285794954907750.png"
    ],
    "extra_info": "{\"username\": \"LangChainAI\", \"tweet_id\": \"1963285794954907750\"}"
  },
  {
    "id": "twitter_TransluceAI_1963286326062846094",
    "source": "Twitter",
    "url": "https://twitter.com/TransluceAI/status/1963286326062846094",
    "title_en": "TransluceAI_AI Agents Successfully Jailbreak Frontier LLMs",
    "summary_en": "Transluce AI announced that its trained investigator agents can effectively identify and exploit specific behaviors in other models. Their research demonstrates that this approach scales to frontier large language models, with an 8B investigator model successfully jailbreaking advanced systems like GPT-5, Claude Opus 4.1, and Gemini 2.5 Pro. This finding underscores the significant potential of using smaller, specialized AI agents for automated red-teaming, offering a scalable method to evaluate and enhance the security and robustness of cutting-edge AI models.",
    "keywords_en": [
      "Transluce AI",
      "AI Agent",
      "Large Language Model",
      "Jailbreak",
      "Security",
      "Red Teaming"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-09-03T17:01:50.000Z",
    "download_time": "2025-09-04 07:51:05",
    "visual_resource": [
      "screenshot/twitter/TransluceAI_1963286326062846094.png"
    ],
    "extra_info": "{\"username\": \"TransluceAI\", \"tweet_id\": \"1963286326062846094\"}"
  },
  {
    "id": "twitter_shawnup_1963335514377130397",
    "source": "Twitter",
    "url": "https://twitter.com/shawnup/status/1963335514377130397",
    "title_en": "shawnup_Acquires OpenPipe: Small Models Outperform Foundation Models via RL",
    "summary_en": "Shawn Lewis announced the successful acquisition of OpenPipe. OpenPipe's ART framework leverages reinforcement learning, enabling smaller open models to easily outperform large foundation models on real-world problems. This acquisition aims to enhance the performance of small models on specific tasks through the ART framework, providing more efficient and cost-effective AI solutions and promoting the democratization of AI technology.",
    "keywords_en": [
      "OpenPipe",
      "ART Framework",
      "Reinforcement Learning",
      "Small Models",
      "Foundation Models",
      "Acquisition"
    ],
    "area_en": [
      "Machine Learning",
      "Tech News",
      "Industry News"
    ],
    "published_time": "2025-09-03T20:17:18.000Z",
    "download_time": "2025-09-04 07:51:23",
    "visual_resource": [
      "screenshot/twitter/shawnup_1963335514377130397.png"
    ],
    "extra_info": "{\"username\": \"shawnup\", \"tweet_id\": \"1963335514377130397\"}"
  },
  {
    "id": "twitter_techhalla_1963333488217919668",
    "source": "Twitter",
    "url": "https://twitter.com/techhalla/status/1963333488217919668",
    "title_en": "techhalla_Creative Video Creation with Freepik Visual Prompting",
    "summary_en": "TechHalla shared their experience creating a creative video using Freepik's visual prompting feature, demonstrating an innovative approach to content generation. The tweet showcased a captivating video about \"nano banana\" entering their favorite arcade, with a promise to reveal the detailed production process. This highlights the growing potential of AI-assisted creative tools in streamlining video content generation, offering new possibilities for personalized narratives and complex scene construction within digital media.",
    "keywords_en": [
      "Visual Prompting",
      "Freepik",
      "Video Creation",
      "Creative Tools",
      "AI Generated Content"
    ],
    "area_en": [
      "Generative AI",
      "Tech News",
      "Multimodal"
    ],
    "published_time": "2025-09-03T20:09:15.000Z",
    "download_time": "2025-09-04 07:51:37",
    "visual_resource": [
      "screenshot/twitter/techhalla_1963333488217919668.png"
    ],
    "extra_info": "{\"username\": \"techhalla\", \"tweet_id\": \"1963333488217919668\"}"
  },
  {
    "id": "S8MwhXj_drrRrxrihXgjYA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/S8MwhXj_drrRrxrihXgjYA",
    "title_en": "Tencent Hunyuan's World Model Tops the WorldScore Rankings",
    "summary_en": "Tencent has officially released and open-sourced its advanced world model, HunyuanWorld-Voyager, marking a significant leap in generative AI. This innovative model is the industry's first to support native 3D reconstruction for ultra-long roaming scenes, capable of generating extensive, world-consistent environments and directly exporting videos into 3D formats. It offers an immersive interactive experience, allowing users to navigate generated scenes with mouse and keyboard, a significant improvement over traditional panoramas. HunyuanWorld-Voyager's core innovation lies in integrating scene depth prediction into the video generation process, enabling native 3D memory and scene reconstruction, which avoids the latency and precision loss of traditional post-processing. This framework ensures precise camera angles and generates 3D point clouds directly, supporting various applications like video scene reconstruction, 3D object texture generation, and style customization. The model has achieved remarkable success, topping the comprehensive capability rankings on Stanford University's WorldScore benchmark, outperforming all existing open-source methods. Its superior performance in camera motion control, spatial consistency, and video generation quality, including the ability to preserve intricate details and achieve high visual realism, has been rigorously validated. The open-sourcing of HunyuanWorld-Voyager, alongside other Tencent AI initiatives, underscores the company's commitment to advancing cutting-edge AI research and making powerful tools accessible to the global developer community.",
    "keywords_en": [
      "World Model",
      "Tencent Hunyuan",
      "3D Reconstruction",
      "Roaming Scenes",
      "WorldScore",
      "Open Source"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Video Understanding"
    ],
    "published_time": "2025-09-03T16:02:25.000Z",
    "download_time": "2025-09-04T15:51:56.007459",
    "visual_resource": [
      "screenshot/wechat/wechat_image_S8MwhXj_drrRrxrihXgjYA.png"
    ],
    "extra_info": null
  },
  {
    "id": "cB4dzM6IUZB5QW1IwrXBsA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/cB4dzM6IUZB5QW1IwrXBsA",
    "title_en": "Is Vector Retrieval More Expensive Than LLMs? Vector Databases Without S3 Support Will Be Eliminated Sooner or Later!",
    "summary_en": "AWS's introduction of S3Vector is disrupting the vector retrieval market with its extremely low storage costs, significantly undercutting traditional solutions. However, S3Vector comes with limitations, including slower query speeds, lower recall rates, and basic functionalities. The article highlights that as large language model (LLM) RAG applications drive an explosion in data volume, vector retrieval costs are soaring, making tiered storageâ€”especially leveraging object storage like S3â€”an inevitable industry trend. Vector databases are evolving from memory- and disk-based eras towards a tiered storage paradigm. Consequently, vector databases that fail to support similar capabilities or deliver extreme cost-effectiveness will face obsolescence. S3Vector's emergence validates the strong demand for vector storage, educates the market, and fosters innovation, prompting professional vector databases like Milvus to accelerate their iterations. Milvus, for instance, is developing \"vector data lake\" solutions to address the dual challenges of cost and performance, aiming to provide efficient and economical data management for AI applications. This shift signifies a new era where cost-efficiency and scalability are paramount for vector database survival and growth.",
    "keywords_en": [
      "Vector Database",
      "S3Vector",
      "Tiered Storage",
      "Vector Retrieval",
      "Cost Optimization",
      "Large Language Model"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-09-03T10:00:41.000Z",
    "download_time": "2025-09-04T15:51:52.826144",
    "visual_resource": [
      "screenshot/wechat/wechat_image_cB4dzM6IUZB5QW1IwrXBsA.png"
    ],
    "extra_info": null
  },
  {
    "id": "MiQ1AFQZpO7aWWExGlK2fQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/MiQ1AFQZpO7aWWExGlK2fQ",
    "title_en": "Large Models Are Smarter with \"Slightly Worse Memory\"! Goldfish Loss Randomly Discards Tokens, Preventing AI from Rote Memorization",
    "summary_en": "Researchers from the University of Maryland and other institutions have introduced \"Goldfish Loss,\" a novel method designed to address the issue of large language models over-memorizing training data. This approach involves randomly discarding a portion of tokens during loss calculation, compelling the model to learn linguistic patterns rather than verbatim replication of the training set. Unlike traditional regularization methods such as Dropout, Goldfish Loss employs a hashing-based masking strategy, ensuring consistent masked positions for identical passages, thereby fundamentally preventing rote memorization. Experiments demonstrate that Goldfish Loss significantly reduces model memorization while maintaining overall performance, enhancing AI's generalization capabilities rather than mere recall. Although it might necessitate more data for compensation, its core principle lies in making models smarter through \"selective forgetting.\"",
    "keywords_en": [
      "Large Language Model",
      "Goldfish Loss",
      "Memorization",
      "Generalization Capability",
      "Loss Function",
      "Hashing Masking"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-09-03T05:17:03.000Z",
    "download_time": "2025-09-04T15:52:03.581287",
    "visual_resource": [
      "screenshot/wechat/wechat_image_MiQ1AFQZpO7aWWExGlK2fQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "149KsdD3NJ7eb5Gvxh56yQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/149KsdD3NJ7eb5Gvxh56yQ",
    "title_en": "UCSD Unveils First AI Agent Browser: Multi-Page Design Revolutionizes Traditional Interaction",
    "summary_en": "The University of California San Diego (UCSD) has unveiled Orca, a groundbreaking AI agent browser that revolutionizes traditional linear tab-based interaction. Addressing the limitations of current AI browsers, which are confined to single pages and struggle with scalable automation workflows, Orca introduces an infinite canvas space. It redefines web pages as 'malleable materials' and the browser as a 'malleable space,' enabling users to simultaneously manage and compare multiple web pages while orchestrating numerous AI agents for information extraction and task execution. This design aims to liberate users from tedious tasks, elevating them to information 'conductors' capable of large-scale browsing. Preliminary studies indicate Orca significantly reduces multi-page management costs, stimulates user exploration, offers an intuitive spatial layout, and enhances user control and trust over AI-generated results. This innovation points towards a promising future for browser design, emphasizing user-driven engagement and empowerment through AI.",
    "keywords_en": [
      "AI Agent Browser",
      "Orca",
      "Spatial Browsing",
      "Web Orchestration",
      "AI Agents",
      "Multi-page"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-03T04:46:37.000Z",
    "download_time": "2025-09-04T15:52:07.825787",
    "visual_resource": [
      "screenshot/wechat/wechat_image_149KsdD3NJ7eb5Gvxh56yQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "b3UryBKzvSKrr-kRz0PEaA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/b3UryBKzvSKrr-kRz0PEaA",
    "title_en": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation",
    "summary_en": "Alibaba introduces FantasyTalking2, marking a qualitative breakthrough in audio-driven portrait animation. This method addresses the complex challenge of balancing motion naturalness, visual fidelity, and lip synchronization through an innovative two-stage training strategy (TLPO). TLPO employs multi-expert decoupled preference alignment, assigning competing preferences to specialized modules, and utilizes a timestep-layer adaptive fusion mechanism to dynamically adjust knowledge injection. The research constructs a high-quality multi-dimensional preference dataset and trains a reward model based on Qwen2.5-Omni. Quantitative and qualitative evaluations, including user studies, consistently demonstrate that FantasyTalking2 significantly outperforms existing state-of-the-art methods in character motion, lip-sync accuracy, and visual quality. By achieving Pareto-optimal outputs, this work provides a robust solution for highly expressive and realistic human animation, highlighting the critical importance of fine-grained preference fusion in diffusion models.",
    "keywords_en": [
      "Audio-Driven Portrait Animation",
      "Multi-Objective Preference Optimization",
      "Diffusion Models",
      "Lip Synchronization",
      "Motion Naturalness",
      "Visual Quality"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Multimodal"
    ],
    "published_time": "2025-09-03T00:02:23.000Z",
    "download_time": "2025-09-04T15:52:18.584848",
    "visual_resource": [
      "screenshot/wechat/wechat_image_b3UryBKzvSKrr-kRz0PEaA.png"
    ],
    "extra_info": null
  },
  {
    "id": "vJXKZCRQYduYr99a57bZHg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/vJXKZCRQYduYr99a57bZHg",
    "title_en": "One Word: Fast! Interpreting Meituan's LongCat-Flash Large Model Technical Report: Algorithm and Infrastructure Innovations",
    "summary_en": "Meituan has released the technical report for its LongCat-Flash large model, a 560B Mixture-of-Experts (MoE) architecture that stands out for its exceptional speed and efficiency, ranking among the fastest hundred-billion-parameter models. Key innovations include \"Zero-computation Experts\" for dynamic computational resource allocation and \"Shortcut-connected MoE\" to optimize communication efficiency. The model ensures large-scale training stability through strategies like hyperparameter transfer and model growth initialization, alongside a multi-stage training process designed to cultivate advanced agentic capabilities. LongCat-Flash demonstrates deep synergy between algorithmic and engineering advancements, achieving high-throughput, low-cost inference. It excels across four core capabilities: general knowledge, agentic tool use, programming, and instruction following, showcasing Meituan's robust technical prowess in the AI domain.",
    "keywords_en": [
      "LongCat-Flash",
      "Large Language Model",
      "MoE",
      "AI Agent",
      "Dynamic Computation",
      "Infrastructure"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-03T00:02:23.000Z",
    "download_time": "2025-09-04T15:52:16.626105",
    "visual_resource": [
      "screenshot/wechat/wechat_image_vJXKZCRQYduYr99a57bZHg.png"
    ],
    "extra_info": null
  },
  {
    "id": "koog",
    "source": "GitHub",
    "url": "https://github.com/JetBrains/koog",
    "title_en": "Koog",
    "summary_en": "Koog, an incubator project by JetBrains, is a Kotlin-based framework designed for building and running AI agents entirely in idiomatic Kotlin. It supports multiplatform deployment across JVM, JS, WasmJS, and iOS, integrating core functionalities such as Model Context Protocol (MCP), embedding capabilities, and custom tool creation. The framework offers intelligent history compression, real-time streaming API, persistent agent memory, and comprehensive tracing. Compatible with major LLM providers like Google and OpenAI, Koog is suitable for developing a wide range of AI agent solutions, from simple chatbots to complex enterprise-level applications.",
    "keywords_en": [
      "Kotlin",
      "AI Agent",
      "Large Language Model",
      "Development Framework",
      "Multiplatform",
      "Agent Memory",
      "Streaming",
      "Embeddings"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-03T10:48:31Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "screenshot/github/koog.png"
    ],
    "extra_info": null
  },
  {
    "id": "awesome-mcp-servers",
    "source": "GitHub",
    "url": "https://github.com/appcypher/awesome-mcp-servers",
    "title_en": "Awesome MCP Servers",
    "summary_en": "This GitHub repository curates a comprehensive list of excellent Model Context Protocol (MCP) servers. MCP is an open protocol enabling AI models to securely interact with local and remote resources. The list features production-ready and experimental MCP server implementations across various domains, including file systems, databases, API integrations, version control, cloud storage, communication, monitoring, search, and automation. It aims to extend AI capabilities and highlights crucial security best practices for running MCP servers.",
    "keywords_en": [
      "Model Context Protocol",
      "AI Servers",
      "AI Agents",
      "Protocol Integration",
      "Resource Management",
      "Security Practices"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-09-03T18:08:18Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "screenshot/github/awesome-mcp-servers.png"
    ],
    "extra_info": null
  },
  {
    "id": "500-AI-Agents-Projects",
    "source": "GitHub",
    "url": "https://github.com/ashishpatel26/500-AI-Agents-Projects",
    "title_en": "ðŸŒŸ 500+ AI Agent Projects / UseCases",
    "summary_en": "This GitHub repository curates over 500 AI agent projects and use cases across various industries, showcasing the practical applications of AI agents in healthcare, finance, education, customer service, and more. It provides detailed use case descriptions and links to corresponding open-source projects, covering mainstream AI frameworks such as CrewAI, AutoGen, Agno, and Langgraph. This resource serves as a valuable hub for developers, researchers, and business enthusiasts seeking inspiration and knowledge in AI agent technology, fostering the widespread adoption and innovation of AI agent solutions.",
    "keywords_en": [
      "AI Agent",
      "AI Applications",
      "Open Source Projects",
      "Industry Solutions",
      "AI Frameworks",
      "Use Case Collection"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-08-01T11:52:42+00:00",
    "download_time": "2024-07-30 08:00:00",
    "visual_resource": [
      "https://github.com/ashishpatel26/500-AI-Agents-Projects/raw/main/images/AIAgentUseCase.jpg",
      "https://github.com/ashishpatel26/500-AI-Agents-Projects/raw/main/images/industry_usecase1.png"
    ],
    "extra_info": null
  },
  {
    "id": "eShop",
    "source": "GitHub",
    "url": "https://github.com/dotnet/eShop",
    "title_en": "eShop Reference Application - \"AdventureWorks\"",
    "summary_en": "eShop is a reference e-commerce application built on .NET Aspire, featuring a services-based architecture to demonstrate how to construct modern, distributed e-commerce solutions using .NET 9. The project integrates development tools like Docker, Visual Studio, and VS Code, and supports deployment to the Azure cloud platform via the Azure Developer CLI. It also offers optional integration with Azure OpenAI services and leverages GPT-35-Turbo and DALLÂ·E 3 for sample data generation, providing developers with a comprehensive guide for building high-performance, scalable cloud-native applications.",
    "keywords_en": [
      ".NET",
      "Aspire",
      "E-commerce Application",
      "Microservices",
      "Docker",
      "Azure",
      "Cloud-Native",
      "OpenAI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-09-03T00:16:26+00:00",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "https://github.com/dotnet/eShop/raw/main/img/eshop_architecture.png",
      "https://github.com/dotnet/eShop/raw/main/img/eshop_homepage.png"
    ],
    "extra_info": null
  },
  {
    "id": "aie-book",
    "source": "GitHub",
    "url": "https://github.com/chiphuyen/aie-book",
    "title_en": "AI Engineering book and other resources",
    "summary_en": "This GitHub repository serves as a comprehensive resource hub for the \"AI Engineering\" book, designed to equip professionals with the knowledge to effectively leverage foundation models for real-world problem-solving. The content delves into the entire lifecycle of AI application development, from adapting foundation models and evaluating their performance to advanced topics like prompt engineering, Retrieval-Augmented Generation (RAG), building and assessing AI agents, and strategic model finetuning. It also addresses critical aspects such as data quality, cost optimization, and security, providing a framework for navigating the complex AI landscape. Emphasizing foundational AI engineering principles rather than transient tools, this book is an invaluable guide for AI/ML engineers, data scientists, and technical managers, offering a systematic methodology to scale AI applications from experimental prototypes to robust production systems, bridging the gap between theoretical understanding and practical implementation.",
    "keywords_en": [
      "AI Engineering",
      "Foundation Models",
      "Large Language Models",
      "Prompt Engineering",
      "Retrieval Augmented Generation",
      "AI Agent",
      "Model Finetuning",
      "AI Application Development"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Machine Learning"
    ],
    "published_time": "2025-02-12T17:48:38Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://github.com/chiphuyen/aie-book/raw/main/assets/aie-cover.png",
      "https://github.com/chiphuyen/aie-book/raw/main/assets/aie-cover-back.png"
    ],
    "extra_info": null
  },
  {
    "id": "serena",
    "source": "GitHub",
    "url": "https://github.com/oraios/serena",
    "title_en": "Serena: A Powerful Coding Agent Toolkit",
    "summary_en": "Serena is a powerful AI coding agent toolkit designed to transform large language models (LLMs) into fully-featured agents that operate directly on codebases. Unlike many other tools, it is LLM-agnostic and framework-independent, offering flexible integration. Serena provides essential semantic code retrieval and editing tools, akin to an IDE's capabilities, by extracting code entities at the symbol level and exploiting relational structures. This approach greatly enhances token efficiency and precision for coding agents. Being free and open-source, Serena augments the capabilities of existing LLMs without additional cost. It supports a wide range of programming languages through Language Server Protocol (LSP) and seamlessly integrates with various clients like Claude Code, Claude Desktop, VSCode, and local GUIs via the Model Context Protocol (MCP), making it a game-changer for navigating and manipulating complex codebases.",
    "keywords_en": [
      "AI Coding Agent",
      "Large Language Model",
      "Semantic Code Analysis",
      "Code Retrieval",
      "Code Editing",
      "Open Source Tool",
      "Language Server Protocol",
      "Model Context Protocol"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-03T19:05:43Z",
    "download_time": "2024-05-16 10:30:00",
    "visual_resource": [
      "https://github.com/oraios/serena/raw/main/resources/serena-logo.svg"
    ],
    "extra_info": null
  },
  {
    "id": "2509.02547",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.02547",
    "title_en": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
    "summary_en": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm\nshift from conventional reinforcement learning applied to large language models\n(LLM RL), reframing LLMs from passive sequence generators into autonomous,\ndecision-making agents embedded in complex, dynamic worlds. This survey\nformalizes this conceptual shift by contrasting the degenerate single-step\nMarkov Decision Processes (MDPs) of LLM-RL with the temporally extended,\npartially observable Markov decision processes (POMDPs) that define Agentic RL.\nBuilding on this foundation, we propose a comprehensive twofold taxonomy: one\norganized around core agentic capabilities, including planning, tool use,\nmemory, reasoning, self-improvement, and perception, and the other around their\napplications across diverse task domains. Central to our thesis is that\nreinforcement learning serves as the critical mechanism for transforming these\ncapabilities from static, heuristic modules into adaptive, robust agentic\nbehavior. To support and accelerate future research, we consolidate the\nlandscape of open-source environments, benchmarks, and frameworks into a\npractical compendium. By synthesizing over five hundred recent works, this\nsurvey charts the contours of this rapidly evolving field and highlights the\nopportunities and challenges that will shape the development of scalable,\ngeneral-purpose AI agents.",
    "keywords_en": [
      "Agentic Reinforcement Learning",
      "Large Language Models",
      "Reinforcement Learning",
      "AI Agents",
      "Agentic Capabilities"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-02T17:46:26.000Z",
    "download_time": "2025-09-03 20:28:56",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02547.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.02547\", \"arxiv_url\": \"https://arxiv.org/abs/2509.02547\"}"
  },
  {
    "id": "2509.01055",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.01055",
    "title_en": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
    "summary_en": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2times speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool.",
    "keywords_en": [
      "Agentic Reinforcement Learning",
      "Tool Use",
      "Multimodal",
      "Unified Framework",
      "Asynchronous Execution"
    ],
    "area_en": [
      "AI Agent",
      "Machine Learning",
      "Multimodal"
    ],
    "published_time": "2025-09-01T01:45:18.000Z",
    "download_time": "2025-09-03 20:28:56",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01055.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.01055\", \"arxiv_url\": \"https://arxiv.org/abs/2509.01055\"}"
  },
  {
    "id": "2509.02522",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.02522",
    "title_en": "Implicit Actor Critic Coupling via a Supervised Learning Framework for\n  RLVR",
    "summary_en": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have\nempowered large language models (LLMs) to tackle challenging reasoning tasks\nsuch as mathematics and programming. RLVR leverages verifiable outcome rewards\nto guide policy optimization, enabling LLMs to progressively improve output\nquality in a grounded and reliable manner. Despite its promise, the RLVR\nparadigm poses significant challenges, as existing methods often suffer from\nsparse reward signals and unstable policy gradient updates, particularly in\nRL-based approaches. To address the challenges, we propose PACS, a\nnovel RLVR framework that achieves imPlicit Actor\nCritic coupling via a Supervised learning framework. By\ntreating the outcome reward as a predictable label, we reformulate the RLVR\nproblem into a supervised learning task over a score function parameterized by\nthe policy model and optimized using cross-entropy loss. A detailed gradient\nanalysis shows that this supervised formulation inherently recovers the\nclassical policy gradient update while implicitly coupling actor and critic\nroles, yielding more stable and efficient training. Benchmarking on challenging\nmathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as\nPPO and GRPO, achieving superior reasoning performance. For instance, PACS\nachieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32\nand 14.36 points over PPO and GRPO. This simple yet powerful framework offers a\npromising avenue for LLMs post-training with verifiable rewards. Our code and\ndata are available as open source at https://github.com/ritzz-ai/PACS.",
    "keywords_en": [
      "Reinforcement Learning with Verifiable Rewards",
      "Large Language Models",
      "Supervised Learning",
      "Actor-Critic",
      "Mathematical Reasoning"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-09-02T17:22:46.000Z",
    "download_time": "2025-09-03 20:28:55",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02522.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.02522\", \"arxiv_url\": \"https://arxiv.org/abs/2509.02522\"}"
  },
  {
    "id": "2509.02460",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.02460",
    "title_en": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
    "summary_en": "Video compositing combines live-action footage to create video production,\nserving as a crucial technique in video creation and film production.\nTraditional pipelines require intensive labor efforts and expert collaboration,\nresulting in lengthy production cycles and high manpower costs. To address this\nissue, we automate this process with generative models, called generative video\ncompositing. This new task strives to adaptively inject identity and motion\ninformation of foreground video to the target video in an interactive manner,\nallowing users to customize the size, motion trajectory, and other attributes\nof the dynamic elements added in final video. Specifically, we designed a novel\nDiffusion Transformer (DiT) pipeline based on its intrinsic properties. To\nmaintain consistency of the target video before and after editing, we revised a\nlight-weight DiT-based background preservation branch with masked token\ninjection. As to inherit dynamic elements from other sources, a DiT fusion\nblock is proposed using full self-attention, along with a simple yet effective\nforeground augmentation for training. Besides, for fusing background and\nforeground videos with different layouts based on user control, we developed a\nnovel position embedding, named Extended Rotary Position Embedding (ERoPE).\nFinally, we curated a dataset comprising 61K sets of videos for our new task,\ncalled VideoComp. This data includes complete dynamic elements and high-quality\ntarget videos. Experiments demonstrate that our method effectively realizes\ngenerative video compositing, outperforming existing possible solutions in\nfidelity and consistency.",
    "keywords_en": [
      "Generative Video Compositing",
      "Diffusion Transformer",
      "Video Compositing",
      "Video Dataset",
      "Video Editing"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-09-02T16:10:13.000Z",
    "download_time": "2025-09-03 20:28:58",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02460.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.02460\", \"arxiv_url\": \"https://arxiv.org/abs/2509.02460\"}"
  },
  {
    "id": "2509.01440",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.01440",
    "title_en": "Benchmarking Optimizers for Large Language Model Pretraining",
    "summary_en": "The recent development of Large Language Models (LLMs) has been accompanied\nby an effervescence of novel ideas and methods to better optimize the loss of\ndeep learning models. Claims from those methods are myriad: from faster\nconvergence to removing reliance on certain hyperparameters. However, the\ndiverse experimental protocols used to validate these claims make direct\ncomparisons between methods challenging. This study presents a comprehensive\nevaluation of recent optimization techniques across standardized LLM\npretraining scenarios, systematically varying model size, batch size, and\ntraining duration. Through careful tuning of each method, we provide guidance\nto practitioners on which optimizer is best suited for each scenario. For\nresearchers, our work highlights promising directions for future optimization\nresearch. Finally, by releasing our code and making all experiments fully\nreproducible, we hope our efforts can help the development and rigorous\nbenchmarking of future methods.",
    "keywords_en": [
      "Large Language Models",
      "Optimizers",
      "Pretraining",
      "Benchmarking",
      "Deep Learning"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-09-01T12:50:30.000Z",
    "download_time": "2025-09-03 20:28:55",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01440.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.01440\", \"arxiv_url\": \"https://arxiv.org/abs/2509.01440\"}"
  },
  {
    "id": "2509.01610",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.01610",
    "title_en": "Improving Large Vision and Language Models by Learning from a Panel of\n  Peers",
    "summary_en": "Traditional alignment methods for Large Vision and Language Models (LVLMs)\nprimarily rely on human-curated preference data. Human-generated preference\ndata is costly; machine-generated preference data is limited in quality; and\nself-supervised preference data often introduces hallucinations. To overcome\nthese limitations, we propose a novel Panel-of-Peers learning framework\ninspired by collaborative learning among humans. This approach leverages a\npanel of LVLMs, each evaluating and learning from their collective outputs\nthrough an iterative self-improvement process. By simulating a peer review\nsystem, our models generate, assess, and refine outputs in response to a\ncurated set of prompts, mimicking a classroom learning environment. We\ndemonstrate that this methodology enhances model performance without requiring\nextensive human-labeled datasets. Our experiments show significant improvement\nacross multiple benchmarks, demonstrating the potential of peer evaluations as\na scalable alternative to self-supervised alignment. Notably, we show that\nPanel-of-Peers increases the average score on fifteen benchmarks from 48% to\n57%",
    "keywords_en": [
      "Large Vision and Language Models",
      "Panel-of-Peers",
      "Model Alignment",
      "Self-Improvement",
      "Peer Evaluation"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Deep Learning"
    ],
    "published_time": "2025-09-01T16:43:48.000Z",
    "download_time": "2025-09-03 20:29:00",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01610.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.01610\", \"arxiv_url\": \"https://arxiv.org/abs/2509.01610\"}"
  }
]