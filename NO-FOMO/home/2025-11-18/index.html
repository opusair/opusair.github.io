<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-11-18</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="." class="active">‰∏≠Êñá</a>
                <a href="en/" class="">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-11-18</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† ËøîÂõû‰∏ªÈ°µ</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ ÊúÄÊñ∞Êó•Êä•</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ ÂÖ≥‰∫éÊàë‰ª¨</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Solving a Million-Step LLM Task with Zero Errors</h2>
                <span class="published-time">Published: 2025-11-18 16:26:28</span>
                
                <p class="summary">A recent research breakthrough demonstrates the unprecedented ability to complete complex, million-step tasks using Large Language Models (LLMs) with zero errors. This advancement addresses a critical challenge in AI development: the accumulation of errors and inconsistencies in LLMs during extended, multi-stage operations. The methodology, though not fully detailed in the provided abstract, implies novel techniques in self-correction, robust planning, and verifiable execution, enabling LLMs to maintain perfect accuracy over an extremely long sequence of computational or logical steps. This development significantly boosts the reliability and trustworthiness of LLM-powered systems, paving the way for their deployment in highly sensitive applications where even minor errors are unacceptable. It marks a crucial step towards creating more dependable and robust AI agents capable of autonomous operation in complex, real-world environments requiring sustained precision and coherence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>LLM Reliability</span><span>Error Correction</span><span>Multi-step Reasoning</span><span>AI Agents</span><span>Zero-Error Systems</span><span>Task Automation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://arxiv.org/abs/2511.09030" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Gemini 3 Pro Model Card</h2>
                <span class="published-time">Published: 2025-11-18 11:40:01</span>
                
                <p class="summary">The Gemini 3 Pro Model Card functions as a foundational document outlining the detailed specifications, enhanced capabilities, and critical ethical considerations associated with Google's presumed next-generation artificial intelligence model, Gemini 3 Pro. While the provided input is concise, a comprehensive model card for such an advanced system would meticulously detail its architectural innovations, sophisticated training methodology, and comprehensive performance benchmarks across a wide array of complex tasks. It would likely emphasize significant advancements in its multimodal functionalities, enabling seamless processing and understanding across text, image, audio, and video inputs, alongside notable improvements in reasoning, problem-solving, and general comprehension. Furthermore, the model card would address responsible deployment by meticulously documenting potential biases, inherent limitations, and proposed mitigation strategies. This level of transparency is crucial for fostering ethical AI development and guiding developers and end-users in the model's appropriate and effective application.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>Multimodal AI</span><span>AI Model Card</span><span>AI Ethics</span><span>Generative AI</span><span>Model Documentation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://pixeldrain.com/u/hwgaNKeH" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Trying out Gemini 3 Pro with audio transcription and a new pelican benchmark</h2>
                <span class="published-time">Published: 2025-11-18 19:05:44</span>
                
                <p class="summary">The article details initial experiments with Google's forthcoming Gemini 3 Pro model, focusing on its capabilities in audio transcription. The author, Simon Willison, conducted practical tests to assess the model's performance in converting spoken language into text, a critical feature for many AI applications. A key aspect of this evaluation involved the introduction of a new "pelican benchmark," designed to provide a standardized and repeatable method for measuring the efficacy and accuracy of advanced AI models like Gemini 3 Pro in specific tasks. This benchmark aims to offer objective data points regarding transcription quality, speed, and robustness across various audio inputs. The findings from these early trials are expected to shed light on Gemini 3 Pro's advancements in multimodal AI processing, particularly its proficiency in handling audio data and integrating it with language understanding. The blog post likely serves as a preliminary assessment, highlighting both the model's strengths and potential areas for future development, contributing valuable insights to the broader AI research community.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini 3 Pro</span><span>audio transcription</span><span>AI models</span><span>benchmarking</span><span>multimodal AI</span><span>performance evaluation</span><span>natural language processing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Multimodal</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://simonwillison.net/2025/Nov/18/gemini-3/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: RowboatX ‚Äì open-source Claude Code for everyday automations</h2>
                <span class="published-time">Published: 2025-11-18 18:50:00</span>
                
                <p class="summary">RowboatX introduces an open-source command-line interface (CLI) tool specifically engineered to extend the application of large language models (LLMs) from coding-centric operations to general everyday automations. Drawing inspiration from Claude Code, RowboatX empowers users to create and manage custom background agents locally, utilizing the file system and standard Unix utilities for their operation and monitoring. These agents can seamlessly integrate with any compatible MCP server to access a broader array of tools and are designed to intelligently process and reason over their generated outputs. A key feature of RowboatX is its capability to run agents with local shell access, enabling them to perform actions such as installing software, executing code, and automating virtually any terminal-based task, all under explicit user consent. Furthermore, its architecture promotes versatility by supporting a wide spectrum of LLMs, including various open-source options, making it a robust platform for intelligent, permission-based automation across diverse computing environments.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>RowboatX</span><span>CLI tool</span><span>automation</span><span>AI agents</span><span>Large Language Model</span><span>open-source</span><span>Unix tools</span><span>shell access</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/rowboatlabs/rowboat" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Autonomous driving in five new cities</h2>
                <span class="published-time">Published: 2025-11-18 14:30:22</span>
                
                <p class="summary">Waymo, a pioneer in autonomous driving technology, has announced a significant expansion of its services into five new cities. This strategic move, detailed in its November 2025 blog post, signifies a major advancement in the commercialization and widespread deployment of self-driving vehicles. The expansion highlights Waymo's confidence in its AI-powered autonomous system's ability to safely navigate diverse and complex urban environments, further solidifying its position as an industry leader. By extending its operational footprint, Waymo aims to make its fully autonomous ride-hailing and logistics services accessible to more communities, thereby transforming urban mobility and setting new standards for vehicle safety and efficiency. This development underscores the robustness of Waymo's sensor fusion, perception algorithms, and decision-making capabilities, which are crucial for Level 4 autonomy. The increased operational scale will also provide invaluable real-world data, accelerating the refinement and advancement of future autonomous vehicle technologies.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Autonomous Driving</span><span>Self-Driving Vehicles</span><span>Waymo</span><span>Robotics</span><span>AI</span><span>Urban Mobility</span><span>Vehicle Automation</span><span>Computer Vision</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Robotics</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://waymo.com/blog/2025/11/safe-routine-ready-autonomous-driving-in-new-cities" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Optimizing LiteLLM with Rust ‚Äì When Expectations Meet Reality</h2>
                <span class="published-time">Published: 2025-11-18 16:32:16</span>
                
                <p class="summary">The "Fast LiteLLM" project introduced a Rust acceleration layer for the popular Python library LiteLLM, focusing on optimizing performance-critical components like token counting, routing, rate limiting, and connection pooling. The initiative was driven by the assumption that Python's inherent characteristics would present significant opportunities for low-hanging performance fruit. The technical approach involved developing Rust implementations using PyO3, specifically incorporating tiktoken-rs for efficient token counting and DashMap to enable lock-free data structures for concurrent operations. Furthermore, the project integrated async-friendly rate limiting and engineered monkeypatch shims for transparently replacing Python functions with their optimized Rust counterparts. To ensure controlled deployment and monitor impact, comprehensive feature flags were implemented alongside performance monitoring tools for real-time tracking of improvements. This work provides valuable learnings for developers seeking to boost the efficiency of existing Python systems through Rust, highlighting the practical challenges and benefits of such an optimization strategy in the context of large language model inference infrastructure.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Rust</span><span>Python Optimization</span><span>LiteLLM</span><span>PyO3</span><span>Performance Engineering</span><span>Token Counting</span><span>Rate Limiting</span><span>Concurrency</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/neul-labs/fast-litellm" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>TrendRadar</h2>
                <span class="published-time">Published: 2025-11-18T11:51:36Z</span>
                
                <p class="summary">TrendRadar is a rapid-deployment hot spot assistant designed to filter information overload and deliver personalized news and trends. It aggregates hot spots from over 11 mainstream platforms (including Zhihu, Douyin, Bilibili, Weibo, Baidu Hot Search, etc.) and allows users to add custom monitoring platforms. The system employs intelligent push strategies, offering daily summaries, currentÊ¶úÂçï monitoring, and incremental updates, coupled with optional time window controls to prevent interruptions.Key features include precise content filtering with support for normal, must-include (`+`), and exclude (`!`) keywords, grouped for independent statistics. A personalized hot spot algorithm re-ranks aggregated news based on high-ranking occurrences (60%), sustained appearance (30%), and ranking quality (10%), enabling users to track evolving trends rather than just current popularity. TrendRadar supports real-time push notifications across multiple channels like WeCom, Feishu, DingTalk, Telegram, Email, and ntfy, and provides multi-device adaptation through GitHub Pages and Docker deployment.A significant addition in v3.0.0 is the AI intelligent analysis system, built on the MCP (Model Context Protocol). This feature offers conversational queries using natural language and leverages 13 analysis tools for basic queries, smart retrieval, trend analysis, data insights, and sentiment analysis. It empowers investors, self-media creators, and PR professionals to gain deep insights into market trends, public opinion, and industry dynamics, all deployable within minutes, reducing dependency on platform algorithms.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Hot Spot Aggregation</span><span>Content Filtering</span><span>Trend Analysis</span><span>AI Analysis</span><span>Multi-channel Notification</span><span>Docker</span><span>GitHub Actions</span><span>Model Context Protocol</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/sansan0/TrendRadar" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent Development Kit (ADK) for Go</h2>
                <span class="published-time">Published: 2025-11-18 13:29:04+00:00</span>
                
                <p class="summary">The Agent Development Kit (ADK) for Go is an open-source, code-first toolkit designed for building, evaluating, and deploying sophisticated AI agents. It applies established software development principles to AI agent creation, offering a flexible and modular framework to simplify the orchestration of agent workflows, from simple tasks to complex multi-agent systems. While optimized for Google's Gemini, ADK is model-agnostic and deployment-agnostic, ensuring compatibility across various frameworks and large language models. This Go version specifically caters to developers building cloud-native agent applications, leveraging Go's inherent strengths in concurrency and performance. Key features include idiomatic Go design, a rich ecosystem for integrating diverse tools, code-first development for ultimate flexibility and testability, and robust support for deployment in cloud-native environments like Google Cloud Run. ADK empowers developers with fine-grained control over agent logic and integration.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Go Programming Language</span><span>Agent Development Kit</span><span>Cloud-Native</span><span>Modular Systems</span><span>Model-Agnostic</span><span>Agent Orchestration</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/google/adk-go" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>‚û§ Cursor Free VIP</h2>
                <span class="published-time">Published: 2025-09-16T03:47:39Z</span>
                
                <p class="summary">Cursor Free VIP is an open-source utility designed to enhance the functionality of the Cursor AI coding assistant. This tool enables users to reset Cursor's configuration and offers broad system compatibility across Windows, macOS, and Linux, alongside multi-language support. Provided for educational and research purposes, it features automated installation scripts for ease of use across different operating systems. The project emphasizes its role as a learning tool, ensuring it does not generate fake accounts or OAuth access, and encourages users to support the original Cursor project while adhering to relevant software usage terms.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Cursor AI</span><span>developer tool</span><span>configuration management</span><span>cross-platform</span><span>automation script</span><span>utility software</span><span>system support</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/yeongpin/cursor-free-vip" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>P1: Mastering Physics Olympiads with Reinforcement Learning</h2>
                <span class="published-time">Published: 2025-11-17T17:18:13.000Z</span>
                
                <p class="summary">Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reinforcement Learning</span><span>Large Language Model</span><span>Physics Reasoning</span><span>AI Agent</span><span>Physics Olympiads</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.13612" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</h2>
                <span class="published-time">Published: 2025-11-16T14:10:55.000Z</span>
                
                <p class="summary">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Omnimodal Large Model</span><span>Mixture-of-Experts</span><span>Multimodal Understanding</span><span>Generative AI</span><span>Reinforcement Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.12609" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</h2>
                <span class="published-time">Published: 2025-11-14T18:52:07.000Z</span>
                
                <p class="summary">We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>MiroThinker</span><span>Open-source agents</span><span>Interactive scaling</span><span>Tool-augmented reasoning</span><span>Reinforcement learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.11793" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</h2>
                <span class="published-time">Published: 2025-11-17T17:58:18.000Z</span>
                
                <p class="summary">Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-G√∂del Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Live-SWE-agent</span><span>Software Engineering Agents</span><span>Self-Evolving</span><span>Large Language Models</span><span>SWE-bench</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.13646" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</h2>
                <span class="published-time">Published: 2025-11-17T18:52:44.000Z</span>
                
                <p class="summary">The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>TiViBench</span><span>Video Generative Models</span><span>Reasoning</span><span>Benchmarking</span><span>Image-to-Video Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.13704" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</h2>
                <span class="published-time">Published: 2025-11-12T18:58:21.000Z</span>
                
                <p class="summary">While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multimodal</span><span>Diffusion Models</span><span>Large Language Model</span><span>Reinforcement Learning</span><span>Thinking-Aware Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Generative AI</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.09611" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>