[
  {
    "id": "twitter_latentspacepod_1956433236021883071",
    "source": "Twitter",
    "url": "https://twitter.com/latentspacepod/status/1956433236021883071",
    "title_en": "latentspacepod_Greg Brockman on OpenAI's Future, GPT-5, and AGI",
    "summary_en": "In a Latent.Space podcast, OpenAI President Greg Brockman discusses the GPT-5 era, evaluating model intelligence, scaling compute, and the path to AGI. He emphasizes \"energy turns into compute, turns into intelligence,\" sharing OpenAI's progress in reasoning evolution, online/offline learning, model routing, pricing optimization, and self-improving agents. The conversation reveals OpenAI's vision for future AI research and engineering, alongside reflections on the value of engineers in the age of AGI.",
    "keywords_en": [
      "OpenAI",
      "GPT-5",
      "Greg Brockman",
      "AGI",
      "Large Language Model",
      "Compute Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-08-15T19:10:06.000Z",
    "download_time": "2025-08-16 09:27:46",
    "visual_resource": [
      "screenshot/twitter/latentspacepod_1956433236021883071.png"
    ],
    "extra_info": "{\"username\": \"latentspacepod\", \"tweet_id\": \"1956433236021883071\"}"
  },
  {
    "id": "twitter_OpenAI_1956212769365352758",
    "source": "Twitter",
    "url": "https://twitter.com/OpenAI/status/1956212769365352758",
    "title_en": "OpenAI_ChatGPT Weekly Updates: GPT-4o and GPT-5 Features & Availability Expansion",
    "summary_en": "OpenAI announced its latest weekly ChatGPT updates, including GPT-4o being default for paid users, who can also enable more legacy models and GPT-5 Thinking mini. GPT-5 introduces Auto, Fast, and Thinking modes for varied response speed and depth. Plus and Team users now receive up to 3,000 GPT-5 Thinking messages per week. Furthermore, GPT-5 is now available for Enterprise and Education users, with a warmer personality coming soon.",
    "keywords_en": [
      "ChatGPT",
      "GPT-4o",
      "GPT-5",
      "Model Updates",
      "Feature Expansion",
      "OpenAI"
    ],
    "area_en": [
      "Product Launch",
      "Large Language Model",
      "Tech News"
    ],
    "published_time": "2025-08-15T04:34:03.000Z",
    "download_time": "2025-08-16 09:27:11",
    "visual_resource": [
      "screenshot/twitter/OpenAI_1956212769365352758.png"
    ],
    "extra_info": "{\"username\": \"OpenAI\", \"tweet_id\": \"1956212769365352758\"}"
  },
  {
    "id": "twitter_Alibaba_Qwen_1956289523421470855",
    "source": "Twitter",
    "url": "https://twitter.com/Alibaba_Qwen/status/1956289523421470855",
    "title_en": "Alibaba_Qwen_Qwen Chat Vision Understanding Major Upgrade",
    "summary_en": "Qwen Chat's vision understanding capabilities have received a significant update, now featuring native 128K context and stronger performance across vision, video, and 3D tasks. Key upgrades include a substantial boost in math and reasoning, more accurate object recognition, OCR support for over 30 languages, enhanced 2D and 3D grounding, and major improvements in video understanding.",
    "keywords_en": [
      "Qwen",
      "Vision Understanding",
      "Multimodal",
      "Large Language Model",
      "Video Understanding",
      "OCR"
    ],
    "area_en": [
      "Large Language Model",
      "Multimodal",
      "Tech News"
    ],
    "published_time": "2025-08-15T09:39:03.000Z",
    "download_time": "2025-08-16 09:29:02",
    "visual_resource": [
      "screenshot/twitter/Alibaba_Qwen_1956289523421470855.png"
    ],
    "extra_info": "{\"username\": \"Alibaba_Qwen\", \"tweet_id\": \"1956289523421470855\"}"
  },
  {
    "id": "twitter_xywang626_1956400403911962757",
    "source": "Twitter",
    "url": "https://twitter.com/xywang626/status/1956400403911962757",
    "title_en": "xywang626_OpenCUA: First Open-Source Computer-Use Agent Foundation Model Framework Released",
    "summary_en": "Xinyuan Wang's team has released OpenCUA, the first from-scratch open-source foundation model framework for computer-use agents, along with the SOTA model OpenCUA-32B. This framework matches top proprietary models on the OSWorld-Verified benchmark and provides full open-source infrastructure and data. OpenCUA aims to address the lack of large-scale open desktop agent datasets and transparent pipelines. It offers a comprehensive stack for scalable data collection, effective data formulation, model training strategies, and reproducible evaluation, powering top open-source models like OpenCUA-7B and OpenCUA-32B.",
    "keywords_en": [
      "Computer-Use Agent",
      "OpenCUA",
      "Open-Source Framework",
      "Foundation Model",
      "Dataset",
      "Benchmark"
    ],
    "area_en": [
      "AI Agent",
      "Open Source",
      "Large Language Model"
    ],
    "published_time": "2025-08-15T16:59:39.000Z",
    "download_time": "2025-08-16 09:27:57",
    "visual_resource": [
      "screenshot/twitter/xywang626_1956400403911962757.png"
    ],
    "extra_info": "{\"username\": \"xywang626\", \"tweet_id\": \"1956400403911962757\"}"
  },
  {
    "id": "twitter__philschmid_1956351654753673252",
    "source": "Twitter",
    "url": "https://twitter.com/_philschmid/status/1956351654753673252",
    "title_en": "_philschmid_Imagen 4 Officially Launched with Performance Boost and Pricing Details",
    "summary_en": "Philipp Schmid announced that Imagen 4 is now generally available in AI Studio and the Gemini API. The service offers three models: Ultra, Standard, and Fast, with pricing starting at $0.02 per image. Imagen 4 boasts up to 10x faster generation compared to its predecessor, supports image output up to 2K resolution, and significantly improves spelling and typography for longer text strings. Users can generate between 1 and 4 images per prompt, with specific pricing at $0.06 for Ultra, $0.04 for Standard, and $0.02 for Fast.",
    "keywords_en": [
      "Imagen 4",
      "AI Studio",
      "Gemini API",
      "Image Generation",
      "Text-to-Image",
      "Product Launch"
    ],
    "area_en": [
      "Generative AI",
      "Product Launch",
      "Computer Vision"
    ],
    "published_time": "2025-08-15T13:45:56.000Z",
    "download_time": "2025-08-16 09:27:54",
    "visual_resource": [
      "screenshot/twitter/_philschmid_1956351654753673252.png"
    ],
    "extra_info": "{\"username\": \"_philschmid\", \"tweet_id\": \"1956351654753673252\"}"
  },
  {
    "id": "twitter_omarsar0_1956325856265326923",
    "source": "Twitter",
    "url": "https://twitter.com/omarsar0/status/1956325856265326923",
    "title_en": "omarsar0_OdysseyBench: Multi-Day Office Task AI Agent Benchmark",
    "summary_en": "The omarsar0 team has introduced OdysseyBench, a novel benchmark and data-generation pipeline designed to evaluate AI agents' performance in realistic, multi-day office tasks. This benchmark spans applications like Word, Excel, PDF, Email, and Calendar, specifically targeting long-horizon, context-dependent workflows rather than atomic tasks. It provides a new paradigm for assessing agent capabilities in complex office environments.",
    "keywords_en": [
      "OdysseyBench",
      "AI Agent",
      "Benchmark",
      "Office Automation",
      "Long-horizon Tasks"
    ],
    "area_en": [
      "AI Agent",
      "Research Progress",
      "Tech News"
    ],
    "published_time": "2025-08-15T12:03:07.000Z",
    "download_time": "2025-08-16 09:28:49",
    "visual_resource": [
      "screenshot/twitter/omarsar0_1956325856265326923.png"
    ],
    "extra_info": "{\"username\": \"omarsar0\", \"tweet_id\": \"1956325856265326923\"}"
  },
  {
    "id": "uxUibyxlYko6rAaHKiDSlg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/uxUibyxlYko6rAaHKiDSlg",
    "title_en": "DINOv3 Arrives: 7 Billion Parameters, 1.7 Billion Images, Self-Supervised Learning Reaches New Milestone",
    "summary_en": "Meta has launched DINOv3, a cutting-edge self-supervised learning (SSL) computer vision model. This new iteration scales unsupervised training to an unprecedented 7 billion parameters and leverages a massive 1.7 billion image dataset. DINOv3 marks a significant breakthrough by being the first single frozen visual backbone to outperform specialized solutions across multiple long-standing dense prediction tasks. Inspired by the success of large language models, DINOv3 emphasizes expanding model capacity and data scale. It introduces innovative techniques like Gram Anchoring to address performance degradation in dense tasks during extended training and incorporates high-resolution adaptation for enhanced feature utilization. The model demonstrates for the first time that self-supervised learning can broadly surpass weakly supervised models, achieving superior performance in both image classification and dense prediction benchmarks. Its real-world impact is already evident, for instance, in forest monitoring applications, signifying a major milestone for self-supervised learning in computer vision.",
    "keywords_en": [
      "DINOv3",
      "Self-supervised learning",
      "Computer Vision",
      "Large Models",
      "Dense Prediction"
    ],
    "area_en": [
      "Computer Vision",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-15T01:13:45.000Z",
    "download_time": "2025-08-16T17:30:27.825204",
    "visual_resource": [
      "screenshot/wechat/wechat_image_uxUibyxlYko6rAaHKiDSlg.png"
    ],
    "extra_info": null
  },
  {
    "id": "IH64apP7SmHVCwHKfTGOsQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/IH64apP7SmHVCwHKfTGOsQ",
    "title_en": "Google Open-Sources Gemma 3 270M, Outperforming Qwen 2.5 Equivalent Models",
    "summary_en": "Google has officially released Gemma 3 270M, a compact language model with only 270 million parameters, specifically designed for fine-tuning on specialized tasks. Inheriting the advanced architecture of the Gemma 3 series, this model demonstrates robust instruction-following and text structuring capabilities, setting new performance benchmarks against equivalent models in IFEval benchmarks. Its key advantages include extreme energy efficiency (consuming only 0.75% battery for 25 conversations on a Pixel 9 Pro), support for production-ready INT4 quantization, and excellent instruction adherence. Gemma 3 270M aims to empower developers to build highly efficient, cost-effective, and offline-capable specialized AI systems. It is particularly suited for high-volume, low-latency, privacy-sensitive, and rapid iteration scenarios, fostering the widespread adoption of small, expert models.",
    "keywords_en": [
      "Gemma 3 270M",
      "Google",
      "Language Model",
      "Fine-tuning",
      "On-device AI",
      "Energy Efficiency"
    ],
    "area_en": [
      "Large Language Model",
      "Generative AI",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-15T04:14:53.000Z",
    "download_time": "2025-08-16T17:30:16.370941",
    "visual_resource": [
      "screenshot/wechat/wechat_image_IH64apP7SmHVCwHKfTGOsQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "ReIDqAtEabUbzMvAu40CgQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ReIDqAtEabUbzMvAu40CgQ",
    "title_en": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation",
    "summary_en": "The Video-BLADE framework, jointly open-sourced by Zhejiang University and Huawei, significantly enhances the inference efficiency of video diffusion models by synergistically integrating Adaptive Block-Sparse Attention (ASA) with a data-free Trajectory Distribution Matching (TDM) distillation process. This innovative framework enables DiT models to achieve up to a 14x speedup while demonstrating superior or maintained generation quality, even surpassing traditional dense baselines on the VBench-2.0 benchmark. At its core, Video-BLADE dynamically prunes attention matrices to focus on critical spatiotemporal interactions and employs distribution-level alignment to ensure the student model efficiently learns the teacher model's generation trajectory. This approach substantially reduces computational costs while preserving high perceptual quality, establishing a new paradigm for efficient video generation.",
    "keywords_en": [
      "Text-to-video",
      "Diffusion models",
      "Model distillation",
      "Sparse attention",
      "Video generation acceleration"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-08-15T16:19:53.000Z",
    "download_time": "2025-08-16T17:29:44.972737",
    "visual_resource": [
      "screenshot/wechat/wechat_image_ReIDqAtEabUbzMvAu40CgQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "3nZGw_6WFDo2lvNjiA8yNA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/3nZGw_6WFDo2lvNjiA8yNA",
    "title_en": "StableAvatar: New Lip-Sync Technology for Infinite-Length, Zero-Frame-Drop Video Generation",
    "summary_en": "Fudan University introduces StableAvatar, the first end-to-end video diffusion Transformer, specifically designed to overcome the limitations of current audio-driven avatar video generation models, which struggle to maintain natural audio synchronization and consistent identity over extended video durations. StableAvatar achieves this breakthrough through several key innovations: a timestep-aware audio adapter that suppresses error accumulation, an audio-native guidance mechanism that significantly improves audio-visual synchronization, and a dynamic weighted sliding window strategy to enhance overall video smoothness. These combined features enable the seamless generation of high-quality, infinite-length videos without the need for post-processing. Furthermore, StableAvatar demonstrates remarkable efficiency, reducing GPU memory consumption by approximately 50% and achieving a 10x inference speedup compared to leading competitors. Its superior performance in facial quality and precise lip-sync accuracy positions StableAvatar as a significant advancement in the field of long-duration avatar video generation.",
    "keywords_en": [
      "StableAvatar",
      "Lip-sync",
      "Video Generation",
      "Diffusion Models",
      "Long-form Video",
      "AI Avatars"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Multimodal"
    ],
    "published_time": "2025-08-15T13:44:57.000Z",
    "download_time": "2025-08-16T17:29:51.337618",
    "visual_resource": [
      "screenshot/wechat/wechat_image_3nZGw_6WFDo2lvNjiA8yNA.png"
    ],
    "extra_info": null
  },
  {
    "id": "3gzb5QcJ8AO-1EDeECUFlQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/3gzb5QcJ8AO-1EDeECUFlQ",
    "title_en": "WebWatcher: The First Open-Source Multimodal Deep Research Agent Surpassing Closed-Source Models",
    "summary_en": "WebWatcher, the first open-source multimodal deep research agent, integrates various tools such as web browsing, image search, and code interpreters. It generates high-quality reasoning trajectories through a fully automated process, optimized by supervised fine-tuning and reinforcement learning. This agent is designed to tackle complex, cross-modal, multi-tool, and multi-step tasks. Its technical approach encompasses building high-difficulty multimodal data, optimizing reasoning trajectories, and applying reinforcement learning. Evaluated against challenging benchmarks like BrowseComp-VL, WebWatcher comprehensively outperforms leading closed-source models such as GPT-4o and Gemini in complex reasoning, information retrieval, knowledge integration, and information aggregation. This performance establishes WebWatcher's leading position as a new generation of open-source multimodal AI agents.",
    "keywords_en": [
      "Deep Research Agent",
      "Multimodal",
      "Open-source",
      "Tool Use",
      "Reinforcement Learning",
      "WebWatcher"
    ],
    "area_en": [
      "AI Agent",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-08-15T06:41:05.000Z",
    "download_time": "2025-08-16T17:29:50.196608",
    "visual_resource": [
      "screenshot/wechat/wechat_image_3gzb5QcJ8AO-1EDeECUFlQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "gfM7kMxSt9Cs7Ark8gaOcA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/gfM7kMxSt9Cs7Ark8gaOcA",
    "title_en": "GPT-5 Surpasses Human Doctors: 24% Higher Reasoning and 29% Stronger Comprehension Than Experts",
    "summary_en": "Recent research indicates that GPT-5 demonstrates exceptional performance in medical image reasoning and comprehension, with accuracy rates 24.23% and 29.40% higher than human experts, respectively. The model comprehensively outperforms GPT-4o and other variants across multiple standardized medical tests, including USMLE, MedXpertQA, and VQA-RAD. Its core capability enhancement stems from an architectural shift from text-dominant to native multimodal deep fusion, utilizing shared tokenization techniques and cross-modal attention mechanisms for seamless information processing. Although GPT-5 excels in ideal testing environments, researchers emphasize that its application in real-world clinical scenarios still requires further practical validation. Currently, in handling complex, unseen real-world cases, AI models, including GPT-5, still lag behind experienced radiologists.",
    "keywords_en": [
      "GPT-5",
      "Medical Imaging",
      "Multimodal",
      "Artificial Intelligence",
      "Medical AI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Multimodal"
    ],
    "published_time": "2025-08-15T06:41:05.000Z",
    "download_time": "2025-08-16T17:30:09.114321",
    "visual_resource": [
      "screenshot/wechat/wechat_image_gfM7kMxSt9Cs7Ark8gaOcA.png"
    ],
    "extra_info": null
  },
  {
    "id": "fastapi_mcp",
    "source": "GitHub",
    "url": "https://github.com/tadata-org/fastapi_mcp",
    "title_en": "FastAPI-MCP",
    "summary_en": "FastAPI-MCP is an innovative Python library designed to seamlessly expose FastAPI API endpoints as Model Context Protocol (MCP) tools, complete with built-in authentication. This project adopts a FastAPI-native approach, distinguishing itself from mere OpenAPI converters, and supports zero or minimal configuration. It automatically preserves the schemas of request and response models, as well as Swagger documentation. Key advantages include efficient communication via FastAPI's ASGI interface and flexible deployment options, allowing it to function either as an extension to an existing FastAPI application or as a standalone service. By offering native dependency management and a unified infrastructure, FastAPI-MCP significantly streamlines the integration of existing FastAPI services into the MCP ecosystem, making it particularly suitable for developing and managing AI-powered tools.",
    "keywords_en": [
      "FastAPI",
      "Model Context Protocol",
      "MCP",
      "API Tools",
      "Authentication",
      "Python",
      "ASGI",
      "AI Tools"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-10T09:07:00Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/fastapi_mcp.png"
    ],
    "extra_info": null
  },
  {
    "id": "SpatialLM",
    "source": "GitHub",
    "url": "https://github.com/manycore-research/SpatialLM",
    "title_en": "SpatialLM",
    "summary_en": "SpatialLM is an innovative 3D large language model specifically engineered to interpret and process complex 3D point cloud data. Its primary function is to generate highly structured 3D scene understanding outputs, which encompass detailed architectural elements like walls, doors, and windows, as well as precisely oriented object bounding boxes complete with their semantic categories. A key advantage of SpatialLM is its versatility in handling point clouds derived from a wide array of sources, including monocular video sequences, RGBD images, and LiDAR sensors, thereby overcoming limitations of previous methods requiring specialized equipment. This multimodal architecture effectively bridges the critical gap between raw, unstructured 3D geometric data and refined, structured 3D representations, offering a high-level semantic understanding of environments. Furthermore, SpatialLM 1.1 introduces advanced capabilities such as doubled point cloud resolution, a more powerful point cloud encoder (Sonata), and the ability to perform detection based on user-specified categories, leveraging the flexibility of LLMs. These features collectively enhance spatial reasoning, making SpatialLM highly valuable for cutting-edge applications in embodied robotics, autonomous navigation, and sophisticated 3D scene analysis tasks.",
    "keywords_en": [
      "3D Large Language Model",
      "Point Cloud Processing",
      "Scene Understanding",
      "Spatial Reasoning",
      "Embodied Robotics",
      "Object Detection"
    ],
    "area_en": [
      "Large Language Model",
      "Computer Vision",
      "Robotics"
    ],
    "published_time": "2025-06-10T02:58:45Z",
    "download_time": "2024-05-15 12:00:00",
    "visual_resource": [
      "https://github.com/manycore-research/SpatialLM/raw/main/figures/stru3d.jpg",
      "https://github.com/manycore-research/SpatialLM/raw/main/figures/scannet.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "magentic-ui",
    "source": "GitHub",
    "url": "https://github.com/microsoft/magentic-ui",
    "title_en": "Magentic-UI",
    "summary_en": "Magentic-UI is a cutting-edge research prototype of a human-centered interface, leveraging a sophisticated multi-agent system to automate complex web tasks while ensuring users retain full control. It excels at browsing and performing actions on the web, generating and executing code, and analyzing various file types, addressing scenarios from form filling to deep website navigation and data-driven code execution. Built on the AutoGen framework, Magentic-UI provides a transparent and highly controllable interaction paradigm, fostering efficient human-in-the-loop involvement. Its core functionalities include collaborative planning (Co-Planning), guided task execution (Co-Tasking), robust security measures (Action Guards), intelligent plan learning and retrieval from past runs, and efficient parallel task execution. This innovative approach significantly boosts human-agent interaction efficiency, making it an ideal solution for intricate web navigation, data extraction, and automated data processing challenges, as demonstrated by its performance on benchmarks like GAIA and AssistantBench.",
    "keywords_en": [
      "Multi-agent System",
      "Human-in-the-loop",
      "Web Automation",
      "Code Execution",
      "File Analysis",
      "LLM Applications",
      "AutoGen",
      "Docker"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-14T17:46:34Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://github.com/microsoft/magentic-ui/blob/main/docs/img/magenticui_running.png?raw=true",
      "https://github.com/microsoft/magentic-ui/blob/main/docs/img/magui-coplanning.png?raw=true"
    ],
    "extra_info": null
  },
  {
    "id": "marker",
    "source": "GitHub",
    "url": "https://github.com/datalab-to/marker",
    "title_en": "Marker",
    "summary_en": "Marker is an efficient and accurate document conversion tool, supporting various file formats such as PDF, images, Office documents, HTML, and EPUB, converting them into Markdown, JSON, chunks, or HTML. Its core features include intelligent recognition and formatting of complex elements like tables, equations, and code blocks, with support for all languages. Marker outperforms existing cloud services and open-source solutions in performance. Furthermore, it can significantly enhance conversion accuracy and structured data extraction capabilities by integrating Large Language Models (LLMs), particularly excelling in table recognition. The tool supports GPU/CPU/MPS operation, offering flexible API and CLI interfaces, making it widely applicable in document digitization, content management, and RAG data preparation.",
    "keywords_en": [
      "Document Conversion",
      "PDF Processing",
      "Structured Extraction",
      "Large Language Model",
      "OCR",
      "Multimodal"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Large Language Model"
    ],
    "published_time": "2025-08-15T23:20:30Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "screenshot/github/marker.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.10433",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.10433",
    "title_en": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual\n  Mathematical Reasoning",
    "summary_en": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.",
    "keywords_en": [
      "Mathematical Reasoning",
      "Multimodal Large Language Models",
      "Reinforcement Learning",
      "Knowledge System",
      "Dataset"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Machine Learning"
    ],
    "published_time": "2025-08-14T08:15:41.000Z",
    "download_time": "2025-08-16 02:30:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10433.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.10433\", \"arxiv_url\": \"https://arxiv.org/abs/2508.10433\"}"
  },
  {
    "id": "2508.10711",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.10711",
    "title_en": "NextStep-1: Toward Autoregressive Image Generation with Continuous\n  Tokens at Scale",
    "summary_en": "Prevailing autoregressive (AR) models for text-to-image generation either\nrely on heavy, computationally-intensive diffusion models to process continuous\nimage tokens, or employ vector quantization (VQ) to obtain discrete tokens with\nquantization loss. In this paper, we push the autoregressive paradigm forward\nwith NextStep-1, a 14B autoregressive model paired with a 157M flow matching\nhead, training on discrete text tokens and continuous image tokens with\nnext-token prediction objectives. NextStep-1 achieves state-of-the-art\nperformance for autoregressive models in text-to-image generation tasks,\nexhibiting strong capabilities in high-fidelity image synthesis. Furthermore,\nour method shows strong performance in image editing, highlighting the power\nand versatility of our unified approach. To facilitate open research, we will\nrelease our code and models to the community.",
    "keywords_en": [
      "Autoregressive models",
      "Text-to-image generation",
      "Continuous tokens",
      "Image synthesis",
      "NextStep-1"
    ],
    "area_en": [
      "Generative AI",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-08-14T14:54:22.000Z",
    "download_time": "2025-08-16 02:30:38",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10711.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.10711\", \"arxiv_url\": \"https://arxiv.org/abs/2508.10711\"}"
  },
  {
    "id": "2508.10881",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.10881",
    "title_en": "ToonComposer: Streamlining Cartoon Production with Generative\n  Post-Keyframing",
    "summary_en": "Traditional cartoon and anime production involves keyframing, inbetweening,\nand colorization stages, which require intensive manual effort. Despite recent\nadvances in AI, existing methods often handle these stages separately, leading\nto error accumulation and artifacts. For instance, inbetweening approaches\nstruggle with large motions, while colorization methods require dense per-frame\nsketches. To address this, we introduce ToonComposer, a generative model that\nunifies inbetweening and colorization into a single post-keyframing stage.\nToonComposer employs a sparse sketch injection mechanism to provide precise\ncontrol using keyframe sketches. Additionally, it uses a cartoon adaptation\nmethod with the spatial low-rank adapter to tailor a modern video foundation\nmodel to the cartoon domain while keeping its temporal prior intact. Requiring\nas few as a single sketch and a colored reference frame, ToonComposer excels\nwith sparse inputs, while also supporting multiple sketches at any temporal\nlocation for more precise motion control. This dual capability reduces manual\nworkload and improves flexibility, empowering artists in real-world scenarios.\nTo evaluate our model, we further created PKBench, a benchmark featuring\nhuman-drawn sketches that simulate real-world use cases. Our evaluation\ndemonstrates that ToonComposer outperforms existing methods in visual quality,\nmotion consistency, and production efficiency, offering a superior and more\nflexible solution for AI-assisted cartoon production.",
    "keywords_en": [
      "ToonComposer",
      "Cartoon Production",
      "Generative AI",
      "Post-Keyframing",
      "Inbetweening"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Generative AI",
      "Video Understanding"
    ],
    "published_time": "2025-08-14T17:50:11.000Z",
    "download_time": "2025-08-16 02:30:41",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10881.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.10881\", \"arxiv_url\": \"https://arxiv.org/abs/2508.10881\"}"
  },
  {
    "id": "2508.09848",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.09848",
    "title_en": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts",
    "summary_en": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.",
    "keywords_en": [
      "Long-context understanding",
      "Benchmark",
      "Global comprehension",
      "Deep reasoning",
      "Large Language Models"
    ],
    "area_en": [
      "Natural Language Processing",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-13T14:28:25.000Z",
    "download_time": "2025-08-16 02:30:37",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09848.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.09848\", \"arxiv_url\": \"https://arxiv.org/abs/2508.09848\"}"
  },
  {
    "id": "2508.10833",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.10833",
    "title_en": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
    "summary_en": "We present UI-Venus, a native UI agent that takes only screenshots as input\nbased on a multimodal large language model. UI-Venus achieves SOTA performance\non both UI grounding and navigation tasks using only several hundred thousand\nhigh-quality training samples through reinforcement finetune (RFT) based on\nQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /\n50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,\nScreenspot-V2 / Pro, surpassing the previous SOTA baselines including\nopen-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and\nplaning ability, we also evaluate it on the AndroidWorld, an online UI\nnavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%\nsuccess rate, also beating existing models.To achieve this, we introduce\ncarefully designed reward functions for both UI grounding and navigation tasks\nand corresponding efficient data cleaning strategies.To further boost\nnavigation performance, we propose Self-Evolving Trajectory History Alignment\n\\& Sparse Action Enhancement that refine historical reasoning traces and\nbalances the distribution of sparse but critical actions, leading to more\ncoherent planning and better generalization in complex UI tasks. Our\ncontributions include the publish of SOTA open-source UI agents, comprehensive\ndata cleaning protocols and a novel self-evolving framework for improving\nnavigation performance, which encourage further research and development in the\ncommunity. Code is available at https://github.com/antgroup/UI-Venus.",
    "keywords_en": [
      "UI Agent",
      "Multimodal Large Language Model",
      "Reinforcement Finetune",
      "UI Grounding",
      "UI Navigation"
    ],
    "area_en": [
      "AI Agent",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-08-14T16:58:07.000Z",
    "download_time": "2025-08-16 02:30:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10833.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.10833\", \"arxiv_url\": \"https://arxiv.org/abs/2508.10833\"}"
  },
  {
    "id": "2508.10875",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.10875",
    "title_en": "A Survey on Diffusion Language Models",
    "summary_en": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
    "keywords_en": [
      "Diffusion Language Models",
      "Natural Language Processing",
      "Generative AI",
      "Survey",
      "Autoregressive Models"
    ],
    "area_en": [
      "Natural Language Processing",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-08-14T17:47:22.000Z",
    "download_time": "2025-08-16 02:30:40",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10875.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.10875\", \"arxiv_url\": \"https://arxiv.org/abs/2508.10875\"}"
  }
]