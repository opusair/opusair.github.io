[
  {
    "id": "twitter_TheTuringPost_1937290428929442105",
    "source": "Twitter",
    "url": "https://x.com/TheTuringPost/status/1937290428929442105",
    "title_en": "TheTuringPost_This Week's Top AI Research Papers",
    "summary_en": "The TuringPost has released its top seven selected AI research papers of the week, covering cutting-edge topics such as LLM reasoning, reinforcement learning, budget guidance, polysemanticity handling, reasoning efficiency optimization, and agent technology. These studies aim to enhance the performance, efficiency, and verifiability of large language models, exploring new training and application paradigms, and offering valuable insights for AI researchers and developers.",
    "keywords_en": [
      "LLM",
      "Reinforcement Learning",
      "Reasoning",
      "AI Agent",
      "Research Papers",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Large Language Model",
      "Research Progress",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-23T23:23:26.000Z",
    "download_time": "2025-06-24 03:58:42",
    "visual_resource": [
      "screenshot/twitter/TheTuringPost_1937290428929442105.png"
    ],
    "extra_info": "{\"username\": \"TheTuringPost\", \"tweet_id\": \"1937290428929442105\"}"
  },
  {
    "id": "twitter_SakanaAILabs_1936965841188425776",
    "source": "Twitter",
    "url": "https://twitter.com/SakanaAILabs/status/1936965841188425776",
    "title_en": "SakanaAILabs_Sakana AI Introduces RLTs: Enhancing LLM Reasoning with Reinforcement Learning",
    "summary_en": "Sakana AI introduces Reinforcement-Learned Teachers (RLTs), revolutionizing how LLMs are taught reasoning. RLT models are trained via reinforcement learning to generate clear, step-by-step \"explanations\" for student models. A 7B RLT remarkably outperforms much larger LLMs in distilling and cold-starting student models on competitive and graduate-level reasoning tasks. It effectively distills even 32B student models, significantly enhancing efficiency in developing reasoning language models.",
    "keywords_en": [
      "Reinforcement-Learned Teachers",
      "RLT",
      "Large Language Models",
      "Reasoning",
      "Reinforcement Learning",
      "Model Distillation"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Research Progress"
    ],
    "published_time": "2025-06-23T01:53:38.000Z",
    "download_time": "2025-06-24 04:19:01",
    "visual_resource": [
      "screenshot/twitter/SakanaAILabs_1936965841188425776.png"
    ],
    "extra_info": "{\"username\": \"SakanaAILabs\", \"tweet_id\": \"1936965841188425776\"}"
  },
  {
    "id": "twitter_brianchristian_1937171802360557817",
    "source": "Twitter",
    "url": "https://x.com/brianchristian/status/1937171802360557817",
    "title_en": "brianchristian_Large-Scale Analysis of LLM Reward Models Reveals Complexity and Biases",
    "summary_en": "Brian Christian conducted the first exhaustive large-scale analysis of 10 leading large language model reward models. The study revealed significant disagreements, base-model imprints, identity-term biases, and mere-exposure quirks among these models, which are considered the \"moral compass\" of LLMs. This research highlights the complex and potentially problematic internal workings of reward models.",
    "keywords_en": [
      "Reward Models",
      "Large Language Models",
      "Model Bias",
      "AI Ethics",
      "Model Analysis"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Research Progress"
    ],
    "published_time": "2025-06-23T15:32:03.000Z",
    "download_time": "2025-06-24 04:04:43",
    "visual_resource": [
      "screenshot/twitter/brianchristian_1937171802360557817.png"
    ],
    "extra_info": "{\"username\": \"brianchristian\", \"tweet_id\": \"1937171802360557817\"}"
  },
  {
    "id": "twitter_Yoshua_Bengio_1937206510708293902",
    "source": "Twitter",
    "url": "https://twitter.com/Yoshua_Bengio/status/1937206510708293902",
    "title_en": "Yoshua_Bengio_Warns of Sharply Rising AI-Driven Cyberattack Risks",
    "summary_en": "Renowned AI scientist Yoshua Bengio warns that as frontier AI systems increase in capability and agency, the risk of AI-driven cyberattacks will likely rise sharply. He emphasizes that tasks once performed by elite hackers may soon be carried out autonomously, demanding urgent attention. The tweet quotes Dawn Song's work, highlighting AI agents' breakthrough in cybersecurity, including discovering 15 zero-day vulnerabilities via CyberGym and solving real-world bug bounty tasks worth tens of thousands of dollars through BountyBench.",
    "keywords_en": [
      "Artificial Intelligence",
      "Cybersecurity",
      "AI Agents",
      "Zero-day",
      "Cyberattacks",
      "Bug Bounty"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Tech News"
    ],
    "published_time": "2025-06-23T17:49:58.000Z",
    "download_time": "2025-06-24 04:19:39",
    "visual_resource": [
      "screenshot/twitter/Yoshua_Bengio_1937206510708293902.png"
    ],
    "extra_info": "{\"username\": \"Yoshua_Bengio\", \"tweet_id\": \"1937206510708293902\"}"
  },
  {
    "id": "twitter_hwchase17_1937194145074020798",
    "source": "Twitter",
    "url": "https://twitter.com/hwchase17/status/1937194145074020798",
    "title_en": "hwchase17_The Rise and Definition of Context Engineering",
    "summary_en": "Harrison Chase introduces \"context engineering\" as building dynamic systems to provide large language models with the right information and tools in the right format for task accomplishment. He notes that while not a new concept, the term's rise aims to draw attention to the skills and tools needed. He highlights LangChain's tools like LangGraph and LangSmith as instrumental in assisting AI engineers with this system building.",
    "keywords_en": [
      "Context Engineering",
      "Large Language Model",
      "AI Engineer",
      "LangChain",
      "AI Agent"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-23T17:00:50.000Z",
    "download_time": "2025-06-24 04:19:14",
    "visual_resource": [
      "screenshot/twitter/hwchase17_1937194145074020798.png"
    ],
    "extra_info": "{\"username\": \"hwchase17\", \"tweet_id\": \"1937194145074020798\"}"
  },
  {
    "id": "twitter_GaryMarcus_1937147381507903552",
    "source": "Twitter",
    "url": "https://x.com/GaryMarcus/status/1937147381507903552",
    "title_en": "GaryMarcus_AI Development Dilemma: Brute Force Fails to Achieve AGI and SOTA",
    "summary_en": "Gary Marcus, quoting Carlos E. Perez, highlights that \"brute force\" alone is insufficient to achieve Artificial General Intelligence (AGI) or State-of-the-Art (SOTA) AI. The tweet suggests that Meta and X are currently in an AI crisis, having realized that their reliance on massive computation and data accumulation is not yielding significant progress. This indicates a growing \"panic\" within the AI field and an increasing reflection on current development methodologies.",
    "keywords_en": [
      "Artificial Intelligence",
      "AGI",
      "SOTA",
      "Brute Force AI",
      "Meta",
      "X"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Tech News",
      "Industry News"
    ],
    "published_time": "2025-06-23T13:55:00.000Z",
    "download_time": "2025-06-24 04:02:41",
    "visual_resource": [
      "screenshot/twitter/GaryMarcus_1937147381507903552.png"
    ],
    "extra_info": "{\"username\": \"GaryMarcus\", \"tweet_id\": \"1937147381507903552\"}"
  },
  {
    "id": "oqognmBzmEqWjiEeRh6WTA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/oqognmBzmEqWjiEeRh6WTA",
    "title_en": "Tencent's Hunyuan GameCraft: AIGC Reshapes Immersive AAA Games with Real-time Precise Control and Million-Scale Data Training",
    "summary_en": "Tencent has introduced the Hunyuan GameCraft framework, leveraging AIGC technology to redefine immersive AAA gaming experiences. This innovative framework achieves real-time precise control over in-game actions by unifying keyboard and mouse inputs into a shared camera representation space. It employs a hybrid historical conditional training strategy to autoregressively extend video sequences while preserving game scene information. To enhance inference efficiency and playability, the model incorporates distillation techniques, maintaining consistency over long sequences for real-time deployment in complex interactive environments. GameCraft is trained on a massive dataset comprising over one million AAA game recordings, further fine-tuned with a meticulously annotated synthetic dataset. This extensive training significantly improves visual fidelity, realism, and action controllability, demonstrating strong potential for natural control in third-person gaming scenarios.",
    "keywords_en": [
      "AIGC",
      "Game Generation",
      "Real-time Control",
      "Immersive Gaming",
      "Game AI",
      "Large-scale Dataset"
    ],
    "area_en": [
      "Generative AI",
      "AI Agent",
      "Computer Vision"
    ],
    "published_time": "2025-06-23T23:45:34.000Z",
    "download_time": "2025-06-24T12:13:40.961954",
    "visual_resource": [
      "screenshot/wechat/wechat_image_oqognmBzmEqWjiEeRh6WTA.png"
    ],
    "extra_info": null
  },
  {
    "id": "XZADpyE7BXg1KnQHpxcbYA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/XZADpyE7BXg1KnQHpxcbYA",
    "title_en": "Tencent Releases Hunyuan-GameCraft! From Static Images to Dynamic Game Worlds",
    "summary_en": "Tencent has unveiled Hunyuan-GameCraft, a novel technical framework designed to overcome existing challenges in dynamic expression, general adaptability, long-term consistency, and operational efficiency for interactive game video generation. This innovative system can generate complete game interaction videos from just a single image and a text prompt. It unifies keyboard and mouse operations, integrating movement commands and viewpoint controls into a shared camera representation space, enabling smooth transitions. The system precisely responds to interaction signals, automatically extending time-consistent and 3D-coherent video content while preserving historical scene information. Hunyuan-GameCraft is trained on data from over a hundred AAA titles, demonstrating superior performance in environments like Minecraft, particularly excelling in 3D consistency and scene coherence. This breakthrough offers new possibilities for immersive gaming experiences, although its current action space is primarily focused on open-world exploration.",
    "keywords_en": [
      "Hunyuan-GameCraft",
      "Game Video Generation",
      "Interactive Video",
      "Diffusion Model",
      "3D Consistency"
    ],
    "area_en": [
      "Generative AI",
      "Video Understanding",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-23T12:30:19.000Z",
    "download_time": "2025-06-24T12:13:40.858787",
    "visual_resource": [
      "screenshot/wechat/wechat_image_XZADpyE7BXg1KnQHpxcbYA.png"
    ],
    "extra_info": null
  },
  {
    "id": "1gnvHOOzBmDRJd8kGWquNg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/1gnvHOOzBmDRJd8kGWquNg",
    "title_en": "Deep Dive | Former Instagram Co-founder, Anthropic CPO: A Truly Excellent AI Product Should Know When to Shut Up",
    "summary_en": "Anthropic CPO Mike Krieger, co-founder of Instagram, shared profound insights on his evolving perception of AI, shifting from viewing it merely as a tool to a \"partner capable of original thought.\" He highlighted that AI now writes 70-90% of code, fundamentally reshaping traditional product development. Product managers are transforming into \"system designers,\" directly involved in post-training models. Krieger emphasized that truly excellent AI products should \"know when to shut up,\" acting as thought partners rather than undermining human judgment. Anthropic differentiates itself by focusing on serving developers and builders. The article further explores AI's rapid advancements, human-AI collaboration paradigms, the cultivation of future skills like curiosity and systemic thinking, the dramatic changes in engineering roles and team collaboration driven by AI, and the core value of product teams in the age of models (understandability design, strategic focus, and educating on possibilities).",
    "keywords_en": [
      "AI Product",
      "Human-AI Collaboration",
      "Product Manager",
      "Large Language Model",
      "Anthropic"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-06-23T12:01:49.000Z",
    "download_time": "2025-06-24T12:13:42.631379",
    "visual_resource": [
      "screenshot/wechat/wechat_image_1gnvHOOzBmDRJd8kGWquNg.png"
    ],
    "extra_info": null
  },
  {
    "id": "vHUZ3Dd0f24gdQ-JwiSk4A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/vHUZ3Dd0f24gdQ-JwiSk4A",
    "title_en": "Tencent Youtu Introduces Incentivizing Reasoning for Complex Instructions, Boosting Performance by 11.74%",
    "summary_en": "Addressing the persistent challenges Large Language Models (LLMs) face in understanding and executing complex instructions, Tencent Youtu's research team has introduced a systematic approach called \"Incentivizing Reasoning.\" This novel method enhances LLMs' deep reasoning capabilities by meticulously decomposing complex instructions and integrating Reinforcement Learning (RL) with rule-driven reward mechanisms. Experimental results demonstrate that Incentivizing Reasoning significantly improves the performance of most LLMs when handling intricate commands. Notably, a 1.5B parameter LLM achieved an impressive 11.74% performance boost, making its capabilities comparable to those of an 8B parameter model. This approach also substantially outperforms conventional methods like Chain-of-Thought (CoT), which often exhibit superficial reasoning patterns. By fostering a more profound understanding and structured thinking, Incentivizing Reasoning offers a promising new avenue for enhancing LLM generalization abilities and instruction-following precision, particularly for smaller models, pushing the boundaries of what compact LLMs can achieve in complex reasoning tasks.",
    "keywords_en": [
      "Incentivizing Reasoning",
      "Large Language Models",
      "Complex Instructions",
      "Reinforcement Learning",
      "Tencent Youtu"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-06-23T08:11:45.000Z",
    "download_time": "2025-06-24T12:13:43.000976",
    "visual_resource": [
      "screenshot/wechat/wechat_image_vHUZ3Dd0f24gdQ-JwiSk4A.png"
    ],
    "extra_info": null
  },
  {
    "id": "LWORE7ZPWW-qZ-_hB-Wf7A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/LWORE7ZPWW-qZ-_hB-Wf7A",
    "title_en": "From Shaving Robots to Dual-Arm Mastery! This Embodied AI Unicorn Ignites a Hundred-Million-Dollar Funding Frenzy",
    "summary_en": "Flexiv, an embodied AI unicorn, recently secured a multi-hundred-million-dollar Series C funding round. The company has achieved significant breakthroughs in physical world interaction and perception-motion strategies with its pioneering adaptive robots. Through end-to-end model training, Flexiv's robots demonstrate highly precise dual-arm collaborative operations and complex tasks like robotic shaving. Flexiv's full-stack R&D and \"general + humanoid\" technological approach enable its robots to exhibit flexibility, anti-interference capabilities, easy deployment, and transferability. These robots are widely applied in challenging scenarios across industries such as automotive, 3C electronics, and healthcare, where traditional automation struggles. The company's general robot base platform and robust ecosystem are driving the industrialization of embodied AI, with thousands of units already delivered, positioning Flexiv as a leader in the field.",
    "keywords_en": [
      "Embodied AI",
      "Adaptive Robot",
      "Flexiv",
      "Robotics",
      "Funding",
      "General Robot"
    ],
    "area_en": [
      "Robotics",
      "Artificial Intelligence",
      "AI Agent"
    ],
    "published_time": "2025-06-23T05:12:32.000Z",
    "download_time": "2025-06-24T12:13:41.969541",
    "visual_resource": [
      "screenshot/wechat/wechat_image_LWORE7ZPWW-qZ-_hB-Wf7A.png"
    ],
    "extra_info": null
  },
  {
    "id": "N5kZkqDo1o40L9JIK98R9Q",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/N5kZkqDo1o40L9JIK98R9Q",
    "title_en": "Reasoning Accuracy Drops 65.5%! Stanford, MIT, etc., Challenge AI's Logic Limits with 'Inequalities'",
    "summary_en": "A joint research team from Stanford, MIT, and UC Berkeley has introduced the IneqMath benchmark to rigorously evaluate large language models' (LLMs) mathematical reasoning capabilities, particularly with inequalities. This benchmark dissects inequality proofs into natural language tasks and employs an innovative LLM-as-Judge framework, featuring five independent \"judges\" to meticulously analyze each step of a model's reasoning chain. Evaluations across 29 mainstream LLMs, including GPT-4 and Claude, reveal a significant disparity: while final answer accuracy might be acceptable, the accuracy of the underlying reasoning process is remarkably low, dropping by up to 65.5%. This highlights a severe structural flaw in current LLMs' logical rigor, indicating that even correct answers do not guarantee sound reasoning. The research team has launched an open leaderboard to foster advancements in LLMs' rigorous mathematical argumentation.",
    "keywords_en": [
      "Large Language Models",
      "Mathematical Reasoning",
      "Inequalities",
      "Logical Rigor",
      "Evaluation Benchmark",
      "LLM-as-Judge"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "published_time": "2025-06-23T05:12:32.000Z",
    "download_time": "2025-06-24T12:13:51.993786",
    "visual_resource": [
      "screenshot/wechat/wechat_image_N5kZkqDo1o40L9JIK98R9Q.png"
    ],
    "extra_info": null
  },
  {
    "id": "void",
    "source": "GitHub",
    "url": "https://github.com/voideditor/void",
    "title_en": "Welcome to Void.",
    "summary_en": "Void is an open-source alternative to Cursor, designed as a powerful code editor integrating AI agent capabilities. It enables users to leverage AI agents directly on their codebase, supporting checkpointing and visualization of code changes, and allowing for local deployment or hosting of various AI models. Emphasizing data privacy, Void communicates directly with AI providers without retaining user data. The project is a fork of VS Code, offering its full source code and actively encouraging community contributions.",
    "keywords_en": [
      "Code Editor",
      "AI Agent",
      "Open Source",
      "VS Code",
      "Code Assistance",
      "Privacy Protection"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Generative AI"
    ],
    "published_time": "2025-06-23T08:05:25Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/voideditor/void/main/src/vs/workbench/browser/parts/editor/media/slice_of_void.png"
    ],
    "extra_info": null
  },
  {
    "id": "suna",
    "source": "GitHub",
    "url": "https://github.com/kortix-ai/suna",
    "title_en": "Suna - Open Source Generalist AI Agent",
    "summary_en": "Suna is an open-source generalist AI agent designed to help users accomplish real-world tasks with ease through natural conversation. It integrates powerful capabilities such as seamless browser automation, file management, web crawling, command-line execution, website deployment, and integration with various APIs and services. Suna understands user needs and delivers results by harmonizing these capabilities to solve complex problems and automate workflows, making it an ideal digital companion for research, data analysis, and everyday challenges. Its architecture is built upon a Python/FastAPI backend, Next.js/React frontend, an isolated Agent Docker environment, and a Supabase database, supporting multiple LLM providers.",
    "keywords_en": [
      "AI Agent",
      "Open Source",
      "Automation",
      "Web Crawling",
      "Task Management",
      "LLM Application",
      "General AI"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-06-23T16:19:12Z",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "https://github.com/kortix-ai/suna/raw/main/frontend/public/banner.png",
      "https://github.com/kortix-ai/suna/raw/main/docs/images/diagram.png"
    ],
    "extra_info": null
  },
  {
    "id": "system-prompts-and-models-of-ai-tools",
    "source": "GitHub",
    "url": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
    "title_en": "FULL v0, Cursor, Manus, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent, VSCode Agent, Dia Browser & Trae AI (And other Open Sourced) System Prompts, Tools & AI Models",
    "summary_en": "This GitHub repository compiles and open-sources system prompts and internal tools from various prominent AI tools and models, including v0, Cursor, Manus, Devin, and Replit Agent, offering over 7000 lines of insights into their structure and functionality. The project aims to help developers and security researchers deeply understand the inner workings of AI systems, while also emphasizing the critical importance of AI system security by offering audit services. It serves as a valuable resource for studying and analyzing the underlying mechanisms of AI agents.",
    "keywords_en": [
      "System Prompts",
      "AI Models",
      "AI Agents",
      "Reverse Engineering",
      "AI Security",
      "Internal Tools"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-21T11:30:10Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date&theme=dark"
    ],
    "extra_info": null
  },
  {
    "id": "Web-Dev-For-Beginners",
    "source": "GitHub",
    "url": "https://github.com/microsoft/Web-Dev-For-Beginners",
    "title_en": "Web Development for Beginners - A Curriculum",
    "summary_en": "This comprehensive 12-week web development curriculum, meticulously crafted by Microsoft Cloud Advocates, provides a robust foundation in JavaScript, CSS, and HTML. It features 24 immersive, hands-on lessons, guiding learners through the creation of diverse projects like interactive terrariums, practical browser extensions, and engaging space games. The pedagogical approach emphasizes active learning through integrated quizzes, collaborative discussions, and practical assignments, ensuring optimal skill development and knowledge retention. This project-based methodology is designed to bridge the gap between theoretical understanding and practical application, preparing students for real-world web development challenges. Moreover, the team has recently launched an exciting new curriculum focused on Generative AI for JavaScript, offering an advanced learning path for developers keen on exploring cutting-edge artificial intelligence applications within the web ecosystem. This expansion highlights the commitment to providing relevant and forward-thinking educational resources.",
    "keywords_en": [
      "Web Development",
      "Front-end Development",
      "JavaScript",
      "HTML",
      "CSS",
      "Project-based Learning",
      "Programming Education",
      "Generative AI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Generative AI",
      "Others"
    ],
    "published_time": "2025-05-29T17:34:21Z",
    "download_time": "2024-07-09 07:20:00",
    "visual_resource": [
      "https://github.com/microsoft/Web-Dev-For-Beginners/raw/main/images/background.png",
      "https://github.com/microsoft/Web-Dev-For-Beginners/raw/main/images/character.png",
      "https://github.com/microsoft/Web-Dev-For-Beginners/raw/main/images/createcodespace.png"
    ],
    "extra_info": null
  },
  {
    "id": "ComfyUI",
    "source": "GitHub",
    "url": "https://github.com/comfyanonymous/ComfyUI",
    "title_en": "ComfyUI",
    "summary_en": "ComfyUI is a powerful and modular visual AI engine and application that enables users to design and execute advanced Stable Diffusion pipelines through a graph/nodes/flowchart-based interface. It supports a wide range of image, video, audio, and 3D models, including SDXL, Stable Cascade, SD3, SVD, Stable Audio, and Hunyuan3D. The platform features an asynchronous queue system, smart memory management, and various optimizations, allowing efficient operation even with low VRAM. ComfyUI facilitates the creation of complex AI workflows without coding, supporting diverse model formats and advanced functionalities like ControlNet, Inpainting, and model merging, making it a robust tool in the field of AI content generation.",
    "keywords_en": [
      "Visual AI",
      "Stable Diffusion",
      "Node-based Programming",
      "Image Generation",
      "Video Generation",
      "Deep Learning",
      "AI Models",
      "Workflow"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-06-23T18:04:49Z",
    "download_time": "2024-07-30 12:34:56",
    "visual_resource": [
      "screenshot/github/ComfyUI.png"
    ],
    "extra_info": null
  },
  {
    "id": "2506.16406",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.16406",
    "title_en": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
    "summary_en": "Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank\nadaptation (LoRA) reduce the cost of customizing large language models (LLMs),\nyet still require a separate optimization run for every downstream dataset. We\nintroduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned\nparameter generator that eliminates per-task training by mapping a handful of\nunlabeled task prompts directly to LoRA weight updates. A lightweight text\nencoder distills each prompt batch into condition embeddings, which are then\ntransformed by a cascaded hyper-convolutional decoder into the full set of LoRA\nmatrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD\nproduces task-specific parameters in seconds, yielding i) up to\n12,000times lower overhead than full fine-tuning, ii) average gains\nup to 30\\% in performance over the strongest training LoRAs on unseen\ncommon-sense reasoning, math, coding, and multimodal benchmarks, and iii)\nrobust cross-domain generalization despite never seeing the target data or\nlabels. Our results demonstrate that prompt-conditioned parameter generation is\na viable alternative to gradient-based adaptation for rapidly specializing\nLLMs. Our project is available at\nhttps://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.",
    "keywords_en": [
      "Large Language Models",
      "Parameter-Efficient Fine-Tuning",
      "LoRA",
      "Prompt-Conditioned Parameter Generation",
      "Zero-Shot"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-06-19T15:38:21.000Z",
    "download_time": "2025-06-23 21:14:12",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16406.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.16406\", \"arxiv_url\": \"https://arxiv.org/abs/2506.16406\"}"
  },
  {
    "id": "2506.09049",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.09049",
    "title_en": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
    "summary_en": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.",
    "keywords_en": [
      "Embodied Multi-Agent",
      "Reinforcement Learning",
      "Vision-Language Model",
      "Cooperation",
      "Benchmark"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Robotics"
    ],
    "published_time": "2025-06-10T17:59:44.000Z",
    "download_time": "2025-06-23 21:14:11",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09049.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.09049\", \"arxiv_url\": \"https://arxiv.org/abs/2506.09049\"}"
  },
  {
    "id": "2506.17201",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.17201",
    "title_en": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
    "summary_en": "Recent advances in diffusion-based and controllable video generation have\nenabled high-quality and temporally coherent video synthesis, laying the\ngroundwork for immersive interactive gaming experiences. However, current\nmethods face limitations in dynamics, generality, long-term consistency, and\nefficiency, which limit the ability to create various gameplay videos. To\naddress these gaps, we introduce Hunyuan-GameCraft, a novel framework for\nhigh-dynamic interactive video generation in game environments. To achieve\nfine-grained action control, we unify standard keyboard and mouse inputs into a\nshared camera representation space, facilitating smooth interpolation between\nvarious camera and movement operations. Then we propose a hybrid\nhistory-conditioned training strategy that extends video sequences\nautoregressively while preserving game scene information. Additionally, to\nenhance inference efficiency and playability, we achieve model distillation to\nreduce computational overhead while maintaining consistency across long\ntemporal sequences, making it suitable for real-time deployment in complex\ninteractive environments. The model is trained on a large-scale dataset\ncomprising over one million gameplay recordings across over 100 AAA games,\nensuring broad coverage and diversity, then fine-tuned on a carefully annotated\nsynthetic dataset to enhance precision and control. The curated game scene data\nsignificantly improves the visual fidelity, realism and action controllability.\nExtensive experiments demonstrate that Hunyuan-GameCraft significantly\noutperforms existing models, advancing the realism and playability of\ninteractive game video generation.",
    "keywords_en": [
      "Interactive video generation",
      "Game video",
      "Diffusion models",
      "Model distillation",
      "Action control"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-06-20T17:50:37.000Z",
    "download_time": "2025-06-23 21:14:10",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17201.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.17201\", \"arxiv_url\": \"https://arxiv.org/abs/2506.17201\"}"
  },
  {
    "id": "2506.15745",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.15745",
    "title_en": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
    "summary_en": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
    "keywords_en": [
      "KV Cache Compression",
      "Streaming Video Understanding",
      "Multimodal LLMs",
      "Memory Optimization",
      "Edge AI"
    ],
    "area_en": [
      "Video Understanding",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-06-18T02:22:14.000Z",
    "download_time": "2025-06-23 21:14:10",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15745.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.15745\", \"arxiv_url\": \"https://arxiv.org/abs/2506.15745\"}"
  },
  {
    "id": "2506.16504",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.16504",
    "title_en": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate\n  Details",
    "summary_en": "In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion\nmodels aimed at generating high-fidelity and detailed textured 3D assets.\nHunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D\n2.0, while demonstrating substantial advancements in both shape and texture\ngeneration. In terms of shape generation, we introduce a new shape foundation\nmodel -- LATTICE, which is trained with scaled high-quality datasets,\nmodel-size, and compute. Our largest model reaches 10B parameters and generates\nsharp and detailed 3D shape with precise image-3D following while keeping mesh\nsurface clean and smooth, significantly closing the gap between generated and\nhandcrafted 3D shapes. In terms of texture generation, it is upgraded with\nphyiscal-based rendering (PBR) via a novel multi-view architecture extended\nfrom Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D\n2.5 significantly outperforms previous methods in both shape and end-to-end\ntexture generation.",
    "keywords_en": [
      "Hunyuan3D 2.5",
      "3D Asset Generation",
      "Diffusion Models",
      "Shape Generation",
      "Texture Generation"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-06-19T17:57:40.000Z",
    "download_time": "2025-06-23 21:14:12",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16504.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.16504\", \"arxiv_url\": \"https://arxiv.org/abs/2506.16504\"}"
  },
  {
    "id": "2506.17113",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.17113",
    "title_en": "MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert\n  Aggregation",
    "summary_en": "Combining pre-trained expert models offers substantial potential for scalable\nmultimodal reasoning, but building a unified framework remains challenging due\nto the increasing diversity of input modalities and task complexity. For\ninstance, medical diagnosis requires precise reasoning over structured clinical\ntables, while financial forecasting depends on interpreting plot-based data to\nmake informed predictions. To tackle this challenge, we introduce MEXA, a\ntraining-free framework that performs modality- and task-aware aggregation of\nmultiple expert models to enable effective multimodal reasoning across diverse\nand distinct domains. MEXA dynamically selects expert models based on the input\nmodality and the task-specific reasoning demands (i.e., skills). Each expert\nmodel, specialized in a modality task pair, generates interpretable textual\nreasoning outputs. MEXA then aggregates and reasons over these outputs using a\nLarge Reasoning Model (LRM) to produce the final answer. This modular design\nallows flexible and transparent multimodal reasoning across diverse domains\nwithout additional training overhead. We extensively evaluate our approach on\ndiverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D\nUnderstanding, and Medical QA. MEXA consistently delivers performance\nimprovements over strong multimodal baselines, highlighting the effectiveness\nand broad applicability of our expert-driven selection and aggregation in\ndiverse multimodal reasoning tasks.",
    "keywords_en": [
      "Multimodal Reasoning",
      "Expert Models",
      "Dynamic Aggregation",
      "Large Reasoning Model",
      "Training-free Framework"
    ],
    "area_en": [
      "Multimodal",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-06-20T16:14:13.000Z",
    "download_time": "2025-06-23 21:14:10",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17113.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.17113\", \"arxiv_url\": \"https://arxiv.org/abs/2506.17113\"}"
  }
]