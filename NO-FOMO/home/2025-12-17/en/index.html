<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-17</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-12-17</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Gemini 3 Flash: frontier intelligence built for speed</h2>
                <span class="published-time">Published: 2025-12-17 16:42:13</span>
                
                <p class="summary">Google has officially unveiled Gemini 3 Flash, a new and highly optimized version of its advanced Gemini frontier intelligence models, specifically engineered for unparalleled speed and efficiency. This development highlights Google's strategic focus on empowering developers with cutting-edge AI capabilities that are not only powerful but also incredibly performant and cost-effective. Gemini 3 Flash is designed as a lightweight, fast-inference model, making it exceptionally well-suited for applications demanding rapid responses and high throughput. This includes scenarios such as real-time content generation, efficient summarization, dynamic chatbot interactions, and sophisticated AI agentic workflows. By prioritizing speed, Google aims to facilitate the quicker deployment and broader scaling of advanced AI solutions, concurrently reducing operational overhead while maintaining a high standard of intelligence and accuracy. This release marks a significant step in the ongoing evolution of AI, balancing robust capabilities with crucial practical considerations like low latency and optimized resource consumption.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini 3 Flash</span><span>Large Language Model</span><span>AI Performance</span><span>Model Optimization</span><span>Frontier AI</span><span>Developer Tools</span><span>Google AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://blog.google/products/gemini/gemini-3-flash/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AWS CEO says replacing junior devs with AI is 'one of the dumbest ideas'</h2>
                <span class="published-time">Published: 2025-12-17 17:08:35</span>
                
                <p class="summary">The CEO of Amazon Web Services (AWS) has publicly stated that the notion of artificial intelligence replacing junior developers is "one of the dumbest ideas." This strong stance from a leading figure in the cloud computing and AI infrastructure industry highlights a prevailing sentiment among some tech leaders regarding the future of AI in the workforce. Rather than envisioning AI as a direct substitute for human talent, especially at entry-level positions, the perspective emphasizes AI's role as an augmentation tool. It suggests that AI should empower developers, streamline workflows, and enhance productivity, allowing human professionals to focus on higher-level problem-solving, creativity, and strategic tasks. This viewpoint underscores the belief that human innovation and nuanced understanding remain indispensable in software development, with AI serving as a powerful assistant to amplify human capabilities rather than diminish the need for human input and development pathways. This perspective offers a critical counter-narrative to fears of widespread job displacement by AI in the technology sector.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Artificial Intelligence</span><span>Software Development</span><span>Developer Productivity</span><span>AI Workforce</span><span>Cloud Computing</span><span>AI Integration</span><span>AWS</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.finalroundai.com/blog/aws-ceo-ai-cannot-replace-junior-developers" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Why outcome-billing makes sense for AI Agents</h2>
                <span class="published-time">Published: 2025-12-17 18:02:12</span>
                
                <p class="summary">The article advocates for outcome-based billing models as an imperative for the successful deployment and monetization of AI Agents, particularly highlighting their advantages over traditional usage or token-based pricing. The core argument posits that AI agents, due to their autonomous and often unpredictable operational paths, present significant challenges for conventional billing, leading to opaque costs and misaligned incentives. Outcome-billing addresses this by proposing compensation directly tied to the measurable business value or specific tasks completed by the agent, such as successfully closing a sale, resolving a customer query, or automating a complex workflow. This paradigm shift is crucial for fostering trust, promoting efficiency, and ensuring accountability, as providers are incentivized to optimize agents for tangible results rather than mere activity. Implementing such a model is seen as vital for the widespread adoption and sustainable growth of the AI agent ecosystem, enabling businesses to confidently invest in AI solutions with clear, demonstrable returns.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Outcome-based billing</span><span>AI monetization</span><span>Autonomous systems</span><span>Value proposition</span><span>Billing models</span><span>Artificial Intelligence</span><span>Business models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Artificial Intelligence</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.valmi.io/blog/an-imperative-for-ai-agents-outcome-billing-with-valmi/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Doublespeed hacked, revealing what its AI-generated accounts are promoting</h2>
                <span class="published-time">Published: 2025-12-17 18:16:58</span>
                
                <p class="summary">A recent security breach has targeted Doublespeed, an a16z-backed company, exposing the intricate operations of its network of AI-generated accounts, predominantly active on TikTok. The hack specifically revealed the nature and scope of content these AI-driven influencers were promoting, thereby shedding critical light on a sophisticated "phone farm" system. This system is designed to autonomously disseminate AI-generated media and simulate user engagement across major social platforms. The incident underscores growing concerns surrounding the proliferation of synthetic media, automated social media manipulation, and the potential for advanced AI technologies to covertly influence public perception on a massive scale. This revelation offers crucial insight into the methods employed by such operations to simulate genuine user interaction and content creation through artificial intelligence, raising significant questions about platform integrity, digital authenticity, and the ethical implications of deploying AI at scale for marketing or influence campaigns. The event further highlights the inherent vulnerabilities within such AI-powered social manipulation systems and their broader implications for the integrity of the digital information ecosystem.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI-generated content</span><span>Social media manipulation</span><span>AI influencers</span><span>Phone farm technology</span><span>Hacking</span><span>Synthetic media</span><span>Digital authenticity</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.404media.co/hack-reveals-the-a16z-backed-phone-farm-flooding-tiktok-with-ai-influencers/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Launch HN: Kenobi (YC W22) ‚Äì Personalize your website for every visitor</h2>
                <span class="published-time">Published: 2025-12-17 16:44:13</span>
                
                <p class="summary">Kenobi, a Y Combinator Winter 2022 startup founded by Rory, Chris, and Felix, has launched its AI-based content personalization platform designed to transform static websites into dynamic, visitor-tailored experiences. The core offering is a personalization widget that site owners can easily integrate using a simple script tag. When visitors interact with the widget, for instance by inputting a company name, Kenobi leverages AI to dynamically adapt the website's content to suit that specific visitor. The founders emphasize that the advanced capabilities of Large Language Models (LLMs) are pivotal in enabling this shift, making it feasible to convert traditionally static HTML into highly responsive and personalized web pages. Kenobi is currently targeting B2B landing pages, aiming to significantly boost top-of-funnel inbound conversions by offering hyper-relevant content. A live demo is provided for prospective users to test the personalization technology firsthand, illustrating its potential to enhance user engagement and commercial outcomes.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI-based content personalization</span><span>Website personalization</span><span>Large Language Models (LLMs)</span><span>B2B marketing</span><span>Dynamic websites</span><span>AI widget</span><span>Inbound conversion</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://news.ycombinator.com/item?id=46301865" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AI's real superpower: consuming, not creating</h2>
                <span class="published-time">Published: 2025-12-17 08:34:00</span>
                
                <p class="summary">The article "AI's real superpower: consuming, not creating" advocates for a reevaluation of artificial intelligence's most impactful capabilities, suggesting that its core strength lies not in generating entirely new content but rather in its extraordinary capacity to consume, process, and synthesize vast quantities of existing information. Challenging the prevailing narrative that often spotlights generative AI's creative prowess, the author posits that AI functions primarily as an advanced information aggregator and analyzer. This consumption-focused paradigm emphasizes AI's unparalleled ability to rapidly absorb and interpret massive datasets, identify intricate patterns, extract critical insights, and efficiently structure disparate knowledge. By prioritizing AI's role in consuming and interpreting the global information landscape, the piece argues that its potential can be more effectively harnessed for complex problem-solving, informed decision-making, and sophisticated knowledge management. This perspective encourages a shift from viewing AI predominantly as a creator to recognizing its fundamental value as a powerful consumer and synthesizer of the world's data.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI capabilities</span><span>Data processing</span><span>Information synthesis</span><span>Knowledge management</span><span>Generative AI</span><span>AI paradigm</span><span>Data consumption</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://msanroman.io/blog/ai-consumption-paradigm" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Olmo 3</h2>
                <span class="published-time">Published: 2025-12-15T23:41:48.000Z</span>
                
                <p class="summary">We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Olmo 3</span><span>language models</span><span>state-of-the-art</span><span>long-context reasoning</span><span>fully-open models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.13961" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Janus: Disaggregating Attention and Experts for Scalable MoE Inference</h2>
                <span class="published-time">Published: 2025-12-15T16:53:49.000Z</span>
                
                <p class="summary">Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Mixture-of-Experts</span><span>Scalable Inference</span><span>GPU Architecture</span><span>Resource Management</span><span>Attention Mechanism</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Deep Learning</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.13525" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SS4D: Native 4D Generative Model via Structured Spacetime Latents</h2>
                <span class="published-time">Published: 2025-12-16T10:45:06.000Z</span>
                
                <p class="summary">We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>4D Generative Model</span><span>Structured Spacetime Latents</span><span>Dynamic 3D Objects</span><span>Monocular Video</span><span>Temporal Coherence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Generative AI</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.14284" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</h2>
                <span class="published-time">Published: 2025-12-16T18:26:38.000Z</span>
                
                <p class="summary">Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6% on long-horizon tasks, +22.0% in 1-shot learning, and enables cross-task generalization -- achieving 20.8% success on unseen tasks without task-specific demonstrations training (vs. 0% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Vision-Language-Action Models</span><span>Test-Time Training</span><span>Environmental Feedback</span><span>Robotic Manipulation</span><span>Adaptive Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>AI Agent</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.14666" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning</h2>
                <span class="published-time">Published: 2025-12-15T09:03:45.000Z</span>
                
                <p class="summary">Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Semi-Supervised Reinforcement Learning</span><span>LLM Reasoning</span><span>Policy Optimization</span><span>Reinforcement Learning with Verifiable Rewards</span><span>Mathematical Reasoning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Machine Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.13106" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MMGR: Multi-Modal Generative Reasoning</h2>
                <span class="published-time">Published: 2025-12-16T18:58:04.000Z</span>
                
                <p class="summary">Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Generative Reasoning</span><span>Multi-modal</span><span>Video Models</span><span>Evaluation Framework</span><span>World Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.14691" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>