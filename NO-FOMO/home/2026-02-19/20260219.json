[
  {
    "id": "hackernews_47076826",
    "source": "Hacker News",
    "url": "https://www.techdirt.com/2026/02/19/doge-bros-grant-review-process-was-literally-just-asking-chatgpt-is-this-dei/",
    "title": "DOGE Bro's Grant Review Process Was Literally Just Asking ChatGPT 'Is This DEI?'",
    "summary": "The story details a controversial grant review process, purportedly involving \"DOGE Bro's,\" where the assessment of Diversity, Equity, and Inclusion (DEI) criteria was executed by simply querying ChatGPT. This method points to a potentially problematic over-reliance on artificial intelligence for evaluating complex and sensitive social and ethical frameworks. The report implicitly critiques the reduction of nuanced DEI considerations to a direct prompt-and-response interaction with a large language model, thereby raising concerns about the depth, fairness, and accountability of such an automated approach in grant allocation. This incident underscores broader debates regarding the appropriate boundaries and limitations of AI tools in decision-making contexts that inherently demand sophisticated human judgment, ethical reasoning, and comprehensive understanding, rather than purely algorithmic classification. The use of a chatbot for such critical evaluations highlights the ongoing challenge of integrating AI responsibly into processes with significant societal impact.",
    "keywords": [
      "ChatGPT",
      "Large Language Model",
      "Artificial Intelligence",
      "Grant Review",
      "DEI",
      "AI Ethics",
      "Automated Decision Making"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2026-02-19 18:01:50",
    "download_time": "2026-02-19 20:00:49",
    "extra_info": "{\"score\": 78, \"by\": \"hn_acker\", \"descendants\": 22, \"story_id\": 47076826}"
  },
  {
    "id": "hackernews_47075124",
    "source": "Hacker News",
    "url": "https://micasa.dev",
    "title": "Show HN: Micasa – track your house from the terminal",
    "summary": "Micasa is a newly launched terminal UI application designed for local home management, storing all data in a single SQLite file without requiring cloud services, accounts, or subscriptions. Developed to address common issues of losing track of home maintenance tasks, quotes, and repairs, Micasa offers features like tracking appliance cleaning schedules or project estimates. A notable aspect of its development is that approximately 99% of the codebase was generated by an AI agent, with the human developer focusing on code review and merging. The application boasts a Vim-style modal user interface, enabling efficient navigation and editing. Key UI functionalities include multicolumn sorting, fuzzy-jump to specific columns, row filtering and pinning, customizable column visibility, and the ability to drill down into related records for detailed information.",
    "keywords": [
      "Terminal UI",
      "Home Management",
      "SQLite",
      "Local-first",
      "AI Agent",
      "Vim-style UI",
      "Productivity Tool"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Others"
    ],
    "published_time": "2026-02-19 15:54:14",
    "download_time": "2026-02-19 20:00:28",
    "extra_info": "{\"score\": 197, \"by\": \"cpcloud\", \"descendants\": 68, \"story_id\": 47075124}"
  },
  {
    "id": "hackernews_47074735",
    "source": "Hacker News",
    "url": "https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-3.1-pro-preview?pli=1",
    "title": "Gemini 3.1 Pro Preview",
    "summary": "Google has officially launched a preview of its Gemini 3.1 Pro model, making it accessible to developers and enterprises through the Vertex AI platform. This release marks a significant advancement in Google's suite of large language models, designed to equip users with enhanced generative AI capabilities for a diverse range of applications. The 'Pro Preview' designation underscores a focus on delivering a robust, enterprise-grade offering, likely incorporating substantial improvements in multimodal understanding, advanced reasoning, and increased efficiency for processing complex tasks. Developers utilizing Vertex AI can seamlessly integrate Gemini 3.1 Pro into their existing and new workflows to architect sophisticated AI-powered solutions, spanning from sophisticated content generation and advanced conversational agents to in-depth data analysis and highly specialized industry applications. This strategic introduction further solidifies Google's dedication to fostering accessible AI innovation and providing powerful, scalable tools that enable the global developer community to accelerate the creation and deployment of next-generation artificial intelligence applications, emphasizing performance and versatility.",
    "keywords": [
      "Gemini 3.1 Pro",
      "Large Language Model",
      "Google AI",
      "Vertex AI",
      "Generative AI",
      "Multimodal AI",
      "AI Development",
      "Foundation Models"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2026-02-19 15:19:57",
    "download_time": "2026-02-19 20:00:45",
    "extra_info": "{\"score\": 164, \"by\": \"MallocVoidstar\", \"descendants\": 85, \"story_id\": 47074735}"
  },
  {
    "id": "hackernews_47073947",
    "source": "Hacker News",
    "url": "https://www.anthropic.com/research/measuring-agent-autonomy",
    "title": "Measuring AI agent autonomy in practice",
    "summary": "Anthropic's research explores the critical challenge of effectively measuring AI agent autonomy within practical applications. The study likely delves into defining different levels of autonomy for AI systems, considering their ability to operate independently, make decisions, and take actions without direct human intervention. This work is essential for developing robust and safe AI agents, as accurate measurement of autonomy allows researchers and developers to understand the capabilities and limitations of these systems in real-world scenarios. The research probably proposes new methodologies or frameworks for quantifying agent autonomy, addressing complexities such as unforeseen circumstances, ethical considerations, and the dynamic nature of AI environments. This initiative aims to provide clearer guidelines and tools for evaluating AI agents, fostering responsible development and deployment, and ultimately enhancing trust in autonomous AI technologies across various domains.",
    "keywords": [
      "AI Agents",
      "Agent Autonomy",
      "AI Evaluation",
      "AI Safety",
      "Human-AI Interaction",
      "AI Ethics"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2026-02-19 14:14:14",
    "download_time": "2026-02-19 20:00:36",
    "extra_info": "{\"score\": 45, \"by\": \"jbredeche\", \"descendants\": 14, \"story_id\": 47073947}"
  },
  {
    "id": "hackernews_47069299",
    "source": "Hacker News",
    "url": "https://code.claude.com/docs/en/legal-and-compliance",
    "title": "Anthropic officially bans using subscription auth for third party use",
    "summary": "Anthropic, a prominent AI research and development company, has officially implemented a ban on utilizing subscription authentication credentials for third-party applications. This new policy is a significant step towards reinforcing the security and integrity of its AI services, particularly those associated with its advanced Claude models. The prohibition aims to prevent the unauthorized leveraging of individual user subscriptions to power external platforms or commercial services that subsequently offer access to Anthropic's premium features. By clearly delineating acceptable usage, Anthropic seeks to protect its intellectual property, ensure fair resource allocation, and prevent potential abuse or circumvention of its official API access methods. This directive encourages developers to pursue legitimate integration pathways, fostering a more secure and compliant ecosystem for applications built with Anthropic's AI. The move underscores the company's commitment to governing how its technology is accessed and utilized by external entities, ensuring sustainable growth and adherence to its terms of service while maintaining control over its valuable AI assets.",
    "keywords": [
      "AI Policy",
      "Subscription Management",
      "API Access",
      "Third-Party Integration",
      "Large Language Models",
      "Platform Security",
      "Developer Guidelines"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2026-02-19 02:52:26",
    "download_time": "2026-02-19 20:01:11",
    "extra_info": "{\"score\": 584, \"by\": \"theahura\", \"descendants\": 717, \"story_id\": 47069299}"
  },
  {
    "id": "hackernews_47069179",
    "source": "Hacker News",
    "url": "https://static.stepfun.com/blog/step-3.5-flash/",
    "title": "Step 3.5 Flash – Open-source foundation model, supports deep reasoning at speed",
    "summary": "Step 3.5 Flash has been launched as a groundbreaking open-source foundation model, explicitly engineered to facilitate deep reasoning capabilities at exceptional speeds. This development represents a pivotal step in making advanced artificial intelligence more accessible and adaptable across various domains. The model's open-source status is particularly significant, fostering a collaborative ecosystem where developers, researchers, and organizations can leverage, modify, and contribute to its evolution, thereby accelerating innovation in AI applications. Its core strength lies in its ability to perform 'deep reasoning,' implying sophisticated cognitive functions that enable nuanced understanding and complex problem-solving. This goes beyond simple data processing, allowing the model to grasp intricate relationships and derive profound insights. The integration of 'speed' alongside deep reasoning positions Step 3.5 Flash as an ideal candidate for real-time analytical tasks, critical decision-making processes, and applications demanding immediate, intelligent responses. This release is anticipated to have a substantial impact on the development of more robust, efficient, and versatile AI systems, pushing the boundaries of what open-source models can achieve in practical, high-stakes environments.",
    "keywords": [
      "Foundation Model",
      "Open-source AI",
      "Deep Reasoning",
      "AI Performance",
      "Machine Learning",
      "Artificial Intelligence"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2026-02-19 02:32:00",
    "download_time": "2026-02-19 20:01:03",
    "extra_info": "{\"score\": 190, \"by\": \"kristianp\", \"descendants\": 85, \"story_id\": 47069179}"
  },
  {
    "id": "2602.14080",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.14080",
    "title": "Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality",
    "summary": "Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.",
    "keywords": [
      "Large Language Models",
      "Factuality Evaluation",
      "Knowledge Recall",
      "Parametric Factuality",
      "Benchmarking"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2026-02-15T10:13:30.000Z",
    "download_time": "2026-02-19 12:01:19",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.14080\", \"arxiv_url\": \"https://arxiv.org/abs/2602.14080\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14080.png\", \"original_title\": \"Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality\"}"
  },
  {
    "id": "2602.16704",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.16704",
    "title": "Reinforced Fast Weights with Next-Sequence Prediction",
    "summary": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",
    "keywords": [
      "Fast Weights",
      "Next-Sequence Prediction",
      "Reinforcement Learning",
      "Long-Context Modeling",
      "Language Models"
    ],
    "area": [
      "Natural Language Processing",
      "Deep Learning",
      "Large Language Model"
    ],
    "published_time": "2026-02-18T18:53:18.000Z",
    "download_time": "2026-02-19 12:01:17",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.16704\", \"arxiv_url\": \"https://arxiv.org/abs/2602.16704\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16704.png\", \"original_title\": \"Reinforced Fast Weights with Next-Sequence Prediction\"}"
  },
  {
    "id": "2602.14979",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.14979",
    "title": "RynnBrain: Open Embodied Foundation Models",
    "summary": "Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.",
    "keywords": [
      "embodied intelligence",
      "foundation models",
      "spatiotemporal",
      "reasoning",
      "planning"
    ],
    "area": [
      "Artificial Intelligence",
      "Robotics",
      "Multimodal"
    ],
    "published_time": "2026-02-13T18:59:56.000Z",
    "download_time": "2026-02-19 12:01:20",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.14979\", \"arxiv_url\": \"https://arxiv.org/abs/2602.14979\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14979.png\", \"original_title\": \"RynnBrain: Open Embodied Foundation Models\"}"
  },
  {
    "id": "2602.16666",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.16666",
    "title": "Towards a Science of AI Agent Reliability",
    "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
    "keywords": [
      "AI Agents",
      "Reliability",
      "Performance Metrics",
      "Robustness",
      "Evaluation"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "AI Agent"
    ],
    "published_time": "2026-02-18T18:05:44.000Z",
    "download_time": "2026-02-19 12:01:17",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.16666\", \"arxiv_url\": \"https://arxiv.org/abs/2602.16666\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16666.png\", \"original_title\": \"Towards a Science of AI Agent Reliability\"}"
  },
  {
    "id": "2602.07345",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.07345",
    "title": "Optimizing Few-Step Generation with Adaptive Matching Distillation",
    "summary": "Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.",
    "keywords": [
      "Adaptive Matching Distillation",
      "Few-Step Generation",
      "Distribution Matching Distillation",
      "Generative Models",
      "Image and Video Generation"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2026-02-07T04:00:20.000Z",
    "download_time": "2026-02-19 12:01:16",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.07345\", \"arxiv_url\": \"https://arxiv.org/abs/2602.07345\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07345.png\", \"original_title\": \"Optimizing Few-Step Generation with Adaptive Matching Distillation\"}"
  },
  {
    "id": "2602.15922",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2602.15922",
    "title": "World Action Models are Zero-shot Policies",
    "summary": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
    "keywords": [
      "World Action Models",
      "Zero-shot Generalization",
      "Video Diffusion",
      "Robotics",
      "Cross-embodiment Transfer"
    ],
    "area": [
      "Robotics",
      "Deep Learning",
      "Video Understanding"
    ],
    "published_time": "2026-02-17T15:04:02.000Z",
    "download_time": "2026-02-19 12:01:21",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2602.15922\", \"arxiv_url\": \"https://arxiv.org/abs/2602.15922\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15922.png\", \"original_title\": \"World Action Models are Zero-shot Policies\"}"
  }
]