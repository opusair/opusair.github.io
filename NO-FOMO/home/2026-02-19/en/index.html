<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2026-02-19</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2026-02-19</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>DOGE Bro's Grant Review Process Was Literally Just Asking ChatGPT 'Is This DEI?'</h2>
                <span class="published-time">Published: 2026-02-19 18:01:50</span>
                
                <p class="summary">The story details a controversial grant review process, purportedly involving "DOGE Bro's," where the assessment of Diversity, Equity, and Inclusion (DEI) criteria was executed by simply querying ChatGPT. This method points to a potentially problematic over-reliance on artificial intelligence for evaluating complex and sensitive social and ethical frameworks. The report implicitly critiques the reduction of nuanced DEI considerations to a direct prompt-and-response interaction with a large language model, thereby raising concerns about the depth, fairness, and accountability of such an automated approach in grant allocation. This incident underscores broader debates regarding the appropriate boundaries and limitations of AI tools in decision-making contexts that inherently demand sophisticated human judgment, ethical reasoning, and comprehensive understanding, rather than purely algorithmic classification. The use of a chatbot for such critical evaluations highlights the ongoing challenge of integrating AI responsibly into processes with significant societal impact.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>ChatGPT</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Grant Review</span><span>DEI</span><span>AI Ethics</span><span>Automated Decision Making</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.techdirt.com/2026/02/19/doge-bros-grant-review-process-was-literally-just-asking-chatgpt-is-this-dei/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Micasa ‚Äì track your house from the terminal</h2>
                <span class="published-time">Published: 2026-02-19 15:54:14</span>
                
                <p class="summary">Micasa is a newly launched terminal UI application designed for local home management, storing all data in a single SQLite file without requiring cloud services, accounts, or subscriptions. Developed to address common issues of losing track of home maintenance tasks, quotes, and repairs, Micasa offers features like tracking appliance cleaning schedules or project estimates. A notable aspect of its development is that approximately 99% of the codebase was generated by an AI agent, with the human developer focusing on code review and merging. The application boasts a Vim-style modal user interface, enabling efficient navigation and editing. Key UI functionalities include multicolumn sorting, fuzzy-jump to specific columns, row filtering and pinning, customizable column visibility, and the ability to drill down into related records for detailed information.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Terminal UI</span><span>Home Management</span><span>SQLite</span><span>Local-first</span><span>AI Agent</span><span>Vim-style UI</span><span>Productivity Tool</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://micasa.dev" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Gemini 3.1 Pro Preview</h2>
                <span class="published-time">Published: 2026-02-19 15:19:57</span>
                
                <p class="summary">Google has officially launched a preview of its Gemini 3.1 Pro model, making it accessible to developers and enterprises through the Vertex AI platform. This release marks a significant advancement in Google's suite of large language models, designed to equip users with enhanced generative AI capabilities for a diverse range of applications. The 'Pro Preview' designation underscores a focus on delivering a robust, enterprise-grade offering, likely incorporating substantial improvements in multimodal understanding, advanced reasoning, and increased efficiency for processing complex tasks. Developers utilizing Vertex AI can seamlessly integrate Gemini 3.1 Pro into their existing and new workflows to architect sophisticated AI-powered solutions, spanning from sophisticated content generation and advanced conversational agents to in-depth data analysis and highly specialized industry applications. This strategic introduction further solidifies Google's dedication to fostering accessible AI innovation and providing powerful, scalable tools that enable the global developer community to accelerate the creation and deployment of next-generation artificial intelligence applications, emphasizing performance and versatility.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini 3.1 Pro</span><span>Large Language Model</span><span>Google AI</span><span>Vertex AI</span><span>Generative AI</span><span>Multimodal AI</span><span>AI Development</span><span>Foundation Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-3.1-pro-preview?pli=1" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Measuring AI agent autonomy in practice</h2>
                <span class="published-time">Published: 2026-02-19 14:14:14</span>
                
                <p class="summary">Anthropic's research explores the critical challenge of effectively measuring AI agent autonomy within practical applications. The study likely delves into defining different levels of autonomy for AI systems, considering their ability to operate independently, make decisions, and take actions without direct human intervention. This work is essential for developing robust and safe AI agents, as accurate measurement of autonomy allows researchers and developers to understand the capabilities and limitations of these systems in real-world scenarios. The research probably proposes new methodologies or frameworks for quantifying agent autonomy, addressing complexities such as unforeseen circumstances, ethical considerations, and the dynamic nature of AI environments. This initiative aims to provide clearer guidelines and tools for evaluating AI agents, fostering responsible development and deployment, and ultimately enhancing trust in autonomous AI technologies across various domains.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Agent Autonomy</span><span>AI Evaluation</span><span>AI Safety</span><span>Human-AI Interaction</span><span>AI Ethics</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.anthropic.com/research/measuring-agent-autonomy" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Anthropic officially bans using subscription auth for third party use</h2>
                <span class="published-time">Published: 2026-02-19 02:52:26</span>
                
                <p class="summary">Anthropic, a prominent AI research and development company, has officially implemented a ban on utilizing subscription authentication credentials for third-party applications. This new policy is a significant step towards reinforcing the security and integrity of its AI services, particularly those associated with its advanced Claude models. The prohibition aims to prevent the unauthorized leveraging of individual user subscriptions to power external platforms or commercial services that subsequently offer access to Anthropic's premium features. By clearly delineating acceptable usage, Anthropic seeks to protect its intellectual property, ensure fair resource allocation, and prevent potential abuse or circumvention of its official API access methods. This directive encourages developers to pursue legitimate integration pathways, fostering a more secure and compliant ecosystem for applications built with Anthropic's AI. The move underscores the company's commitment to governing how its technology is accessed and utilized by external entities, ensuring sustainable growth and adherence to its terms of service while maintaining control over its valuable AI assets.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Policy</span><span>Subscription Management</span><span>API Access</span><span>Third-Party Integration</span><span>Large Language Models</span><span>Platform Security</span><span>Developer Guidelines</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://code.claude.com/docs/en/legal-and-compliance" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Step 3.5 Flash ‚Äì Open-source foundation model, supports deep reasoning at speed</h2>
                <span class="published-time">Published: 2026-02-19 02:32:00</span>
                
                <p class="summary">Step 3.5 Flash has been launched as a groundbreaking open-source foundation model, explicitly engineered to facilitate deep reasoning capabilities at exceptional speeds. This development represents a pivotal step in making advanced artificial intelligence more accessible and adaptable across various domains. The model's open-source status is particularly significant, fostering a collaborative ecosystem where developers, researchers, and organizations can leverage, modify, and contribute to its evolution, thereby accelerating innovation in AI applications. Its core strength lies in its ability to perform 'deep reasoning,' implying sophisticated cognitive functions that enable nuanced understanding and complex problem-solving. This goes beyond simple data processing, allowing the model to grasp intricate relationships and derive profound insights. The integration of 'speed' alongside deep reasoning positions Step 3.5 Flash as an ideal candidate for real-time analytical tasks, critical decision-making processes, and applications demanding immediate, intelligent responses. This release is anticipated to have a substantial impact on the development of more robust, efficient, and versatile AI systems, pushing the boundaries of what open-source models can achieve in practical, high-stakes environments.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Foundation Model</span><span>Open-source AI</span><span>Deep Reasoning</span><span>AI Performance</span><span>Machine Learning</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://static.stepfun.com/blog/step-3.5-flash/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality</h2>
                <span class="published-time">Published: 2026-02-15T10:13:30.000Z</span>
                
                <p class="summary">Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Factuality Evaluation</span><span>Knowledge Recall</span><span>Parametric Factuality</span><span>Benchmarking</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.14080" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Reinforced Fast Weights with Next-Sequence Prediction</h2>
                <span class="published-time">Published: 2026-02-18T18:53:18.000Z</span>
                
                <p class="summary">Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Fast Weights</span><span>Next-Sequence Prediction</span><span>Reinforcement Learning</span><span>Long-Context Modeling</span><span>Language Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Deep Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.16704" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RynnBrain: Open Embodied Foundation Models</h2>
                <span class="published-time">Published: 2026-02-13T18:59:56.000Z</span>
                
                <p class="summary">Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>embodied intelligence</span><span>foundation models</span><span>spatiotemporal</span><span>reasoning</span><span>planning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Robotics</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.14979" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Towards a Science of AI Agent Reliability</h2>
                <span class="published-time">Published: 2026-02-18T18:05:44.000Z</span>
                
                <p class="summary">AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Reliability</span><span>Performance Metrics</span><span>Robustness</span><span>Evaluation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.16666" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Optimizing Few-Step Generation with Adaptive Matching Distillation</h2>
                <span class="published-time">Published: 2026-02-07T04:00:20.000Z</span>
                
                <p class="summary">Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Adaptive Matching Distillation</span><span>Few-Step Generation</span><span>Distribution Matching Distillation</span><span>Generative Models</span><span>Image and Video Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.07345" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>World Action Models are Zero-shot Policies</h2>
                <span class="published-time">Published: 2026-02-17T15:04:02.000Z</span>
                
                <p class="summary">State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>World Action Models</span><span>Zero-shot Generalization</span><span>Video Diffusion</span><span>Robotics</span><span>Cross-embodiment Transfer</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>Deep Learning</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2602.15922" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>