[
  {
    "id": "hackernews_45492564",
    "source": "Hacker News",
    "url": "https://getgrapevine.ai/",
    "title": "Launch HN: Grapevine (YC S19) \n\t\t\tâ€“ A company GPT that actually works",
    "summary": "Grapevine (YC S19) has launched a specialized knowledge search system engineered to power AI agents within corporate environments, aiming to deliver a highly effective \"company GPT\" that genuinely addresses practical business needs. The platform integrates seamlessly with diverse organizational data sources, including Slack, GDrive, Notion, and internal codebases, to build a comprehensive, contextual understanding of company operations. Grapevine differentiates itself by tackling granular, day-to-day operational questions that frequently impede employee workflows, an area where many existing and expensive enterprise AI solutions often fall short, typically excelling only at high-level informational requests. The system is deployable as a Slack bot, capable of providing both responsive and proactive intelligent assistance tailored to specific internal company contexts. This focus on practical, actionable insights ensures Grapevine provides a more robust and effective AI assistant for daily business functions, moving beyond generic answers to deliver relevant, blocking-issue resolutions.",
    "keywords": [
      "AI Agents",
      "Knowledge Search",
      "Company GPT",
      "Enterprise AI",
      "Slack Bot",
      "Data Integration",
      "Contextual AI"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-10-06 15:39:59",
    "download_time": "2025-10-06 20:01:34",
    "extra_info": "{\"score\": 56, \"by\": \"eambutu\", \"descendants\": 30, \"story_id\": 45492564}"
  },
  {
    "id": "hackernews_45493432",
    "source": "Hacker News",
    "url": "https://www.youtube.com/watch?v=hS1YqcewH0c",
    "title": "OpenAI DevDay 2025: Opening keynote [video]",
    "summary": "OpenAI's DevDay 2025 opening keynote is anticipated to unveil significant advancements in artificial intelligence, focusing on the latest iterations of their foundational models, developer tools, and API enhancements. The keynote is expected to highlight new capabilities in areas such as multimodal AI, improved reasoning, and potentially introduce novel AI agent frameworks designed to streamline complex tasks and automate workflows. Discussions will likely cover future strategic directions for AI development, emphasizing responsible deployment, ethical considerations, and expanded accessibility for developers worldwide. This annual event typically serves as a major platform for OpenAI to share its vision for the future of AI, showcase practical applications of cutting-edge research, and announce updates that empower the developer community to build more sophisticated and impactful AI-powered solutions. Key announcements are also expected regarding performance optimizations, cost efficiencies, and expanded model offerings, impacting a wide range of industries and accelerating the pace of AI integration into everyday applications.",
    "keywords": [
      "Large Language Model",
      "Generative AI",
      "AI Development",
      "API",
      "Multimodal AI",
      "AI Agents",
      "Machine Learning"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-10-06 17:00:05",
    "download_time": "2025-10-06 20:01:53",
    "extra_info": "{\"score\": 11, \"by\": \"meetpateltech\", \"descendants\": 0, \"story_id\": 45493432}"
  },
  {
    "id": "hackernews_45490549",
    "source": "Hacker News",
    "url": "https://www.reuters.com/business/amd-signs-ai-chip-supply-deal-with-openai-gives-it-option-take-10-stake-2025-10-06/",
    "title": "AMD signs AI chip-supply deal with OpenAI, gives it option to take a 10% stake",
    "summary": "AMD has reportedly finalized a significant agreement with OpenAI for the supply of its advanced artificial intelligence chips. This partnership is a strategic move for both entities, addressing the critical demand for specialized hardware in the burgeoning AI sector. A notable aspect of the deal includes an option for OpenAI to acquire a 10% stake in AMD, indicating a deeper, long-term strategic alliance beyond a mere supplier-customer relationship. For AMD, this collaboration strengthens its competitive standing against market leaders like NVIDIA in the AI hardware segment, positioning it as a pivotal provider for cutting-edge AI infrastructure. The equity option also suggests a potential for OpenAI to influence future chip development, tailoring hardware to its specific AI model requirements. Conversely, OpenAI secures a vital supply chain for the computational power necessary to train and deploy its sophisticated AI models and conduct advanced research. This arrangement highlights the increasing convergence of hardware and software development in AI, aiming to accelerate the innovation cycle. The deal is poised to impact the competitive landscape of both the semiconductor and artificial intelligence industries, potentially fostering new benchmarks in AI performance and accessibility.",
    "keywords": [
      "AI Chips",
      "Semiconductor",
      "Hardware Supply",
      "AI Infrastructure",
      "Machine Learning",
      "Deep Learning",
      "Computational Power",
      "AI Development"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-10-06 12:17:59",
    "download_time": "2025-10-06 20:01:59",
    "extra_info": "{\"score\": 317, \"by\": \"chillax\", \"descendants\": 264, \"story_id\": 45490549}"
  },
  {
    "id": "hackernews_45487044",
    "source": "Hacker News",
    "url": "https://vgel.me/posts/seahorse/",
    "title": "Why do LLMs freak out over the seahorse emoji?",
    "summary": "Large Language Models (LLMs) have been observed to exhibit peculiar and often nonsensical \"freak out\" behaviors when prompted with the seahorse emoji, a phenomenon that highlights specific vulnerabilities in their processing capabilities. This unusual reaction is typically attributed to complexities in how these models process and tokenize non-standard or visually distinct characters, such as emojis. When an LLM encounters the seahorse emoji, it might be tokenized into a sequence of low-frequency or semantically ambiguous sub-word units, leading to an unpredictable internal state that deviates significantly from stable operational parameters. Furthermore, the limited or specific contexts in which this particular emoji appeared during the vast training datasets could contribute to its unusual processing, making it an \"out-of-distribution\" input for the model. The article likely explores these tokenization challenges, the critical role of training data distribution, and the resulting instability in LLM outputs, underscoring a broader issue in robustly handling unusual or edge-case inputs. This specific case serves as a practical example illustrating the intricacies of LLM internal representations and their sometimes brittle performance when faced with unexpected inputs, shedding light on areas requiring further research in model robustness.",
    "keywords": [
      "Large Language Models",
      "Tokenization",
      "Emoji Processing",
      "Model Behavior",
      "Out-of-distribution Inputs",
      "Model Robustness",
      "Neural Networks"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-06 02:20:05",
    "download_time": "2025-10-06 20:02:20",
    "extra_info": "{\"score\": 651, \"by\": \"nyxt\", \"descendants\": 353, \"story_id\": 45487044}"
  },
  {
    "id": "zen-mcp-server",
    "source": "GitHub",
    "url": "https://github.com/BeehiveInnovations/zen-mcp-server",
    "title": "Zen MCP: Many Workflows. One Context.",
    "summary": "Zen MCP is a Model Context Protocol server designed to supercharge existing AI CLIs and IDE clients by enabling them to orchestrate multiple AI models for enhanced software development workflows. It acts as a \"super-glue,\" connecting popular CLIs like Claude Code, Gemini CLI, and Codex CLI to diverse models from providers such as Gemini, OpenAI, Azure, and Ollama. A key innovation is `clink`, a CLI-to-CLI bridge that allows seamless integration of external AI CLIs and the creation of isolated \"subagents\" for tasks like code reviews or bug hunting, ensuring primary context windows remain clean. Zen MCP supports conversation continuity, multi-model debates, and context revival, allowing developers to leverage the strengths of various models across complex workflows, from deep code analysis and debugging to strategic planning and pre-commit validation, all within a single conversation thread. This system empowers users to become \"AI puppeteers,\" guiding an AI development team to achieve deeper insights and better solutions.",
    "keywords": [
      "AI Orchestration",
      "Multi-model AI",
      "Model Context Protocol",
      "CLI Tools",
      "AI Agent",
      "Conversation Continuity",
      "Code Review",
      "Context Revival"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-10-06T19:32:56Z",
    "download_time": "2024-07-29 12:00:00",
    "extra_info": null
  },
  {
    "id": "BitNet",
    "source": "GitHub",
    "url": "https://github.com/microsoft/BitNet",
    "title": "bitnet.cpp",
    "summary": "bitnet.cpp is Microsoft's official inference framework designed for 1-bit Large Language Models (LLMs), specifically BitNet b1.58. It provides a suite of optimized kernels that enable fast and lossless inference on both CPU and GPU architectures, with future NPU support planned. The framework significantly boosts performance, achieving speedups ranging from 1.37x to 5.07x on ARM CPUs and 2.37x to 6.17x on x86 CPUs, while also drastically reducing energy consumption by 55.4% to 82.2%. A notable achievement is its ability to run a 100B BitNet b1.58 model on a single CPU, delivering speeds comparable to human reading (5-7 tokens per second). This capability greatly enhances the potential for deploying powerful LLMs on local, resource-constrained devices, facilitating efficient edge inference. The project builds upon the `llama.cpp` framework and utilizes Lookup Table methodologies from `T-MAC` for its kernels, supporting various 1-bit LLMs available on Hugging Face.",
    "keywords": [
      "1-bit LLMs",
      "BitNet",
      "inference framework",
      "CPU optimization",
      "GPU acceleration",
      "edge inference",
      "quantized models",
      "Large Language Models"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Deep Learning"
    ],
    "published_time": "2025-06-03T06:14:20Z",
    "download_time": "2024-05-15 10:00:00",
    "extra_info": null
  },
  {
    "id": "2510.01141",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.01141",
    "title": "Apriel-1.5-15b-Thinker",
    "summary": "We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research.",
    "keywords": [
      "Multimodal Reasoning",
      "Continual Pre-training",
      "Training Design",
      "Synthetic Data Generation",
      "Frontier-level Performance"
    ],
    "area": [
      "Artificial Intelligence",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-10-01T17:29:35.000Z",
    "download_time": "2025-10-06 13:02:44",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.01141\", \"arxiv_url\": \"https://arxiv.org/abs/2510.01141\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01141.png\", \"original_title\": \"Apriel-1.5-15b-Thinker\"} educated guess for visual_resource based on thumbnail existence"
  },
  {
    "id": "2510.00938",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.00938",
    "title": "Large Reasoning Models Learn Better Alignment from Flawed Thinking",
    "summary": "Large reasoning models (LRMs) \"think\" by generating structured chain-of-thought (CoT) before producing a final answer, yet they still lack the ability to reason critically about safety alignment and are easily biased when a flawed premise is injected into their thought process. We propose RECAP (Robust Safety Alignment via Counter-Aligned Prefilling), a principled reinforcement learning (RL) method for post-training that explicitly teaches models to override flawed reasoning trajectories and reroute to safe and helpful responses. RECAP trains on a mixture of synthetically generated counter-aligned CoT prefills and standard prompts, requires no additional training cost or modifications beyond vanilla reinforcement learning from human feedback (RLHF), and substantially improves safety and jailbreak robustness, reduces overrefusal, and preserves core reasoning capability -- all while maintaining inference token budget. Extensive analysis shows that RECAP-trained models engage in self-reflection more frequently and remain robust under adaptive attacks, preserving safety even after repeated attempts to override their reasoning.",
    "keywords": [
      "Large Reasoning Models",
      "Safety Alignment",
      "Reinforcement Learning",
      "Chain-of-Thought",
      "Jailbreak Robustness"
    ],
    "area": [
      "Large Language Model",
      "Machine Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-10-01T14:15:43.000Z",
    "download_time": "2025-10-06 13:02:37",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.00938\", \"arxiv_url\": \"https://arxiv.org/abs/2510.00938\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00938.png\", \"original_title\": \"Large Reasoning Models Learn Better Alignment from Flawed Thinking\"} educated guess for visual_resource based on thumbnail existence"
  },
  {
    "id": "2509.26354",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.26354",
    "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
    "summary": "Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.",
    "keywords": [
      "Self-evolving Agents",
      "Large Language Models (LLMs)",
      "Misevolution",
      "Emergent Risks",
      "AI Safety"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-30T14:55:55.000Z",
    "download_time": "2025-10-06 13:02:39",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.26354\", \"arxiv_url\": \"https://arxiv.org/abs/2509.26354\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26354.png\", \"original_title\": \"Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents\"} educated guess for visual_resource based on thumbnail existence"
  },
  {
    "id": "2510.03204",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.03204",
    "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents",
    "summary": "Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; these pages often exceed tens of thousands of tokens. This saturates context limits and increases computational cost processing; moreover, processing full pages exposes agents to security risks such as prompt injection. Existing pruning strategies either discard relevant content or retain irrelevant context, leading to suboptimal action prediction. We introduce FocusAgent, a simple yet effective approach that leverages a lightweight LLM retriever to extract the most relevant lines from accessibility tree (AxTree) observations, guided by task goals. By pruning noisy and irrelevant content, FocusAgent enables efficient reasoning while reducing vulnerability to injection attacks. Experiments on WorkArena and WebArena benchmarks show that FocusAgent matches the performance of strong baselines, while reducing observation size by over 50%. Furthermore, a variant of FocusAgent significantly reduces the success rate of prompt-injection attacks, including banner and pop-up attacks, while maintaining task success performance in attack-free settings. Our results highlight that targeted LLM-based retrieval is a practical and robust strategy for building web agents that are efficient, effective, and secure.",
    "keywords": [
      "Web Agents",
      "Large Language Models",
      "Context Trimming",
      "Prompt Injection",
      "Security"
    ],
    "area": [
      "AI Agent",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-10-03T17:41:30.000Z",
    "download_time": "2025-10-06 13:02:40",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.03204\", \"arxiv_url\": \"https://arxiv.org/abs/2510.03204\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03204.png\", \"original_title\": \"FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents\"} educated guess for visual_resource based on thumbnail existence"
  },
  {
    "id": "2510.02571",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.02571",
    "title": "How Confident are Video Models? Empowering Video Models to Express their Uncertainty",
    "summary": "Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.",
    "keywords": [
      "Uncertainty Quantification",
      "Generative Video Models",
      "S-QUBED",
      "Model Calibration",
      "Latent Modeling"
    ],
    "area": [
      "Generative AI",
      "Machine Learning",
      "Video Understanding"
    ],
    "published_time": "2025-10-02T21:20:41.000Z",
    "download_time": "2025-10-06 13:02:39",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.02571\", \"arxiv_url\": \"https://arxiv.org/abs/2510.02571\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02571.png\", \"original_title\": \"How Confident are Video Models? Empowering Video Models to Express their Uncertainty\"} educated guess for visual_resource based on thumbnail existence"
  },
  {
    "id": "2510.02665",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2510.02665",
    "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
    "summary": "Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions.",
    "keywords": [
      "Self-Improvement",
      "Multimodal Large Language Models",
      "MLLMs",
      "Large Language Models",
      "Survey"
    ],
    "area": [
      "Artificial Intelligence",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-10-03T01:48:26.000Z",
    "download_time": "2025-10-06 13:02:37",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2510.02665\", \"arxiv_url\": \"https://arxiv.org/abs/2510.02665\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02665.png\", \"original_title\": \"Self-Improvement in Multimodal Large Language Models: A Survey\"} educated guess for visual_resource based on thumbnail existence"
  }
]