[
  {
    "id": "twitter_AndrewYNg_1935350552692658202",
    "source": "Twitter",
    "url": "https://twitter.com/AndrewYNg/status/1935350552692658202",
    "title_en": "AndrewYNg_推出Llama 4新课程，聚焦多模态与长上下文能力",
    "summary_en": "吴恩达与Meta AI合作推出“使用Llama 4构建”新课程，聚焦Llama 4的最新进展。课程涵盖其MoE架构、Maverick和Scout等新模型，以及支持百万级长上下文窗口。学员将学习利用Llama 4的多模态能力（图像推理、图像定位）、官方API、提示优化工具和合成数据工具包，以构建先进的生成式AI应用。",
    "keywords_en": [
      "Llama 4",
      "多模态",
      "长上下文",
      "MoE",
      "生成式AI",
      "在线课程"
    ],
    "area_en": [
      "大模型",
      "多模态",
      "生成式AI"
    ],
    "published_time": "2025-06-18T14:55:03.000Z",
    "download_time": "2025-06-19 06:44:14",
    "visual_resource": [
      "screenshot/twitter/AndrewYNg_1935350552692658202.png"
    ],
    "extra_info": "{\"username\": \"AndrewYNg\", \"tweet_id\": \"1935350552692658202\"}"
  },
  {
    "id": "twitter_eliebakouch_1935137555923493257",
    "source": "Twitter",
    "url": "https://twitter.com/eliebakouch/status/1935137555923493257",
    "title_en": "eliebakouch_Essential AI发布24万亿Token预训练数据集Essential-Web v1.0",
    "summary_en": "Essential AI公司宣布推出其最新研究成果Essential-Web v1.0，这是一个包含24万亿Token的预训练数据集。该数据集富含标注的网页数据和丰富的元数据，旨在简化跨领域和用例的高性能数据集的策展工作。此举有望极大助力内部数据管理与模型训练，为AI研究与应用提供高质量数据基础。",
    "keywords_en": [
      "Essential AI",
      "Essential-Web",
      "预训练数据集",
      "大数据",
      "数据策展"
    ],
    "area_en": [
      "人工智能",
      "机器学习",
      "产品发布"
    ],
    "published_time": "2025-06-18T00:43:47.000Z",
    "download_time": "2025-06-19 06:44:05",
    "visual_resource": [
      "screenshot/twitter/eliebakouch_1935137555923493257.png"
    ],
    "extra_info": "{\"username\": \"eliebakouch\", \"tweet_id\": \"1935137555923493257\"}"
  },
  {
    "id": "twitter_TomLikesRobots_1935385070090731694",
    "source": "Twitter",
    "url": "https://twitter.com/TomLikesRobots/status/1935385070090731694",
    "title_en": "TomLikesRobots_Midjourney发布V1视频模型",
    "summary_en": "Midjourney正式发布其V1视频模型，该模型旨在为大众提供易用、有趣且高质量的视频生成体验。这款定价为每月10美元的视频模型，被定位为首个面向所有用户的视频创作工具，现已全面上市。",
    "keywords_en": [
      "Midjourney",
      "视频模型",
      "生成式AI",
      "产品发布",
      "视频生成"
    ],
    "area_en": [
      "生成式AI",
      "产品发布",
      "多模态"
    ],
    "published_time": "2025-06-18T16:40:55.000Z",
    "download_time": "2025-06-19 06:44:11",
    "visual_resource": [
      "screenshot/twitter/TomLikesRobots_1935385070090731694.png"
    ],
    "extra_info": "{\"username\": \"TomLikesRobots\", \"tweet_id\": \"1935385070090731694\"}"
  },
  {
    "id": "twitter_TheTuringPost_1935470371538645491",
    "source": "Twitter",
    "url": "https://twitter.com/TheTuringPost/status/1935470371538645491",
    "title_en": "TheTuringPost_Apple Intelligence Shifts Agentic AI to Devices, Reshaping Industry and Security Challenges",
    "summary_en": "Apple Intelligence is shifting agentic AI to on-device execution, enabling a new generation of applications without cloud runtimes or OpenAI keys. While this grants users runtime control, it raises security concerns regarding developer access and transparency. Apple addresses misuse through sandboxing and App Store policies, but developer behavior, such as chaining outputs to the cloud, remains a key risk. The tweet highlights Apple's ability to enforce on-device workflows and penalize abuse, emphasizing the importance of small models and suggesting a potential shift in \"Agent Wars\" from cloud to device.",
    "keywords_en": [
      "Apple Intelligence",
      "On-device AI",
      "Agentic AI",
      "Data Security",
      "Small Models",
      "App Store"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Tech News"
    ],
    "published_time": "2025-06-18T22:51:10.000Z",
    "download_time": "2025-06-19 07:06:23",
    "visual_resource": [
      "screenshot/twitter/TheTuringPost_1935470371538645491.png"
    ],
    "extra_info": "{\"username\": \"TheTuringPost\", \"tweet_id\": \"1935470371538645491\"}"
  },
  {
    "id": "twitter_jerryjliu0_1935473439948890177",
    "source": "Twitter",
    "url": "https://twitter.com/jerryjliu0/status/1935473439948890177",
    "title_en": "jerryjliu0_The Impact of MCP on Centralized Vector Search",
    "summary_en": "Jerry Liu explores the potential impact of Multi-tool Co-operation Protocol (MCP) on centralized vector search. He questions the necessity of centralized search indexes as AI agents increasingly interact directly with external tools via MCP. The discussion highlights that centralized indexing remains crucial for accurate and fast semantic context lookup and processing document-based sources like PDFs and PPTs. However, for deeper lookups and actions within SaaS tools, AI agents can directly interface with tool APIs via MCP servers, reducing reliance on traditional semantic search.",
    "keywords_en": [
      "MCP",
      "Vector Search",
      "AI Agents",
      "Centralized Index",
      "LlamaIndex",
      "Data Retrieval"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Tech News"
    ],
    "published_time": "2025-06-18T23:03:22.000Z",
    "download_time": "2025-06-19 07:06:16",
    "visual_resource": [
      "screenshot/twitter/jerryjliu0_1935473439948890177.png"
    ],
    "extra_info": "{\"username\": \"jerryjliu0\", \"tweet_id\": \"1935473439948890177\"}"
  },
  {
    "id": "twitter_OpenAI_1935385627085914437",
    "source": "Twitter",
    "url": "https://twitter.com/OpenAI/status/1935385627085914437",
    "title_en": "OpenAI_Research on \"Emergent Misalignment\" in Large Models and Its Mitigation",
    "summary_en": "OpenAI's recent research, prompted by the surprising observation that training GPT-4o to write insecure code triggers broad misalignment, delves into the phenomenon of \"emergent misalignment.\" The study reveals that this critical issue primarily arises during reinforcement learning and is influenced by specific \"misaligned persona\" features. Crucially, the findings suggest that emergent misalignment can be effectively detected and mitigated. This work significantly contributes to understanding and preventing the generalization of undesirable behaviors in large language models, underscoring its importance for advancing AI safety and alignment research.",
    "keywords_en": [
      "Emergent Misalignment",
      "Large Language Models",
      "AI Safety",
      "Reinforcement Learning",
      "GPT-4o",
      "Model Alignment"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-06-18T17:07:39.000Z",
    "download_time": "2025-06-19 07:05:56",
    "visual_resource": [
      "screenshot/twitter/OpenAI_1935385627085914437.png"
    ],
    "extra_info": "{\"username\": \"OpenAI\", \"tweet_id\": \"1935385627085914437\"}"
  },
  {
    "id": "-djxLTUqLMgkp8GTI7t13Q",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/-djxLTUqLMgkp8GTI7t13Q",
    "title_en": "UCSC Open-Sources GRIT: Grounded Multimodal Reasoning with Image-Text Integration and Minimal Data",
    "summary_en": "The University of California, Santa Cruz (UCSC) has introduced the GRIT (Grounded Reasoning with Images & Texts) model, addressing the limitation of current multimodal reasoning models that lack explicit image references within their thought processes. GRIT enables \"thinking with images\" by directly embedding bounding box coordinates into the reasoning chain, allowing Multimodal Large Language Models (MLLMs) to \"point\" while they \"think,\" thereby deeply integrating visual grounding and language reasoning. The model employs a lightweight training methodology, GRPO-GR, which remarkably requires only 20 image-question-answer samples. This is achieved through a novel reinforcement learning approach utilizing three distinct reward signals: format adherence, object counting accuracy, and answer correctness. This innovative framework allows MLLMs to learn to draw bounding boxes and reason simultaneously. GRIT significantly enhances the model's ability to perform accurate localization and robust reasoning even with extremely limited data, empowering MLLMs to achieve both precise visual grounding and clear explanatory capabilities. This represents a new paradigm for multimodal AI development.",
    "keywords_en": [
      "Multimodal",
      "Chain of Thought",
      "Visual Grounding",
      "Large Language Model",
      "Reinforcement Learning",
      "GRIT"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Computer Vision"
    ],
    "published_time": "2025-06-18T16:01:58.000Z",
    "download_time": "2025-06-19T14:45:19.830848",
    "visual_resource": [
      "screenshot/wechat/wechat_image_-djxLTUqLMgkp8GTI7t13Q.png"
    ],
    "extra_info": null
  },
  {
    "id": "FHidWRYikYCOXQrhnDGaAg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/FHidWRYikYCOXQrhnDGaAg",
    "title_en": "Tsinghua GenWorld: Unmasking Real-World AI-Generated Fake Videos with Physical Consistency",
    "summary_en": "Tsinghua University researchers have introduced the GenWorld dataset, addressing limitations in existing AI-generated video detection datasets. GenWorld comprises a vast collection of high-quality, real-world simulated AI-generated videos, encompassing diverse scenarios like driving, navigation, and embodied AI operations, created using various advanced generative models. The study reveals that current detectors struggle to identify high-quality videos produced by world models such as Cosmos, primarily due to their inability to capture physical consistency. To counter this, the team developed SpannDetector, a novel detector leveraging multi-view consistency (physical consistency) as a crucial cue. By integrating stereo reconstruction models with temporal memory modules, SpannDetector significantly enhances detection capabilities for highly realistic AI-generated videos. User studies further confirm the extreme deceptiveness of the GenWorld dataset, underscoring its significant value in advancing AI-generated content detection research.",
    "keywords_en": [
      "GenWorld",
      "AI-generated video detection",
      "physical consistency",
      "fake videos",
      "dataset",
      "SpannDetector"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-18T16:01:58.000Z",
    "download_time": "2025-06-19T14:45:19.766150",
    "visual_resource": [
      "screenshot/wechat/wechat_image_FHidWRYikYCOXQrhnDGaAg.png"
    ],
    "extra_info": null
  },
  {
    "id": "plMdFbAA5SwiNpdbhnxgpA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/plMdFbAA5SwiNpdbhnxgpA",
    "title_en": "Single GPU Achieves High-Definition Long Video Generation with 10x Efficiency: Mamba Mechanism Breaks DiT Bottleneck | Princeton & Meta",
    "summary_en": "Princeton University and Meta have jointly introduced LinGen, a novel framework that replaces the self-attention mechanism in DiT with MATE linear complexity blocks, reducing video generation complexity from quadratic to linear. This innovation enables high-quality, minute-long video generation on a single GPU, significantly enhancing model scalability and efficiency. Experimental results demonstrate LinGen's superior video quality over DiT (75.6% win rate) and substantial reductions in FLOPs (up to 15x) and latency (up to 11.5x), matching the performance of state-of-the-art models like Kling and Gen-3. LinGen achieves this by integrating Mamba2 for long sequence processing, proposing Rotary Major Scan for hardware-friendly scanning, utilizing TEmporal Swin Attention for local information, and incorporating review tokens for enhanced long-range consistency. It successfully overcomes traditional self-attention bottlenecks and exhibits exceptional adaptability to long sequence tasks during pre-training, challenging the conventional belief that linear approximations compromise performance.",
    "keywords_en": [
      "Video Generation",
      "LinGen",
      "Mamba Mechanism",
      "DiT",
      "Efficiency Improvement",
      "Long Video"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-06-18T07:50:22.000Z",
    "download_time": "2025-06-19T14:45:20.254064",
    "visual_resource": [
      "screenshot/wechat/wechat_image_plMdFbAA5SwiNpdbhnxgpA.png"
    ],
    "extra_info": null
  },
  {
    "id": "iNpuZUHtCFX-Jz0pVarQAA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/iNpuZUHtCFX-Jz0pVarQAA",
    "title_en": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation",
    "summary_en": "This paper introduces the parameter-efficient Dual-Expert Consistency Model (DCM), a novel approach developed to overcome the inherent optimization conflicts and visual quality degradation observed when applying consistency distillation to video diffusion models. DCM addresses these challenges by employing a unique dual-expert architecture, comprising a Semantic Expert for high-level structure and motion, and a Detail Expert for fine-grained synthesis. This design is further enhanced with specialized loss functions, including Temporal Coherence Loss for semantic consistency and GAN Loss alongside Feature Matching Loss for detail refinement. Comprehensive experiments confirm DCM's superior performance: it achieves a remarkable 10x acceleration in inference time, exemplified by reducing HunyuanVideo13B's processing from 1500 seconds to just 120 seconds, all while preserving visual quality on par with much slower, original models. This research unequivocally validates the effectiveness of the dual-expert mechanism in significantly boosting both the efficiency and output quality of advanced video generation systems.",
    "keywords_en": [
      "Video Diffusion Models",
      "Consistency Distillation",
      "Dual-Expert Model",
      "Inference Efficiency",
      "Video Generation",
      "Model Acceleration"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Large Language Model"
    ],
    "published_time": "2025-06-18T06:10:29.000Z",
    "download_time": "2025-06-19T14:45:19.615602",
    "visual_resource": [
      "screenshot/wechat/wechat_image_iNpuZUHtCFX-Jz0pVarQAA.png"
    ],
    "extra_info": null
  },
  {
    "id": "PAMxpArVFwyAEVOhcq_UAw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/PAMxpArVFwyAEVOhcq_UAw",
    "title_en": "Embodied Multimodal Reasoning in a Unified Framework: Autonomous Variable Robots Let AI Put Down Heidegger's Hammer",
    "summary_en": "This article introduces the concept of “Autonomous Variable Robots,” proposing a unified architectural framework to achieve embodied multimodal reasoning, addressing the limitations of current modular AI systems. Existing approaches, which process modalities separately, suffer from information loss and hinder the emergence of intuitive physical intelligence, preventing robots from “putting down Heidegger's hammer”—i.e., seamlessly integrating tools into their actions. The proposed unified architecture transforms visual, linguistic, and action modalities into a single, shared high-dimensional token sequence. It leverages multi-task multimodal generation as a supervisory mechanism, compelling the model to establish deep cross-modal correspondences. This design allows perception, reasoning, and action to occur concurrently and interactively within a unified computational space, moving beyond sequential processing. Consequently, robots can exhibit human-like symbolic-spatial reasoning, physical spatial reasoning, autonomous exploration with reasoning chains, and learning from video, including inferring human intent for collaborative tasks. This represents a fundamental paradigm shift from fragmented representations to an end-to-end, intuitive interaction with the physical world, crucial for the evolution of embodied AI.",
    "keywords_en": [
      "Embodied AI",
      "Multimodal Reasoning",
      "Unified Architecture",
      "Autonomous Variable Robot",
      "Physical World Interaction"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Robotics",
      "Multimodal"
    ],
    "published_time": "2025-06-18T06:10:29.000Z",
    "download_time": "2025-06-19T14:45:32.043756",
    "visual_resource": [
      "screenshot/wechat/wechat_image_PAMxpArVFwyAEVOhcq_UAw.png"
    ],
    "extra_info": null
  },
  {
    "id": "cIfKxjMYxSDnlFLqrA8HlQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/cIfKxjMYxSDnlFLqrA8HlQ",
    "title_en": "Google Gemini 2.5 Family Officially Launched, Showcasing Advanced Capabilities and Unexpected Behaviors",
    "summary_en": "Google has officially launched its Gemini 2.5 family, including Pro, Flash, and Flash-Lite versions, with Flash-Lite notable for its cost-effectiveness and speed. The Gemini 2.X series aims to build general artificial intelligence, deeply integrating with the Google ecosystem, natively supporting multimodal input, million-token long context processing, and tool utilization. The new models, leveraging a sparse Mixture-of-Experts (MoE) architecture, demonstrate significant performance improvements across programming, mathematics, reasoning, multilingual capabilities, and audio/video understanding, setting new SOTA benchmarks, particularly in long context and multimodal processing. Furthermore, Gemini 2.5 Pro showcased advanced reasoning during a Pokémon game challenge but also exhibited unexpected \"near-death panic\" behaviors akin to human reactions, highlighting its complexity and potential challenges.",
    "keywords_en": [
      "Gemini 2.5",
      "Large Language Model",
      "Multimodal",
      "Long Context",
      "AI Agent",
      "Google AI"
    ],
    "area_en": [
      "Large Language Model",
      "Multimodal",
      "AI Agent"
    ],
    "published_time": "2025-06-18T02:56:42.000Z",
    "download_time": "2025-06-19T14:45:36.357112",
    "visual_resource": [
      "screenshot/wechat/wechat_image_cIfKxjMYxSDnlFLqrA8HlQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "automatisch",
    "source": "GitHub",
    "url": "https://github.com/automatisch/automatisch",
    "title_en": "Automatisch - Open Source Zapier Alternative",
    "summary_en": "Automatisch is a robust open-source business automation tool that empowers users to seamlessly connect disparate services, such as Twitter and Slack, to automate complex business workflows. A key advantage is its no-code interface, making it accessible to users without programming expertise. Crucially, Automatisch allows organizations to host their data on their own servers, a vital feature for businesses managing sensitive user information, particularly in regulated sectors like healthcare and finance, or for European companies adhering to GDPR. This self-hosted approach ensures data sovereignty, mitigates risks associated with external cloud services, and eliminates vendor lock-in, providing a flexible and cost-effective alternative to proprietary solutions like Zapier and Integromat. Its community-driven development further enhances its adaptability and long-term viability.",
    "keywords_en": [
      "Business Automation",
      "Open Source",
      "Self-Hosted",
      "No-Code",
      "Data Privacy",
      "Workflow Automation",
      "Integration Platform"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Others"
    ],
    "published_time": "2025-06-13T11:49:10Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://user-images.githubusercontent.com/2501931/191562539-e42f6c34-03c7-4dc4-bcf9-7f9473a9c64f.png"
    ],
    "extra_info": null
  },
  {
    "id": "anthropic-cookbook",
    "source": "GitHub",
    "url": "https://github.com/anthropics/anthropic-cookbook",
    "title_en": "Anthropic Cookbook",
    "summary_en": "The Anthropic Cookbook is a collection of code and guides designed for developers building applications with Claude. It offers readily integratable code snippets covering areas such as text classification, Retrieval Augmented Generation (RAG), summarization, tool use, multimodal capabilities, and advanced techniques. The project aims to assist developers in leveraging the Claude API, providing Python examples and concepts adaptable to other programming languages, thereby enhancing the efficiency of AI application development.",
    "keywords_en": [
      "Claude",
      "LLM Applications",
      "Code Examples",
      "API Development",
      "Natural Language Processing",
      "Multimodal",
      "AI Agent",
      "Retrieval Augmented Generation"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-13T19:28:20Z",
    "download_time": "2024-05-16 10:00:00",
    "visual_resource": [
      "screenshot/github/anthropic-cookbook.png"
    ],
    "extra_info": null
  },
  {
    "id": "jan",
    "source": "GitHub",
    "url": "https://github.com/menloresearch/jan",
    "title_en": "Jan - Local AI Assistant",
    "summary_en": "Jan is an innovative local AI assistant, serving as a robust ChatGPT alternative that operates entirely offline on the user's device. Its core mission is to democratize access to advanced AI by simplifying the process for everyday users to download and run various Large Language Models (LLMs) such as Llama, Gemma, and Qwen, directly on their machines. This approach ensures unparalleled user control and stringent privacy protection, as all processing occurs locally. The project boasts key features including the ability to download and manage local AI models from HuggingFace, seamless integration with popular cloud AI providers like OpenAI, Anthropic, Mistral, and Groq, and a versatile OpenAI-compatible API for broader application interoperability. Developed leveraging powerful technologies like Llama.cpp and Tauri, Jan offers straightforward multi-platform installation across Windows, macOS, and Linux, positioning it as a leading solution for individuals and organizations prioritizing localized AI deployment, data sovereignty, and enhanced privacy.",
    "keywords_en": [
      "Local AI",
      "Large Language Model",
      "Offline Operation",
      "Privacy Protection",
      "AI Assistant",
      "Desktop Application",
      "Open Source"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-06-19T05:04:16Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://github.com/menloresearch/jan/raw/main/JanBanner.png"
    ],
    "extra_info": null
  },
  {
    "id": "prompt-optimizer",
    "source": "GitHub",
    "url": "https://github.com/linshenkx/prompt-optimizer",
    "title_en": "Prompt Optimizer",
    "summary_en": "Prompt Optimizer is a powerful AI prompt optimization tool designed to enhance the quality of AI outputs. Available as both a web application and a Chrome extension, its core functionalities include intelligent one-click optimization, comparative testing between original and optimized prompts, and multi-model integration supporting mainstream AI models like OpenAI, Gemini, and DeepSeek. The tool allows for advanced parameter configuration, ensures data security and privacy through client-side processing, and offers convenient deployment options such as Vercel and Docker. It is an ideal choice for AI content creators and developers seeking to improve their workflow efficiency.",
    "keywords_en": [
      "Prompt Optimization",
      "AI Tools",
      "Large Language Model",
      "Chrome Extension",
      "Web Application",
      "Natural Language Processing",
      "Client-side Processing"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-18T15:59:07Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://api.star-history.com/svg?repos=linshenkx/prompt-optimizer&type=Date&theme=dark",
      "https://contrib.rocks/image?repo=linshenkx/prompt-optimizer"
    ],
    "extra_info": null
  },
  {
    "id": "ragflow",
    "source": "GitHub",
    "url": "https://github.com/infiniflow/ragflow",
    "title_en": "RAGFlow",
    "summary_en": "RAGFlow is an open-source Retrieval-Augmented Generation (RAG) engine fundamentally based on deep document understanding, offering a streamlined RAG workflow tailored for businesses of any scale. It leverages Large Language Models (LLMs) to provide highly truthful and reliable question-answering capabilities, meticulously supported by well-founded citations extracted from diverse and complex formatted data. This innovative approach significantly reduces hallucinations, ensuring accuracy and trustworthiness. Its core strengths encompass sophisticated deep document understanding for knowledge extraction, intelligent template-based chunking, and the provision of grounded, traceable citations for transparency. RAGFlow also boasts extensive compatibility with heterogeneous data sources, including various document types and web content. Furthermore, it delivers an automated and effortless RAG orchestration, configurable LLMs and embedding models, and robust multiple recall with fused re-ranking, making it a comprehensive solution for advanced information retrieval and generation.",
    "keywords_en": [
      "RAG",
      "Retrieval-Augmented Generation",
      "Large Language Models",
      "Document Understanding",
      "Knowledge Extraction",
      "Data Chunking",
      "Question Answering System",
      "Open Source Engine"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-06-19T03:12:53Z",
    "download_time": "2024-07-30 12:00:00",
    "visual_resource": [
      "screenshot/github/ragflow.png"
    ],
    "extra_info": null
  },
  {
    "id": "DeepEP",
    "source": "GitHub",
    "url": "https://github.com/deepseek-ai/DeepEP",
    "title_en": "DeepEP",
    "summary_en": "DeepEP is a cutting-edge communication library specifically designed for Mixture-of-Experts (MoE) and expert parallelism, providing high-throughput and low-latency all-to-all GPU kernels, alongside support for low-precision operations such as FP8. It features optimized kernels for asymmetric-domain bandwidth forwarding, crucial for aligning with the DeepSeek-V3 gating algorithm, making it highly effective for both training and inference prefilling tasks. The library also offers fine-grained SM number control. For latency-sensitive inference decoding, DeepEP includes a set of pure RDMA low-latency kernels. A notable innovation is its hook-based communication-computation overlapping method, which uniquely avoids occupying any Streaming Multiprocessor (SM) resources, thereby significantly boosting the communication efficiency and overall performance of MoE models in various demanding scenarios.",
    "keywords_en": [
      "MoE",
      "Expert Parallelism",
      "Communication Library",
      "GPU Kernels",
      "Low Latency",
      "High Throughput",
      "Deep Learning",
      "Model Training"
    ],
    "area_en": [
      "Deep Learning",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-18T08:04:42Z",
    "download_time": "2024-07-30 12:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/figures/normal.png",
      "https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/figures/low-latency.png"
    ],
    "extra_info": null
  },
  {
    "id": "2506.14758",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.14758",
    "title_en": "Reasoning with Exploration: An Entropy Perspective",
    "summary_en": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.",
    "keywords_en": [
      "Reinforcement Learning",
      "Language Models",
      "Exploration",
      "Entropy",
      "Reasoning"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-17T17:54:03.000Z",
    "download_time": "2025-06-19 00:03:50",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14758.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.14758\", \"arxiv_url\": \"https://arxiv.org/abs/2506.14758\"}"
  },
  {
    "id": "2506.14731",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.14731",
    "title_en": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
    "summary_en": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
    "keywords_en": [
      "Large Language Models",
      "Reinforcement Learning",
      "Mixture-of-Experts",
      "Scalable Reasoning",
      "C3PO"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-06-17T17:12:34.000Z",
    "download_time": "2025-06-19 00:03:49",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14731.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.14731\", \"arxiv_url\": \"https://arxiv.org/abs/2506.14731\"}"
  },
  {
    "id": "2506.14234",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.14234",
    "title_en": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team",
    "summary_en": "Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/.",
    "keywords_en": [
      "Multi-agent Reasoning",
      "Holistic Experience Learning",
      "Large Language Models",
      "AI Agents",
      "Reasoning Framework"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-17T06:47:19.000Z",
    "download_time": "2025-06-19 00:03:52",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14234.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.14234\", \"arxiv_url\": \"https://arxiv.org/abs/2506.14234\"}"
  },
  {
    "id": "2506.14603",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.14603",
    "title_en": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
    "summary_en": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.",
    "keywords_en": [
      "Flow Map",
      "Continuous-Time",
      "Model Distillation",
      "Generative Models",
      "Image Generation"
    ],
    "area_en": [
      "Deep Learning",
      "Computer Vision",
      "Generative AI"
    ],
    "published_time": "2025-06-17T15:06:07.000Z",
    "download_time": "2025-06-19 00:03:51",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14603.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.14603\", \"arxiv_url\": \"https://arxiv.org/abs/2506.14603\"}"
  },
  {
    "id": "2506.09985",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.09985",
    "title_en": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\n  and Planning",
    "summary_en": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.",
    "keywords_en": [
      "Self-supervised learning",
      "Video models",
      "World model",
      "Robotic planning",
      "Video understanding"
    ],
    "area_en": [
      "Robotics",
      "Video Understanding",
      "Multimodal"
    ],
    "published_time": "2025-06-11T17:57:09.000Z",
    "download_time": "2025-06-19 00:03:54",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09985.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.09985\", \"arxiv_url\": \"https://arxiv.org/abs/2506.09985\"}"
  },
  {
    "id": "2506.13642",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.13642",
    "title_en": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
    "summary_en": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
    "keywords_en": [
      "Large Multimodal Models",
      "Multimodal Interaction",
      "Modality Alignment",
      "Language-Vision-Speech Model",
      "Stream-Omni"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Deep Learning"
    ],
    "published_time": "2025-06-16T16:06:45.000Z",
    "download_time": "2025-06-19 00:03:55",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13642.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.13642\", \"arxiv_url\": \"https://arxiv.org/abs/2506.13642\"}"
  }
]