<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-11-17</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2025-11-17</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Project Gemini</h2>
                <span class="published-time">Published: 2025-11-17 15:50:04</span>
                
                <p class="summary">Project Gemini represents Google's ambitious and sophisticated multimodal artificial intelligence model, developed by Google DeepMind. Engineered from the ground up to be natively multimodal, Gemini demonstrates a profound capability to seamlessly understand, operate on, and combine information across diverse modalities, including text, code, audio, imagery, and video. This strategic design allows Gemini to tackle complex tasks that require integrated reasoning across different data types, setting a new benchmark for AI versatility. The model is offered in various sizes, ranging from Gemini Ultra, tailored for highly complex and demanding applications, to Gemini Nano, optimized for efficient on-device deployment, and Gemini Pro, designed for broad scalability and enterprise use. This comprehensive family of models underscores Google's significant investment in pushing the frontiers of AI research and development, aiming to create more capable, adaptable, and intuitive AI systems that can compete at the highest level within the rapidly evolving landscape of artificial intelligence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Google Gemini</span><span>Multimodal AI</span><span>Large Language Model</span><span>AI Research</span><span>Generative AI</span><span>AI Capabilities</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://geminiprotocol.net/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Replicate is joining Cloudflare</h2>
                <span class="published-time">Published: 2025-11-17 14:11:57</span>
                
                <p class="summary">Replicate, a prominent platform specializing in the deployment and hosting of machine learning models via API, has announced its acquisition by Cloudflare, a leading provider of internet infrastructure, security, and edge computing services. This strategic move is poised to significantly enhance Cloudflare's capabilities in the artificial intelligence domain, particularly concerning AI inference and model serving at the edge. By integrating Replicate's expertise in easily running and scaling open-source machine learning models with Cloudflare's extensive global network and advanced edge computing infrastructure, the combined entity aims to offer developers a more robust, faster, and globally distributed solution for deploying sophisticated AI applications. This acquisition could accelerate the adoption of AI-driven services by providing a seamless experience for developers looking to operationalize their models with improved performance, reduced latency, and enhanced security, leveraging Cloudflare's existing developer ecosystem and serverless offerings. The integration is expected to foster innovation in edge AI and bring advanced machine learning capabilities closer to end-users globally.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Machine Learning</span><span>AI Deployment</span><span>Edge Computing</span><span>Cloud Infrastructure</span><span>AI Inference</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://replicate.com/blog/replicate-cloudflare" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>TrendRadar</h2>
                <span class="published-time">Published: 2025-11-16T08:35:44Z</span>
                
                <p class="summary">TrendRadar is a lightweight and easily deployable hot news assistant designed to aggregate and filter information from multiple online platforms. It aims to eliminate information overload by providing personalized news feeds based on user-defined keywords and intelligent push strategies. Key features include aggregation from 11+ mainstream platforms (e.g., Zhihu, Douyin, Weibo, Baidu), three distinct push modes (daily summary, current ranking, incremental monitoring), and precise content filtering using ordinary, mandatory, and exclusion keywords. The system employs a personalized hotness algorithm that prioritizes high-ranking news, consistent topics, and ranking quality for reordering aggregated content. It supports multi-channel real-time notifications via platforms like WeChat Work, Feishu, DingTalk, Telegram, Email, and ntfy. A significant addition in v3.0.0 is AI intelligent analysis, leveraging the Model Context Protocol (MCP) for natural language querying and deep data insights, including trend tracking, cross-platform data comparison, and smart summarization. The project emphasizes zero-threshold deployment via GitHub Pages and Docker, offering multi-device adaptation and data persistence, catering to investors, self-media professionals, and general users seeking to proactively control their news consumption.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Hot News Aggregation</span><span>Content Filtering</span><span>AI Analysis</span><span>Multi-channel Notification</span><span>Docker Deployment</span><span>Trend Analysis</span><span>Keyword Monitoring</span><span>Model Context Protocol</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/sansan0/TrendRadar" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Agent Development Kit (ADK) for Go</h2>
                <span class="published-time">Published: 2025-11-14T16:18:14Z</span>
                
                <p class="summary">The Agent Development Kit (ADK) for Go is an open-source, code-first toolkit designed to streamline the building, evaluation, and deployment of sophisticated AI agents. It applies software development principles to AI agent creation, offering a flexible and modular framework for orchestrating agent workflows from simple tasks to complex systems. While optimized for Gemini, ADK is model and deployment-agnostic, ensuring broad compatibility. This Go-specific version capitalizes on Go's strengths in concurrency and performance, making it ideal for developing cloud-native agent applications. Key features include an idiomatic Go design, a rich ecosystem for integrating diverse tools, and code-first development for enhanced flexibility and testability. It supports the creation of modular multi-agent systems and robust deployment, especially for cloud-native platforms like Google Cloud Run, empowering developers to build scalable and controllable AI solutions.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Agent</span><span>Go Programming</span><span>Agent Development Kit</span><span>Cloud-Native</span><span>Multi-Agent Systems</span><span>Code-First Development</span><span>Machine Learning Framework</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/google/adk-go" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>
 Cursor Free VIP</h2>
                <span class="published-time">Published: 2025-09-16T03:47:39Z</span>
                
                <p class="summary">Cursor Free VIP is an open-source tool designed to enhance the functionality and accessibility of the Cursor AI code editor across Windows, macOS, and Linux operating systems. Positioned for educational and research use, it offers features such as resetting Cursor's configuration, multi-language support (English, Simplified Chinese, Traditional Chinese, Vietnamese), and automated installation via shell and PowerShell scripts. The tool provides extensive configuration options, allowing users to customize parameters related to browser paths, captcha handling, storage locations for Cursor's data (like storage.json, state.vscdb, machineId), and various timing settings for operations. It also includes experimental support for temporary email services for verification purposes, though explicitly states it does not generate fake email accounts or OAuth access. Emphasizing optimal performance when run with administrative privileges, Cursor Free VIP aims to provide 'VIP' features to users while encouraging support for the original Cursor project.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Cursor AI</span><span>Code Editor</span><span>Automation Script</span><span>System Utility</span><span>Multi-platform Support</span><span>Configuration Management</span><span>Productivity Tool</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/yeongpin/cursor-free-vip" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Experience-Guided Adaptation of Inference-Time Reasoning Strategies</h2>
                <span class="published-time">Published: 2025-11-14T17:45:28.000Z</span>
                
                <p class="summary">Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Inference-Time Reasoning</span><span>Agentic AI Systems</span><span>LLM-based Meta-Strategy</span><span>Adaptive Problem Solving</span><span>Experience-Guided Learning</span></div>
                    <div class="area"><span class="label">Areas：</span><span>AI Agent</span><span>Large Language Model</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.11519" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism</h2>
                <span class="published-time">Published: 2025-11-14T14:52:34.000Z</span>
                
                <p class="summary">Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Reinforcement Learning</span><span>Multi-Agent Reasoning Systems</span><span>Large Language Models</span><span>Agentic Pipeline Parallelism</span><span>Reasoning Tasks</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.11373" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Building the Web for Agents: A Declarative Framework for Agent-Web Interaction</h2>
                <span class="published-time">Published: 2025-11-14T13:23:34.000Z</span>
                
                <p class="summary">The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Agents</span><span>Web Interaction</span><span>Declarative Framework</span><span>Agentic Web</span><span>Human-AI Collaboration</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.11287" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation</h2>
                <span class="published-time">Published: 2025-11-11T13:00:09.000Z</span>
                
                <p class="summary">User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code^N, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code^N establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>UI-to-Code Generation</span><span>Visual Language Models</span><span>Multimodal Coding</span><span>Interactive UI</span><span>Reinforcement Learning</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Multimodal</span><span>Generative AI</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.08195" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LiteAttention: A Temporal Sparse Attention for Diffusion Transformers</h2>
                <span class="published-time">Published: 2025-11-14T08:26:55.000Z</span>
                
                <p class="summary">Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step t typically remain so at step t+δ. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Diffusion Transformers</span><span>Temporal Sparse Attention</span><span>Video Generation</span><span>Attention Complexity</span><span>Sparsity Patterns</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Deep Learning</span><span>Generative AI</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.11062" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DoPE: Denoising Rotary Position Embedding</h2>
                <span class="published-time">Published: 2025-11-12T09:32:35.000Z</span>
                
                <p class="summary">Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Denoising Positional Encoding</span><span>Rotary Position Embedding</span><span>Transformer Models</span><span>Length Generalization</span><span>Attention Sink Phenomenon</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2511.09146" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>