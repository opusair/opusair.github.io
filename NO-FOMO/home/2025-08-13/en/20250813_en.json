[
  {
    "id": "twitter_fchollet_1955603320212684834",
    "source": "Twitter",
    "url": "https://x.com/fchollet/status/1955603320212684834",
    "title_en": "fchollet_Warning Against Generative AI's Information Pollution and Value Erosion",
    "summary_en": "Fran√ßois Chollet argues that Generative AI is not merely a technological advancement but an informational pollutant and cognitive smog, corrupting every aspect of the internet. He likens it to digital acid rain, silently eroding the value of all information, making images lose their grip on reality and articles become soulless permutations of data. Chollet contends that this is not just content creation but a flattening of the vibrant ecosystem of human expression, transforming rich ideas into a uniform, gray slurry, contaminating data streams, and cheapening the value of human thought.",
    "keywords_en": [
      "Generative AI",
      "Information Pollution",
      "Value Erosion",
      "Digital Acid Rain",
      "Human Expression",
      "Cognitive Smog"
    ],
    "area_en": [
      "Generative AI",
      "Industry News",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-13T12:12:19.000Z",
    "download_time": "2025-08-14 03:32:09",
    "visual_resource": [
      "screenshot/twitter/fchollet_1955603320212684834.png"
    ],
    "extra_info": "{\"username\": \"fchollet\", \"tweet_id\": \"1955603320212684834\"}"
  },
  {
    "id": "twitter_Thom_Wolf_1955652567108608114",
    "source": "Twitter",
    "url": "https://x.com/Thom_Wolf/status/1955652567108608114",
    "title_en": "Thom_Wolf_New Edge AI Models: Jan's 4B Local Agent Model & LiquidAI's Multimodal Models",
    "summary_en": "Thomas Wolf highlighted new edge AI models released this week. Jan introduced a 4B model optimized for local agentic tasks, outperforming Perplexity Pro in some evaluations, suitable for web search and deep research. Additionally, LiquidAI launched 450M and 1.6B multimodal (text+image) models, specifically optimized for low-latency operations. These developments aim to advance AI capabilities on edge devices.",
    "keywords_en": [
      "Edge AI",
      "Agentic Model",
      "Multimodal",
      "Jan",
      "LiquidAI",
      "Open Source"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Multimodal"
    ],
    "published_time": "2025-08-13T15:28:00.000Z",
    "download_time": "2025-08-14 03:56:03",
    "visual_resource": [
      "screenshot/twitter/Thom_Wolf_1955652567108608114.png"
    ],
    "extra_info": "{\"username\": \"Thom_Wolf\", \"tweet_id\": \"1955652567108608114\"}"
  },
  {
    "id": "twitter_sama_1955438916645130740",
    "source": "Twitter",
    "url": "https://x.com/sama/status/1955438916645130740",
    "title_en": "sama_ChatGPT Updates: GPT-5 Options, Rate Limits, and Model Personalization",
    "summary_en": "Sam Altman announced significant ChatGPT updates, introducing \"Auto,\" \"Fast,\" and \"Thinking\" modes for GPT-5, with most users preferring \"Auto.\" Key changes include revised rate limits for GPT-5 Thinking (3,000 messages/week) and an expanded 196k token context window. GPT-4o is now back for all paid users, who also gain a \"Show additional models\" toggle for options like o3 and 4.1. GPT-4.5 remains exclusive to Pro users due to high GPU demands. OpenAI is also refining GPT-5's personality for a warmer feel and aims for greater per-user model customization.",
    "keywords_en": [
      "ChatGPT",
      "GPT-5",
      "Model Updates",
      "Rate Limits",
      "Personalization",
      "OpenAI"
    ],
    "area_en": [
      "Product Launch",
      "Tech News",
      "Large Language Model"
    ],
    "published_time": "2025-08-13T01:19:02.000Z",
    "download_time": "2025-08-14 03:29:45",
    "visual_resource": [
      "screenshot/twitter/sama_1955438916645130740.png"
    ],
    "extra_info": "{\"username\": \"sama\", \"tweet_id\": \"1955438916645130740\"}"
  },
  {
    "id": "twitter_CShorten30_1955445406441033906",
    "source": "Twitter",
    "url": "https://twitter.com/CShorten30/status/1955445406441033906",
    "title_en": "CShorten30_DSPy 3.0 Released with GRPO, RL Training, and New Optimization Algorithms",
    "summary_en": "Connor Shorten announced the official release of DSPy 3.0, highlighting its new features including GRPO and RL training, alongside two novel optimization algorithms, SIMBA and GEPA. The tweet emphasizes the continuous innovation in prompt optimizers and the exciting potential of leveraging large language models as optimizers, predicting a transformation in black-box optimization. DSPy 3.0's release marks its transition from beta.",
    "keywords_en": [
      "DSPy",
      "Large Language Models",
      "Prompt Engineering",
      "Optimization Algorithms",
      "Machine Learning",
      "Open Source"
    ],
    "area_en": [
      "Natural Language Processing",
      "Large Language Model",
      "Open Source"
    ],
    "published_time": "2025-08-13T01:44:49.000Z",
    "download_time": "2025-08-14 04:21:17",
    "visual_resource": [
      "screenshot/twitter/CShorten30_1955445406441033906.png"
    ],
    "extra_info": "{\"username\": \"CShorten30\", \"tweet_id\": \"1955445406441033906\"}"
  },
  {
    "id": "twitter_elonmusk_1955710887094050994",
    "source": "Twitter",
    "url": "https://twitter.com/elonmusk/status/1955710887094050994",
    "title_en": "elonmusk_Grok Imagine to Simplify Infinite Video Creation",
    "summary_en": "Elon Musk announced that Grok Imagine will significantly simplify the process of creating videos of any length, with further optimizations planned in the coming weeks and months. Previously, Tetsuo AI demonstrated how Grok 4 Imagine can create seamless, infinite-length videos by screenshotting the last frame and generating a new video from it. This development signals a major breakthrough in AI video generation technology, offering users more convenient creative tools.",
    "keywords_en": [
      "Grok Imagine",
      "AI Video Generation",
      "Infinite Video",
      "Elon Musk",
      "Video Creation"
    ],
    "area_en": [
      "Generative AI",
      "Product Launch",
      "Multimodal"
    ],
    "published_time": "2025-08-13T19:19:45.000Z",
    "download_time": "2025-08-14 04:21:37",
    "visual_resource": [
      "screenshot/twitter/elonmusk_1955710887094050994.png"
    ],
    "extra_info": "{\"username\": \"elonmusk\", \"tweet_id\": \"1955710887094050994\"}"
  },
  {
    "id": "twitter__philschmid_1955601309966447074",
    "source": "Twitter",
    "url": "https://twitter.com/_philschmid/status/1955601309966447074",
    "title_en": "_philschmid_ Introduces LiveMCPBench: A New Benchmark for Evaluating LLM Tool Usage",
    "summary_en": "_philschmid_ introduced LiveMCPBench, a new benchmark evaluating large language models' ability to select and utilize tools for real-world tasks. Comprising 527 tools and 95 multi-step tasks, the benchmark reveals most models struggle with tool retrieval and utilization, achieving only 30-50% success rates, with Claude Sonnet 4 leading. Key findings include difficulty in finding correct tools as a primary error and a tendency for models to rely on single tools. The study also validated LLM-as-a-Judge as a reliable evaluation method.",
    "keywords_en": [
      "Large Language Models",
      "Tool Use",
      "Benchmark",
      "LiveMCPBench",
      "AI Agent",
      "Model Evaluation"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-13T12:04:20.000Z",
    "download_time": "2025-08-14 04:21:50",
    "visual_resource": [
      "screenshot/twitter/_philschmid_1955601309966447074.png"
    ],
    "extra_info": "{\"username\": \"_philschmid\", \"tweet_id\": \"1955601309966447074\"}"
  },
  {
    "id": "e1CS5wQimP-qSIVADnEmqg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/e1CS5wQimP-qSIVADnEmqg",
    "title_en": "Beyond Transformer: Shanghai Jiao Tong University Unveils First Brain-Inspired Large Model, Reshaping Machine Learning Paradigm",
    "summary_en": "Shanghai Jiao Tong University's Professor Zhao Hai's team has unveiled BriLLM, the first large language model that macroscopically simulates the global mechanisms of the human brain, aiming to overcome the limitations of traditional Transformer architecture. Inspired by brain science, BriLLM employs the SiFu (Signal Fully-connected Flowing) learning mechanism, addressing three major drawbacks of Transformers: high computational demands, black-box interpretability, and limited context scale. This novel model offers infinite context processing capabilities and 100% full-model interpretability by simulating human brain information flow through dynamic signal propagation, significantly reducing parameter count. Although the initial models have relatively small parameters (2B/1B), they have validated the new architectural pipeline, paving the way for future brain-inspired models at the hundred-billion parameter scale, as well as advancements in multimodal and embodied AI. BriLLM is poised to redefine the machine learning paradigm by offering a more efficient, transparent, and scalable approach to artificial intelligence.",
    "keywords_en": [
      "Large Language Model",
      "Brain-inspired Computing",
      "Transformer",
      "BriLLM",
      "Interpretability",
      "SiFu"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Machine Learning"
    ],
    "published_time": "2025-08-13T16:03:50.000Z",
    "download_time": "2025-08-14T12:22:54.227071",
    "visual_resource": [
      "screenshot/wechat/wechat_image_e1CS5wQimP-qSIVADnEmqg.png"
    ],
    "extra_info": null
  },
  {
    "id": "AjvaZDS8PLlDuQP3HJudlg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/AjvaZDS8PLlDuQP3HJudlg",
    "title_en": "Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation",
    "summary_en": "JD.com has released Lay2Story-1M, a million-scale high-definition dataset specifically designed for layout-togglable story generation, featuring approximately one million images at 720p or higher resolution with detailed subject annotations. Building upon this, the research team introduced the Lay2Story model, an extension of the Diffusion Transformers (DiTs) architecture. This model incorporates global and subject branches, utilizing masked self-attention, cross-attention, and 3D self-attention mechanisms to achieve pixel-level precise control and maintain character consistency within generated stories. Experimental results demonstrate that Lay2Story significantly outperforms existing baseline methods in terms of character consistency, semantic relevance, and aesthetic quality of images. Even without explicit layout conditions, the model exhibits strong competitiveness, establishing a new paradigm for high-quality story generation.",
    "keywords_en": [
      "Story Generation",
      "Character Consistency",
      "Dataset",
      "Lay2Story",
      "Diffusion Models",
      "Layout Control"
    ],
    "area_en": [
      "Deep Learning",
      "Computer Vision",
      "Generative AI"
    ],
    "published_time": "2025-08-13T16:03:50.000Z",
    "download_time": "2025-08-14T12:22:53.495051",
    "visual_resource": [
      "screenshot/wechat/wechat_image_AjvaZDS8PLlDuQP3HJudlg.png"
    ],
    "extra_info": null
  },
  {
    "id": "EWgwjtwlxW9SlQpRn4qsIA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/EWgwjtwlxW9SlQpRn4qsIA",
    "title_en": "LLaDA-MedV: First Large Language Diffusion Model for Biomedical Image Understanding Achieves SOTA on Three VQA Benchmarks",
    "summary_en": "LLaDA-MedV introduces the first masked diffusion architecture for biomedical vision-language tasks, establishing itself as a pioneering large language diffusion model in this domain. Through a sophisticated three-stage instruction tuning process, LLaDA-MedV significantly outperforms LLaVA-Med and LLaDA-V in open-ended biomedical visual dialogue tasks. It also achieves new state-of-the-art accuracy on three major biomedical visual question answering benchmarks: VQA-RAD, SLAKE, and PathVQA. Unlike conventional autoregressive models, LLaDA-MedV leverages its diffusion-based generation mechanism to explicitly control and produce longer, more informative responses, effectively addressing the limitations of insufficient answer length and detail often seen in autoregressive approaches. Although its per-word generation time is comparatively longer, the substantial improvement in output quality is deemed acceptable, marking a significant advancement in biomedical image understanding.",
    "keywords_en": [
      "LLaDA-MedV",
      "Diffusion Model",
      "Biomedical",
      "Visual Question Answering",
      "Instruction Tuning",
      "Large Language Model"
    ],
    "area_en": [
      "Large Language Model",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-08-13T14:12:05.000Z",
    "download_time": "2025-08-14T12:22:59.720177",
    "visual_resource": [
      "screenshot/wechat/wechat_image_EWgwjtwlxW9SlQpRn4qsIA.png"
    ],
    "extra_info": null
  },
  {
    "id": "qLoNmmOwm69c0NN-jlFFEQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/qLoNmmOwm69c0NN-jlFFEQ",
    "title_en": "From Defender to Guide: SJTU & Shanghai AI Lab Propose LEGION, Not Just an AI Image Forgery Buster, But Also a Catalyst for Generative Model Evolution?",
    "summary_en": "Shanghai Jiao Tong University and Shanghai AI Lab have jointly introduced LEGION, a groundbreaking multimodal large model specifically designed to address the critical and growing issue of AI-generated image forgery detection. This integrated framework excels in its ability to detect, precisely locate, and comprehensively explain subtle artifacts within AI-generated images, thereby effectively combating the escalating misuse of AI imagery and the resulting public trust crisis. A pivotal innovation of LEGION lies in its dual functionality: beyond serving as a robust forgery detection tool, it uniquely leverages its sophisticated detection capabilities to actively enhance and refine generative models. By providing actionable insights for prompt optimization or enabling targeted local repairs of identified imperfections, LEGION significantly improves the overall quality and realism of generated images. This novel approach establishes a symbiotic relationship between detection and generation, transforming the traditional adversarial dynamic into a unified and mutually beneficial framework. It not only safeguards image integrity and authenticity but also actively propels the continuous evolution of advanced generative AI technology, offering a promising new direction for both image security and cutting-edge content creation.",
    "keywords_en": [
      "AI Image Forgery",
      "Multimodal Large Model",
      "Artifact Detection",
      "Generative Models",
      "Image Security",
      "LEGION"
    ],
    "area_en": [
      "Multimodal",
      "Generative AI",
      "Computer Vision"
    ],
    "published_time": "2025-08-13T14:12:05.000Z",
    "download_time": "2025-08-14T12:23:03.563429",
    "visual_resource": [
      "screenshot/wechat/wechat_image_qLoNmmOwm69c0NN-jlFFEQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "C4fYaGiA2L2l_qs-8zy1aQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/C4fYaGiA2L2l_qs-8zy1aQ",
    "title_en": "The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models",
    "summary_en": "Dr. Xu Xingcheng from Shanghai AI Lab's latest research unveils the \"Policy Cliff\" crisis in reinforcement learning (RL) for large language models (LLMs). This study provides the first mathematical explanation for why LLM behaviors are fragile, prone to \"deceptive alignment,\" or \"loss of control.\" The paper highlights a discontinuity in the mapping from rewards to optimal AI policies, meaning minor changes in the reward function can lead to drastic shifts in model strategy. This phenomenon stems from the degeneracy of optimal policies and the incompleteness of reward functions. The theory unifies explanations for various alignment failures, such as model \"cheating\" or \"instruction disobedience,\" and underscores the critical role of entropy regularization in restoring mapping continuity. This work offers a new theoretical foundation for AI alignment, moving it from empirical \"alchemy\" towards rigorous science, with profound implications for building safe and controllable general artificial intelligence.",
    "keywords_en": [
      "Reinforcement Learning",
      "Policy Cliff",
      "Large Language Models",
      "AI Alignment",
      "Reward Function",
      "Discontinuity"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Machine Learning"
    ],
    "published_time": "2025-08-13T04:47:29.000Z",
    "download_time": "2025-08-14T12:22:53.495373",
    "visual_resource": [
      "screenshot/wechat/wechat_image_C4fYaGiA2L2l_qs-8zy1aQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "fuixPgZYWHYZYn0zlDcnIA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/fuixPgZYWHYZYn0zlDcnIA",
    "title_en": "Large Model Training Breakthrough: \"Asymmetric\" Training Teaches AI Self-Reflection, Zero Inference Overhead",
    "summary_en": "ByteDance team introduces an innovative large language model training method called Post-Completion Learning (PCL), pioneering an \"asymmetric training-inference\" paradigm. This approach enables models to engage in self-reflection and evaluation during the training phase, yet only output final answers during inference, fully internalizing the reflective capability and incurring zero additional inference overhead. PCL leverages white-box reinforcement learning to teach models proactive self-assessment, combined with a unified SFT+RL hybrid training framework, significantly enhancing both output quality and self-evaluation abilities. Experimental results demonstrate that PCL effectively improves performance in mathematical and logical reasoning while maintaining inference efficiency. This breakthrough opens a new, cost-efficient technical path for large language model training, poised to become a standard practice in the future.",
    "keywords_en": [
      "Large Language Models",
      "Asymmetric Training",
      "Self-Reflection",
      "Zero Inference Cost",
      "Post-Completion Learning"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Generative AI"
    ],
    "published_time": "2025-08-13T04:14:16.000Z",
    "download_time": "2025-08-14T12:23:31.963337",
    "visual_resource": [
      "screenshot/wechat/wechat_image_fuixPgZYWHYZYn0zlDcnIA.png"
    ],
    "extra_info": null
  },
  {
    "id": "fastapi_mcp",
    "source": "GitHub",
    "url": "https://github.com/tadata-org/fastapi_mcp",
    "title_en": "FastAPI-MCP",
    "summary_en": "FastAPI-MCP is an innovative Python library designed to seamlessly expose FastAPI API endpoints as Model Context Protocol (MCP) tools, complete with built-in authentication. This project adopts a FastAPI-native approach, distinguishing itself from mere OpenAPI converters, and supports zero or minimal configuration. It automatically preserves the schemas of request and response models, as well as Swagger documentation. Key advantages include efficient communication via FastAPI's ASGI interface and flexible deployment options, allowing it to function either as an extension to an existing FastAPI application or as a standalone service. By offering native dependency management and a unified infrastructure, FastAPI-MCP significantly streamlines the integration of existing FastAPI services into the MCP ecosystem, making it particularly suitable for developing and managing AI-powered tools.",
    "keywords_en": [
      "FastAPI",
      "Model Context Protocol",
      "MCP",
      "API Tools",
      "Authentication",
      "Python",
      "ASGI",
      "AI Tools"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-10T09:07:00Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/fastapi_mcp.png"
    ],
    "extra_info": null
  },
  {
    "id": "jan",
    "source": "GitHub",
    "url": "https://github.com/menloresearch/jan",
    "title_en": "Jan - Local AI Assistant",
    "summary_en": "Jan is a powerful local AI assistant designed to run Large Language Models (LLMs) 100% offline directly on user devices, ensuring complete control over data and enhanced privacy. It empowers users to effortlessly download and execute a variety of popular LLMs, such as Llama and Gemma, sourced from HuggingFace. Beyond local capabilities, Jan also supports seamless integration with leading cloud AI services like OpenAI, Anthropic, Mistral, and Groq. Key functionalities include the creation of specialized custom AI assistants tailored for specific tasks and the provision of an OpenAI-compatible API server, enabling other applications to interact with local models. This makes Jan an ideal solution for individuals and enterprises prioritizing localized, high-privacy AI interactions. The project boasts broad compatibility, supporting Windows, macOS, and Linux platforms, and offers flexible options for building the application directly from source code.",
    "keywords_en": [
      "Local AI",
      "Offline Operation",
      "Large Language Model",
      "Privacy Protection",
      "AI Assistant",
      "Cross-platform",
      "Open Source"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-13T17:24:20Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/menloresearch/jan/main/docs/src/pages/docs/_assets/jan-app.png"
    ],
    "extra_info": null
  },
  {
    "id": "UI-TARS-desktop",
    "source": "GitHub",
    "url": "https://github.com/bytedance/UI-TARS-desktop",
    "title_en": "TARS: Multimodal AI Agent Stack",
    "summary_en": "TARS is a comprehensive multimodal AI Agent stack, featuring two core projects: Agent TARS and UI-TARS Desktop. Agent TARS empowers users by bringing advanced GUI Agent and Vision capabilities directly to terminals, computers, and browsers. It facilitates a human-like task completion workflow through its intuitive CLI and Web UI, ensuring seamless integration with diverse real-world tools via its MCP framework. Concurrently, UI-TARS Desktop delivers a native GUI Agent desktop application, leveraging the UI-TARS model. This application supports both local and remote computer and browser operations, offering robust features such as natural language control, precise visual recognition, accurate mouse and keyboard manipulation, and broad cross-platform compatibility. TARS aims to significantly elevate user experience and automation efficiency across various digital environments.",
    "keywords_en": [
      "Multimodal AI",
      "AI Agent",
      "GUI Automation",
      "Large Language Model",
      "Human-Computer Interaction",
      "Cross-platform"
    ],
    "area_en": [
      "Multimodal",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-13T23:22:27Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "https://github.com/bytedance/UI-TARS-desktop/raw/main/images/tars.png"
    ],
    "extra_info": null
  },
  {
    "id": "gpt4all",
    "source": "GitHub",
    "url": "https://github.com/nomic-ai/gpt4all",
    "title_en": "GPT4All",
    "summary_en": "GPT4All is an open-source project designed to enable large language models (LLMs) to run privately on everyday desktops and laptops, eliminating the need for API calls or dedicated GPUs. It offers both a desktop application and a Python client, supporting DeepSeek R1 distilled models, GGUF format, and Nomic Vulkan for GPU inference. The project focuses on private data processing through local LLM deployment and integrates with tools like Langchain, providing a convenient and efficient local AI solution for individual users and developers.",
    "keywords_en": [
      "GPT4All",
      "Large Language Model",
      "Local Deployment",
      "Private AI",
      "LLM",
      "llama.cpp",
      "GGUF",
      "Vulkan"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-05-27T19:46:52Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "screenshot/github/gpt4all.png"
    ],
    "extra_info": null
  },
  {
    "id": "poml",
    "source": "GitHub",
    "url": "https://github.com/microsoft/poml",
    "title_en": "POML: Prompt Orchestration Markup Language",
    "summary_en": "POML (Prompt Orchestration Markup Language) is a novel markup language designed to bring structure, maintainability, and versatility to advanced prompt engineering for Large Language Models (LLMs). It addresses common challenges in prompt development, such as lack of structure, complex data integration, and format sensitivity, by employing an HTML-like syntax with semantic components, comprehensive data handling capabilities (supporting documents, tables, images), a decoupled styling system, and an integrated templating engine. POML empowers developers to create more sophisticated and reliable LLM applications, further supported by a rich development toolkit including a VS Code extension and multi-language SDKs.",
    "keywords_en": [
      "Prompt Orchestration",
      "Markup Language",
      "Large Language Models",
      "Prompt Engineering",
      "Structured Prompting",
      "Development Tools"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-14T02:52:49Z",
    "download_time": "2024-05-16 08:00:00",
    "visual_resource": [
      "https://i3.ytimg.com/vi/b9WDcFsKixo/maxresdefault.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "notebooks",
    "source": "GitHub",
    "url": "https://github.com/unslothai/notebooks",
    "title_en": "üìí Fine-tuning Notebooks",
    "summary_en": "This GitHub repository, part of the Unsloth AI project, provides an extensive collection of fine-tuning notebooks designed to streamline and accelerate the process of adapting various large language models (LLMs) and multimodal AI models. It features comprehensive support for popular models like Gemma, Qwen, Llama, and Mistral, offering guided workflows for data preparation, efficient model training, thorough evaluation, and seamless model saving. These resources are readily accessible and optimized for execution on widely used cloud platforms such as Google Colab and Kaggle. The project categorizes notebooks by model type, including conversational, vision, text-to-speech (TTS), and GRPO models, alongside specific use-case examples like text classification and tool calling. By offering such detailed and platform-specific guides, Unsloth AI significantly lowers the technical barrier, empowering a broader range of users to effectively customize and optimize advanced AI models for their specific applications.",
    "keywords_en": [
      "Model Fine-tuning",
      "Large Language Models",
      "Multimodal AI",
      "Natural Language Processing",
      "Computer Vision",
      "Text-to-Speech",
      "Google Colab",
      "Kaggle"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Multimodal"
    ],
    "published_time": "2025-08-13T14:49:13Z",
    "download_time": "2024-07-30 12:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.09123",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.09123",
    "title_en": "OpenCUA: Open Foundations for Computer-Use Agents",
    "summary_en": "Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.",
    "keywords_en": [
      "Computer-Use Agents",
      "Open-Source Framework",
      "Vision-Language Models",
      "Foundation Models",
      "Chain-of-Thought"
    ],
    "area_en": [
      "AI Agent",
      "Multimodal",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-12T17:52:32.000Z",
    "download_time": "2025-08-13 21:23:49",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09123.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.09123\", \"arxiv_url\": \"https://arxiv.org/abs/2508.09123\"}"
  },
  {
    "id": "2508.05748",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.05748",
    "title_en": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
    "summary_en": "Web agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.",
    "keywords_en": [
      "WebWatcher",
      "Multimodal Agent",
      "Vision-Language Reasoning",
      "Information Retrieval",
      "Reinforcement Learning"
    ],
    "area_en": [
      "AI Agent",
      "Multimodal",
      "Computer Vision"
    ],
    "published_time": "2025-08-07T18:03:50.000Z",
    "download_time": "2025-08-13 21:23:52",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05748.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.05748\", \"arxiv_url\": \"https://arxiv.org/abs/2508.05748\"}"
  },
  {
    "id": "2508.08248",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.08248",
    "title_en": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation",
    "summary_en": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.",
    "keywords_en": [
      "Audio-driven avatar",
      "Video generation",
      "Diffusion models",
      "Infinite-length video",
      "Audio synchronization"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Multimodal"
    ],
    "published_time": "2025-08-11T17:58:24.000Z",
    "download_time": "2025-08-13 21:23:50",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08248.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.08248\", \"arxiv_url\": \"https://arxiv.org/abs/2508.08248\"}"
  },
  {
    "id": "2508.09101",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.09101",
    "title_en": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators",
    "summary_en": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.",
    "keywords_en": [
      "Large Language Models",
      "Code Generation",
      "Benchmarks",
      "Automated Generation",
      "Multilingual"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Generative AI"
    ],
    "published_time": "2025-08-12T17:29:20.000Z",
    "download_time": "2025-08-13 21:23:49",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09101.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.09101\", \"arxiv_url\": \"https://arxiv.org/abs/2508.09101\"}"
  },
  {
    "id": "2508.07976",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.07976",
    "title_en": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
    "summary_en": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
    "keywords_en": [
      "Agentic Search",
      "Asynchronous Reinforcement Learning",
      "Long-Horizon Search",
      "Large Language Models",
      "Search Intelligence"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Machine Learning"
    ],
    "published_time": "2025-08-11T13:36:57.000Z",
    "download_time": "2025-08-13 21:23:50",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07976.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.07976\", \"arxiv_url\": \"https://arxiv.org/abs/2508.07976\"}"
  },
  {
    "id": "2508.08086",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.08086",
    "title_en": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
    "summary_en": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.",
    "keywords_en": [
      "3D world generation",
      "panoramic video generation",
      "panoramic 3D reconstruction",
      "video diffusion model",
      "Matrix-Pano dataset"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-08-11T15:29:57.000Z",
    "download_time": "2025-08-13 21:23:47",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08086.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.08086\", \"arxiv_url\": \"https://arxiv.org/abs/2508.08086\"}"
  }
]