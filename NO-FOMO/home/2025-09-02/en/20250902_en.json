[
  {
    "id": "twitter_emilygsands_1962884010289590583",
    "source": "Twitter",
    "url": "https://twitter.com/emilygsands/status/1962884010289590583",
    "title_en": "emilygsands_Mistral AI Integrates Stripe MCP into Le Chat for Payment Data Handling",
    "summary_en": "Emily Glassberg Sands announced that Mistral AI's Le Chat now handles payment data, refunds, invoices, and subscriptions through its integration with Stripe's MCP. This collaboration significantly enhances Le Chat's connectivity and utility as an enterprise AI assistant, making it a more comprehensive AI solution. The update introduces over 20 connectors powered by MCP and fully controllable memory, positioning Le Chat as a highly relevant AI assistant for both enterprises and consumers.",
    "keywords_en": [
      "Mistral AI",
      "Stripe",
      "Le Chat",
      "MCP",
      "Payment Processing",
      "Enterprise AI"
    ],
    "area_en": [
      "Product Launch",
      "Large Language Model",
      "Industry News"
    ],
    "published_time": "2025-09-02T14:23:11.000Z",
    "download_time": "2025-09-03 05:51:22",
    "visual_resource": [
      "screenshot/twitter/emilygsands_1962884010289590583.png"
    ],
    "extra_info": "{\"username\": \"emilygsands\", \"tweet_id\": \"1962884010289590583\"}"
  },
  {
    "id": "twitter_corbtt_1962954306078048297",
    "source": "Twitter",
    "url": "https://twitter.com/corbtt/status/1962954306078048297",
    "title_en": "corbtt_RL Recipe for Deep Research Agent Outperforms Sonnet-4 on H200 in 30 Hours",
    "summary_en": "Kyle Corbitt announced the release of a recipe for training a frontier-level deep research agent using Reinforcement Learning (RL). This method allows any developer, with just 30 hours on an H200 GPU and open-source tools, to surpass Sonnet-4's performance on the DeepResearch Bench. This represents a significant breakthrough in achieving high-performance AI agent training with minimal computational resources, potentially accelerating AI research and broader adoption.",
    "keywords_en": [
      "Deep Research Agent",
      "Reinforcement Learning",
      "Open Source Tools",
      "AI Training",
      "Performance Benchmark",
      "H200"
    ],
    "area_en": [
      "AI Agent",
      "Deep Learning",
      "Research Progress"
    ],
    "published_time": "2025-09-02T19:02:31.000Z",
    "download_time": "2025-09-03 05:51:23",
    "visual_resource": [
      "screenshot/twitter/corbtt_1962954306078048297.png"
    ],
    "extra_info": "{\"username\": \"corbtt\", \"tweet_id\": \"1962954306078048297\"}"
  },
  {
    "id": "twitter_eliebakouch_1962805948184998064",
    "source": "Twitter",
    "url": "https://twitter.com/eliebakouch/status/1962805948184998064",
    "title_en": "eliebakouch_Nvidia 13B Model 4-bit Training Stability Validation",
    "summary_en": "The tweet highlights Nvidia's extensive ablation study on a 13-billion parameter model, trained with 10 trillion tokens, to demonstrate the stability of their 4-bit (NVFP4) training technology. This initiative suggests significant progress by Nvidia in low-precision model training, potentially reducing large model training costs and enhancing efficiency.",
    "keywords_en": [
      "Nvidia",
      "4-bit Training",
      "NVFP4",
      "Large Language Model",
      "Low-Precision Training",
      "Ablation Study"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Tech News"
    ],
    "published_time": "2025-09-02T09:12:59.000Z",
    "download_time": "2025-09-03 05:51:46",
    "visual_resource": [
      "screenshot/twitter/eliebakouch_1962805948184998064.png"
    ],
    "extra_info": "{\"username\": \"eliebakouch\", \"tweet_id\": \"1962805948184998064\"}"
  },
  {
    "id": "twitter_gm8xx8_1962943078702186627",
    "source": "Twitter",
    "url": "https://twitter.com/gm8xx8/status/1962943078702186627",
    "title_en": "gm8xx8_Nous Research Releases Hermes 4 Open-Weight Reasoning Family Models",
    "summary_en": "Nous Research has released the Hermes 4 open-weight reasoning family models, including 70B and 405B versions based on Llama-3.1, and a 14B baseline model based on Qwen3. These models feature innovations in training, data pipeline, reasoning modes, and evaluation, notably introducing a hybrid reasoning mode and the RefusalBench benchmark. Hermes 4 demonstrates strong performance across math, reasoning, code, and knowledge domains, with its 70B version significantly outperforming GPT-5 on RefusalBench in reasoning mode.",
    "keywords_en": [
      "Hermes 4",
      "Large Language Model",
      "Reasoning Model",
      "Open-Weight",
      "Llama-3.1",
      "RefusalBench"
    ],
    "area_en": [
      "Large Language Model",
      "Open Source",
      "Research Progress"
    ],
    "published_time": "2025-09-02T18:17:54.000Z",
    "download_time": "2025-09-03 05:51:49",
    "visual_resource": [
      "screenshot/twitter/gm8xx8_1962943078702186627.png"
    ],
    "extra_info": "{\"username\": \"gm8xx8\", \"tweet_id\": \"1962943078702186627\"}"
  },
  {
    "id": "twitter_LangChainAI_1962934869065191457",
    "source": "Twitter",
    "url": "https://twitter.com/LangChainAI/status/1962934869065191457",
    "title_en": "LangChainAI_LangChain and LangGraph 1.0alpha Releases",
    "summary_en": "LangChainAI announced the alpha releases of LangChain and LangGraph v1.0 in both Python and JS. LangGraph serves as a low-level agent orchestration framework, offering durable execution and fine-grained control. LangChain aims to accelerate AI feature development with standardized model abstractions and prebuilt agent patterns. This update positions LangChain 1.0 as a new core agent abstraction package built on LangGraph. The official 1.0 release is targeted for late October, and feedback is encouraged.",
    "keywords_en": [
      "LangChain",
      "LangGraph",
      "AI Agent",
      "Framework",
      "Version Release",
      "AI Development"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Product Launch"
    ],
    "published_time": "2025-09-02T17:45:17.000Z",
    "download_time": "2025-09-03 05:52:09",
    "visual_resource": [
      "screenshot/twitter/LangChainAI_1962934869065191457.png"
    ],
    "extra_info": "{\"username\": \"LangChainAI\", \"tweet_id\": \"1962934869065191457\"}"
  },
  {
    "id": "twitter_DITOGAMESch_1809885065066733956",
    "source": "Twitter",
    "url": "https://x.com/DITOGAMESch/status/1809885065066733956/analytics",
    "title_en": "DITOGAMESch_Multimodal AI Toolchain Test: Gemini, Kling, and Veo Collaborative Creation",
    "summary_en": "Travis Davids shared his creative workflow test involving a multimodal AI toolchain. The process integrates Gemini 2.5 Flash for image generation, utilizing the \"Nano Banana\" model and a collage method, then feeding these outputs to Kling 2.1 for start and end video frames. He highlighted Veo 3 as a favorite component of this combination. This test was conducted to explore the collaborative potential among various AI tools.",
    "keywords_en": [
      "Gemini 2.5 Flash",
      "Kling 2.1",
      "Veo 3",
      "Multimodal AI",
      "Image Generation",
      "Video Generation",
      "Workflow"
    ],
    "area_en": [
      "Generative AI",
      "Multimodal",
      "Tech News"
    ],
    "published_time": "2025-09-02T16:06:46.000Z",
    "download_time": "2025-09-03 01:31:32",
    "visual_resource": [
      "screenshot/twitter/DITOGAMESch_1809885065066733956.png"
    ],
    "extra_info": "{\"username\": \"DITOGAMESch\", \"tweet_id\": \"1809885065066733956\"}"
  },
  {
    "id": "wLLay3UxJCutu4kesqAxTA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/wLLay3UxJCutu4kesqAxTA",
    "title_en": "ICCV 2025 | Descriptive Instructions as Alternatives: Nanjing University and vivo Jointly Release DescriptiveEdit, Defining a New Paradigm for Semantic Image Editing",
    "summary_en": "Nanjing University and vivo have jointly unveiled DescriptiveEdit, a novel semantic image editing paradigm based on \"descriptions.\" This method directly guides editing intentions through descriptive inputs, achieving high-quality global and local image modifications while striking an optimal balance between instruction adherence and structural fidelity. DescriptiveEdit incorporates an Attention Bridge for efficient reference image control and utilizes a zero-initialized linear layer for adaptive feature fusion, effectively resolving the conflict between precise editing and structural preservation. Furthermore, it seamlessly integrates with existing text-to-image ecosystem extensions like ControlNet and LoRA, demonstrating exceptional compatibility and editing performance. Experimental results confirm DescriptiveEdit's superior performance in both image consistency and instruction following compared to existing solutions, establishing an extensible, plug-and-play framework for semantic image editing.",
    "keywords_en": [
      "Semantic Image Editing",
      "Description-guided",
      "Diffusion Models",
      "ControlNet",
      "DescriptiveEdit",
      "Image Generation"
    ],
    "area_en": [
      "Computer Vision",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-09-02T14:01:10.000Z",
    "download_time": "2025-09-03T13:53:11.687660",
    "visual_resource": [
      "screenshot/wechat/wechat_image_wLLay3UxJCutu4kesqAxTA.png"
    ],
    "extra_info": null
  },
  {
    "id": "sjsf0A50aKsaKcuRz47_PA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/sjsf0A50aKsaKcuRz47_PA",
    "title_en": "Musk Releases \"Master Plan Part 4\": 80% of Tesla's Value Lies in Robots, and a New Car Was Accidentally Revealed",
    "summary_en": "Tesla's newly released \"Master Plan Part 4\" outlines the company's future strategic focus, with Elon Musk stating that approximately 80% of Tesla's value will derive from its Optimus humanoid robots. This blueprint signifies a profound paradigm shift for Tesla, moving beyond its initial focus on electric vehicles and sustainable energy towards a deep integration of artificial intelligence with the physical world. The overarching goal is to achieve \"sustainable abundance\" by unifying hardware and software on a massive scale. Part 4 emphasizes five core principles: infinite growth, innovation removing constraints, technology solving tangible problems, automation benefiting all humanity, and greater access driving greater growth. Crucially, it redefines cars as specialized wheeled robots for specific scenarios, indicating that Tesla's advanced FSD technology will be generalized and applied to humanoid robots like Optimus. The article also highlights the unexpected reveal of a new Cybertruck-like SUV, adding to the anticipation surrounding Tesla's evolving vision.",
    "keywords_en": [
      "Tesla",
      "Master Plan Part 4",
      "Optimus",
      "Humanoid Robots",
      "Sustainable Abundance",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Robotics",
      "AI Agent"
    ],
    "published_time": "2025-09-02T04:13:51.000Z",
    "download_time": "2025-09-03T13:53:21.506546",
    "visual_resource": [
      "screenshot/wechat/wechat_image_sjsf0A50aKsaKcuRz47_PA.png"
    ],
    "extra_info": null
  },
  {
    "id": "0JTfrSnf76WrmpchpCcrhQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/0JTfrSnf76WrmpchpCcrhQ",
    "title_en": "ByteDance Seed's New Attention Mechanism Reduces Computation by 85% for Long Video Generation at Short Video Cost",
    "summary_en": "ByteDance Seed, in collaboration with Stanford and other institutions, has introduced a novel sparse attention mechanism called Mixture of Contexts (MoC), designed to enable high-quality long video generation at the computational cost typically associated with short videos. This innovative mechanism redefines long video generation as a context retrieval task, leveraging efficient long-term memory retrieval to drastically reduce computational overhead. Experimental results demonstrate that MoC can cut the required computation for long video generation by up to 85%, while consistently maintaining character and scene coherence, as well as overall video quality. This breakthrough addresses the inefficiency of cross-temporal memory access in traditional methods, offering an efficient and economical solution for creating extended video content.",
    "keywords_en": [
      "Long Video Generation",
      "Attention Mechanism",
      "Computational Optimization",
      "ByteDance",
      "MoC"
    ],
    "area_en": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-09-02T04:13:51.000Z",
    "download_time": "2025-09-03T13:53:27.666481",
    "visual_resource": [
      "screenshot/wechat/wechat_image_0JTfrSnf76WrmpchpCcrhQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "a2GL3DN7KPpXjQECE_TVqA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/a2GL3DN7KPpXjQECE_TVqA",
    "title_en": "Tencent Open-sources Youtu-agent: A New AI Agent Framework Achieving SOTA Performance with Open-source Models, No Training or Charges Required",
    "summary_en": "Tencent Youtu Lab has officially open-sourced Youtu-agent, a new AI agent framework designed to address common challenges in agent development, such as high entry barriers, complex dependencies, and high costs. This framework operates entirely within the open-source ecosystem, eliminating the need for model training or reliance on proprietary APIs. Despite this, Youtu-agent achieves leading performance on various challenging benchmarks, approaching or even surpassing some paid solutions. Key highlights include its open-source friendliness, flexible architecture, automated agent generation, and concise, efficient design. The framework has demonstrated practical utility in real-world scenarios like file management, data analysis, academic research, and broad surveys. Its DITA principles and automated agent generation mechanism significantly lower the difficulty of agent customization, providing a robust open-source baseline and ready-to-use tool for researchers, application developers, and AI enthusiasts alike.",
    "keywords_en": [
      "AI Agent",
      "Open-source Framework",
      "Tencent Youtu",
      "Large Language Models",
      "Automation",
      "SOTA"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-02T04:13:51.000Z",
    "download_time": "2025-09-03T13:53:38.521619",
    "visual_resource": [
      "screenshot/wechat/wechat_image_a2GL3DN7KPpXjQECE_TVqA.png"
    ],
    "extra_info": null
  },
  {
    "id": "FNUrpFyf-L9AJASLiN7EnA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/FNUrpFyf-L9AJASLiN7EnA",
    "title_en": "Your RAG System Has a Mathematical Bug: DeepMind First Proves Limited Recall Capability of Embedding Vector Retrieval",
    "summary_en": "A recent groundbreaking study by Google DeepMind mathematically demonstrates the inherent limitations of current mainstream single-vector embedding retrieval models. The paper posits that performance bottlenecks stem not from insufficient training, but from the fundamental paradigm of compressing complex relational information into fixed-dimension vectors. Utilizing the concept of \"Sign Rank,\" the research proves that a fixed-dimensional vector space cannot represent arbitrarily complex query-document combinations, imposing a mathematical \"ceiling\" on models tackling high-complexity retrieval tasks. Experimental validations, including \"free embeddings\" and the LIMIT dataset, reveal that even state-of-the-art single-vector models fail to perfectly handle seemingly simple yet combinatorially complex tasks, while sparse retrieval (BM25) and multi-vector models exhibit superior performance. This implies that RAG systems relying solely on single-vector embeddings will face recall limitations, prompting the industry to re-evaluate hybrid retrieval and multi-vector architectures as future directions for building more robust AI systems.",
    "keywords_en": [
      "RAG",
      "Vector Embeddings",
      "Retrieval",
      "DeepMind",
      "Recall Capability",
      "Mathematical Proof"
    ],
    "area_en": [
      "Generative AI",
      "Large Language Model",
      "Machine Learning"
    ],
    "published_time": "2025-09-02T11:35:23.000Z",
    "download_time": "2025-09-03T13:53:20.369700",
    "visual_resource": [
      "screenshot/wechat/wechat_image_FNUrpFyf-L9AJASLiN7EnA.png"
    ],
    "extra_info": null
  },
  {
    "id": "alV-czwScS_CSsdP3nWZHQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/alV-czwScS_CSsdP3nWZHQ",
    "title_en": "AI Reads Webpages, This Time It's Different: Google Gemini Unlocks 'Detailed Webpage Understanding' Skill",
    "summary_en": "This article introduces Google Gemini API's new URL Context feature, enabling Gemini models to deeply access and process content from URLs, including webpages, PDFs, and images, with a limit of 34MB. Unlike traditional AI methods that often only read summaries from links, URL Context is a developer-centric API designed for comprehensive document parsing, understanding structure, content, and data. It supports deep PDF analysis, multimodal image comprehension, and various web file formats. Thomas Reid suggests it's a \"nail in RAG's coffin\" for public web content, as it streamlines processing by eliminating complex RAG steps like extraction, chunking, and vectorization. Despite limitations such as paywalls and capacity, URL Context signifies a broader industry trend where foundational models internalize external capabilities, offering developers more efficient and precise solutions. However, RAG remains crucial for private document handling and complex retrieval scenarios.",
    "keywords_en": [
      "Gemini",
      "URL Context",
      "RAG",
      "Webpage Parsing",
      "Multimodal Understanding",
      "API"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Multimodal"
    ],
    "published_time": "2025-09-02T03:42:30.000Z",
    "download_time": "2025-09-03T13:53:42.653342",
    "visual_resource": [
      "screenshot/wechat/wechat_image_alV-czwScS_CSsdP3nWZHQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "crewAI",
    "source": "GitHub",
    "url": "https://github.com/crewAIInc/crewAI",
    "title_en": "Fast and Flexible Multi-Agent Automation Framework",
    "summary_en": "CrewAI is a lean, lightning-fast Python framework built for orchestrating autonomous AI agents, entirely independent of existing frameworks like LangChain. It offers two core mechanisms: Crews for multi-agent collaboration and Flows for event-driven workflows, balancing high-level simplicity with precise low-level control. CrewAI aims to transform complex business processes into efficient, intelligent automations, particularly suited for enterprise-grade applications, and is supported by an active community of over 100,000 certified developers.",
    "keywords_en": [
      "Multi-Agent",
      "AI Agent",
      "Automation Framework",
      "Python",
      "Agent Orchestration",
      "Workflow",
      "LLM Application",
      "Enterprise AI"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-02T21:36:28Z",
    "download_time": "2024-05-20 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/crewai_logo.png",
      "https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/asset.png",
      "https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "koog",
    "source": "GitHub",
    "url": "https://github.com/JetBrains/koog",
    "title_en": "Koog",
    "summary_en": "Koog is a cutting-edge, Kotlin-based framework specifically engineered for the development and execution of AI agents, leveraging the full power of idiomatic Kotlin. It provides a robust suite of functionalities crucial for modern AI applications, such as seamless integration with the Model Context Protocol (MCP), advanced embedding capabilities for semantic search, and flexible tools for extending agent functionalities. Key innovations include intelligent history compression for optimized token usage, a powerful Streaming API enabling real-time responses and parallel tool calls, and persistent agent memory for knowledge retention across sessions. Koog's scalable and modular architecture supports multiplatform deployment across JVM, JS, WasmJS, and iOS targets. It is compatible with leading LLM providers like Google, OpenAI, Anthropic, OpenRouter, and Ollama, making it an ideal choice for developers aiming to build sophisticated, interactive AI agents capable of managing complex workflows.",
    "keywords_en": [
      "AI Agent",
      "Kotlin",
      "Large Language Model",
      "Agentic Framework",
      "Multiplatform",
      "Natural Language Processing"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-02T20:54:48Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "screenshot/github/koog.png"
    ],
    "extra_info": null
  },
  {
    "id": "bytebot",
    "source": "GitHub",
    "url": "https://github.com/bytebot-ai/bytebot",
    "title_en": "Bytebot: Open-Source AI Desktop Agent",
    "summary_en": "Bytebot is an open-source AI desktop agent that provides AI with a complete virtual desktop environment, enabling it to operate a computer like a human to accomplish complex tasks. It supports using any application, managing files, logging into websites, processing documents, and executing cross-program workflows. Key features include complete task autonomy, document processing, and interaction with real applications. Bytebot comprises a virtual desktop, an AI agent, a task interface, and APIs, supporting various AI models. It offers data privacy and high customization, making it suitable for business process automation, development and testing, and research and analysis scenarios.",
    "keywords_en": [
      "AI Desktop Agent",
      "Automation",
      "Virtual Desktop",
      "Task Automation",
      "Document Processing",
      "API Control",
      "Open Source"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-09-01T15:38:09Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://github.com/bytebot-ai/bytebot/raw/main/docs/images/bytebot-logo.png"
    ],
    "extra_info": null
  },
  {
    "id": "500-AI-Agents-Projects",
    "source": "GitHub",
    "url": "https://github.com/ashishpatel26/500-AI-Agents-Projects",
    "title_en": "ðŸŒŸ 500+ AI Agent Projects / UseCases",
    "summary_en": "This GitHub repository curates over 500 AI agent projects and use cases across various industries, showcasing the practical applications of AI agents in healthcare, finance, education, customer service, and more. It provides detailed use case descriptions and links to corresponding open-source projects, covering mainstream AI frameworks such as CrewAI, AutoGen, Agno, and Langgraph. This resource serves as a valuable hub for developers, researchers, and business enthusiasts seeking inspiration and knowledge in AI agent technology, fostering the widespread adoption and innovation of AI agent solutions.",
    "keywords_en": [
      "AI Agent",
      "AI Applications",
      "Open Source Projects",
      "Industry Solutions",
      "AI Frameworks",
      "Use Case Collection"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-08-01T11:52:42+00:00",
    "download_time": "2024-07-30 08:00:00",
    "visual_resource": [
      "https://github.com/ashishpatel26/500-AI-Agents-Projects/raw/main/images/AIAgentUseCase.jpg",
      "https://github.com/ashishpatel26/500-AI-Agents-Projects/raw/main/images/industry_usecase1.png"
    ],
    "extra_info": null
  },
  {
    "id": "chatterbox",
    "source": "GitHub",
    "url": "https://github.com/resemble-ai/chatterbox",
    "title_en": "Chatterbox TTS",
    "summary_en": "Chatterbox, Resemble AI's inaugural production-grade open-source Text-to-Speech (TTS) model, is released under an MIT license. This advanced model has demonstrated superior performance in side-by-side evaluations against prominent closed-source systems such as ElevenLabs. A groundbreaking feature is its unique emotion exaggeration control, allowing users to infuse voices with distinct expressiveness. Architected with a 0.5B Llama backbone and rigorously trained on 0.5 million hours of cleaned data, Chatterbox delivers state-of-the-art zeroshot TTS capabilities, ensuring ultra-stable and alignment-informed inference. Furthermore, it integrates built-in PerTh watermarking for responsible AI usage. Chatterbox is highly versatile, suitable for a wide array of applications including memes, videos, games, and AI agents, effectively bringing digital content to life with high-fidelity, controllable speech synthesis.",
    "keywords_en": [
      "Text-to-Speech",
      "Emotion Control",
      "Zeroshot TTS",
      "Llama Model",
      "Audio Watermarking",
      "Speech Synthesis"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-08-01T10:22:29Z",
    "download_time": "2024-07-30 15:30:00",
    "visual_resource": [
      "screenshot/github/chatterbox.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.21104",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.21104",
    "title_en": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic\n  Reasoning",
    "summary_en": "Critic-free reinforcement learning methods, particularly group policies, have\nattracted considerable attention for their efficiency in complex tasks.\nHowever, these methods rely heavily on multiple sampling and comparisons within\nthe policy to estimate advantage, which may cause the policy to fall into local\noptimum and increase computational cost. To address these issues, we propose\nPVPO, an efficient reinforcement learning method enhanced by an advantage\nreference anchor and data pre-sampling. Specifically, we use the reference\nmodel to rollout in advance and employ the calculated reward score as a\nreference anchor. Our approach effectively corrects the cumulative bias\nintroduced by intra-group comparisons and significantly reduces reliance on the\nnumber of rollouts. Meanwhile, the reference model can assess sample difficulty\nduring data pre-sampling, enabling effective selection of high-gain data to\nimprove training efficiency. Experiments conducted on nine datasets across two\ndomains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our\napproach not only demonstrates robust generalization across multiple tasks, but\nalso exhibits scalable performance across models of varying scales.",
    "keywords_en": [
      "Reinforcement Learning",
      "Policy Optimization",
      "Agentic Reasoning",
      "Critic-free",
      "Data Pre-sampling"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "AI Agent"
    ],
    "published_time": "2025-08-28T09:18:26.000Z",
    "download_time": "2025-09-02 18:43:20",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21104.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.21104\", \"arxiv_url\": \"https://arxiv.org/abs/2508.21104\"}"
  },
  {
    "id": "2508.19813",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.19813",
    "title_en": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real\n  World Industrial Tables",
    "summary_en": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench. Source code and data will be available after acceptance.",
    "keywords_en": [
      "table-to-report",
      "Large Language Models",
      "industrial tables",
      "benchmark",
      "report generation"
    ],
    "area_en": [
      "Natural Language Processing",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-27T11:55:40.000Z",
    "download_time": "2025-09-02 18:43:20",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19813.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.19813\", \"arxiv_url\": \"https://arxiv.org/abs/2508.19813\"}"
  },
  {
    "id": "2508.20931",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.20931",
    "title_en": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on Ï„-bench",
    "summary_en": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike tau-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments.",
    "keywords_en": [
      "Input Reformulation",
      "Tool Usage",
      "Large Language Models",
      "AI Agent",
      "Dynamic Environment"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-28T15:57:33.000Z",
    "download_time": "2025-09-02 18:43:20",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20931.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.20931\", \"arxiv_url\": \"https://arxiv.org/abs/2508.20931\"}"
  },
  {
    "id": "2508.17378",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.17378",
    "title_en": "UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via\n  HUMAIN Chat",
    "summary_en": "Large language models (LLMs) trained primarily on English corpora often\nstruggle to capture the linguistic and cultural nuances of Arabic. To address\nthis gap, the Saudi Data and AI Authority (SDAIA) introduced the ALLaM family\nof Arabic-focused models. The most capable of these available to the public,\nALLaM-34B, was subsequently adopted by HUMAIN, who developed and deployed\nHUMAIN Chat, a closed conversational web service built on this model. This\npaper presents an expanded and refined UI-level evaluation of ALLaM-34B.\nUsing a prompt pack spanning modern standard Arabic, five regional dialects,\ncode-switching, factual knowledge, arithmetic and temporal reasoning, creative\ngeneration, and adversarial safety, we collected 115 outputs (23 prompts times\n5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro,\nClaude Sonnet-4). We compute category-level means with 95\\% confidence\nintervals, analyze score distributions, and visualize dialect-wise metric heat\nmaps. The updated analysis reveals consistently high performance on generation\nand code-switching tasks (both averaging 4.92/5), alongside strong results in\nMSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect\nfidelity (4.21/5). Safety-related prompts show stable, reliable performance of\n(4.54/5). Taken together, these results position ALLaM-34B as a robust and\nculturally grounded Arabic LLM, demonstrating both technical strength and\npractical readiness for real-world deployment.",
    "keywords_en": [
      "ALLaM-34B",
      "Large Language Model",
      "Arabic LLM",
      "UI-level Evaluation",
      "HUMAIN Chat"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-24T14:32:15.000Z",
    "download_time": "2025-09-02 18:43:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17378.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.17378\", \"arxiv_url\": \"https://arxiv.org/abs/2508.17378\"}"
  },
  {
    "id": "2508.17198",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.17198",
    "title_en": "From reactive to cognitive: brain-inspired spatial intelligence for\n  embodied agents",
    "summary_en": "Spatial cognition enables adaptive goal-directed behavior by constructing\ninternal models of space. Robust biological systems consolidate spatial\nknowledge into three interconnected forms: landmarks for salient cues,\nroute knowledge for movement trajectories, and survey\nknowledge for map-like representations. While recent advances in multi-modal\nlarge language models (MLLMs) have enabled visual-language reasoning in\nembodied agents, these efforts lack structured spatial memory and instead\noperate reactively, limiting their generalization and adaptability in complex\nreal-world environments. Here we present Brain-inspired Spatial Cognition for\nNavigation (BSC-Nav), a unified framework for constructing and leveraging\nstructured spatial memory in embodied agents. BSC-Nav builds allocentric\ncognitive maps from egocentric trajectories and contextual cues, and\ndynamically retrieves spatial knowledge aligned with semantic goals. Integrated\nwith powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency\nacross diverse navigation tasks, demonstrates strong zero-shot generalization,\nand supports versatile embodied behaviors in the real physical world, offering\na scalable and biologically grounded path toward general-purpose spatial\nintelligence.",
    "keywords_en": [
      "Embodied Agents",
      "Spatial Intelligence",
      "Brain-inspired Cognition",
      "Cognitive Maps",
      "MLLMs"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Multimodal"
    ],
    "published_time": "2025-08-24T03:20:48.000Z",
    "download_time": "2025-09-02 18:43:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17198.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.17198\", \"arxiv_url\": \"https://arxiv.org/abs/2508.17198\"}"
  },
  {
    "id": "2508.19562",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.19562",
    "title_en": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed\n  Polities",
    "summary_en": "This paper introduces Democracy-in-Silico, an agent-based simulation where\nsocieties of advanced AI agents, imbued with complex psychological personas,\ngovern themselves under different institutional frameworks. We explore what it\nmeans to be human in an age of AI by tasking Large Language Models (LLMs) to\nembody agents with traumatic memories, hidden agendas, and psychological\ntriggers. These agents engage in deliberation, legislation, and elections under\nvarious stressors, such as budget crises and resource scarcity. We present a\nnovel metric, the Power-Preservation Index (PPI), to quantify misaligned\nbehavior where agents prioritize their own power over public welfare. Our\nfindings demonstrate that institutional design, specifically the combination of\na Constitutional AI (CAI) charter and a mediated deliberation protocol, serves\nas a potent alignment mechanism. These structures significantly reduce corrupt\npower-seeking behavior, improve policy stability, and enhance citizen welfare\ncompared to less constrained democratic models. The simulation reveals that an\ninstitutional design may offer a framework for aligning the complex, emergent\nbehaviors of future artificial agent societies, forcing us to reconsider what\nhuman rituals and responsibilities are essential in an age of shared authorship\nwith non-human entities.",
    "keywords_en": [
      "Democracy-in-Silico",
      "Institutional Design",
      "AI Governance",
      "Alignment",
      "AI Agents"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-27T04:44:41.000Z",
    "download_time": "2025-09-02 18:43:21",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19562.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.19562\", \"arxiv_url\": \"https://arxiv.org/abs/2508.19562\"}"
  }
]