<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-09</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-12-09</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Show HN: Gemini Pro 3 hallucinates the HN front page 10 years from now</h2>
                <span class="published-time">Published: 2025-12-09 15:00:38</span>
                
                <p class="summary">A recent "Show HN" presentation unveiled an intriguing experiment where the Gemini Pro 3 large language model was prompted to generate a fictional Hacker News front page, imagining how it might look a decade from now. This demonstration vividly illustrates the advanced generative capabilities of contemporary AI models, showcasing their ability to synthesize information and produce contextually coherent, albeit entirely fabricated, content. The project effectively highlights the creative potential of AI in generating speculative scenarios and engaging narratives. Crucially, the term "hallucinates" in the title emphasizes a key characteristic of large language models: their propensity to produce outputs that are plausible and grammatically correct but lack factual accuracy or a basis in real-world foresight. This case serves as a valuable example for understanding the distinction between sophisticated content generation and genuine, verifiable prediction, underscoring the imaginative yet non-veridical nature of AI-generated future scenarios.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini Pro 3</span><span>Large Language Model</span><span>AI Hallucination</span><span>Generative AI</span><span>Content Generation</span><span>AI Experimentation</span><span>Future Prediction</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://dosaygo-studio.github.io/hn-front-page-2035/news" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Mistral Releases Devstral 2 (72.2% SWE-Bench Verified) and Vibe CLI</h2>
                <span class="published-time">Published: 2025-12-09 14:45:01</span>
                
                <p class="summary">Mistral AI has announced the release of Devstral 2, its latest large language model, alongside Vibe CLI, a new command-line interface designed to enhance developer interaction with their AI ecosystem. Devstral 2 distinguishes itself with an impressive 72.2% verification rate on the SWE-Bench benchmark, a rigorous evaluation for software engineering tasks. This high score signifies Devstral 2's advanced capabilities in understanding, generating, and debugging code, positioning it as a powerful tool for automated software development, code completion, and complex problem-solving within programming environments. The SWE-Bench verification underscores Mistral's commitment to developing highly capable AI models tailored for technical applications, particularly in the realm of software engineering and agentic workflows. Concurrently, the introduction of Vibe CLI provides developers with a streamlined interface, facilitating easier integration and deployment of Mistral's models, including Devstral 2, into existing development pipelines. This dual release reinforces Mistral AI's dedication to pushing the boundaries of AI utility for professional developers, offering practical tools to augment productivity and innovation in software creation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Devstral 2</span><span>SWE-Bench</span><span>Code Generation</span><span>Large Language Model</span><span>AI Agent</span><span>Mistral AI</span><span>Vibe CLI</span><span>Software Engineering</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mistral.ai/news/devstral-2-vibe-cli" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Donating the Model Context Protocol and Establishing the Agentic AI Foundation</h2>
                <span class="published-time">Published: 2025-12-09 17:05:42</span>
                
                <p class="summary">Anthropic has announced a significant initiative involving the donation of its Model Context Protocol (MCP) and the establishment of the Agentic AI Foundation. This strategic move aims to foster an open and collaborative ecosystem for the development of advanced AI agents. The Model Context Protocol is designed to standardize how AI models interact with and leverage external tools, data, and environments, thereby enhancing their capabilities for complex, multi-step tasks. By open-sourcing the MCP, Anthropic seeks to accelerate innovation in agentic AI across the industry, enabling developers and researchers to build more sophisticated and reliable AI applications. The newly formed Agentic AI Foundation will serve as a neutral entity dedicated to guiding the evolution of agentic AI. Its mission includes promoting responsible development, establishing best practices, and facilitating research into the safety and utility of AI agents. This foundation is expected to bring together various stakeholders, including AI developers, ethicists, and policymakers, to ensure that the growth of agentic AI benefits society while mitigating potential risks. This dual announcement underscores a commitment to advancing AI through shared infrastructure and community governance.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Model Context Protocol</span><span>Agentic AI Foundation</span><span>Open Source AI</span><span>AI Governance</span><span>AI Standardization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Artificial Intelligence</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Launch HN: Mentat (YC F24) ‚Äì Controlling LLMs with Runtime Intervention</h2>
                <span class="published-time">Published: 2025-12-09 16:37:55</span>
                
                <p class="summary">CTGT has launched Mentat, an API designed to provide developers with deterministic control over Large Language Model (LLM) behavior. Mentat enables real-time steering of LLM reasoning and the removal of bias through runtime intervention, offering a more efficient and robust alternative to expensive fine-tuning or fragile prompt engineering methods. The technology employs feature-level intervention and graph-based verification to effectively mitigate issues like hallucinations and enforce complex operational policies. This capability is particularly critical for highly regulated sectors, such as financial services, where integrating intricate compliance policies into Generative AI for tasks like scanning communications is essential. Mentat aims to facilitate the scalable and controlled application of AI in contexts where the accuracy and reliability of AI output are paramount.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLM control</span><span>Runtime intervention</span><span>Feature-level intervention</span><span>Graph-based verification</span><span>Hallucination correction</span><span>Bias removal</span><span>Policy enforcement</span><span>AI API</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Generative AI</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://news.ycombinator.com/item?id=46207017" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Why the A.I. Boom Is Unlike the Dot-Com Boom</h2>
                <span class="published-time">Published: 2025-12-09 19:39:05</span>
                
                <p class="summary">The article, "Why the A.I. Boom Is Unlike the Dot-Com Boom," argues that the current burgeoning period of artificial intelligence innovation and investment fundamentally differs from the speculative bubble witnessed during the late 1990s dot-com era. Rather than being driven primarily by unproven business models and premature market enthusiasm, the contemporary AI boom is rooted in profound, tangible technological advancements. Key drivers include breakthroughs in machine learning algorithms, the proliferation of sophisticated neural networks, and the rapid evolution of large language models. These foundational technologies are enabling demonstrable, real-world applications across diverse sectors, including healthcare diagnostics, financial analytics, autonomous systems, and content generation. The present landscape is marked by sustained research and development, significant capital expenditure in high-performance computing, and a clearer pathway to commercial viability and societal impact. This perspective suggests that while rapid growth and investment are characteristic, the AI boom's enduring value is anchored in its foundational technical capabilities and its potential for deep, pervasive transformation, distinguishing it from past speculative cycles.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Neural Networks</span><span>Large Language Models</span><span>Technological Advancements</span><span>Economic Comparison</span><span>Tech Boom</span><span>Sustainable Growth</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.nytimes.com/2025/12/09/technology/ai-boom-unlike-dot-com-boom.html" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Horses: AI progress is steady. Human equivalence is sudden</h2>
                <span class="published-time">Published: 2025-12-09 00:26:35</span>
                
                <p class="summary">The article 'Horses: AI progress is steady. Human equivalence is sudden' delves into a critical perspective on the developmental trajectory and public perception of artificial intelligence. It argues that the underlying advancements in AI technology are typically characterized by consistent, incremental progress rather than dramatic leaps. However, the point at which AI systems achieve or surpass human-level performance in specific domains often appears to be sudden and abrupt. This perceived discontinuity can be attributed to several factors, including the exponential nature of technological growth, where sustained minor improvements can lead to an unexpected critical mass of capabilities, or simply a shift in human perception once a significant benchmark is unequivocally crossed. The piece encourages a more nuanced understanding of AI's evolution, highlighting the potential for rapid, transformative shifts in capability that may seem to emerge suddenly despite years of steady foundational work.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Progress</span><span>Human Equivalence</span><span>Technological Advancement</span><span>Emergent AI</span><span>AI Breakthroughs</span><span>Exponential Growth</span><span>AI Philosophy</span><span>Cognitive Computing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://andyljones.com/posts/horses.html" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning</h2>
                <span class="published-time">Published: 2025-12-08T11:39:43.000Z</span>
                
                <p class="summary">We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from "cold-start" format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Native Parallel Reasoner</span><span>Large Language Models</span><span>Parallel Reasoning</span><span>Reinforcement Learning</span><span>AI Agent</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.07461" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs</h2>
                <span class="published-time">Published: 2025-12-08T12:59:54.000Z</span>
                
                <p class="summary">Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Rotary Position Embeddings</span><span>Large Language Models</span><span>Long-context modeling</span><span>Positional encoding</span><span>Complex attention</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.07525" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</h2>
                <span class="published-time">Published: 2025-12-08T18:59:01.000Z</span>
                
                <p class="summary">Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video Generation</span><span>Multi-Modal Learning</span><span>Multi-Task Learning</span><span>World-Aware</span><span>Zero-shot Generalization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.07831" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>LongCat-Image Technical Report</h2>
                <span class="published-time">Published: 2025-12-08T14:26:40.000Z</span>
                
                <p class="summary">We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LongCat-Image</span><span>Image Generation</span><span>Multilingual Text Rendering</span><span>Foundation Model</span><span>Open-Source</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.07584" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</h2>
                <span class="published-time">Published: 2025-12-08T18:12:10.000Z</span>
                
                <p class="summary">Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Reasoning Language Models</span><span>Reinforcement Learning</span><span>Pre-training</span><span>Mid-training</span><span>Generalization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.07783" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</h2>
                <span class="published-time">Published: 2025-12-07T18:57:15.000Z</span>
                
                <p class="summary">Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Robot Manipulation</span><span>Generalization</span><span>Video Generative Models</span><span>Vision-Language-Action</span><span>Diffusion Transformer</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>Generative AI</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.06963" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>