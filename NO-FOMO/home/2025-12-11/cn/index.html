<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-11</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2025-12-11</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>GPT-5.2</h2>
                <span class="published-time">Published: 2025-12-11 18:04:47</span>
                
                <p class="summary">OpenAI has officially announced the introduction of its newest large language model, GPT-5.2, marking a significant advancement in the field of artificial intelligence. This release is accompanied by comprehensive documentation accessible on the OpenAI platform and a detailed system card, which outlines the model's technical specifications, intended capabilities, and safety implementations. While specific performance metrics and novel features are expected to be detailed further, the launch of GPT-5.2 suggests substantial improvements over previous iterations, likely focusing on enhanced reasoning, broader knowledge integration, and potentially expanded multimodal functionalities. The system card is crucial for understanding the model's design principles, its limitations, and the robust safety measures adopted to ensure responsible deployment across various applications. This development reinforces OpenAI's commitment to pushing the boundaries of AI technology and making sophisticated models available to researchers and developers globally.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>GPT-5.2</span><span>Large Language Model</span><span>OpenAI</span><span>Generative AI</span><span>AI Development</span><span>AI System Card</span><span>Model Release</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://openai.com/index/introducing-gpt-5-2/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Disney making $1B investment in OpenAI, will allow characters on Sora AI</h2>
                <span class="published-time">Published: 2025-12-11 14:12:14</span>
                
                <p class="summary">The Walt Disney Company has announced a significant $1 billion investment in OpenAI, signaling a strategic partnership aimed at integrating advanced artificial intelligence into its expansive entertainment ecosystem. This collaboration is specifically highlighted by Disney's intent to permit the use of its iconic characters on OpenAI's Sora AI platform. Sora, known for its text-to-video generative capabilities, is poised to revolutionize content creation within the media giant. This move suggests Disney is exploring innovative methods for storytelling, animation, and digital content production, leveraging AI to bring its beloved characters to life in novel ways. The substantial investment underscores the growing intersection between traditional entertainment and cutting-edge AI technologies, positioning Disney as a leader in adopting generative AI for creative endeavors. For OpenAI, this partnership provides significant capital and a high-profile application for its advanced models, potentially accelerating the development and refinement of its video generation capabilities. This strategic alliance is expected to pave the way for new forms of interactive experiences and media content, redefining how audiences engage with Disney's intellectual property through AI-powered platforms.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>OpenAI</span><span>Sora AI</span><span>Generative AI</span><span>Video Generation</span><span>Investment</span><span>Entertainment Industry</span><span>AI Collaboration</span><span>Digital Content</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Last quarter I rolled out Microsoft Copilot to 4k employees</h2>
                <span class="published-time">Published: 2025-12-11 19:18:27</span>
                
                <p class="summary">A recent post highlights a significant enterprise-wide deployment of Microsoft Copilot, an advanced AI assistant, to approximately 4,000 employees within the last fiscal quarter. This initiative underscores the increasing trend of integrating sophisticated artificial intelligence tools into large corporate environments to enhance productivity and streamline operational workflows. Such a large-scale rollout demonstrates an organizational commitment to leveraging generative AI technologies for practical business applications across diverse functions, including content creation, data analysis, and communication management. The successful deployment on this scale suggests a strategic investment in digital transformation, aiming to foster improved efficiency and adaptability within the workforce. While specific metrics on the impact are not detailed, the substantial number of users indicates a robust adoption strategy and a belief in the potential of AI to revolutionize daily tasks and support decision-making processes in a corporate setting.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Microsoft Copilot</span><span>Enterprise AI</span><span>AI Adoption</span><span>Workplace Productivity</span><span>Generative AI</span><span>Software Deployment</span><span>Digital Transformation</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://twitter.com/gothburz/status/1999124665801880032" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Rivian Unveils Custom Silicon, R2 Lidar Roadmap, and Universal Hands Free</h2>
                <span class="published-time">Published: 2025-12-11 18:17:19</span>
                
                <p class="summary">Rivian, the electric vehicle manufacturer, has announced significant advancements in its autonomous driving capabilities with the unveiling of custom-designed silicon, a detailed roadmap for its R2 Lidar technology, and a new 'Universal Hands Free' system. These developments are central to Rivian's next-generation autonomy platform, aiming to enhance the safety, performance, and scalability of its advanced driver-assistance systems (ADAS). The custom silicon is expected to provide superior processing power and efficiency for real-time sensor data interpretation and decision-making, crucial for complex driving scenarios. The R2 Lidar roadmap outlines the evolution of Rivian's perception systems, emphasizing improved range, resolution, and reliability, which are vital for robust environmental sensing. Furthermore, the Universal Hands Free feature suggests a more refined and broadly applicable semi-autonomous driving experience, potentially expanding the operational design domain of its hands-free driving functions. These strategic technological investments underscore Rivian's commitment to developing sophisticated in-house solutions to accelerate its progress in the competitive autonomous vehicle landscape, promising a more integrated and advanced user experience for its future lineup.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Custom Silicon</span><span>Lidar Technology</span><span>Autonomous Driving</span><span>Hands-Free Driving</span><span>ADAS</span><span>Next-Gen Autonomy Platform</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Robotics</span><span>Artificial Intelligence</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://riviantrackr.com/news/rivian-unveils-custom-silicon-r2-lidar-roadmap-universal-hands-free-and-its-next-gen-autonomy-platform/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>A Developer Accidentally Found CSAM in AI Data. Google Banned Him for It</h2>
                <span class="published-time">Published: 2025-12-11 16:02:32</span>
                
                <p class="summary">A significant incident has recently highlighted severe challenges in AI data integrity and content moderation, as a developer reportedly discovered Child Sexual Abuse Material (CSAM) within an AI dataset. This accidental finding underscores the immense difficulties associated with thoroughly curating and sanitizing the vast quantities of data essential for training advanced artificial intelligence models. Following this unintended discovery, Google implemented a ban on the developer from its platforms, a decision that has ignited considerable debate. The situation raises critical questions regarding the ethical responsibilities of AI developers when encountering illegal content, the efficacy of current data filtering mechanisms employed by leading technology firms, and the adequacy of platform policies concerning user accounts in such sensitive contexts. There are concerns that such punitive actions could potentially deter researchers and developers from disclosing crucial vulnerabilities or illicit content within AI ecosystems, thereby impeding collective efforts to enhance data safety and promote ethical AI development. This case accentuates the urgent necessity for robust data governance frameworks, proactive content screening technologies, and transparent, supportive protocols for developers who encounter prohibited material, ensuring accountability while fostering a secure environment for reporting.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Data Security</span><span>Data Hygiene</span><span>Content Moderation Systems</span><span>Ethical AI Development</span><span>AI Training Data</span><span>Platform Policy</span><span>Dataset Filtering</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.404media.co/a-developer-accidentally-found-csam-in-ai-data-google-banned-him-for-it/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: GPULlama3.java Llama Compilied to PTX/OpenCL Now Integrated in Quarkus</h2>
                <span class="published-time">Published: 2025-12-11 15:59:33</span>
                
                <p class="summary">The GPULlama3.java project unveils a significant advancement in deploying Llama models, specifically enabling their compilation to PTX/OpenCL for accelerated execution on GPUs. This capability is now seamlessly integrated into Quarkus, a popular Java framework renowned for its cloud-native and serverless application development. The core objective of this project is to drastically improve the performance and operational efficiency of running large language models within Java environments by effectively utilizing GPU hardware. Detailed instructions guide users through the entire setup process, encompassing the download and configuration of TornadoVM, the establishment of project-specific environment paths, building the project with Maven, and finally, executing the Llama model. This integration marks a crucial step forward for efficient AI model deployment and execution within Java-centric ecosystems, providing developers with a streamlined methodology to leverage the robust computational power of GPUs for demanding AI tasks. It particularly emphasizes TornadoVM's role in compiling and executing Java code across diverse hardware, including GPUs.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Llama</span><span>Java</span><span>GPU</span><span>PTX</span><span>OpenCL</span><span>Quarkus</span><span>TornadoVM</span><span>Large Language Model</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://news.ycombinator.com/item?id=46233009" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</h2>
                <span class="published-time">Published: 2025-12-10T18:59:32.000Z</span>
                
                <p class="summary">Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Vision-Language-Action Models</span><span>Robotic Manipulation</span><span>Motion Representation</span><span>Temporal Reasoning</span><span>Long-horizon Manipulation</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Robotics</span><span>Multimodal</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.09928" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</h2>
                <span class="published-time">Published: 2025-12-10T17:50:29.000Z</span>
                
                <p class="summary">Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Autonomous Driving</span><span>Understanding Generation Planning</span><span>Vision-Language Models</span><span>Video Generation</span><span>Trajectory Planning</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Robotics</span><span>Computer Vision</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.09864" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Composing Concepts from Images and Videos via Concept-prompt Binding</h2>
                <span class="published-time">Published: 2025-12-10T16:57:31.000Z</span>
                
                <p class="summary">Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Visual Concept Composition</span><span>Concept-Prompt Binding</span><span>Diffusion Transformers</span><span>Temporal Disentanglement</span><span>Cross-Attention</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Computer Vision</span><span>Generative AI</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.09824" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting</h2>
                <span class="published-time">Published: 2025-12-10T14:01:02.000Z</span>
                
                <p class="summary">Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Multimodal Large Language Models</span><span>Infrared Images</span><span>Benchmarking</span><span>Generative Visual Prompting</span><span>Image Understanding</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Multimodal</span><span>Large Language Model</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.09663" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Learning Unmasking Policies for Diffusion Language Models</h2>
                <span class="published-time">Published: 2025-12-09T20:44:33.000Z</span>
                
                <p class="summary">Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Diffusion Language Models</span><span>Reinforcement Learning</span><span>Masked Diffusion</span><span>Sampling Procedures</span><span>Large Language Models</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Natural Language Processing</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.09106" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation</h2>
                <span class="published-time">Published: 2025-12-10T06:50:16.000Z</span>
                
                <p class="summary">The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Monocular-to-Stereo Video Generation</span><span>Geometry-Aware</span><span>Stereo Video</span><span>Video Generator</span><span>XR Devices</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Computer Vision</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.09363" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>