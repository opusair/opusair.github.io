<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-10-27</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-10-27</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Claude for Excel</h2>
                <span class="published-time">Published: 2025-10-27 16:09:22</span>
                
                <p class="summary">Anthropic has introduced 'Claude for Excel,' an integration designed to bring advanced artificial intelligence capabilities directly into Microsoft Excel. This new offering aims to significantly enhance productivity and streamline data management tasks for users by leveraging Claude's powerful natural language processing and sophisticated reasoning abilities. The integration allows users to automate complex spreadsheet operations, generate accurate formulas, analyze intricate data patterns, and interpret information using intuitive natural language prompts, effectively transforming how data is handled and processed within Excel. This development signifies a strategic move towards embedding sophisticated AI agents into ubiquitous business applications, making AI-driven data analysis more accessible and intuitive for a broader audience. By substantially reducing manual effort and providing intelligent, context-aware insights, Claude for Excel seeks to empower users to derive greater value from their datasets, accelerate informed decision-making processes, and improve overall efficiency in data-intensive workflows.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Claude</span><span>Excel</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Productivity</span><span>Data Analysis</span><span>Automation</span><span>Spreadsheet</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.claude.com/claude-for-excel" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Artificial Writing and Automated Detection</h2>
                <span class="published-time">Published: 2025-10-27 17:09:05</span>
                
                <p class="summary">This research paper, titled 'Artificial Writing and Automated Detection,' critically examines the dual emergence of sophisticated AI-driven text generation and the imperative for effective automated detection systems. The proliferation of large language models has enabled the creation of highly coherent and contextually relevant artificial writing, blurring the lines between human and machine authorship. This development introduces considerable challenges across sectors such as education, journalism, and scientific research, where the authenticity of content is paramount. The paper likely investigates the technical mechanisms behind both the generation of such artificial text and the various methodologies employed for its identification, including statistical analysis, linguistic patterns, and potentially watermarking techniques. It highlights the dynamic and often adversarial relationship between advancements in generative AI and the development of robust detection tools. The work provides insights into the societal implications of widespread AI-generated content and the ongoing efforts to maintain transparency and integrity in written communication.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Artificial Intelligence</span><span>Large Language Models</span><span>Text Generation</span><span>AI Content Detection</span><span>Natural Language Processing</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.nber.org/system/files/working_papers/w34223/w34223.pdf" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Creating an All-Weather Driver</h2>
                <span class="published-time">Published: 2025-10-27 18:57:57</span>
                
                <p class="summary">Waymo's blog post, "Creating an All-Weather Driver," details the complex engineering challenges and innovative solutions involved in developing a robust autonomous driving system capable of operating safely and reliably across diverse and challenging weather conditions. The article likely explores how Waymo addresses critical issues such as obscured sensor data from rain, snow, or fog, and the impact of reduced visibility on perception and prediction systems. Key technical strategies could include advanced sensor fusion techniques, which integrate data from lidar, radar, cameras, and other sensors to create a comprehensive environmental model less susceptible to individual sensor limitations. Furthermore, the discussion would encompass sophisticated machine learning algorithms for object detection and tracking, specifically trained on vast datasets encompassing various meteorological scenarios. The ultimate goal is to ensure that Waymo's autonomous vehicles can maintain consistent performance and safety standards, irrespective of environmental factors, thereby expanding the operational design domain and bringing fully autonomous mobility closer to widespread reality. This initiative underscores the significant progress in AI and robotics required to achieve truly all-weather self-driving capabilities.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Autonomous Driving</span><span>Self-Driving Cars</span><span>Perception Systems</span><span>Sensor Fusion</span><span>Adverse Weather Conditions</span><span>Robotics</span><span>AI in Automotive</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>Computer Vision</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://waymo.com/blog/2025/10/creating-an-all-weather-driver" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>It's insulting to read AI-generated blog posts</h2>
                <span class="published-time">Published: 2025-10-27 15:27:38</span>
                
                <p class="summary">The sentiment expressed in the Hacker News story title, 'It's insulting to read AI-generated blog posts,' reflects a growing critical stance among readers towards content perceived as originating from artificial intelligence. This perspective suggests that such material often lacks the depth, nuance, and genuine human insight expected from quality writing, leading to a negative user experience. Readers may interpret AI-generated posts as a form of intellectual laziness or a disrespect for their time and intelligence, fostering a sense of insult. The proliferation of easily identifiable AI-written content could erode trust in online publications and content creators, prompting a reevaluation of content generation strategies. This highlights a crucial challenge for Generative AI applications in writing: balancing efficiency with maintaining authenticity, originality, and a human connection to avoid alienating audiences. The critique underscores the importance of human oversight and unique value proposition in an increasingly automated content landscape, advocating for AI as an augmentative tool rather than a wholesale replacement for human creativity.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Generative AI</span><span>AI-generated content</span><span>Content quality</span><span>User experience</span><span>Natural Language Generation</span><span>Content authenticity</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://blog.pabloecortez.com/its-insulting-to-read-your-ai-generated-blog-post/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>WorldGrow: Generating Infinite 3D World</h2>
                <span class="published-time">Published: 2025-10-27 09:31:29</span>
                
                <p class="summary">WorldGrow introduces a novel approach to generating infinite 3D worlds, offering a robust framework for creating expansive and diverse virtual environments. The project focuses on procedural content generation techniques, enabling the dynamic creation of landscapes, structures, and interactive elements without manual intervention. This system aims to address challenges in developing vast virtual spaces for applications such as gaming, simulations, and virtual reality, where traditional asset creation can be time-consuming and resource-intensive. By leveraging algorithmic methods, WorldGrow facilitates the on-the-fly generation of unique and evolving environments, promising significant advancements in scalability and immersion. Its core idea revolves around automating the world-building process, providing developers with tools to rapidly prototype and deploy complex 3D scenes. The potential impact spans various digital domains requiring dynamic and endless spatial experiences.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Procedural Generation</span><span>3D Graphics</span><span>Virtual Worlds</span><span>Game Development</span><span>Content Creation</span><span>Environment Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Machine Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/world-grow/WorldGrow" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Microsoft needs to open up more about its OpenAI dealings</h2>
                <span class="published-time">Published: 2025-10-27 11:19:27</span>
                
                <p class="summary">The Wall Street Journal article highlights a critical need for Microsoft to disclose more details about its substantial dealings and strategic partnership with OpenAI. This demand for increased transparency emerges amid growing public and regulatory scrutiny over the extensive influence Microsoft exerts within the rapidly expanding artificial intelligence industry. Concerns are particularly focused on the implications of Microsoft's significant financial investment and operational integration with OpenAI, a frontrunner in AI research and deployment. Industry observers and critics are increasingly vocal about potential impacts on competitive fairness, the risk of monopolistic tendencies, and the broader challenges associated with governing a highly integrated relationship between two pivotal players in the AI domain. Enhanced transparency regarding their specific agreements, operational protocols, and joint strategic planning is considered vital for building public confidence, ensuring robust accountability, and fostering an equitable and innovative environment for AI development and application. This openness is essential to address issues of market concentration and to guide the responsible evolution of advanced AI technologies.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Microsoft</span><span>OpenAI</span><span>AI partnership</span><span>Transparency</span><span>Regulatory scrutiny</span><span>AI governance</span><span>Market competition</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.wsj.com/tech/ai/microsoft-needs-to-open-up-more-about-its-openai-dealings-59102de8" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>Agent Lightning‚ö°</h2>
                <span class="published-time">Published: 2025-10-27T16:01:31Z</span>
                
                <p class="summary">Agent Lightning is a Microsoft-developed training framework designed to optimize AI agents with minimal code changes. It supports a wide array of agent frameworks, including LangChain, OpenAI Agent SDK, AutoGen, and CrewAI, or can be used with agents built without specific frameworks. A key feature is its ability to selectively optimize agents within complex multi-agent systems, leveraging algorithms such as Reinforcement Learning, Automatic Prompt Optimization, and Supervised Fine-tuning. The framework operates by tracing agent interactions
‚Äîprompts, tool calls, and rewards
‚Äîinto a central LightningStore. This data fuels algorithms that refine agent behavior, producing updated resources like improved prompt templates or policy weights. The Trainer component orchestrates the learning loop, streaming data and applying improvements to the inference engine. This streamlined architecture enables a clear path from initial deployment to continuous performance enhancement, facilitating the development of robust and adaptable AI agents, as demonstrated in projects tackling long-horizon, sparse-reward tasks.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Agents</span><span>Reinforcement Learning</span><span>Prompt Optimization</span><span>Multi-agent Systems</span><span>Agent Training</span><span>Machine Learning</span><span>AI Frameworks</span><span>Deep Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/microsoft/agent-lightning" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>AFFiNE.ProWrite, Draw and Plan All at Once</h2>
                <span class="published-time">Published: 2025-10-23T08:35:08Z</span>
                
                <p class="summary">AFFiNE is an open-source, local-first, and privacy-focused all-in-one workspace designed as a robust alternative to Notion and Miro. It uniquely hyper-merges writing, drawing, and planning functionalities into a single platform, enabling users to manage docs, canvases, and tables seamlessly. A core feature is its "true canvas for blocks," allowing the integration of diverse elements like rich text, sticky notes, embedded web pages, and multi-view databases onto an edgeless canvas. The platform is augmented by AFFiNE AI, a multimodal partner capable of assisting with tasks from generating professional reports and slides to summarizing articles into mindmaps and prototyping applications from prompts, leveraging advanced Canvas AI capabilities. Emphasizing data ownership, AFFiNE supports local storage alongside real-time collaboration and cross-platform synchronization. Users have the freedom to self-host, fork, and customize their AFFiNE instance, with an upcoming plugin community. Built upon open-source projects like Blocksuite and OctoBase, AFFiNE aims to provide a comprehensive and extensible knowledge management system.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Open-source</span><span>Local-first</span><span>Workspace</span><span>Knowledge Management</span><span>AI Assistant</span><span>Real-time Collaboration</span><span>Block-based Editor</span><span>Self-hosting</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Multimodal</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/toeverything/AFFiNE" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>DeepAgent: A General Reasoning Agent with Scalable Toolsets</h2>
                <span class="published-time">Published: 2025-10-24T16:24:01.000Z</span>
                
                <p class="summary">Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>DeepAgent</span><span>Reasoning Agents</span><span>Tool Use</span><span>Reinforcement Learning</span><span>Large Language Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.21618" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Video-As-Prompt: Unified Semantic Control for Video Generation</h2>
                <span class="published-time">Published: 2025-10-23T17:59:52.000Z</span>
                
                <p class="summary">Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video generation</span><span>Semantic control</span><span>Video-As-Prompt</span><span>Diffusion Transformer</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.20888" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model</h2>
                <span class="published-time">Published: 2025-10-22T06:58:55.000Z</span>
                
                <p class="summary">Discrete diffusion models have emerged as a promising direction for vision-language tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by a train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering a chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, a refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features a two-stage training process: first, we instill a foundational revision capability by training the model to revise synthetic errors; second, we implement a novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an expert's corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at https://rediff-hku.github.io/.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Vision-Language Diffusion Model</span><span>Diffusion Models</span><span>Self-Correction</span><span>Error Cascades</span><span>Parallel Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Generative AI</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.19871" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>A Definition of AGI</h2>
                <span class="published-time">Published: 2025-10-21T01:28:35.000Z</span>
                
                <p class="summary">The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify both rapid progress and the substantial gap remaining before AGI.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Artificial General Intelligence</span><span>Cognitive Assessment</span><span>Psychometrics</span><span>AI Evaluation</span><span>Human Cognition</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.18212" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning</h2>
                <span class="published-time">Published: 2025-10-23T07:18:32.000Z</span>
                
                <p class="summary">GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>GUI grounding</span><span>Instruction-as-Reasoning</span><span>Multi-perspective reasoning</span><span>UI agents</span><span>Reinforcement learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Natural Language Processing</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.20286" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling</h2>
                <span class="published-time">Published: 2025-10-23T04:45:09.000Z</span>
                
                <p class="summary">Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present RAPO++, a cross-stage prompt optimization framework that unifies training-data--aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In Stage 1, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. Stage 2 introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback -- including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow -- yielding progressively improved video generation quality. Stage 3 leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at https://github.com/Vchitect/RAPO.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Text-to-Video Generation</span><span>Prompt Optimization</span><span>Large Language Model</span><span>Diffusion Models</span><span>Data Alignment</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Video Understanding</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2510.20206" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>