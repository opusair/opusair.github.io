<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI 日报 - 2025-06-11</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter, Noto Sans SC', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }

        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.3s ease, transform 0.2s ease;
            border: 2px solid transparent;
            font-size: 0.9em;
        }

        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }

        .language-switch a.active {
            background: var(--secondary-color);
            border-color: var(--border-color);
        }

        @media (max-width: 768px) {
            .language-switch {
                position: static;
                justify-content: center;
                margin-bottom: 20px;
            }
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="./" class="active">中文</a>
                <a href="en/">English</a>
            </div>

            <h1>AI 日报</h1>
            <p class="date">2025-06-11</p>
            <p class="theme-info">关于我们: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">🏠 返回主页</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">📅 最新日报</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">👤 关于我们</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>「Next-Token」范式改变！刚刚，强化学习预训练来了</h2>
                <span class="published-time">发布时间: 2025-06-11T03:55:03.000Z</span>
                <img src="screenshot/wechat/wechat_image_UABVUoHYTDlFWWNvD5R9Og.png" alt="「Next-Token」范式改变！刚刚，强化学习预训练来了">
                <p class="summary">微软提出“强化预训练”（RPT）新范式，将下一token预测重构为强化学习推理任务。该范式在预训练阶段融入强化学习，模型通过正确预测下一token获得可验证奖励，无需外部标注，利用海量无标注文本数据进行通用强化学习。RPT显著提升语言模型预测准确性，为后续强化微调奠定基础，并有效降低奖励欺骗风险。实验证明，RPT在语言建模能力和零样本性能上均优于基线模型，展现了其推动大语言模型预训练发展、促进模型深层理解和泛化能力的巨大潜力。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>强化学习</span><span>预训练</span><span>大模型</span><span>Next-Token</span><span>语言模型</span><span>RPT</span></div>
                    <div class="area"><span class="label">区域：</span><span>强化学习</span><span>大模型</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/UABVUoHYTDlFWWNvD5R9Og" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>103K「硬核」题，让大模型突破数学推理瓶颈</h2>
                <span class="published-time">发布时间: 2025-06-11T03:55:03.000Z</span>
                <img src="screenshot/20250611/wechat/wechat_image_EkVeW5pLRM8_T6hrrs7lsA.png" alt="103K「硬核」题，让大模型突破数学推理瓶颈">
                <p class="summary">腾讯AI Lab与上海交大团队发布DeepMath-103K数据集，旨在解决当前大语言模型在数学推理，特别是强化学习训练中面临的数据瓶颈，如数据集难度不足、答案难验证、数据污染及缺乏新颖性等问题。DeepMath-103K包含逾10万道高难度、严格去污染且答案可验证的数学题，专为推动AI数学推理极限设计。其独特结构支持强化学习，并已助力DeepMath系列模型在多个基准测试中达到SOTA，展现出从数学到科学领域的泛化推理能力，预示着AI将实现更深层次的思考与更广泛的科学探索。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>DeepMath-103K</span><span>数学推理</span><span>大语言模型</span><span>数据集</span><span>强化学习</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>人工智能</span><span>机器学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/EkVeW5pLRM8_T6hrrs7lsA" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Mistral的首个强推理模型：拥抱开源，推理速度快10倍</h2>
                <span class="published-time">发布时间: 2025-06-11T03:55:03.000Z</span>
                <img src="screenshot/20250611/wechat/wechat_image_Go5dXv4DA3hy5lGhxxE8SA.png" alt="Mistral的首个强推理模型：拥抱开源，推理速度快10倍">
                <p class="summary">Mistral AI发布了其首个强推理大模型Magistral系列，包含专有版Magistral Medium和开源版Magistral Small，该系列模型展现卓越推理能力，能自我反思并解决复杂任务。Magistral模型擅长多语言高保真推理，通过Flash Answers实现高达10倍的token吞吐量，显著提升实时推理效率。其核心设计是使用与用户相同的语言进行推理。Magistral Medium定价虽高但具竞争力，旨在加速模型迭代并推动大规模实时推理应用。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大模型</span><span>推理能力</span><span>Mistral AI</span><span>开源</span><span>实时推理</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>人工智能</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/Go5dXv4DA3hy5lGhxxE8SA" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAI深夜放大招！o3-Pro震撼发布：性能飙升，价格暴降80%！</h2>
                <span class="published-time">发布时间: 2025-06-11T01:08:23.000Z</span>
                <img src="screenshot/20250611/wechat/wechat_image_uQCOXmZS_Zo6QL30TXq5cQ.png" alt="OpenAI深夜放大招！o3-Pro震撼发布：性能飙升，价格暴降80%！">
                <p class="summary">OpenAI深夜发布o3-pro模型，面向ChatGPT Pro用户及API开放。作为o3的升级版，o3-pro专为深度思考和提供可靠响应设计，在数学、科学、编程等学术评估中表现卓越，并支持网络搜索、文件分析、视觉推理等多种工具。专家评估显示，o3-pro在清晰度、准确性等方面显著优于o3。同时，o3 API价格直降80%，o3-pro API价格相比o1-pro也下降87%，大幅降低了使用成本，使其成为可靠性优先场景的理想选择。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>o3-pro</span><span>OpenAI</span><span>大模型</span><span>性能提升</span><span>API降价</span><span>可靠性</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>人工智能</span><span>生成式AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/uQCOXmZS_Zo6QL30TXq5cQ" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PolyVivid实现多主体视频定制，身份一致性超越现有模型</h2>
                <span class="published-time">发布时间: 2025-06-11T00:30:38.000Z</span>
                <img src="screenshot/20250611/wechat/wechat_image_4FiiPR7Ds7ZZTykEKV73Wg.png" alt="PolyVivid实现多主体视频定制，身份一致性超越现有模型">
                <p class="summary">上交与腾讯联合推出PolyVivid框架，旨在解决现有视频生成模型在多主体定制中身份一致性差、交互不自然的问题。PolyVivid通过VLLM融合模块实现视觉身份与文本空间精确对齐，引入3D-RoPE增强图文融合，并采用注意力继承式身份注入模块缓解身份漂移。其MLLM数据流水线提升主体区分与生成质量，同时提出的SaRA参数高效微调方法优化大模型应用。该框架在身份保真度、视频真实感及主体对齐方面显著优于现有基线，为灵活且身份一致的多主体视频生成提供了创新解决方案。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>视频生成</span><span>多主体定制</span><span>身份一致性</span><span>PolyVivid</span><span>参数高效微调</span><span>大模型</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>多模态</span><span>计算机视觉</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/4FiiPR7Ds7ZZTykEKV73Wg" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>🌟 Awesome LLM Apps</h2>
                <span class="published-time">发布时间: 2025-06-06T22:50:54Z</span>
                <img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" alt="🌟 Awesome LLM Apps">
                <p class="summary">该GitHub仓库“Awesome LLM Apps”汇集了大量基于RAG、AI智能体、多智能体团队、MCP及语音智能体等技术构建的LLM应用。它展示了如何利用OpenAI、Anthropic、Google及开源模型（如DeepSeek、Qwen、Llama）解决实际问题，涵盖从代码库到邮件处理等多样化场景。项目旨在提供实用且创新的LLM应用范例，推动大模型技术在不同领域的落地与发展，并鼓励社区贡献，共同构建丰富的开源LLM应用生态。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大语言模型</span><span>AI智能体</span><span>RAG</span><span>多智能体系统</span><span>生成式AI</span><span>应用开发</span><span>机器学习</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/Shubhamsaboo/awesome-llm-apps" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>✨ YouTube Transcript API ✨</h2>
                <span class="published-time">发布时间: 2025-06-11T22:24:04Z</span>
                <img src="screenshot/github/youtube-transcript-api.png" alt="✨ YouTube Transcript API ✨">
                <p class="summary">YouTube Transcript API是一个Python库，旨在高效获取YouTube视频的字幕和转录文本，包括自动生成字幕。该API无需依赖无头浏览器，支持字幕翻译，并提供灵活的API接口和命令行工具。它能处理IP封禁问题，通过代理配置确保服务稳定性，并支持多种输出格式。该项目为视频内容分析、多语言处理及自动化数据提取提供了便捷的解决方案。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>YouTube</span><span>字幕</span><span>转录</span><span>API</span><span>Python</span><span>视频处理</span><span>数据提取</span><span>代理</span></div>
                    <div class="area"><span class="label">区域：</span><span>视频理解</span><span>自然语言处理</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/jdepoix/youtube-transcript-api" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>开源大模型食用指南</h2>
                <span class="published-time">发布时间: 2025-06-11T14:30:51Z</span>
                <img src="https://contrib.rocks/image?repo=datawhalechina/self-llm" alt="开源大模型食用指南">
                <p class="summary">本项目是面向国内初学者的开源大模型教程，基于Linux平台，提供从环境配置、本地部署到高效微调的全流程指导。它简化了开源大模型的应用流程，涵盖LLaMA、ChatGLM、InternLM等主流模型，并指导命令行调用、在线Demo部署及LangChain集成。旨在帮助学生和研究者掌握开源大模型的“食用”方法，促进其在学习和实践中的普及应用。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大模型</span><span>开源</span><span>环境配置</span><span>模型部署</span><span>模型微调</span><span>自然语言处理</span><span>Linux</span><span>LangChain</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>自然语言处理</span><span>深度学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/datawhalechina/self-llm" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiniCPM</h2>
                <span class="published-time">发布时间: 2025-06-11T06:36:09Z</span>
                <img src="https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm_logo.png" alt="MiniCPM">
                <p class="summary">MiniCPM是一个极致高效的端侧大模型系列，由面壁智能、清华大学和中国人民大学联合开发。它通过创新的模型架构（如InfLLM v2稀疏注意力）、高效学习算法（如BitCPM三值量化）和优化推理系统（CPM.cu），实现了卓越的效率提升。MiniCPM系列模型在保持同等规模最优性能的同时，在典型端侧芯片上实现了5倍以上生成加速，并在工具调用、代码解释器、长文本处理等多个任务上超越同级别甚至更大参数量的模型，为端侧AI应用提供了强大的解决方案。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大模型</span><span>端侧AI</span><span>高效推理</span><span>稀疏注意力</span><span>模型量化</span><span>工具调用</span><span>长文本</span><span>自然语言处理</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>深度学习</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/OpenBMB/MiniCPM" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>大型语言模型中的地缘政治偏见：当代语言模型如何界定“好”与“坏”国家</h2>
                <span class="published-time">发布时间: 2025-06-07T10:45:17.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06751.png" alt="大型语言模型中的地缘政治偏见：当代语言模型如何界定“好”与“坏”国家">
                <p class="summary">本文通过分析大型语言模型（LLMs）对具有冲突国家视角（美国、英国、苏联和中国）的历史事件的解读，评估了其在不同国家方面的地缘政治偏见。我们引入了一个新颖的数据集，其中包含中立的事件描述和来自不同国家的对比观点。我们的研究结果表明，模型存在显著的地缘政治偏见，倾向于特定的国家叙事。此外，简单的去偏见提示在减少这些偏见方面的效果有限。对操纵参与者标签的实验揭示了模型对归因的敏感性，有时会放大偏见或识别不一致性，尤其是在标签互换的情况下。这项工作突出了大型语言模型中的国家叙事偏见，挑战了简单去偏见方法的有效性，并为未来的地缘政治偏见研究提供了框架和数据集。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大型语言模型</span><span>地缘政治偏见</span><span>国家叙事</span><span>偏见评估</span><span>数据集</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>自然语言处理</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.06751" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>自回归语义视觉重建助力视觉语言模型提升理解能力</h2>
                <span class="published-time">发布时间: 2025-06-10T17:57:50.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09040.png" alt="自回归语义视觉重建助力视觉语言模型提升理解能力">
                <p class="summary">典型的视觉语言大模型（LVLMs）仅对文本序列应用自回归监督，未能将视觉模态充分融入学习过程。这导致了三个关键局限性：（1）无法在没有配套字幕的情况下利用图像；（2）字幕可能遗漏关键视觉细节；（3）某些以视觉为中心的内容无法通过文本充分传达。因此，当前的LVLMs通常优先考虑视觉到语言的对齐，而可能忽视细粒度的视觉信息。尽管一些现有工作探索了自回归图像生成，但有效利用自回归视觉监督来增强图像理解仍然是一个开放的挑战。在本文中，我们引入了自回归语义视觉重建（ASVR），它使得在统一的自回归框架内实现视觉和文本模态的联合学习成为可能。我们发现，自回归地重建图像的原始视觉外观并不能增强，甚至可能损害多模态理解。相反，自回归地重建图像的语义表示则能持续提升理解能力。值得注意的是，我们发现即使模型输入连续图像特征，它们也能有效地重建离散的语义标记，从而在广泛的多模态理解基准上带来稳定且持续的改进。我们的方法在不同数据规模（556k-2M）和不同LLM骨干网络类型上均实现了显著的性能提升。具体而言，ASVR使LLaVA-1.5在14个多模态基准测试中的平均得分提高了5%。代码已开源。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>自回归语义视觉重建</span><span>视觉语言模型</span><span>多模态理解</span><span>图像理解</span><span>语义表示</span></div>
                    <div class="area"><span class="label">区域：</span><span>多模态</span><span>大模型</span><span>计算机视觉</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.09040" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>看见声音：使用Mirage从音频生成A-Roll视频</h2>
                <span class="published-time">发布时间: 2025-06-09T22:56:02.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08279.png" alt="看见声音：使用Mirage从音频生成A-Roll视频">
                <p class="summary">从专业电影制作到用户生成内容，创作者和消费者长期以来都认识到，视频的力量取决于我们所听（视频的音轨）与我们所看（视频的图像序列）的和谐整合。当前的视频生成方法要么忽略声音，专注于通用但无声的图像序列生成，要么同时处理视觉和音频元素，但仅限于配音等受限应用领域。我们引入了Mirage，一个音频到视频的基础模型，它擅长根据音频输入从头开始生成逼真、富有表现力的输出图像。当与现有的语音合成方法（文本到语音，即TTS）结合时，Mirage能够生成引人入胜的多模态视频。当在人们讲话的音视频素材（A-roll）上进行训练，并以包含语音的音频为条件时，Mirage能够生成人们对输入音频中隐含的表演进行可信诠释的视频。我们核心的技术贡献是一种统一的方法，用于训练基于自注意力机制的音频到视频生成模型，无论是从头开始训练还是基于现有权重。这种方法使Mirage作为一种音频到视频生成方法保持了通用性，同时生成的主观质量优于那些结合了特定音频架构或特定于人物、语音或图像/音频捕获细节的方法。我们鼓励读者亲自观看和聆听Mirage的成果（请参阅论文和评论中的链接）。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>音频到视频生成</span><span>基础模型</span><span>多模态</span><span>视频生成</span><span>自注意力</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>多模态</span><span>深度学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08279" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ECoRAG：长上下文RAG的证据引导压缩</h2>
                <span class="published-time">发布时间: 2025-06-05T15:43:49.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05167.png" alt="ECoRAG：长上下文RAG的证据引导压缩">
                <p class="summary">大型语言模型（LLMs）通过检索增强生成（RAG）利用外部文档，在开放域问答（ODQA）中展现出卓越性能。为减少RAG开销，对于较长上下文，上下文压缩是必要的。然而，先前的压缩方法并未专注于过滤非证据性信息，这限制了其在基于LLM的RAG中的性能。因此，我们提出了证据引导的RAG，即ECoRAG框架。ECoRAG通过基于证据性压缩检索到的文档来提升LLM性能，确保答案生成由正确证据支持。作为额外步骤，ECoRAG会评估压缩内容是否提供了充分证据，如果不足，则继续检索直至充分。实验表明，ECoRAG在ODQA任务上提升了LLM性能，优于现有压缩方法。此外，ECoRAG具有极高的成本效益，因为它不仅降低了延迟，还通过仅保留生成正确答案所需的信息来最大限度地减少token使用。代码可在https://github.com/ldilab/ECoRAG获取。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>检索增强生成</span><span>大型语言模型</span><span>上下文压缩</span><span>证据性</span><span>开放域问答</span></div>
                    <div class="area"><span class="label">区域：</span><span>自然语言处理</span><span>大模型</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.05167" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>基于多模态大语言模型中接地推理的可解释且可靠的AI生成图像检测</h2>
                <span class="published-time">发布时间: 2025-06-08T08:47:44.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07045.png" alt="基于多模态大语言模型中接地推理的可解释且可靠的AI生成图像检测">
                <p class="summary">图像生成技术的快速发展，加剧了对可解释且鲁棒的检测方法的需求。尽管现有方法通常能达到高精度，但它们通常作为黑箱运行，无法提供人类可理解的解释。多模态大语言模型（MLLMs）虽然最初并非用于伪造检测，但展现出强大的分析和推理能力。经过适当微调后，它们可以有效地识别AI生成图像并提供有意义的解释。然而，现有MLLMs仍然存在幻觉问题，并且常常无法将其视觉解释与实际图像内容和人类推理对齐。为了弥合这一差距，我们构建了一个AI生成图像数据集，其中包含标注了合成伪影的边界框和描述性标题，为人类对齐的视觉-文本接地推理奠定了基础。随后，我们通过多阶段优化策略对MLLMs进行微调，该策略逐步平衡了准确检测、视觉定位和连贯文本解释的目标。最终模型在检测AI生成图像和定位视觉缺陷方面均取得了卓越性能，显著优于基线方法。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>AI生成图像检测</span><span>多模态大语言模型</span><span>可解释性</span><span>接地推理</span><span>伪影检测</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>多模态</span><span>计算机视觉</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07045" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>思考与行动：通过扩展测试时交互进行推理的智能体</h2>
                <span class="published-time">发布时间: 2025-06-09T17:50:02.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07976.png" alt="思考与行动：通过扩展测试时交互进行推理的智能体">
                <p class="summary">当前测试时扩展的范式依赖于在生成响应之前产生冗长的推理轨迹（即“更多思考”）。在需要交互的智能体问题中，这可以通过在与世界交互之前生成思考轨迹来实现。然而，这一过程不允许智能体从环境中获取新信息或随时间调整其行为。在这项工作中，我们提出扩展测试时交互，这是测试时扩展一个尚未开发的维度，它增加了智能体的交互视野，从而能够在单次推演中运行探索、回溯和动态重新规划等丰富的行为。为了展示这一扩展维度的潜力，我们研究了网络智能体领域。我们首先表明，即使是基于提示的交互扩展，在没有任何训练的情况下，也能显著提高网络基准上的任务成功率。在此基础上，我们引入了TTI（测试时交互），这是一种基于课程的在线强化学习（RL）方法，通过自适应调整智能体的推演长度来训练它们。TTI使用Gemma 3 12B模型，在WebVoyager和WebArena基准上产生了最先进的开源、开放数据网络智能体。我们进一步表明，TTI使智能体能够自适应地平衡探索与利用。我们的结果确立了交互扩展作为与每步计算扩展互补的强大轴线，为训练自适应智能体提供了新途径。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>智能体</span><span>测试时交互</span><span>在线强化学习</span><span>网络智能体</span><span>探索与利用</span></div>
                    <div class="area"><span class="label">区域：</span><span>智能体</span><span>机器学习</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07976" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            由 AI 助手生成
        </footer>
    </div>
</body>
</html>