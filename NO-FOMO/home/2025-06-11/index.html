<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI 日报 - 2025-06-11</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <script src="../js/analytics.js"></script>
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter, Noto Sans SC', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI 日报</h1>
            <p class="date">2025-06-11</p>
            <p class="theme-info">关于我们: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>103K「硬核」题，让大模型突破数学推理瓶颈</h2>
                <span class="published-time">发布时间: 2025-06-11T03:55:03.000Z</span>
                <img src="screenshot/wechat/wechat_image_EkVeW5pLRM8_T6hrrs7lsA.png" alt="103K「硬核」题，让大模型突破数学推理瓶颈">
                <p class="summary">腾讯AI Lab与上海交大团队发布DeepMath-103K数据集，旨在解决当前大语言模型在数学推理，特别是强化学习训练中面临的数据瓶颈，如数据集难度不足、答案难验证、数据污染及缺乏新颖性等问题。DeepMath-103K包含逾10万道高难度、严格去污染且答案可验证的数学题，专为推动AI数学推理极限设计。其独特结构支持强化学习，并已助力DeepMath系列模型在多个基准测试中达到SOTA，展现出从数学到科学领域的泛化推理能力，预示着AI将实现更深层次的思考与更广泛的科学探索。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>DeepMath-103K</span><span>数学推理</span><span>大语言模型</span><span>数据集</span><span>强化学习</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>人工智能</span><span>机器学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/EkVeW5pLRM8_T6hrrs7lsA" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Mistral的首个强推理模型：拥抱开源，推理速度快10倍</h2>
                <span class="published-time">发布时间: 2025-06-11T03:55:03.000Z</span>
                <img src="screenshot/wechat/wechat_image_Go5dXv4DA3hy5lGhxxE8SA.png" alt="Mistral的首个强推理模型：拥抱开源，推理速度快10倍">
                <p class="summary">Mistral AI发布了其首个强推理大模型Magistral系列，包含专有版Magistral Medium和开源版Magistral Small，该系列模型展现卓越推理能力，能自我反思并解决复杂任务。Magistral模型擅长多语言高保真推理，通过Flash Answers实现高达10倍的token吞吐量，显著提升实时推理效率。其核心设计是使用与用户相同的语言进行推理。Magistral Medium定价虽高但具竞争力，旨在加速模型迭代并推动大规模实时推理应用。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大模型</span><span>推理能力</span><span>Mistral AI</span><span>开源</span><span>实时推理</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>人工智能</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/Go5dXv4DA3hy5lGhxxE8SA" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAI深夜放大招！o3-Pro震撼发布：性能飙升，价格暴降80%！</h2>
                <span class="published-time">发布时间: 2025-06-11T01:08:23.000Z</span>
                <img src="screenshot/wechat/wechat_image_uQCOXmZS_Zo6QL30TXq5cQ.png" alt="OpenAI深夜放大招！o3-Pro震撼发布：性能飙升，价格暴降80%！">
                <p class="summary">OpenAI深夜发布o3-pro模型，面向ChatGPT Pro用户及API开放。作为o3的升级版，o3-pro专为深度思考和提供可靠响应设计，在数学、科学、编程等学术评估中表现卓越，并支持网络搜索、文件分析、视觉推理等多种工具。专家评估显示，o3-pro在清晰度、准确性等方面显著优于o3。同时，o3 API价格直降80%，o3-pro API价格相比o1-pro也下降87%，大幅降低了使用成本，使其成为可靠性优先场景的理想选择。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>o3-pro</span><span>OpenAI</span><span>大模型</span><span>性能提升</span><span>API降价</span><span>可靠性</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>人工智能</span><span>生成式AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/uQCOXmZS_Zo6QL30TXq5cQ" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PolyVivid实现多主体视频定制，身份一致性超越现有模型</h2>
                <span class="published-time">发布时间: 2025-06-11T00:30:38.000Z</span>
                <img src="screenshot/wechat/wechat_image_4FiiPR7Ds7ZZtIkEKV73Wg.png" alt="PolyVivid实现多主体视频定制，身份一致性超越现有模型">
                <p class="summary">上交与腾讯联合推出PolyVivid框架，旨在解决现有视频生成模型在多主体定制中身份一致性差、交互不自然的问题。PolyVivid通过VLLM融合模块实现视觉身份与文本空间精确对齐，引入3D-RoPE增强图文融合，并采用注意力继承式身份注入模块缓解身份漂移。其MLLM数据流水线提升主体区分与生成质量，同时提出的SaRA参数高效微调方法优化大模型应用。该框架在身份保真度、视频真实感及主体对齐方面显著优于现有基线，为灵活且身份一致的多主体视频生成提供了创新解决方案。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>视频生成</span><span>多主体定制</span><span>身份一致性</span><span>PolyVivid</span><span>参数高效微调</span><span>大模型</span></div>
                    <div class="area"><span class="label">区域：</span><span>生成式AI</span><span>多模态</span><span>计算机视觉</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/4FiiPR7Ds7ZZtIkEKV73Wg" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>🌟 Awesome LLM Apps</h2>
                <span class="published-time">发布时间: 2025-06-06T22:50:54Z</span>
                <img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" alt="🌟 Awesome LLM Apps">
                <p class="summary">该GitHub仓库“Awesome LLM Apps”汇集了大量基于RAG、AI智能体、多智能体团队、MCP及语音智能体等技术构建的LLM应用。它展示了如何利用OpenAI、Anthropic、Google及开源模型（如DeepSeek、Qwen、Llama）解决实际问题，涵盖从代码库到邮件处理等多样化场景。项目旨在提供实用且创新的LLM应用范例，推动大模型技术在不同领域的落地与发展，并鼓励社区贡献，共同构建丰富的开源LLM应用生态。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大语言模型</span><span>AI智能体</span><span>RAG</span><span>多智能体系统</span><span>生成式AI</span><span>应用开发</span><span>机器学习</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>大模型</span><span>智能体</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/Shubhamsaboo/awesome-llm-apps" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>✨ YouTube Transcript API ✨</h2>
                <span class="published-time">发布时间: 2025-04-23T07:30:48Z</span>
                <img src="screenshot/github/youtube-transcript-api.png" alt="✨ YouTube Transcript API ✨">
                <p class="summary">YouTube Transcript API是一个Python库，用于获取YouTube视频的字幕和转录文本。它支持自动生成字幕和多语言翻译，无需依赖无头浏览器，显著提升了效率。该API还提供了处理IP封锁的代理功能、年龄限制视频的Cookie认证，并支持多种输出格式（如JSON、SRT），同时提供命令行工具，极大方便了开发者和用户获取和处理YouTube视频内容。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>YouTube字幕</span><span>视频转录</span><span>Python库</span><span>API</span><span>字幕翻译</span><span>代理</span><span>命令行工具</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>自然语言处理</span><span>视频理解</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/jdepoix/youtube-transcript-api" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>开源大模型食用指南</h2>
                <span class="published-time">发布时间: 2025-06-11T14:30:51Z</span>
                <img src="https://contrib.rocks/image?repo=datawhalechina/self-llm" alt="开源大模型食用指南">
                <p class="summary">本项目是面向国内初学者的开源大模型教程，基于Linux平台，提供从环境配置、本地部署到高效微调的全流程指导。它简化了开源大模型的应用流程，涵盖LLaMA、ChatGLM、InternLM等主流模型，并指导命令行调用、在线Demo部署及LangChain集成。旨在帮助学生和研究者掌握开源大模型的“食用”方法，促进其在学习和实践中的普及应用。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大模型</span><span>开源</span><span>环境配置</span><span>模型部署</span><span>模型微调</span><span>自然语言处理</span><span>Linux</span><span>LangChain</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>自然语言处理</span><span>深度学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/datawhalechina/self-llm" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiniCPM</h2>
                <span class="published-time">发布时间: 2025-06-11T06:36:09Z</span>
                <img src="https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm_logo.png" alt="MiniCPM">
                <p class="summary">MiniCPM是一个极致高效的端侧大模型系列，由面壁智能、清华大学和中国人民大学联合开发。它通过创新的模型架构（如InfLLM v2稀疏注意力）、高效学习算法（如BitCPM三值量化）和优化推理系统（CPM.cu），实现了卓越的效率提升。MiniCPM系列模型在保持同等规模最优性能的同时，在典型端侧芯片上实现了5倍以上生成加速，并在工具调用、代码解释器、长文本处理等多个任务上超越同级别甚至更大参数量的模型，为端侧AI应用提供了强大的解决方案。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大模型</span><span>端侧AI</span><span>高效推理</span><span>稀疏注意力</span><span>模型量化</span><span>工具调用</span><span>长文本</span><span>自然语言处理</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>深度学习</span><span>自然语言处理</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/OpenBMB/MiniCPM" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>大型语言模型中的地缘政治偏见：当代语言模型如何界定“好”与“坏”国家</h2>
                <span class="published-time">发布时间: 2025-06-07T10:45:17.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06751.png" alt="大型语言模型中的地缘政治偏见：当代语言模型如何界定“好”与“坏”国家">
                <p class="summary">本文通过分析大型语言模型（LLMs）对具有冲突国家视角（美国、英国、苏联和中国）的历史事件的解读，评估了其在不同国家方面的地缘政治偏见。我们引入了一个新颖的数据集，其中包含中立的事件描述和来自不同国家的对比观点。我们的研究结果表明，模型存在显著的地缘政治偏见，倾向于特定的国家叙事。此外，简单的去偏见提示在减少这些偏见方面的效果有限。对操纵参与者标签的实验揭示了模型对归因的敏感性，有时会放大偏见或识别不一致性，尤其是在标签互换的情况下。这项工作突出了大型语言模型中的国家叙事偏见，挑战了简单去偏见方法的有效性，并为未来的地缘政治偏见研究提供了框架和数据集。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>大型语言模型</span><span>地缘政治偏见</span><span>国家叙事</span><span>偏见评估</span><span>数据集</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>自然语言处理</span><span>人工智能</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.06751" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RuleReasoner：通过领域感知动态采样强化的基于规则的推理</h2>
                <span class="published-time">发布时间: 2025-06-10T10:31:21.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08672.png" alt="RuleReasoner：通过领域感知动态采样强化的基于规则的推理">
                <p class="summary">基于规则的推理被认为是推理领域的基本问题之一，然而，在实际应用中，规则格式、类型和复杂性的偏差带来了严峻挑战。最近的研究表明，大型推理模型（LRMs）具有卓越的推理能力，并且其性能通过强化学习（RL）得到了显著提升。然而，小型推理模型（SRMs）是否能有效学习基于规则的推理，并在不同任务和领域中展现出强大的泛化能力，仍然是一个悬而未决的问题。为了解决这个问题，我们引入了强化基于规则的推理，即RuleReasoner，这是一种通过大量精选任务和新颖的领域感知动态采样方法进行基于规则推理的简单而有效的方法。具体而言，RuleReasoner通过根据历史奖励更新不同领域的采样权重来重新采样每个训练批次。这有助于领域增强和强化学习的灵活在线学习调度，从而避免了现有方法中所需的人工预先设计的混合训练方案。在分布内（ID）和分布外（OOD）基准测试上的实证评估表明，RuleReasoner显著优于前沿的大型推理模型（在八个ID任务上平均提高4.1%，在三个OOD任务上平均提高10.4%，均优于OpenAI-o1）。值得注意的是，与之前用于强化学习的动态采样方法相比，我们的方法还表现出更高的计算效率。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>基于规则的推理</span><span>强化学习</span><span>动态采样</span><span>领域感知</span><span>推理模型</span></div>
                    <div class="area"><span class="label">区域：</span><span>人工智能</span><span>机器学习</span><span>深度学习</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08672" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>自回归语义视觉重建助力视觉语言模型提升理解能力</h2>
                <span class="published-time">发布时间: 2025-06-10T17:57:50.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09040.png" alt="自回归语义视觉重建助力视觉语言模型提升理解能力">
                <p class="summary">典型的视觉语言大模型（LVLMs）仅对文本序列应用自回归监督，未能将视觉模态充分融入学习过程。这导致了三个关键局限性：（1）无法在没有配套字幕的情况下利用图像；（2）字幕可能遗漏关键视觉细节；（3）某些以视觉为中心的内容无法通过文本充分传达。因此，当前的LVLMs通常优先考虑视觉到语言的对齐，而可能忽视细粒度的视觉信息。尽管一些现有工作探索了自回归图像生成，但有效利用自回归视觉监督来增强图像理解仍然是一个开放的挑战。在本文中，我们引入了自回归语义视觉重建（ASVR），它使得在统一的自回归框架内实现视觉和文本模态的联合学习成为可能。我们发现，自回归地重建图像的原始视觉外观并不能增强，甚至可能损害多模态理解。相反，自回归地重建图像的语义表示则能持续提升理解能力。值得注意的是，我们发现即使模型输入连续图像特征，它们也能有效地重建离散的语义标记，从而在广泛的多模态理解基准上带来稳定且持续的改进。我们的方法在不同数据规模（556k-2M）和不同LLM骨干网络类型上均实现了显著的性能提升。具体而言，ASVR使LLaVA-1.5在14个多模态基准测试中的平均得分提高了5%。代码已开源。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>自回归语义视觉重建</span><span>视觉语言模型</span><span>多模态理解</span><span>图像理解</span><span>语义表示</span></div>
                    <div class="area"><span class="label">区域：</span><span>多模态</span><span>大模型</span><span>计算机视觉</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.09040" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>自强制：弥合自回归视频扩散模型中的训练-测试鸿沟</h2>
                <span class="published-time">发布时间: 2025-06-09T17:59:55.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08009.png" alt="自强制：弥合自回归视频扩散模型中的训练-测试鸿沟">
                <p class="summary">我们引入了自强制（Self Forcing），这是一种用于自回归视频扩散模型的新型训练范式。它解决了长期存在的暴露偏差问题，即模型在训练时依赖真实上下文，但在推理时必须基于自身不完美的输出生成序列。与以往基于真实上下文帧去噪未来帧的方法不同，自强制通过在训练期间执行带有键值（KV）缓存的自回归展开，将每一帧的生成条件设定为先前自生成的输出。这种策略通过视频层面的整体损失实现监督，直接评估整个生成序列的质量，而非仅仅依赖传统的逐帧目标。为确保训练效率，我们采用了少步扩散模型以及随机梯度截断策略，有效平衡了计算成本和性能。我们还引入了一种滚动KV缓存机制，以实现高效的自回归视频外推。大量实验表明，我们的方法在单GPU上实现了亚秒级延迟的实时流式视频生成，同时达到甚至超越了显著更慢且非因果扩散模型的生成质量。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>自强制</span><span>自回归视频扩散</span><span>暴露偏差</span><span>键值缓存</span><span>视频生成</span></div>
                    <div class="area"><span class="label">区域：</span><span>深度学习</span><span>计算机视觉</span><span>生成式AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08009" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>思考与行动：通过扩展测试时交互进行推理的智能体</h2>
                <span class="published-time">发布时间: 2025-06-09T17:50:02.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07976.png" alt="思考与行动：通过扩展测试时交互进行推理的智能体">
                <p class="summary">当前测试时扩展的范式依赖于在生成响应之前产生冗长的推理轨迹（即“更多思考”）。在需要交互的智能体问题中，这可以通过在与世界交互之前生成思考轨迹来实现。然而，这一过程不允许智能体从环境中获取新信息或随时间调整其行为。在这项工作中，我们提出扩展测试时交互，这是测试时扩展一个尚未开发的维度，它增加了智能体的交互视野，从而能够在单次推演中运行探索、回溯和动态重新规划等丰富的行为。为了展示这一扩展维度的潜力，我们研究了网络智能体领域。我们首先表明，即使是基于提示的交互扩展，在没有任何训练的情况下，也能显著提高网络基准上的任务成功率。在此基础上，我们引入了TTI（测试时交互），这是一种基于课程的在线强化学习（RL）方法，通过自适应调整智能体的推演长度来训练它们。TTI使用Gemma 3 12B模型，在WebVoyager和WebArena基准上产生了最先进的开源、开放数据网络智能体。我们进一步表明，TTI使智能体能够自适应地平衡探索与利用。我们的结果确立了交互扩展作为与每步计算扩展互补的强大轴线，为训练自适应智能体提供了新途径。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>智能体</span><span>测试时交互</span><span>在线强化学习</span><span>网络智能体</span><span>探索与利用</span></div>
                    <div class="area"><span class="label">区域：</span><span>智能体</span><span>机器学习</span><span>大模型</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07976" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DRAGged into Conflicts: 搜索增强型大语言模型中冲突源的检测与处理</h2>
                <span class="published-time">发布时间: 2025-06-10T06:52:57.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08500.png" alt="DRAGged into Conflicts: 搜索增强型大语言模型中冲突源的检测与处理">
                <p class="summary">检索增强生成（RAG）是一种常用的方法，用于通过相关和最新信息增强大型语言模型（LLM）。然而，检索到的来源常常包含冲突信息，并且模型应如何处理此类差异仍不清楚。在这项工作中，我们首先提出了RAG中知识冲突类型的新颖分类法，以及每种类型所需的模型行为。然后，我们引入了CONFLICTS，这是一个高质量的基准测试，在真实的RAG设置中包含冲突类型的专家标注。CONFLICTS是第一个能够跟踪模型如何处理各种知识冲突进展的基准测试。我们对该基准进行了广泛的实验，结果表明LLM通常难以适当地解决来源之间的冲突。虽然提示LLM明确地对检索文档中潜在的冲突进行推理显著提高了其响应的质量和适当性，但未来的研究仍有很大的改进空间。</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">关键词：</span><span>检索增强生成</span><span>大语言模型</span><span>知识冲突</span><span>基准测试</span><span>冲突解决</span></div>
                    <div class="area"><span class="label">区域：</span><span>大模型</span><span>自然语言处理</span><span>生成式AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08500" target="_blank" rel="noopener noreferrer">查看详情...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            由 AI 助手生成
        </footer>
    </div>
</body>
</html>