<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-06-11</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <script src="../../js/analytics.js"></script>
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }

        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.3s ease, transform 0.2s ease;
            border: 2px solid transparent;
            font-size: 0.9em;
        }

        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }

        .language-switch a.active {
            background: var(--secondary-color);
            border-color: var(--border-color);
        }

        @media (max-width: 768px) {
            .language-switch {
                position: static;
                justify-content: center;
                margin-bottom: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../cn/">‰∏≠Êñá</a>
                <a href="../en/" class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-06-11</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning</h2>
                <span class="published-time">Published: 2025-06-11T03:55:03.000Z</span>
                <img src="../screenshot/wechat/wechat_image_EkVeW5pLRM8_T6hrrs7lsA.png" alt="DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning">
                <p class="summary">Tencent AI Lab and Shanghai Jiao Tong University have introduced DeepMath-103K, a novel large-scale, high-difficulty, strictly decontaminated, and verifiable mathematical dataset. This initiative addresses the critical data bottleneck faced by current large language models (LLMs) in mathematical reasoning, especially when trained with reinforcement learning, due to issues like insufficient difficulty, unverifiable answers, data contamination, and lack of novelty in existing datasets. Comprising over 103,000 problems, DeepMath-103K is specifically designed to push the boundaries of AI mathematical reasoning. Its unique structure provides rich information, including verifiable final answers and multiple reasoning paths, crucial for RL-based training. DeepMath-103K has enabled the DeepMath series models to achieve state-of-the-art (SOTA) performance across various mathematical benchmarks and demonstrate strong generalization capabilities to broader scientific reasoning, paving the way for more profound AI thinking and scientific exploration.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>DeepMath-103K</span><span>Mathematical Reasoning</span><span>Large Language Models</span><span>Dataset</span><span>Reinforcement Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/EkVeW5pLRM8_T6hrrs7lsA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Mistral's First Powerful Reasoning Model: Embracing Open Source, 10x Faster Inference Speed</h2>
                <span class="published-time">Published: 2025-06-11T03:55:03.000Z</span>
                <img src="../screenshot/wechat/wechat_image_Go5dXv4DA3hy5lGhxxE8SA.png" alt="Mistral's First Powerful Reasoning Model: Embracing Open Source, 10x Faster Inference Speed">
                <p class="summary">Mistral AI has launched its first powerful reasoning large language model series, Magistral, which includes a proprietary Magistral Medium and an open-source Magistral Small (24B parameters). This new series demonstrates exceptional reasoning capabilities, enabling advanced self-reflection and solving more complex tasks, performing remarkably well across various benchmarks. Magistral models excel in high-fidelity multilingual reasoning, supporting languages like English, Chinese, and French. Furthermore, they achieve up to 10 times token throughput via Flash Answers, significantly boosting real-time inference efficiency. A core design principle is to reason in the user's language, effectively minimizing code switching. Although Magistral Medium is priced higher than its predecessors, its inference costs are highly competitive compared to rivals like OpenAI and Gemini, positioning Mistral to rapidly iterate on these models and drive large-scale real-time inference applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>Reasoning</span><span>Mistral AI</span><span>Open Source</span><span>Real-time Inference</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/Go5dXv4DA3hy5lGhxxE8SA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>OpenAI Unveils o3-Pro: Performance Soars, Price Drops 80%</h2>
                <span class="published-time">Published: 2025-06-11T01:08:23.000Z</span>
                <img src="../screenshot/wechat/wechat_image_uQCOXmZS_Zo6QL30TXq5cQ.png" alt="OpenAI Unveils o3-Pro: Performance Soars, Price Drops 80%">
                <p class="summary">OpenAI has launched its new o3-pro model overnight, making it available to ChatGPT Pro users and via API. Positioned as an upgraded version of o3, o3-pro is specifically engineered for deep thinking and delivering highly reliable responses. It demonstrates superior performance in academic evaluations across fields such as mathematics, science, and programming. The model integrates extensive tool support, including web search, file analysis, visual reasoning, and Python programming. Expert assessments consistently indicate that o3-pro significantly surpasses o3 in terms of clarity, comprehensiveness, instruction adherence, and accuracy. Furthermore, OpenAI has drastically reduced the API pricing for o3 by 80%, and the o3-pro API price has seen an 87% reduction compared to o1-pro. This substantial price cut makes o3-pro an exceptionally cost-effective solution, particularly for complex scenarios where reliability and quality are prioritized over speed, offering premium output for a minimal wait time.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>o3-pro</span><span>OpenAI</span><span>Large Language Model</span><span>Performance Improvement</span><span>API Price Reduction</span><span>Reliability</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Artificial Intelligence</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/uQCOXmZS_Zo6QL30TXq5cQ" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PolyVivid Achieves Multi-Subject Video Customization with Identity Consistency Surpassing Existing Models</h2>
                <span class="published-time">Published: 2025-06-11T00:30:38.000Z</span>
                <img src="../screenshot/wechat/wechat_image_4FiiPR7Ds7ZZtIkEKV73Wg.png" alt="PolyVivid Achieves Multi-Subject Video Customization with Identity Consistency Surpassing Existing Models">
                <p class="summary">Shanghai Jiao Tong University and Tencent have jointly introduced PolyVivid, a novel multi-subject video customization framework designed to overcome limitations in existing video generation models, particularly regarding identity consistency and natural interaction in multi-subject scenarios. PolyVivid achieves precise semantic alignment by embedding visual identities into the text space via a VLLM fusion module and enhances graphic-text integration with 3D-RoPE. It employs an attention-inherited identity injection module to mitigate identity drift and utilizes an MLLM data pipeline for improved subject differentiation and generation quality. Furthermore, the framework introduces SaRA, a parameter-efficient fine-tuning method optimized for large models. PolyVivid significantly outperforms current baseline methods in identity fidelity, video realism, and subject alignment, offering an innovative solution for flexible and identity-consistent multi-subject video generation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Video Generation</span><span>Multi-Subject Customization</span><span>Identity Consistency</span><span>PolyVivid</span><span>Parameter-Efficient Fine-tuning</span><span>Large Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/4FiiPR7Ds7ZZtIkEKV73Wg" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>üåü Awesome LLM Apps</h2>
                <span class="published-time">Published: 2025-06-06T22:50:54Z</span>
                <img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" alt="üåü Awesome LLM Apps">
                <p class="summary">The "Awesome LLM Apps" GitHub repository presents a meticulously curated collection of practical large language model (LLM) applications. These applications are ingeniously built utilizing advanced techniques such as Retrieval-Augmented Generation (RAG), sophisticated AI Agents, collaborative Multi-agent Teams, Multi-Context Processing (MCP), and intuitive Voice Agents. The repository showcases the versatile integration of leading LLM providers like OpenAI, Anthropic, and Google, alongside powerful open-source models including DeepSeek, Qwen, and Llama, which can even be run locally. It illustrates how LLMs can address real-world challenges across diverse domains, from analyzing code repositories to managing email inboxes. This project's core objective is to offer tangible, innovative LLM application examples, thereby accelerating the practical deployment and advancement of large model technologies across various industries. Furthermore, it actively encourages community contributions, aiming to cultivate a vibrant and comprehensive open-source ecosystem for LLM-powered solutions.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>RAG</span><span>Multi-agent Systems</span><span>Generative AI</span><span>Application Development</span><span>Machine Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/Shubhamsaboo/awesome-llm-apps" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>‚ú® YouTube Transcript API ‚ú®</h2>
                <span class="published-time">Published: 2025-04-23T07:30:48Z</span>
                <img src="../screenshot/github/youtube-transcript-api.png" alt="‚ú® YouTube Transcript API ‚ú®">
                <p class="summary">The YouTube Transcript API is a Python library designed to retrieve transcripts and subtitles for YouTube videos. It supports both automatically generated subtitles and multi-language translation, operating efficiently without the need for headless browsers, which significantly improves performance. The API also features proxy support to circumvent IP blocks, cookie authentication for age-restricted content, and offers various output formats like JSON and SRT. Additionally, it includes a command-line interface, greatly simplifying the process for developers and users to access and process YouTube video content.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>YouTube Subtitles</span><span>Video Transcription</span><span>Python Library</span><span>API</span><span>Subtitle Translation</span><span>Proxy</span><span>Command Line Tool</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Video Understanding</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/jdepoix/youtube-transcript-api" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Open-source Large Language Model Handbook</h2>
                <span class="published-time">Published: 2025-06-11T14:30:51Z</span>
                <img src="https://contrib.rocks/image?repo=datawhalechina/self-llm" alt="Open-source Large Language Model Handbook">
                <p class="summary">This project presents a dedicated open-source large language model (LLM) tutorial, specifically designed for beginners in China and optimized for Linux platforms. It provides comprehensive, full-process guidance encompassing essential skills such as environment configuration, local deployment, and efficient fine-tuning for a wide array of open-source LLMs. By simplifying the complex deployment, usage, and application workflows, the initiative aims to make advanced LLM technologies more accessible to a broader audience of students and researchers. The tutorial covers mainstream models like LLaMA, ChatGLM, and InternLM, offering practical instructions on command-line invocation, setting up online demonstrations, and integrating with frameworks like LangChain. Furthermore, it delves into advanced topics such as distributed full fine-tuning, LoRA, and P-tuning methods. This resource is crucial for fostering the adoption of open-source, free large models, enabling learners to seamlessly incorporate them into their studies and future professional endeavors.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Open-source</span><span>Environment Configuration</span><span>Model Deployment</span><span>Model Fine-tuning</span><span>Natural Language Processing</span><span>Linux</span><span>LangChain</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/datawhalechina/self-llm" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MiniCPM</h2>
                <span class="published-time">Published: 2025-06-11T06:36:09Z</span>
                <img src="https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm_logo.png" alt="MiniCPM">
                <p class="summary">MiniCPM is an ultra-efficient series of large language models designed for edge devices, co-developed by ModelBest, Tsinghua University, and Renmin University of China. It achieves exceptional efficiency through innovative model architectures like InfLLM v2 sparse attention, efficient learning algorithms such as BitCPM 3-value quantization, and optimized inference systems like CPM.cu. While maintaining state-of-the-art performance for its size, MiniCPM models deliver over 5x generation speedup on typical edge chips. They also surpass similarly sized and even larger models in tasks like tool calling, code interpretation, and long-context processing, offering a powerful solution for edge AI applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Model</span><span>Edge AI</span><span>Efficient Inference</span><span>Sparse Attention</span><span>Model Quantization</span><span>Tool Calling</span><span>Long Context</span><span>Natural Language Processing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/OpenBMB/MiniCPM" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Geopolitical biases in LLMs: what are the "good" and the "bad" countries
  according to contemporary language models</h2>
                <span class="published-time">Published: 2025-06-07T10:45:17.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06751.png" alt="Geopolitical biases in LLMs: what are the "good" and the "bad" countries
  according to contemporary language models">
                <p class="summary">This paper evaluates geopolitical biases in LLMs with respect to various
countries though an analysis of their interpretation of historical events with
conflicting national perspectives (USA, UK, USSR, and China). We introduce a
novel dataset with neutral event descriptions and contrasting viewpoints from
different countries. Our findings show significant geopolitical biases, with
models favoring specific national narratives. Additionally, simple debiasing
prompts had a limited effect in reducing these biases. Experiments with
manipulated participant labels reveal models' sensitivity to attribution,
sometimes amplifying biases or recognizing inconsistencies, especially with
swapped labels. This work highlights national narrative biases in LLMs,
challenges the effectiveness of simple debiasing methods, and offers a
framework and dataset for future geopolitical bias research.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Geopolitical Biases</span><span>National Narratives</span><span>Bias Evaluation</span><span>Dataset</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.06751" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic
  Sampling</h2>
                <span class="published-time">Published: 2025-06-10T10:31:21.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08672.png" alt="RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic
  Sampling">
                <p class="summary">Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1%
average points on eight ID tasks and Delta10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Rule-based reasoning</span><span>Reinforcement Learning</span><span>Dynamic Sampling</span><span>Domain-aware</span><span>Reasoning Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08672" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Autoregressive Semantic Visual Reconstruction Helps VLMs Understand
  Better</h2>
                <span class="published-time">Published: 2025-06-10T17:57:50.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09040.png" alt="Autoregressive Semantic Visual Reconstruction Helps VLMs Understand
  Better">
                <p class="summary">Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Autoregressive Semantic Visual Reconstruction</span><span>Visual Language Models</span><span>Multimodal Understanding</span><span>Image Understanding</span><span>Semantic Representation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Large Language Model</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.09040" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Self Forcing: Bridging the Train-Test Gap in Autoregressive Video
  Diffusion</h2>
                <span class="published-time">Published: 2025-06-09T17:59:55.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08009.png" alt="Self Forcing: Bridging the Train-Test Gap in Autoregressive Video
  Diffusion">
                <p class="summary">We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Self Forcing</span><span>Autoregressive Video Diffusion</span><span>Exposure Bias</span><span>KV Caching</span><span>Video Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Deep Learning</span><span>Computer Vision</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08009" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction</h2>
                <span class="published-time">Published: 2025-06-09T17:50:02.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07976.png" alt="Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction">
                <p class="summary">The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Agents</span><span>Test-Time Interaction</span><span>Online Reinforcement Learning</span><span>Web Agents</span><span>Exploration-Exploitation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.07976" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in
  Search-Augmented LLMs</h2>
                <span class="published-time">Published: 2025-06-10T06:52:57.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08500.png" alt="DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in
  Search-Augmented LLMs">
                <p class="summary">Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Retrieval Augmented Generation</span><span>Large Language Models</span><span>Knowledge Conflict</span><span>Benchmark</span><span>Conflict Resolution</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2506.08500" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>