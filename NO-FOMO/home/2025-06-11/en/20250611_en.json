[
  {
    "id": "UABVUoHYTDlFWWNvD5R9Og",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/UABVUoHYTDlFWWNvD5R9Og",
    "title_en": "Next-Token Paradigm Shift! Reinforcement Learning Pre-Training is Here",
    "summary_en": "Microsoft introduces a novel paradigm called \"Reinforcement Pre-Training (RPT),\" which redefines the next-token prediction task as a reinforcement learning-trained inference process. This innovative approach integrates reinforcement learning directly into the pre-training phase of large language models, allowing models to receive verifiable rewards for accurately predicting the next token within a given context. A key advantage of RPT is its scalability and generality, as it leverages vast amounts of unlabeled text data for universal reinforcement learning without requiring external annotations or domain-specific reward functions. The method significantly enhances language modeling accuracy and provides a robust foundation for subsequent reinforcement fine-tuning, while inherently minimizing reward hacking risks. Experimental results demonstrate that RPT consistently outperforms baseline models in next-token prediction accuracy and zero-shot performance, even matching larger models. This research highlights RPT's substantial potential to advance large language model pre-training by fostering deeper understanding and generalization beyond mere token association, marking a significant shift in the \"next-token\" paradigm.",
    "keywords_en": [
      "Reinforcement Learning",
      "Pre-training",
      "Large Language Models",
      "Next-Token",
      "Language Model",
      "RPT"
    ],
    "area_en": [
      "Reinforcement Learning",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-11T03:55:03.000Z",
    "download_time": "2025-06-12T12:32:16.136310",
    "visual_resource": [
      "screenshot/wechat/wechat_image_UABVUoHYTDlFWWNvD5R9Og.png"
    ],
    "extra_info": null
  },
  {
    "id": "EkVeW5pLRM8_T6hrrs7lsA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/EkVeW5pLRM8_T6hrrs7lsA",
    "title_en": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning",
    "summary_en": "Tencent AI Lab and Shanghai Jiao Tong University have introduced DeepMath-103K, a novel large-scale, high-difficulty, strictly decontaminated, and verifiable mathematical dataset. This initiative addresses the critical data bottleneck faced by current large language models (LLMs) in mathematical reasoning, especially when trained with reinforcement learning, due to issues like insufficient difficulty, unverifiable answers, data contamination, and lack of novelty in existing datasets. Comprising over 103,000 problems, DeepMath-103K is specifically designed to push the boundaries of AI mathematical reasoning. Its unique structure provides rich information, including verifiable final answers and multiple reasoning paths, crucial for RL-based training. DeepMath-103K has enabled the DeepMath series models to achieve state-of-the-art (SOTA) performance across various mathematical benchmarks and demonstrate strong generalization capabilities to broader scientific reasoning, paving the way for more profound AI thinking and scientific exploration.",
    "keywords_en": [
      "DeepMath-103K",
      "Mathematical Reasoning",
      "Large Language Models",
      "Dataset",
      "Reinforcement Learning"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Machine Learning"
    ],
    "published_time": "2025-06-11T03:55:03.000Z",
    "download_time": "2025-06-12T12:32:17.027147",
    "visual_resource": [
      "screenshot/20250611/wechat/wechat_image_EkVeW5pLRM8_T6hrrs7lsA.png"
    ],
    "extra_info": null
  },
  {
    "id": "Go5dXv4DA3hy5lGhxxE8SA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Go5dXv4DA3hy5lGhxxE8SA",
    "title_en": "Mistral's First Powerful Reasoning Model: Embracing Open Source, 10x Faster Inference Speed",
    "summary_en": "Mistral AI has launched its first powerful reasoning large language model series, Magistral, which includes a proprietary Magistral Medium and an open-source Magistral Small (24B parameters). This new series demonstrates exceptional reasoning capabilities, enabling advanced self-reflection and solving more complex tasks, performing remarkably well across various benchmarks. Magistral models excel in high-fidelity multilingual reasoning, supporting languages like English, Chinese, and French. Furthermore, they achieve up to 10 times token throughput via Flash Answers, significantly boosting real-time inference efficiency. A core design principle is to reason in the user's language, effectively minimizing code switching. Although Magistral Medium is priced higher than its predecessors, its inference costs are highly competitive compared to rivals like OpenAI and Gemini, positioning Mistral to rapidly iterate on these models and drive large-scale real-time inference applications.",
    "keywords_en": [
      "Large Language Model",
      "Reasoning",
      "Mistral AI",
      "Open Source",
      "Real-time Inference"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-11T03:55:03.000Z",
    "download_time": "2025-06-12T12:32:40.447764",
    "visual_resource": [
      "screenshot/20250611/wechat/wechat_image_Go5dXv4DA3hy5lGhxxE8SA.png"
    ],
    "extra_info": null
  },
  {
    "id": "uQCOXmZS_Zo6QL30TXq5cQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/uQCOXmZS_Zo6QL30TXq5cQ",
    "title_en": "OpenAI Unveils o3-Pro: Performance Soars, Price Drops 80%",
    "summary_en": "OpenAI has launched its new o3-pro model overnight, making it available to ChatGPT Pro users and via API. Positioned as an upgraded version of o3, o3-pro is specifically engineered for deep thinking and delivering highly reliable responses. It demonstrates superior performance in academic evaluations across fields such as mathematics, science, and programming. The model integrates extensive tool support, including web search, file analysis, visual reasoning, and Python programming. Expert assessments consistently indicate that o3-pro significantly surpasses o3 in terms of clarity, comprehensiveness, instruction adherence, and accuracy. Furthermore, OpenAI has drastically reduced the API pricing for o3 by 80%, and the o3-pro API price has seen an 87% reduction compared to o1-pro. This substantial price cut makes o3-pro an exceptionally cost-effective solution, particularly for complex scenarios where reliability and quality are prioritized over speed, offering premium output for a minimal wait time.",
    "keywords_en": [
      "o3-pro",
      "OpenAI",
      "Large Language Model",
      "Performance Improvement",
      "API Price Reduction",
      "Reliability"
    ],
    "area_en": [
      "Large Language Model",
      "Artificial Intelligence",
      "Generative AI"
    ],
    "published_time": "2025-06-11T01:08:23.000Z",
    "download_time": "2025-06-12T12:32:18.966933",
    "visual_resource": [
      "screenshot/20250611/wechat/wechat_image_uQCOXmZS_Zo6QL30TXq5cQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "awesome-llm-apps",
    "source": "GitHub",
    "url": "https://github.com/Shubhamsaboo/awesome-llm-apps",
    "title_en": "ðŸŒŸ Awesome LLM Apps",
    "summary_en": "The \"Awesome LLM Apps\" GitHub repository presents a meticulously curated collection of practical large language model (LLM) applications. These applications are ingeniously built utilizing advanced techniques such as Retrieval-Augmented Generation (RAG), sophisticated AI Agents, collaborative Multi-agent Teams, Multi-Context Processing (MCP), and intuitive Voice Agents. The repository showcases the versatile integration of leading LLM providers like OpenAI, Anthropic, and Google, alongside powerful open-source models including DeepSeek, Qwen, and Llama, which can even be run locally. It illustrates how LLMs can address real-world challenges across diverse domains, from analyzing code repositories to managing email inboxes. This project's core objective is to offer tangible, innovative LLM application examples, thereby accelerating the practical deployment and advancement of large model technologies across various industries. Furthermore, it actively encourages community contributions, aiming to cultivate a vibrant and comprehensive open-source ecosystem for LLM-powered solutions.",
    "keywords_en": [
      "Large Language Model",
      "AI Agent",
      "RAG",
      "Multi-agent Systems",
      "Generative AI",
      "Application Development",
      "Machine Learning"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-06-06T22:50:54Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png",
      "https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&type=Date",
      "https://trendshift.io/api/badge/repositories/9876"
    ],
    "extra_info": null
  },
  {
    "id": "youtube-transcript-api",
    "source": "GitHub",
    "url": "https://github.com/jdepoix/youtube-transcript-api",
    "title_en": "âœ¨ YouTube Transcript API âœ¨",
    "summary_en": "The YouTube Transcript API is a Python library designed for efficient retrieval of YouTube video transcripts and subtitles, including automatically generated ones. This API eliminates the need for headless browsers, supports subtitle translation, and offers flexible API interfaces and command-line tools. It addresses IP blocking issues by supporting proxy configurations to ensure service stability and provides various output formats. The project offers a convenient solution for video content analysis, multilingual processing, and automated data extraction.",
    "keywords_en": [
      "YouTube",
      "Transcript",
      "Subtitles",
      "API",
      "Python",
      "Video Processing",
      "Data Extraction",
      "Proxy"
    ],
    "area_en": [
      "Video Understanding",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-11T22:24:04Z",
    "download_time": "2024-07-09 07:00:00",
    "visual_resource": [
      "screenshot/github/youtube-transcript-api.png"
    ],
    "extra_info": null
  },
  {
    "id": "self-llm",
    "source": "GitHub",
    "url": "https://github.com/datawhalechina/self-llm",
    "title_en": "Open-source Large Language Model Handbook",
    "summary_en": "This project presents a dedicated open-source large language model (LLM) tutorial, specifically designed for beginners in China and optimized for Linux platforms. It provides comprehensive, full-process guidance encompassing essential skills such as environment configuration, local deployment, and efficient fine-tuning for a wide array of open-source LLMs. By simplifying the complex deployment, usage, and application workflows, the initiative aims to make advanced LLM technologies more accessible to a broader audience of students and researchers. The tutorial covers mainstream models like LLaMA, ChatGLM, and InternLM, offering practical instructions on command-line invocation, setting up online demonstrations, and integrating with frameworks like LangChain. Furthermore, it delves into advanced topics such as distributed full fine-tuning, LoRA, and P-tuning methods. This resource is crucial for fostering the adoption of open-source, free large models, enabling learners to seamlessly incorporate them into their studies and future professional endeavors.",
    "keywords_en": [
      "Large Language Models",
      "Open-source",
      "Environment Configuration",
      "Model Deployment",
      "Model Fine-tuning",
      "Natural Language Processing",
      "Linux",
      "LangChain"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-06-11T14:30:51Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://contrib.rocks/image?repo=datawhalechina/self-llm"
    ],
    "extra_info": null
  },
  {
    "id": "MiniCPM",
    "source": "GitHub",
    "url": "https://github.com/OpenBMB/MiniCPM",
    "title_en": "MiniCPM",
    "summary_en": "MiniCPM is an ultra-efficient series of large language models designed for edge devices, co-developed by ModelBest, Tsinghua University, and Renmin University of China. It achieves exceptional efficiency through innovative model architectures like InfLLM v2 sparse attention, efficient learning algorithms such as BitCPM 3-value quantization, and optimized inference systems like CPM.cu. While maintaining state-of-the-art performance for its size, MiniCPM models deliver over 5x generation speedup on typical edge chips. They also surpass similarly sized and even larger models in tasks like tool calling, code interpretation, and long-context processing, offering a powerful solution for edge AI applications.",
    "keywords_en": [
      "Large Language Model",
      "Edge AI",
      "Efficient Inference",
      "Sparse Attention",
      "Model Quantization",
      "Tool Calling",
      "Long Context",
      "Natural Language Processing"
    ],
    "area_en": [
      "Large Language Model",
      "Deep Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-06-11T06:36:09Z",
    "download_time": "2024-07-10 07:00:00",
    "visual_resource": [
      "https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm_logo.png",
      "https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm4/efficiency.png",
      "https://github.com/OpenBMB/MiniCPM/raw/main/assets/minicpm4/benchmark.png"
    ],
    "extra_info": null
  },
  {
    "id": "2506.06751",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.06751",
    "title_en": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
    "summary_en": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
    "keywords_en": [
      "Large Language Models",
      "Geopolitical Biases",
      "National Narratives",
      "Bias Evaluation",
      "Dataset"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-07T10:45:17.000Z",
    "download_time": "2025-06-11 21:32:49",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06751.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.06751\", \"arxiv_url\": \"https://arxiv.org/abs/2506.06751\"}"
  },
  {
    "id": "2506.09040",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.09040",
    "title_en": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
    "summary_en": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.",
    "keywords_en": [
      "Autoregressive Semantic Visual Reconstruction",
      "Visual Language Models",
      "Multimodal Understanding",
      "Image Understanding",
      "Semantic Representation"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Computer Vision"
    ],
    "published_time": "2025-06-10T17:57:50.000Z",
    "download_time": "2025-06-11 21:32:54",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09040.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.09040\", \"arxiv_url\": \"https://arxiv.org/abs/2506.09040\"}"
  },
  {
    "id": "2506.08279",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.08279",
    "title_en": "Seeing Voices: Generating A-Roll Video from Audio with Mirage",
    "summary_en": "From professional filmmaking to user-generated content, creators and\nconsumers have long recognized that the power of video depends on the\nharmonious integration of what we hear (the video's audio track) with what we\nsee (the video's image sequence). Current approaches to video generation either\nignore sound to focus on general-purpose but silent image sequence generation\nor address both visual and audio elements but focus on restricted application\ndomains such as re-dubbing. We introduce Mirage, an audio-to-video foundation\nmodel that excels at generating realistic, expressive output imagery from\nscratch given an audio input. When integrated with existing methods for speech\nsynthesis (text-to-speech, or TTS), Mirage results in compelling multimodal\nvideo. When trained on audio-video footage of people talking (A-roll) and\nconditioned on audio containing speech, Mirage generates video of people\ndelivering a believable interpretation of the performance implicit in input\naudio. Our central technical contribution is a unified method for training\nself-attention-based audio-to-video generation models, either from scratch or\ngiven existing weights. This methodology allows Mirage to retain generality as\nan approach to audio-to-video generation while producing outputs of superior\nsubjective quality to methods that incorporate audio-specific architectures or\nloss components specific to people, speech, or details of how images or audio\nare captured. We encourage readers to watch and listen to the results of Mirage\nfor themselves (see paper and comments for links).",
    "keywords_en": [
      "Audio-to-video generation",
      "Foundation model",
      "Multimodal",
      "Video generation",
      "Self-attention"
    ],
    "area_en": [
      "Generative AI",
      "Multimodal",
      "Deep Learning"
    ],
    "published_time": "2025-06-09T22:56:02.000Z",
    "download_time": "2025-06-11 21:32:57",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08279.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.08279\", \"arxiv_url\": \"https://arxiv.org/abs/2506.08279\"}"
  },
  {
    "id": "2506.05167",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.05167",
    "title_en": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
    "summary_en": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
    "keywords_en": [
      "Retrieval-Augmented Generation",
      "Large Language Models",
      "Context Compression",
      "Evidentiality",
      "Open-Domain Question Answering"
    ],
    "area_en": [
      "Natural Language Processing",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-06-05T15:43:49.000Z",
    "download_time": "2025-06-11 21:32:53",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05167.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.05167\", \"arxiv_url\": \"https://arxiv.org/abs/2506.05167\"}"
  },
  {
    "id": "2506.07045",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.07045",
    "title_en": "Interpretable and Reliable Detection of AI-Generated Images via Grounded\n  Reasoning in MLLMs",
    "summary_en": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods.",
    "keywords_en": [
      "AI-generated image detection",
      "Multimodal Large Language Models",
      "Interpretability",
      "Grounded reasoning",
      "Synthesis artifacts"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "Computer Vision"
    ],
    "published_time": "2025-06-08T08:47:44.000Z",
    "download_time": "2025-06-11 21:32:52",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07045.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.07045\", \"arxiv_url\": \"https://arxiv.org/abs/2506.07045\"}"
  },
  {
    "id": "2506.07976",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2506.07976",
    "title_en": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
    "summary_en": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.",
    "keywords_en": [
      "Agents",
      "Test-Time Interaction",
      "Online Reinforcement Learning",
      "Web Agents",
      "Exploration-Exploitation"
    ],
    "area_en": [
      "AI Agent",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-06-09T17:50:02.000Z",
    "download_time": "2025-06-11 21:32:52",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07976.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2506.07976\", \"arxiv_url\": \"https://arxiv.org/abs/2506.07976\"}"
  }
]