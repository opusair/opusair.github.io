<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2026-01-06</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2026-01-06</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Locating a Photo of a Vehicle in 30 Seconds with GeoSpy</h2>
                <span class="published-time">Published: 2026-01-06 18:00:27</span>
                
                <p class="summary">GeoSpy has showcased a groundbreaking capability to precisely geolocate a vehicle from a photograph in an astonishingly short span of 30 seconds. This technological feat underscores significant advancements in the convergence of geospatial artificial intelligence and sophisticated computer vision techniques. The system appears to employ advanced image processing algorithms, integrating them with vast repositories of geographic data and robust machine learning models. By meticulously analyzing subtle environmental and contextual cues embedded within an image, GeoSpy can rapidly identify and pinpoint the exact location. This tool carries profound implications across diverse sectors, including law enforcement, intelligence gathering, and investigative journalism, by dramatically accelerating visual intelligence analysis. It exemplifies the increasing effectiveness of AI-driven solutions in transforming raw visual information into actionable insights, thereby revolutionizing open-source intelligence (OSINT) methodologies and forensic analysis.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Geo-location</span><span>Computer Vision</span><span>OSINT</span><span>Image Analysis</span><span>Artificial Intelligence</span><span>Geospatial AI</span><span>Vehicle Identification</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Computer Vision</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://geospy.ai/blog/locating-a-photo-of-a-vehicle-in-30-seconds-with-geospy" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Launch HN: Tamarind Bio (YC W24) – AI Inference Provider for Drug Discovery</h2>
                <span class="published-time">Published: 2026-01-06 17:49:56</span>
                
                <p class="summary">Tamarind Bio, a Y Combinator W24 startup, has launched as an AI inference provider specifically tailored for drug discovery applications. Founded by Deniz and Sherry, the company offers biopharma entities a streamlined platform to leverage a library of leading open-source AI models, including prominent ones like AlphaFold, for the computational design of new medicines. The inspiration for Tamarind Bio stemmed from direct experience with the inefficiencies inherent in traditional computational biology workflows, where significant model execution tasks involving thousands of inputs were often handled manually within university clusters. Recognizing the unsustainability of such ad-hoc processes for organizational-level computational work, Tamarind Bio was developed to centralize and automate these critical AI inference tasks. This initiative aims to democratize access to advanced AI for drug discovery, moving beyond reliance on individual specialists to a more robust and scalable platform solution, thereby accelerating the research and development pipeline for novel therapeutic compounds.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Inference</span><span>Drug Discovery</span><span>Computational Biology</span><span>AlphaFold</span><span>Biotechnology</span><span>Machine Learning</span><span>Open-source AI Models</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://news.ycombinator.com/item?id=46515777" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Why agents matter more than other AI</h2>
                <span class="published-time">Published: 2026-01-06 00:26:20</span>
                
                <p class="summary">The article "Why agents matter more than other AI" argues for the profound significance of autonomous AI agents as a distinct and superior form of artificial intelligence compared to conventional AI systems. It posits that while many AI applications excel at specific, narrow tasks, AI agents possess the unique ability to perceive environments, reason, plan, and execute multi-step actions autonomously, learning and adapting over time. This capability allows them to tackle complex, real-world problems that require continuous interaction and decision-making, moving beyond simple data processing or pattern recognition. The increasing sophistication of agents in areas like task automation, goal-oriented problem-solving, and adaptive behavior is highlighted as a critical differentiator, suggesting they are poised to drive the next wave of innovation and practical utility across diverse sectors, making them a more impactful and transformative technology.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>AI Agents</span><span>Autonomous AI</span><span>Intelligent Systems</span><span>Task Automation</span><span>Adaptive AI</span><span>Goal-Oriented AI</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://substack.com/home/post/p-182047799" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>My Tamagotchi is an RL agent playing Slither.io</h2>
                <span class="published-time">Published: 2026-01-06 06:22:30</span>
                
                <p class="summary">A recent project showcases a Reinforcement Learning (RL) agent engineered to autonomously play the popular online game Slither.io. Dubbed a 'Tamagotchi' due to its continuous, self-preserving nature, the agent learns to navigate the game's dynamic environment, consume pellets to grow, and avoid collisions with other players. This initiative highlights the practical application of RL techniques in complex, real-time gaming scenarios, demonstrating an AI's capacity to develop sophisticated strategies for survival and progression. The system likely employs a deep reinforcement learning architecture, allowing the agent to interpret visual input and execute actions within the game's physics. The project serves as an engaging example of how AI agents can be trained to master challenging interactive environments, mimicking the persistent and adaptive behavior of a virtual pet while illustrating the power of machine learning in creating intelligent game-playing entities. It offers insights into agent design and training methodologies for environments requiring continuous learning and adaptation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Reinforcement Learning</span><span>AI Agent</span><span>Slither.io</span><span>Game AI</span><span>Deep Reinforcement Learning</span><span>Agent-based systems</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://nkasmanoff.github.io/#/blog/tamagotchi-rl-slitherio" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Hierarchical Autoregressive Modeling for Memory-Efficient Language Generation</h2>
                <span class="published-time">Published: 2026-01-06 18:02:01</span>
                
                <p class="summary">This research introduces Hierarchical Autoregressive Modeling, a novel framework designed to significantly improve memory efficiency in language generation tasks. The increasing size and complexity of modern large language models often lead to substantial memory consumption, posing challenges for both training and deployment, especially on resource-constrained hardware. The proposed hierarchical approach addresses this by decomposing the sequential generation process into multiple, smaller autoregressive steps or layers. This allows for more efficient management of intermediate representations and activations, effectively reducing the overall memory footprint required for generating long and coherent sequences of text. The methodology is particularly beneficial for applications demanding extensive context handling, such as advanced document generation, sophisticated chatbots, and real-time content creation. By demonstrating how a structured hierarchical design can lead to considerable memory savings while maintaining or even enhancing generation quality, this work provides a crucial advancement for developing more scalable and accessible generative AI systems.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Hierarchical Autoregressive Modeling</span><span>Language Generation</span><span>Memory Efficiency</span><span>Large Language Models</span><span>Natural Language Processing</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Natural Language Processing</span><span>Large Language Model</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://arxiv.org/abs/2512.20687" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: ccrider - Search and Resume Your Claude Code Sessions – TUI / MCP / CLI</h2>
                <span class="published-time">Published: 2026-01-06 14:14:24</span>
                
                <p class="summary">ccrider is a newly developed open-source tool engineered to significantly enhance session management for users interacting with Claude Code. This utility, implemented as a single Go binary, meticulously stores and synchronizes the entire history of Claude Code sessions within a SQLite database, providing robust capabilities for efficient retrieval and seamless continuation of previous coding interactions. It presents a versatile set of interfaces, including a Text User Interface (TUI) offering intuitive session browsing and full-text search functionalities, a Command Line Interface (CLI) for programmatic control, and an MCP (Multi-Process Communication) server for advanced operations. Through the TUI, users can easily search, navigate within specific sessions, resume their work, or export session content to markdown. Crucially, the MCP server empowers Claude to access and leverage prior session data or pre-compacted context, greatly improving continuity and overall effectiveness in AI-assisted development workflows. This simple yet highly effective solution addresses the challenge of maintaining comprehensive historical context for AI coding assistants.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Claude Code</span><span>Session Management</span><span>Developer Tools</span><span>TUI</span><span>CLI</span><span>Go programming language</span><span>SQLite</span><span>AI Agent Tools</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/neilberkman/ccrider" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation</h2>
                <span class="published-time">Published: 2026-01-05T15:27:04.000Z</span>
                
                <p class="summary">We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>NextFlow</span><span>Multimodal</span><span>Autoregressive Transformer</span><span>Image Generation</span><span>Next-scale Prediction</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Multimodal</span><span>Generative AI</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.02204" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Recursive Language Models</h2>
                <span class="published-time">Published: 2025-12-31T03:43:41.000Z</span>
                
                <p class="summary">We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Recursive Language Models</span><span>Large Language Models</span><span>long prompts</span><span>inference strategy</span><span>context windows</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24601" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents</h2>
                <span class="published-time">Published: 2026-01-05T18:05:29.000Z</span>
                
                <p class="summary">As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (φ) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (ρ) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>LLM Agents</span><span>Faithfulness</span><span>Structural Causal Models</span><span>Explainable AI</span><span>Causal Decoupling</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>AI Agent</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.02314" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GARDO: Reinforcing Diffusion Models without Reward Hacking</h2>
                <span class="published-time">Published: 2025-12-30T10:55:45.000Z</span>
                
                <p class="summary">Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Diffusion Models</span><span>Reinforcement Learning</span><span>Reward Hacking</span><span>Generative AI</span><span>Diversity-aware Optimization</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24138" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling</h2>
                <span class="published-time">Published: 2026-01-05T18:44:27.000Z</span>
                
                <p class="summary">This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Reasoning-optimized Model</span><span>Small Language Models</span><span>Parameter Efficiency</span><span>Test-Time Scaling</span><span>Hybrid Architecture</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Deep Learning</span><span>Natural Language Processing</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.02346" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models</h2>
                <span class="published-time">Published: 2025-12-28T10:58:36.000Z</span>
                
                <p class="summary">Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Concept Erasure</span><span>Diffusion Models</span><span>Multimodal Evaluation</span><span>Generative AI</span><span>Text-to-Image</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Multimodal</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.22877" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>