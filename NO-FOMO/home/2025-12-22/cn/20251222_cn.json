[
  {
    "id": "hackernews_46357675",
    "source": "Hacker News",
    "url": "https://jalammar.github.io/illustrated-transformer/",
    "title": "The Illustrated Transformer",
    "summary": "The Illustrated Transformer, a highly acclaimed article by Jay Alammar, provides an exceptionally clear and visual explanation of the Transformer architecture, a groundbreaking deep learning model essential for modern Natural Language Processing. It meticulously breaks down the intricate components such as self-attention, multi-head attention, positional encoding, and the overall encoder-decoder structure. Through intuitive diagrams and analogies, the article demystifies how these elements interact to process sequential data more effectively than previous recurrent neural networks. This resource is widely recognized for making complex concepts accessible, serving as a foundational guide for anyone looking to understand the underlying mechanics of large language models like GPT and BERT. It highlights the paradigm shift brought by attention mechanisms, significantly contributing to the rapid advancements in AI.",
    "keywords": [
      "Transformer",
      "Self-Attention",
      "Deep Learning",
      "Natural Language Processing",
      "Neural Networks",
      "Encoder-Decoder",
      "Positional Encoding"
    ],
    "area": [
      "Deep Learning",
      "Natural Language Processing",
      "Machine Learning"
    ],
    "published_time": "2025-12-22 19:15:56",
    "download_time": "2025-12-22 20:00:27",
    "extra_info": "{\"score\": 31, \"by\": \"auraham\", \"descendants\": 8, \"story_id\": 46357675}"
  },
  {
    "id": "hackernews_46354970",
    "source": "Hacker News",
    "url": "https://blog.kierangill.xyz/oversight-and-guidance",
    "title": "Scaling LLMs to Larger Codebases",
    "summary": "The effective scaling of Large Language Models (LLMs) to operate within extensive codebases presents a significant challenge and a critical area for advancement. Traditional LLM architectures often encounter limitations regarding context window size and the inherent complexity of large-scale software projects. Addressing this requires sophisticated strategies for managing vast amounts of code, dependencies, and documentation. Key methodologies include the implementation of advanced retrieval-augmented generation (RAG) systems, which dynamically fetch relevant code snippets to extend the LLM's effective context without exceeding token limits. Additionally, hierarchical context processing, enabling LLMs to analyze code at varying levels of abstraction, is vital. The development of specialized code embeddings and fine-tuning on diverse code corpuses are also essential for enhancing LLM comprehension of programming paradigms and project-specific conventions. These efforts aim to bolster LLM capabilities for tasks such as code generation, debugging, refactoring, and comprehensive analysis across large software repositories, ultimately boosting developer productivity.",
    "keywords": [
      "Large Language Models",
      "Codebases",
      "Scaling LLMs",
      "Context Window",
      "Retrieval-Augmented Generation",
      "Software Engineering",
      "Code Analysis"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-12-22 15:38:37",
    "download_time": "2025-12-22 20:00:29",
    "extra_info": "{\"score\": 152, \"by\": \"kierangill\", \"descendants\": 69, \"story_id\": 46354970}"
  },
  {
    "id": "hackernews_46355165",
    "source": "Hacker News",
    "url": "https://github.com/anthropics/claude-code/blob/main/CHANGELOG.md",
    "title": "Claude Code gets native LSP support",
    "summary": "Anthropic's Claude Code, an advanced artificial intelligence tool designed to assist with various coding tasks, has officially announced the integration of native Language Server Protocol (LSP) support. This significant update profoundly enhances the tool's interoperability with a wide range of popular Integrated Development Environments (IDEs) and code editors, making it a more versatile and integrated part of the developer's toolkit. By leveraging LSP, Claude Code can now offer a more robust and responsive development experience, enabling critical features such as precise code completion, real-time error checking, go-to-definition functionalities, and efficient refactoring capabilities directly within the user's preferred coding environment. This native support signifies a strategic move towards deeper integration into the existing developer workflow, promising increased productivity and a smoother experience for engineers utilizing AI assistance in their daily work. The enhancement is expected to significantly reduce context switching, improve the accuracy and relevance of AI-generated code suggestions and analyses, and ultimately position Claude Code as a more integral and powerful companion for software developers aiming to streamline their development process and improve code quality.",
    "keywords": [
      "Language Server Protocol",
      "Claude Code",
      "AI Assistant",
      "Code Generation",
      "Developer Tools",
      "Software Development",
      "IDE Integration"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-12-22 15:59:01",
    "download_time": "2025-12-22 20:00:35",
    "extra_info": "{\"score\": 143, \"by\": \"JamesSwift\", \"descendants\": 77, \"story_id\": 46355165}"
  },
  {
    "id": "hackernews_46357287",
    "source": "Hacker News",
    "url": "https://z.ai/blog/glm-4.7",
    "title": "GLM-4.7: Advancing the Coding Capability",
    "summary": "The introduction of GLM-4.7 marks a significant update in the landscape of artificial intelligence models, specifically targeting advancements in coding capabilities. While detailed specifications are pending, the model's designation suggests it is a refined iteration, likely building upon the foundational strengths of its predecessors within the GLM series. The primary focus on \"advancing the coding capability\" implies a robust suite of improvements aimed at enhancing developer productivity and efficiency across various programming tasks. This could encompass more accurate and contextually relevant code generation, superior debugging assistance, improved understanding of complex codebases, and potentially even support for automated refactoring or vulnerability detection. The release of GLM-4.7 is poised to offer developers and organizations a more powerful tool for accelerating software development cycles, reducing errors, and enabling more sophisticated applications through highly capable AI-driven coding assistance. This advancement highlights the ongoing trend in large language model development towards specialized, high-performance applications that cater to specific professional domains, pushing the boundaries of what AI can achieve in practical, real-world development environments.",
    "keywords": [
      "GLM-4.7",
      "Large Language Model",
      "Code Generation",
      "AI Development Tools",
      "Software Engineering AI",
      "Model Improvement"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-12-22 18:46:32",
    "download_time": "2025-12-22 20:00:38",
    "extra_info": "{\"score\": 13, \"by\": \"pretext\", \"descendants\": 0, \"story_id\": 46357287}"
  },
  {
    "id": "hackernews_46350075",
    "source": "Hacker News",
    "url": "https://ym2132.github.io/ONNX_MLProgram_NN_exploration",
    "title": "ONNX Runtime and CoreML May Silently Convert Your Model to FP16",
    "summary": "A recent analysis reveals a significant operational nuance within ONNX Runtime and CoreML, where machine learning models may be silently converted to FP16 (half-precision floating-point) format during the deployment and inference stages. This automatic conversion, while often beneficial for accelerating performance and reducing memory footprint on hardware optimized for FP16, such as GPUs and Apple's Neural Engine, carries the inherent risk of introducing precision loss. Such silent modifications can lead to subtle degradation in model accuracy, affect numerical stability, and create unexpected discrepancies between development and deployment environments. For practitioners in machine learning engineering, this necessitates a heightened awareness and proactive approach to model validation, ensuring that explicit control over data types is maintained. Debugging issues stemming from implicit FP16 conversion can be challenging, underscoring the importance of understanding framework behaviors to guarantee model integrity and predictable performance across diverse inference platforms.",
    "keywords": [
      "ONNX Runtime",
      "CoreML",
      "FP16 Conversion",
      "Machine Learning Models",
      "Inference Engine",
      "Data Precision",
      "Neural Network Optimization",
      "Model Deployment"
    ],
    "area": [
      "Machine Learning",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "published_time": "2025-12-22 00:27:04",
    "download_time": "2025-12-22 20:01:05",
    "extra_info": "{\"score\": 91, \"by\": \"Two_hands\", \"descendants\": 15, \"story_id\": 46350075}"
  },
  {
    "id": "2512.16969",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.16969",
    "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
    "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
    "keywords": [
      "Scientific General Intelligence",
      "LLMs",
      "SGI-Bench",
      "Scientist-Aligned Workflows",
      "Test-Time Reinforcement Learning"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-12-18T12:44:36.000Z",
    "download_time": "2025-12-22 12:01:14",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.16969\", \"arxiv_url\": \"https://arxiv.org/abs/2512.16969\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png\", \"original_title\": \"Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows\"}"
  },
  {
    "id": "2512.16793",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.16793",
    "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
    "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
    "keywords": [
      "Physical Intelligence",
      "Egocentric Data",
      "Vision Language Models",
      "Robotics",
      "Embodied AI"
    ],
    "area": [
      "Robotics",
      "Computer Vision",
      "Multimodal"
    ],
    "published_time": "2025-12-18T17:27:03.000Z",
    "download_time": "2025-12-22 12:01:15",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.16793\", \"arxiv_url\": \"https://arxiv.org/abs/2512.16793\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16793.png\", \"original_title\": \"PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence\"}"
  },
  {
    "id": "2512.17901",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.17901",
    "title": "When Reasoning Meets Its Laws",
    "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/",
    "keywords": [
      "Large Reasoning Models",
      "Laws of Reasoning",
      "Reasoning Performance",
      "Compute Law",
      "Compositionality"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-12-19T18:59:11.000Z",
    "download_time": "2025-12-22 12:01:16",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.17901\", \"arxiv_url\": \"https://arxiv.org/abs/2512.17901\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17901.png\", \"original_title\": \"When Reasoning Meets Its Laws\"}"
  },
  {
    "id": "2512.17909",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.17909",
    "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.",
    "keywords": [
      "Text-to-Image Generation",
      "Image Editing",
      "Latent Diffusion Models",
      "Representation Encoders",
      "Semantic-Pixel Reconstruction"
    ],
    "area": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-12-19T18:59:57.000Z",
    "download_time": "2025-12-22 12:01:15",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.17909\", \"arxiv_url\": \"https://arxiv.org/abs/2512.17909\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17909.png\", \"original_title\": \"Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing\"}"
  },
  {
    "id": "2512.17008",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.17008",
    "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
    "summary": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
    "keywords": [
      "Reinforcement Learning",
      "LLM Agents",
      "Multi-Turn RL",
      "PPO",
      "Advantage Estimation"
    ],
    "area": [
      "Machine Learning",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-12-18T19:07:25.000Z",
    "download_time": "2025-12-22 12:01:14",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.17008\", \"arxiv_url\": \"https://arxiv.org/abs/2512.17008\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17008.png\", \"original_title\": \"Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs\"}"
  },
  {
    "id": "2512.16978",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.16978",
    "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
    "summary": "Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.",
    "keywords": [
      "Omni-Modal Reasoning",
      "Tool Use",
      "Long Videos",
      "Benchmarks",
      "Agentic Framework"
    ],
    "area": [
      "Multimodal",
      "Video Understanding",
      "AI Agent"
    ],
    "published_time": "2025-12-18T18:59:27.000Z",
    "download_time": "2025-12-22 12:01:15",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.16978\", \"arxiv_url\": \"https://arxiv.org/abs/2512.16978\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16978.png\", \"original_title\": \"A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos\"}"
  }
]