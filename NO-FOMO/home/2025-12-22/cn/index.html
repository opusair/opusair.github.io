<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-22</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="report-header">
            <h1>AI Daily Report</h1>
            <p class="date">2025-12-22</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>The Illustrated Transformer</h2>
                <span class="published-time">Published: 2025-12-22 19:15:56</span>
                
                <p class="summary">The Illustrated Transformer, a highly acclaimed article by Jay Alammar, provides an exceptionally clear and visual explanation of the Transformer architecture, a groundbreaking deep learning model essential for modern Natural Language Processing. It meticulously breaks down the intricate components such as self-attention, multi-head attention, positional encoding, and the overall encoder-decoder structure. Through intuitive diagrams and analogies, the article demystifies how these elements interact to process sequential data more effectively than previous recurrent neural networks. This resource is widely recognized for making complex concepts accessible, serving as a foundational guide for anyone looking to understand the underlying mechanics of large language models like GPT and BERT. It highlights the paradigm shift brought by attention mechanisms, significantly contributing to the rapid advancements in AI.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Transformer</span><span>Self-Attention</span><span>Deep Learning</span><span>Natural Language Processing</span><span>Neural Networks</span><span>Encoder-Decoder</span><span>Positional Encoding</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Deep Learning</span><span>Natural Language Processing</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Scaling LLMs to Larger Codebases</h2>
                <span class="published-time">Published: 2025-12-22 15:38:37</span>
                
                <p class="summary">The effective scaling of Large Language Models (LLMs) to operate within extensive codebases presents a significant challenge and a critical area for advancement. Traditional LLM architectures often encounter limitations regarding context window size and the inherent complexity of large-scale software projects. Addressing this requires sophisticated strategies for managing vast amounts of code, dependencies, and documentation. Key methodologies include the implementation of advanced retrieval-augmented generation (RAG) systems, which dynamically fetch relevant code snippets to extend the LLM's effective context without exceeding token limits. Additionally, hierarchical context processing, enabling LLMs to analyze code at varying levels of abstraction, is vital. The development of specialized code embeddings and fine-tuning on diverse code corpuses are also essential for enhancing LLM comprehension of programming paradigms and project-specific conventions. These efforts aim to bolster LLM capabilities for tasks such as code generation, debugging, refactoring, and comprehensive analysis across large software repositories, ultimately boosting developer productivity.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Large Language Models</span><span>Codebases</span><span>Scaling LLMs</span><span>Context Window</span><span>Retrieval-Augmented Generation</span><span>Software Engineering</span><span>Code Analysis</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://blog.kierangill.xyz/oversight-and-guidance" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Claude Code gets native LSP support</h2>
                <span class="published-time">Published: 2025-12-22 15:59:01</span>
                
                <p class="summary">Anthropic's Claude Code, an advanced artificial intelligence tool designed to assist with various coding tasks, has officially announced the integration of native Language Server Protocol (LSP) support. This significant update profoundly enhances the tool's interoperability with a wide range of popular Integrated Development Environments (IDEs) and code editors, making it a more versatile and integrated part of the developer's toolkit. By leveraging LSP, Claude Code can now offer a more robust and responsive development experience, enabling critical features such as precise code completion, real-time error checking, go-to-definition functionalities, and efficient refactoring capabilities directly within the user's preferred coding environment. This native support signifies a strategic move towards deeper integration into the existing developer workflow, promising increased productivity and a smoother experience for engineers utilizing AI assistance in their daily work. The enhancement is expected to significantly reduce context switching, improve the accuracy and relevance of AI-generated code suggestions and analyses, and ultimately position Claude Code as a more integral and powerful companion for software developers aiming to streamline their development process and improve code quality.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Language Server Protocol</span><span>Claude Code</span><span>AI Assistant</span><span>Code Generation</span><span>Developer Tools</span><span>Software Development</span><span>IDE Integration</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/anthropics/claude-code/blob/main/CHANGELOG.md" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GLM-4.7: Advancing the Coding Capability</h2>
                <span class="published-time">Published: 2025-12-22 18:46:32</span>
                
                <p class="summary">The introduction of GLM-4.7 marks a significant update in the landscape of artificial intelligence models, specifically targeting advancements in coding capabilities. While detailed specifications are pending, the model's designation suggests it is a refined iteration, likely building upon the foundational strengths of its predecessors within the GLM series. The primary focus on "advancing the coding capability" implies a robust suite of improvements aimed at enhancing developer productivity and efficiency across various programming tasks. This could encompass more accurate and contextually relevant code generation, superior debugging assistance, improved understanding of complex codebases, and potentially even support for automated refactoring or vulnerability detection. The release of GLM-4.7 is poised to offer developers and organizations a more powerful tool for accelerating software development cycles, reducing errors, and enabling more sophisticated applications through highly capable AI-driven coding assistance. This advancement highlights the ongoing trend in large language model development towards specialized, high-performance applications that cater to specific professional domains, pushing the boundaries of what AI can achieve in practical, real-world development environments.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>GLM-4.7</span><span>Large Language Model</span><span>Code Generation</span><span>AI Development Tools</span><span>Software Engineering AI</span><span>Model Improvement</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://z.ai/blog/glm-4.7" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>ONNX Runtime and CoreML May Silently Convert Your Model to FP16</h2>
                <span class="published-time">Published: 2025-12-22 00:27:04</span>
                
                <p class="summary">A recent analysis reveals a significant operational nuance within ONNX Runtime and CoreML, where machine learning models may be silently converted to FP16 (half-precision floating-point) format during the deployment and inference stages. This automatic conversion, while often beneficial for accelerating performance and reducing memory footprint on hardware optimized for FP16, such as GPUs and Apple's Neural Engine, carries the inherent risk of introducing precision loss. Such silent modifications can lead to subtle degradation in model accuracy, affect numerical stability, and create unexpected discrepancies between development and deployment environments. For practitioners in machine learning engineering, this necessitates a heightened awareness and proactive approach to model validation, ensuring that explicit control over data types is maintained. Debugging issues stemming from implicit FP16 conversion can be challenging, underscoring the importance of understanding framework behaviors to guarantee model integrity and predictable performance across diverse inference platforms.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>ONNX Runtime</span><span>CoreML</span><span>FP16 Conversion</span><span>Machine Learning Models</span><span>Inference Engine</span><span>Data Precision</span><span>Neural Network Optimization</span><span>Model Deployment</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Machine Learning</span><span>Deep Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://ym2132.github.io/ONNX_MLProgram_NN_exploration" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</h2>
                <span class="published-time">Published: 2025-12-18T12:44:36.000Z</span>
                
                <p class="summary">Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Scientific General Intelligence</span><span>LLMs</span><span>SGI-Bench</span><span>Scientist-Aligned Workflows</span><span>Test-Time Reinforcement Learning</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.16969" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence</h2>
                <span class="published-time">Published: 2025-12-18T17:27:03.000Z</span>
                
                <p class="summary">Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9%), demonstrating effective transfer from human egocentric supervision to downstream robot control.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Physical Intelligence</span><span>Egocentric Data</span><span>Vision Language Models</span><span>Robotics</span><span>Embodied AI</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Robotics</span><span>Computer Vision</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.16793" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>When Reasoning Meets Its Laws</h2>
                <span class="published-time">Published: 2025-12-19T18:59:11.000Z</span>
                
                <p class="summary">Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Large Reasoning Models</span><span>Laws of Reasoning</span><span>Reasoning Performance</span><span>Compute Law</span><span>Compositionality</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.17901" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</h2>
                <span class="published-time">Published: 2025-12-19T18:59:57.000Z</span>
                
                <p class="summary">Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Text-to-Image Generation</span><span>Image Editing</span><span>Latent Diffusion Models</span><span>Representation Encoders</span><span>Semantic-Pixel Reconstruction</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Generative AI</span><span>Computer Vision</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.17909" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</h2>
                <span class="published-time">Published: 2025-12-18T19:07:25.000Z</span>
                
                <p class="summary">Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Reinforcement Learning</span><span>LLM Agents</span><span>Multi-Turn RL</span><span>PPO</span><span>Advantage Estimation</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Machine Learning</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.17008" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</h2>
                <span class="published-time">Published: 2025-12-18T18:59:27.000Z</span>
                
                <p class="summary">Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">Keywords：</span><span>Omni-Modal Reasoning</span><span>Tool Use</span><span>Long Videos</span><span>Benchmarks</span><span>Agentic Framework</span></div>
                    <div class="area"><span class="label">Areas：</span><span>Multimodal</span><span>Video Understanding</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.16978" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>