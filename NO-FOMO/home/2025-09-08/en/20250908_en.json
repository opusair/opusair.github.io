[
  {
    "id": "twitter_googleaidevs_1965160822260318702",
    "source": "Twitter",
    "url": "https://x.com/googleaidevs/status/1965160822260318702",
    "title_en": "googleaidevs_Veo 3 and Veo 3 Fast Generally Available in Gemini API",
    "summary_en": "Google AI Developers announced that their video generation models, Veo 3 and Veo 3 Fast, are now generally available within the Gemini API. Alongside this release, the pricing for both models has been reduced by approximately 50%, and new output formats, including 9:16 vertical and 1080 HD, are now supported. This provides developers with more cost-effective and flexible video generation capabilities.",
    "keywords_en": [
      "Veo 3",
      "Veo 3 Fast",
      "Gemini API",
      "Video Generation",
      "Price Reduction",
      "Product Launch"
    ],
    "area_en": [
      "Product Launch",
      "Generative AI",
      "Video Understanding"
    ],
    "published_time": "2025-09-08T21:10:25.000Z",
    "download_time": "2025-09-09 05:45:53",
    "visual_resource": [
      "screenshot/twitter/googleaidevs_1965160822260318702.png"
    ],
    "extra_info": "{\"username\": \"googleaidevs\", \"tweet_id\": \"1965160822260318702\"}"
  },
  {
    "id": "twitter_fchollet_1964900285698252903",
    "source": "Twitter",
    "url": "https://x.com/fchollet/status/1964900285698252903",
    "title_en": "fchollet_Generative AI's Impact on Future Data and Culture",
    "summary_en": "Prominent AI researcher Fran√ßois Chollet warns that as generative AI content proliferates, the internet will be flooded with low-quality information. He predicts that future AI models will inevitably be trained predominantly on this 'slop,' with only verifiable reasoning tasks potentially being trained in simulated environments. Chollet suggests this trend will lead to human culture being continuously remixed from low-quality content, raising profound concerns about AI data quality and cultural evolution.",
    "keywords_en": [
      "Generative AI",
      "Data Contamination",
      "Model Training",
      "Cultural Evolution",
      "Data Quality"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Generative AI",
      "Tech News"
    ],
    "published_time": "2025-09-08T03:55:08.000Z",
    "download_time": "2025-09-09 05:41:40",
    "visual_resource": [
      "screenshot/twitter/fchollet_1964900285698252903.png"
    ],
    "extra_info": "{\"username\": \"fchollet\", \"tweet_id\": \"1964900285698252903\"}"
  },
  {
    "id": "twitter_GoogleDeepMind_1965040645103407572",
    "source": "Twitter",
    "url": "https://x.com/GoogleDeepMind/status/1965040645103407572",
    "title_en": "GoogleDeepMind_RoboBallet AI System Achieves Robot Collaboration Choreography",
    "summary_en": "Google DeepMind, in collaboration with IntrinsicAI and UCL, has successfully developed an innovative AI system named RoboBallet. This advanced system excels at precisely choreographing a team of up to eight robot arms, enabling them to work together seamlessly without collisions. RoboBallet automates complex task and motion planning, significantly streamlining multi-robot operations. Notably, it demonstrates superior performance, outperforming traditional methods by approximately 25%. This development marks a significant advancement in AI-driven robotics, enhancing efficiency and precision in collaborative robotic tasks.",
    "keywords_en": [
      "RoboBallet",
      "Robot Collaboration",
      "AI System",
      "Robot Arms",
      "Motion Planning",
      "Google DeepMind"
    ],
    "area_en": [
      "Robotics",
      "Artificial Intelligence",
      "Research Progress"
    ],
    "published_time": "2025-09-08T13:12:53.000Z",
    "download_time": "2025-09-09 05:46:37",
    "visual_resource": [
      "screenshot/twitter/GoogleDeepMind_1965040645103407572.png"
    ],
    "extra_info": "{\"username\": \"GoogleDeepMind\", \"tweet_id\": \"1965040645103407572\"}"
  },
  {
    "id": "twitter_cognition_1965086655821525280",
    "source": "Twitter",
    "url": "https://twitter.com/cognition/status/1965086655821525280",
    "title_en": "Cognition_Raises Over $400M at $10.2B Valuation for AI Coding Agents",
    "summary_en": "AI company Cognition announced it has successfully raised over $400 million at a $10.2 billion post-money valuation, aiming to advance the frontier of AI coding agents. The round was led by Founders Fund, with existing investors like Lux, 8VC, and Neo doubling down, and new investors including Bain Capital Ventures and D1 Capital joining. This funding will accelerate their innovation in the AI coding domain.",
    "keywords_en": [
      "Cognition",
      "Funding",
      "AI Coding Agents",
      "Valuation",
      "Founders Fund",
      "Artificial Intelligence"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Industry News"
    ],
    "published_time": "2025-09-08T16:15:42.000Z",
    "download_time": "2025-09-09 06:01:22",
    "visual_resource": [
      "screenshot/twitter/cognition_1965086655821525280.png"
    ],
    "extra_info": "{\"username\": \"cognition\", \"tweet_id\": \"1965086655821525280\"}"
  },
  {
    "id": "twitter_MinqiJiang_1965055909605916892",
    "source": "Twitter",
    "url": "https://twitter.com/MinqiJiang/status/1965055909605916892",
    "title_en": "MinqiJiang_ExIt: An RL Method for LLM Iterated Self-Improvement",
    "summary_en": "Minqi Jiang introduces ExIt, a novel RL-based automatic curriculum method from FAIR, designed to efficiently train LLMs for iterated self-improvement at inference-time. ExIt leverages \"self-divergence\" by upcycling the LLM's own responses to bootstrap diverse training distributions, enabling multi-step self-improvement from single-step tasks. This approach significantly reduces training costs and demonstrates superior performance over GRPO in contest math, multi-turn tasks, and Kaggle competitions, notably achieving a 22% improvement in MLE-bench.",
    "keywords_en": [
      "LLM",
      "Self-improvement",
      "Reinforcement Learning",
      "Automatic Curriculum",
      "ExIt",
      "FAIR"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Research Progress"
    ],
    "published_time": "2025-09-08T14:13:32.000Z",
    "download_time": "2025-09-09 06:01:53",
    "visual_resource": [
      "screenshot/twitter/MinqiJiang_1965055909605916892.png"
    ],
    "extra_info": "{\"username\": \"MinqiJiang\", \"tweet_id\": \"1965055909605916892\"}"
  },
  {
    "id": "twitter_UnslothAI_1965047729991860396",
    "source": "Twitter",
    "url": "https://twitter.com/UnslothAI/status/1965047729991860396",
    "title_en": "UnslothAI_Grok 2.5 Local Deployment Optimization and Model Compression",
    "summary_en": "UnslothAI announced that its technology enables efficient local execution of the xAI Grok 2.5 large language model, requiring only 120GB RAM. The 270 billion parameter model achieves approximately 5 tokens/second on a 128GB Mac, utilizing Unsloth's Dynamic 3-bit GGUF technology. This innovation compresses the original 539GB model to 118GB, an 80% reduction, while retaining key layers in higher 8-bit precision to preserve performance.",
    "keywords_en": [
      "Grok 2.5",
      "Large Language Model",
      "Local Deployment",
      "Model Compression",
      "GGUF",
      "UnslothAI"
    ],
    "area_en": [
      "Large Language Model",
      "Tech News",
      "Open Source"
    ],
    "published_time": "2025-09-08T13:41:02.000Z",
    "download_time": "2025-09-09 06:02:35",
    "visual_resource": [
      "screenshot/twitter/UnslothAI_1965047729991860396.png"
    ],
    "extra_info": "{\"username\": \"UnslothAI\", \"tweet_id\": \"1965047729991860396\"}"
  },
  {
    "id": "9QkXcLHUz3ei32L_tJwp_g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/9QkXcLHUz3ei32L_tJwp_g",
    "title_en": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping",
    "summary_en": "The newly open-sourced Inpaint4Drag framework from the University of Hong Kong revolutionizes drag-based image editing. This novel approach decouples complex drag editing into two specialized subtasks: bidirectional warping and standard image inpainting. By employing a pixel-space bidirectional warping algorithm and SAM-based boundary refinement, Inpaint4Drag achieves precise and coherent deformation of image content. A key innovation is its real-time preview capability, which significantly enhances user interaction by allowing iterative refinement of masks and control points before computationally intensive inpainting operations. This addresses the inherent ambiguity of sparse control points in previous methods, leading to more predictable and accurate results. Experimental evaluations demonstrate Inpaint4Drag's superior performance in drag accuracy and image consistency, boasting processing speeds nearly 600 times faster than DragDiffusion while maintaining high-quality inpainting results. Its compatibility with any inpainting model ensures future adaptability and continuous improvement, marking a significant advancement in interactive image manipulation.",
    "keywords_en": [
      "Image Editing",
      "Image Inpainting",
      "Bidirectional Warping",
      "Real-time Interaction",
      "Generative AI"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-08T15:55:57.000Z",
    "download_time": "2025-09-09T14:03:09.919047",
    "visual_resource": [
      "screenshot/wechat/wechat_image_9QkXcLHUz3ei32L_tJwp_g.png"
    ],
    "extra_info": null
  },
  {
    "id": "Yw7v6ggMB5NtwfYICUPAtg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Yw7v6ggMB5NtwfYICUPAtg",
    "title_en": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
    "summary_en": "A new paradigm, Agentic Reinforcement Learning (Agentic RL), is emerging for Large Language Models (LLMs), shifting from traditional Preference-based Reinforcement Fine-tuning (PBRFT). Agentic RL aims to evolve LLMs from passive alignment to active decision-making agents capable of planning, acting, and continuous learning in dynamic environments. A comprehensive 100-page survey, co-authored by institutions including Oxford and NUS, systematically reviews Agentic RL. It covers its theoretical framework, evolutionary trajectory, six core capabilities (planning, tool use, memory, self-improvement, reasoning, perception), and diverse applications. The survey also discusses future challenges such as trustworthiness, scalability, and complex environments. It emphasizes that Agentic RL is crucial for LLMs to transition from \"speaking\" to \"doing,\" marking the \"second half\" of LLM development. This paradigm shift enables LLMs to become more autonomous and versatile, addressing the limitations of previous training methods and paving the way for more robust and interactive AI systems.",
    "keywords_en": [
      "Agentic Reinforcement Learning",
      "Large Language Models",
      "Reinforcement Learning",
      "AI Agent",
      "Survey",
      "Decision Making"
    ],
    "area_en": [
      "Large Language Model",
      "Reinforcement Learning",
      "AI Agent"
    ],
    "published_time": "2025-09-08T15:55:57.000Z",
    "download_time": "2025-09-09T14:03:09.881817",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Yw7v6ggMB5NtwfYICUPAtg.png"
    ],
    "extra_info": null
  },
  {
    "id": "3TaeQTC41BExDZtsPOPDLw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/3TaeQTC41BExDZtsPOPDLw",
    "title_en": "Full-Process Domestic GPU, 100x Context Acceleration! Chinese Academy of Sciences Releases 'Linear Complexity' Brain-Inspired Large Model",
    "summary_en": "The team led by Li Guoqi and Xu Bo from the Chinese Academy of Sciences' Institute of Automation has unveiled SpikingBrain (Áû¨ÊÇâ)-1.0, a domestically developed and controllable brain-inspired spiking large model. Drawing inspiration from the intricate mechanisms of brain neurons, this model adopts a linear or near-linear complexity architecture, effectively addressing the high computational overhead of traditional Transformer models in processing long sequences. SpikingBrain-1.0 was entirely trained and inferred on domestic GPU platforms, achieving performance comparable to mainstream models with significantly less data. Notably, it demonstrates up to a 100-fold acceleration in ultra-long context processing. This research explores an \"endogenous complexity\" approach to general intelligence, aiming to construct low-power, high-performance brain-inspired general intelligent computing models. It provides crucial insights for future brain-inspired chip design, with parts of the model already open-sourced to foster ecosystem development.",
    "keywords_en": [
      "Brain-Inspired Large Model",
      "Linear Complexity",
      "Domestic GPU",
      "Long Sequence Processing",
      "Spiking Neural Network"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-09-08T11:28:15.000Z",
    "download_time": "2025-09-09T14:03:12.015854",
    "visual_resource": [
      "screenshot/wechat/wechat_image_3TaeQTC41BExDZtsPOPDLw.png"
    ],
    "extra_info": null
  },
  {
    "id": "whBNr0pTomNAKhIrfkUYYQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/whBNr0pTomNAKhIrfkUYYQ",
    "title_en": "GPT-5 Revolutionizes Fashion and Revitalizes Excel! OpenAI Hackathon Winners Announced",
    "summary_en": "OpenAI's recent GPT-5 hackathon showcased the immense potential of its latest large language model across diverse applications. The Korean startup Gentoo secured first place with a GPT-5-powered marketing simulation system, enabling businesses to pre-evaluate campaign effectiveness and even predict return rates without real-world financial risk. Other finalist projects demonstrated GPT-5's versatility, including an AI-driven fashion styling platform that combines diffusion models with GPT-5 for 3D avatar makeovers, an intelligent background agent orchestration tool for Excel that automates financial modeling and data interaction, and a knowledge visualization system that transforms complex texts into engaging educational videos. Furthermore, projects featured a GPT-5-based computer use agent capable of automating interactions across various applications and games, and a sophisticated multi-agent system for optimizing power grid operations. These innovative solutions collectively highlight GPT-5's groundbreaking capabilities in multi-domain applications, multimodal processing, and complex task orchestration, signaling a profound integration and transformation of AI across sectors like e-commerce, education, and energy management.",
    "keywords_en": [
      "GPT-5",
      "Hackathon",
      "AI Agent",
      "Marketing Simulation",
      "Automation",
      "Multimodal"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Generative AI"
    ],
    "published_time": "2025-09-08T11:28:15.000Z",
    "download_time": "2025-09-09T14:03:15.545120",
    "visual_resource": [
      "screenshot/wechat/wechat_image_whBNr0pTomNAKhIrfkUYYQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "q-T8xtQNqmqj6PkXfASBsA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/q-T8xtQNqmqj6PkXfASBsA",
    "title_en": "Has Zuckerberg's Big Bet Paid Off? Meta's New Method Accelerates LLM Long-Context Processing by 30x",
    "summary_en": "Meta Superintelligence Labs has introduced REFRAG, an efficient decoding framework designed to address the critical efficiency bottleneck of Large Language Models (LLMs) when processing long context inputs, particularly in Retrieval-Augmented Generation (RAG) applications. The framework significantly reduces the computational overhead of attention mechanisms and KV Cache memory consumption by compressing text into chunk vectors and selectively preserving crucial information. Experimental results demonstrate that REFRAG can accelerate the Time-To-First-Token (TTFT) generation by up to 30.8 times and extend effective context size by 16 times, all while maintaining or even improving model accuracy. This innovation transforms \"large-context RAG\" from a theoretical concept into a practical reality, offering an efficient solution for LLMs to handle massive amounts of information. While the framework shows promising results, its ultimate value awaits broader validation in real-world applications.",
    "keywords_en": [
      "REFRAG",
      "Large Language Models",
      "Long Context",
      "Retrieval-Augmented Generation",
      "Attention Mechanism",
      "Efficiency Optimization"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-09-08T06:20:12.000Z",
    "download_time": "2025-09-09T14:03:25.985943",
    "visual_resource": [
      "screenshot/wechat/wechat_image_q-T8xtQNqmqj6PkXfASBsA.png"
    ],
    "extra_info": null
  },
  {
    "id": "Sk5lEfj-1R5zhV6tNVPI2A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Sk5lEfj-1R5zhV6tNVPI2A",
    "title_en": "Leading Global Image-to-Video Rankings: How Aishi Technology's PixVerse V5 Transforms Video Creation for 100 Million Users",
    "summary_en": "Aishi Technology's PixVerse V5 model has achieved global first place in image-to-video generation and ranks second in text-to-video, demonstrating its leading position in the AI video generation landscape. Serving over 100 million users worldwide, the product emphasizes \"creativity\" and user accessibility, exemplified by innovative features like the Agent creation assistant, which simplifies complex prompt design and enables widespread AI video creation. From a technical standpoint, the V5 version delivers significant advancements in intelligent understanding, achieving more accurate command responses and consistency; rapid, seconds-level generation, with some videos created in as little as 5 seconds; and remarkably enhanced realism, thanks to expanded model parameters and high-quality training data. These improvements are underpinned by core innovations such as a unified feature space for multimodal data, extreme diffusion distillation for accelerated generation, and a proprietary DiT architecture. Aishi Technology's commitment to rapid iteration and continuous technological breakthroughs is propelling AI video generation into a new era of widespread and accessible application, empowering ordinary users to effortlessly transform their imaginative ideas into high-quality video content.",
    "keywords_en": [
      "PixVerse V5",
      "Video Generation",
      "Aishi Technology",
      "Image-to-Video",
      "AI Creation",
      "Large Model"
    ],
    "area_en": [
      "Generative AI",
      "Large Language Model",
      "Computer Vision"
    ],
    "published_time": "2025-09-08T06:20:12.000Z",
    "download_time": "2025-09-09T14:03:27.458874",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Sk5lEfj-1R5zhV6tNVPI2A.png"
    ],
    "extra_info": null
  },
  {
    "id": "parlant",
    "source": "GitHub",
    "url": "https://github.com/emcie-co/parlant",
    "title_en": "Finally, LLM agents that actually follow instructions",
    "summary_en": "Parlant is an innovative AI agent framework designed to address the common challenges of Large Language Model (LLM) agents, such as failing to follow instructions, hallucinating, and exhibiting inconsistent behavior in real-world applications. It ensures reliable execution of predefined tasks and consistent agent conduct by introducing core functionalities like behavioral guidelines, conversational journeys, robust tool integration, domain adaptation, and explainability. This framework is particularly suited for industries demanding high compliance and precision, including financial services, healthcare, e-commerce, and legal tech, enabling developers to build production-ready, predictable, and scalable AI agents.",
    "keywords_en": [
      "AI Agent",
      "Large Language Model",
      "Instruction Following",
      "Behavioral Guidelines",
      "Tool Integration",
      "Explainability",
      "AI Framework"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-08T19:25:59Z",
    "download_time": "2024-07-30 08:00:00",
    "visual_resource": [
      "https://github.com/emcie-co/parlant/blob/develop/docs/demo.gif?raw=true",
      "https://api.star-history.com/svg?repos=emcie-co/parlant&type=Date",
      "https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentDark.png?raw=true"
    ],
    "extra_info": null
  },
  {
    "id": "UI-TARS-desktop",
    "source": "GitHub",
    "url": "https://github.com/bytedance/UI-TARS-desktop",
    "title_en": "TARS",
    "summary_en": "TARS is a comprehensive multimodal AI Agent stack, featuring two core projects: Agent TARS and UI-TARS Desktop. Agent TARS functions as a versatile multimodal AI Agent, extending GUI Agent and Vision capabilities to various environments including terminals, personal computers, web browsers, and integrated products. It provides both a command-line interface (CLI) and a Web UI for user interaction, aiming to streamline task completion through advanced multimodal large language models (LLMs) and seamless integration with real-world Multi-Component Protocol (MCP) tools. UI-TARS Desktop, built upon the UI-TARS model, is a native desktop application offering robust GUI Agent functionalities for both local and remote computer and browser operations. Key features include natural language control, precise screenshot and visual recognition, accurate mouse and keyboard manipulation, cross-platform compatibility, real-time feedback, and secure local processing. This stack is designed to significantly enhance user experience by automating complex interactions and enabling more intuitive control over digital environments.",
    "keywords_en": [
      "Multimodal AI",
      "AI Agent",
      "GUI Automation",
      "Vision Recognition",
      "Natural Language Processing",
      "Cross-platform"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "AI Agent"
    ],
    "published_time": "2025-09-08T19:08:21Z",
    "download_time": "2024-07-30 08:00:00",
    "visual_resource": [
      "https://github.com/bytedance/UI-TARS-desktop/raw/main/images/tars.png"
    ],
    "extra_info": null
  },
  {
    "id": "kilocode",
    "source": "GitHub",
    "url": "https://github.com/Kilo-Org/kilocode",
    "title_en": "üöÄ Kilo Code",
    "summary_en": "Kilo Code is an open-source AI agent specifically designed for Visual Studio Code, empowering developers with intelligent code generation from natural language, automated task execution, and sophisticated code refactoring. It integrates directly with state-of-the-art large language models, including Gemini 2.5 Pro, Claude 4 Sonnet & Opus, and GPT-5, offering a seamless AI-powered coding experience without requiring users to manage their own API keys. A key differentiator is its MCP Server Marketplace, which enables easy discovery and integration of extensions to enhance agent capabilities. While building upon the foundations of projects like Roo Code and Cline, Kilo Code has evolved with its own vision, providing unique features such as multi-mode operation (Architect, Coder, Debugger) and assisted commit messages. This comprehensive tool significantly streamlines the software development lifecycle, enhancing productivity and efficiency for programmers.",
    "keywords_en": [
      "AI Agent",
      "Code Generation",
      "VS Code",
      "Automation",
      "Large Language Model",
      "Code Refactoring",
      "Developer Tool"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-08T21:30:13Z",
    "download_time": "2024-07-30 08:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/Kilo-Org/kilocode/refs/heads/main/kilo.gif"
    ],
    "extra_info": null
  },
  {
    "id": "jaaz",
    "source": "GitHub",
    "url": "https://github.com/11cafe/jaaz",
    "title_en": "Jaaz.app",
    "summary_en": "Jaaz.app is an open-source multimodal creative assistant designed as a privacy-focused, locally runnable alternative to tools like Canva. It offers AI-powered image and video generation, supporting one-prompt creation and prompt-free methods like \"Magic Canvas\" and \"Magic Video.\" The platform features an infinite canvas for visual storyboarding and integrates a smart AI agent system compatible with both local and cloud models. Key advantages include its local-first, open-source, and no-tracking privacy policy, making it safe for commercial use with full data ownership for users.",
    "keywords_en": [
      "Multimodal",
      "Generative AI",
      "Creative Assistant",
      "Image Video Generation",
      "AI Agent",
      "Local Deployment",
      "Open Source"
    ],
    "area_en": [
      "Multimodal",
      "Generative AI",
      "AI Agent"
    ],
    "published_time": "2025-09-09T03:26:33Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/jaaz.png"
    ],
    "extra_info": null
  },
  {
    "id": "generative-ai-for-beginners",
    "source": "GitHub",
    "url": "https://github.com/microsoft/generative-ai-for-beginners",
    "title_en": "Generative AI for Beginners (Version 3) - A Course",
    "summary_en": "This GitHub repository offers a 21-lesson comprehensive course developed by Microsoft Cloud Advocates, designed to teach how to build Generative AI applications. The curriculum covers foundational concepts of Large Language Models (LLMs), prompt engineering, Retrieval Augmented Generation (RAG), function calling, open-source models, AI agents, and model fine-tuning. It provides code examples in both Python and TypeScript, supporting Azure OpenAI, GitHub Marketplace Model Catalog, and OpenAI API, enabling developers to master the creation of applications for text generation, chat, search, and image generation.",
    "keywords_en": [
      "Generative AI",
      "Large Language Models",
      "Prompt Engineering",
      "Retrieval Augmented Generation",
      "AI Agents",
      "Model Fine-tuning",
      "Python",
      "TypeScript"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-09-08T19:05:54Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://github.com/microsoft/Generative-AI-For-Beginners/raw/main/images/repo-thumbnailv4-fixed.png"
    ],
    "extra_info": null
  },
  {
    "id": "XLeRobot",
    "source": "GitHub",
    "url": "https://github.com/Vector-Wangel/XLeRobot",
    "title_en": "XLeRobot ü§ñ",
    "summary_en": "XLeRobot is an open-source, low-cost embodied AI dual-arm mobile robot project designed to democratize embodied AI technology. Starting at approximately $660 with less than 4 hours of assembly time, it offers an economical option for individuals and research institutions. Built upon established projects like LeRobot, it supports various control methods including keyboard, Xbox controller, and Switch Joycon. The project provides comprehensive simulation environments and real-world deployment guides, enabling the robot to perform household tasks. XLeRobot aims to accelerate the adoption and application of embodied AI, making advanced robotics accessible to a wider audience.",
    "keywords_en": [
      "Robotics",
      "Embodied AI",
      "Low-cost",
      "Open-source Hardware",
      "Robotic Arm",
      "Household Tasks",
      "Simulation",
      "Sim2Real"
    ],
    "area_en": [
      "Robotics",
      "Artificial Intelligence",
      "AI Agent"
    ],
    "published_time": "2025-09-09T00:22:16Z",
    "download_time": "2024-07-30 12:00:00",
    "visual_resource": [
      "screenshot/github/XLeRobot.png"
    ],
    "extra_info": null
  },
  {
    "id": "2509.04664",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.04664",
    "title_en": "Why Language Models Hallucinate",
    "summary_en": "Like students facing hard exam questions, large language models sometimes\nguess when uncertain, producing plausible yet incorrect statements instead of\nadmitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art\nsystems and undermine trust. We argue that language models hallucinate because\nthe training and evaluation procedures reward guessing over acknowledging\nuncertainty, and we analyze the statistical causes of hallucinations in the\nmodern training pipeline. Hallucinations need not be mysterious -- they\noriginate simply as errors in binary classification. If incorrect statements\ncannot be distinguished from facts, then hallucinations in pretrained language\nmodels will arise through natural statistical pressures. We then argue that\nhallucinations persist due to the way most evaluations are graded -- language\nmodels are optimized to be good test-takers, and guessing when uncertain\nimproves test performance. This \"epidemic\" of penalizing uncertain responses\ncan only be addressed through a socio-technical mitigation: modifying the\nscoring of existing benchmarks that are misaligned but dominate leaderboards,\nrather than introducing additional hallucination evaluations. This change may\nsteer the field toward more trustworthy AI systems.",
    "keywords_en": [
      "Language Models",
      "Hallucination",
      "Training and Evaluation",
      "Statistical Causes",
      "Trustworthy AI"
    ],
    "area_en": [
      "Natural Language Processing",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-09-04T21:26:31.000Z",
    "download_time": "2025-09-08 22:54:13",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04664.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.04664\", \"arxiv_url\": \"https://arxiv.org/abs/2509.04664\"}"
  },
  {
    "id": "2509.05208",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.05208",
    "title_en": "Symbolic Graphics Programming with Large Language Models",
    "summary_en": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding.",
    "keywords_en": [
      "Large Language Models",
      "Symbolic Graphics Programming",
      "SVG",
      "Reinforcement Learning",
      "Cross-modal Grounding"
    ],
    "area_en": [
      "Large Language Model",
      "Multimodal",
      "Generative AI"
    ],
    "published_time": "2025-09-05T16:10:53.000Z",
    "download_time": "2025-09-08 22:54:15",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05208.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.05208\", \"arxiv_url\": \"https://arxiv.org/abs/2509.05208\"}"
  },
  {
    "id": "2509.03680",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.03680",
    "title_en": "LuxDiT: Lighting Estimation with Video Diffusion Transformer",
    "summary_en": "Estimating scene lighting from a single image or video remains a longstanding\nchallenge in computer vision and graphics. Learning-based approaches are\nconstrained by the scarcity of ground-truth HDR environment maps, which are\nexpensive to capture and limited in diversity. While recent generative models\noffer strong priors for image synthesis, lighting estimation remains difficult\ndue to its reliance on indirect visual cues, the need to infer global\n(non-local) context, and the recovery of high-dynamic-range outputs. We propose\nLuxDiT, a novel data-driven approach that fine-tunes a video diffusion\ntransformer to generate HDR environment maps conditioned on visual input.\nTrained on a large synthetic dataset with diverse lighting conditions, our\nmodel learns to infer illumination from indirect visual cues and generalizes\neffectively to real-world scenes. To improve semantic alignment between the\ninput and the predicted environment map, we introduce a low-rank adaptation\nfinetuning strategy using a collected dataset of HDR panoramas. Our method\nproduces accurate lighting predictions with realistic angular high-frequency\ndetails, outperforming existing state-of-the-art techniques in both\nquantitative and qualitative evaluations.",
    "keywords_en": [
      "Lighting Estimation",
      "Video Diffusion Transformer",
      "HDR Environment Maps",
      "Low-rank adaptation",
      "Generative Models"
    ],
    "area_en": [
      "Computer Vision",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-09-03T19:59:20.000Z",
    "download_time": "2025-09-08 22:54:14",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03680.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.03680\", \"arxiv_url\": \"https://arxiv.org/abs/2509.03680\"}"
  },
  {
    "id": "2509.05263",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.05263",
    "title_en": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
    "summary_en": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n90times increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
    "keywords_en": [
      "3D world generation",
      "Multimodal Large Language Model",
      "Interactive world",
      "Physics simulation",
      "Production efficiency"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-09-05T17:22:33.000Z",
    "download_time": "2025-09-08 22:54:15",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05263.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.05263\", \"arxiv_url\": \"https://arxiv.org/abs/2509.05263\"}"
  },
  {
    "id": "2509.02437",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.02437",
    "title_en": "U-ARM : Ultra low-cost general teleoperation interface for robot\n  manipulation",
    "summary_en": "We propose U-Arm, a low-cost and rapidly adaptable leader-follower\nteleoperation framework designed to interface with most of commercially\navailable robotic arms. Our system supports teleoperation through three\nstructurally distinct 3D-printed leader arms that share consistent control\nlogic, enabling seamless compatibility with diverse commercial robot\nconfigurations. Compared with previous open-source leader-follower interfaces,\nwe further optimized both the mechanical design and servo selection, achieving\na bill of materials (BOM) cost of only \\50.5 for the 6-DoF leader arm and\n56.8 for the 7-DoF version. To enhance usability, we mitigate the common\nchallenge in controlling redundant degrees of freedom by %engineering methods\nmechanical and control optimizations. Experimental results demonstrate that\nU-Arm achieves 39\\% higher data collection efficiency and comparable task\nsuccess rates across multiple manipulation scenarios compared with Joycon,\nanother low-cost teleoperation interface. We have open-sourced all CAD models\nof three configs and also provided simulation support for validating\nteleoperation workflows. We also open-sourced real-world manipulation data\ncollected with U-Arm. The project website is\nhttps://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.",
    "keywords_en": [
      "Teleoperation",
      "Robot Manipulation",
      "Low-cost",
      "Open-source",
      "Robotic Arms"
    ],
    "area_en": [
      "Robotics",
      "Artificial Intelligence",
      "AI Agent"
    ],
    "published_time": "2025-09-02T15:39:38.000Z",
    "download_time": "2025-09-08 22:54:14",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02437.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.02437\", \"arxiv_url\": \"https://arxiv.org/abs/2509.02437\"}"
  },
  {
    "id": "2509.04575",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2509.04575",
    "title_en": "Bootstrapping Task Spaces for Self-Improvement",
    "summary_en": "Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training.",
    "keywords_en": [
      "Self-improvement",
      "Reinforcement Learning",
      "Large Language Models",
      "Autocurriculum",
      "Exploratory Iteration"
    ],
    "area_en": [
      "Machine Learning",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-09-04T18:01:00.000Z",
    "download_time": "2025-09-08 22:54:16",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04575.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2509.04575\", \"arxiv_url\": \"https://arxiv.org/abs/2509.04575\"}"
  }
]