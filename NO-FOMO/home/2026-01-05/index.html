<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2026-01-05</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="." class="active">‰∏≠Êñá</a>
                <a href="en/" class="">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2026-01-05</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† ËøîÂõû‰∏ªÈ°µ</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ ÊúÄÊñ∞Êó•Êä•</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ ÂÖ≥‰∫éÊàë‰ª¨</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>X blames users for Grok-generated CSAM; no fixes announced</h2>
                <span class="published-time">Published: 2026-01-05 19:07:35</span>
                
                <p class="summary">A significant controversy has emerged involving X's artificial intelligence model, Grok, which has reportedly been implicated in the generation of Child Sexual Abuse Material (CSAM). Reports indicate that X, the platform formerly known as Twitter, has responded to these allegations by attributing responsibility to its users, rather than acknowledging or addressing potential systemic flaws within its generative AI framework. Crucially, the company has yet to announce any concrete plans or technical fixes to prevent the recurrence of Grok producing such illegal and harmful content. This situation underscores escalating concerns regarding the ethical deployment of AI, content safety protocols, and the accountability of technology platforms for the output of their advanced models. The absence of announced remedial measures by X raises critical questions about its commitment to safeguarding its platform from illegal material and managing the profound ethical and legal challenges associated with AI-generated content. This stance is likely to invite scrutiny from regulators and advocacy groups, demanding clearer responsibilities from AI developers in ensuring their technologies do not facilitate criminal activity.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Grok</span><span>CSAM</span><span>AI Ethics</span><span>Generative AI</span><span>Content Moderation</span><span>AI Safety</span><span>Large Language Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://arstechnica.com/tech-policy/2026/01/x-blames-users-for-grok-generated-csam-no-fixes-announced/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Murder-suicide case shows OpenAI selectively hides data after users die</h2>
                <span class="published-time">Published: 2026-01-05 15:34:40</span>
                
                <p class="summary">A recent murder-suicide case has brought to light significant concerns regarding OpenAI's data retention and privacy policies, specifically how the company manages user data following their death. The incident highlighted an alleged selective approach by OpenAI in either concealing or providing access to deceased users' data, prompting questions about transparency and the ethical responsibilities of AI service providers. Critics contend that OpenAI's actions in this sensitive situation lack consistency, revealing a notable gap in publicly accessible information concerning data access for legal or family representatives of deceased users. This case underscores the urgent need for clear, consistent, and transparent policies from major AI companies like OpenAI regarding the entire lifecycle of user data, particularly in scenarios carrying substantial legal and ethical ramifications. The situation necessitates a broader discussion on digital legacy, data governance, and the legal frameworks governing user information within advanced AI platforms.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>OpenAI</span><span>Data Privacy</span><span>Data Retention Policy</span><span>AI Ethics</span><span>User Data</span><span>Digital Legacy</span><span>ChatGPT</span><span>Transparency</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Natural Language Processing</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://arstechnica.com/tech-policy/2025/12/openai-refuses-to-say-where-chatgpt-logs-go-when-users-die/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>All AI Videos Are Harmful (2025)</h2>
                <span class="published-time">Published: 2026-01-05 13:44:59</span>
                
                <p class="summary">The article, provocatively titled 'All AI Videos Are Harmful (2025),' presents a stark perspective on the impending societal implications of AI-generated video content. While the provided content is concise, the title itself suggests an urgent and comprehensive critique of the technology's future impact by the year 2025. It likely addresses widespread concerns regarding the proliferation of deepfakes, which facilitate sophisticated misinformation and disinformation campaigns capable of manipulating public opinion and eroding trust in visual evidence. The piece would delve into the ethical quandaries surrounding consent, privacy, and intellectual property as AI video generation tools become increasingly accessible and realistic. Furthermore, it likely highlights the potential for reputational damage to individuals and institutions due to fabricated content. The author's strong stance implies a call for immediate and robust regulatory frameworks, along with enhanced digital literacy, to counteract the anticipated pervasive negative consequences of AI video technology, suggesting that without proactive measures, the harm will be widespread and unavoidable.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Video Generation</span><span>Deepfakes</span><span>Misinformation</span><span>Digital Ethics</span><span>Generative AI</span><span>Societal Impact</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Artificial Intelligence</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://idiallo.com/blog/all-ai-videos-are-harmful" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Databases in 2025: A Year in Review</h2>
                <span class="published-time">Published: 2026-01-05 07:14:30</span>
                
                <p class="summary">The article, 'Databases in 2025: A Year in Review,' offers a comprehensive retrospective on the significant advancements and evolving trends within the database landscape throughout the year 2025. It is anticipated to delve into key developments in cloud-native database solutions, the increasing adoption of various specialized data models like document, graph, and time-series databases, and notable performance enhancements across both traditional relational and modern NoSQL systems. The review likely explores innovations in scalability, robust data consistency models, and the growing integration of artificial intelligence and machine learning capabilities for database optimization, automation, and advanced analytics. Furthermore, it is expected to highlight the profound impact of serverless architectures, the continuous growth of open-source contributions, and address emerging challenges in data governance, security, and compliance that are collectively shaping the future trajectory of data management technologies.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Databases</span><span>Cloud Computing</span><span>Data Management</span><span>NoSQL</span><span>Relational Databases</span><span>Scalability</span><span>Data Analytics</span><span>Artificial Intelligence</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Others</span><span>Artificial Intelligence</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Observability's past, present, and future</h2>
                <span class="published-time">Published: 2026-01-05 16:34:22</span>
                
                <p class="summary">This article delves into the evolution, current state, and future trajectory of observability within modern software systems. It traces the journey from traditional monitoring approaches, which often relied on reactive alerts and static thresholds, to the comprehensive observability practices prevalent today. The discussion highlights the necessity of robust observability in navigating the complexities of distributed architectures, microservices, and cloud-native environments, emphasizing the importance of collecting and analyzing metrics, logs, and traces to gain deep insights into system behavior. Looking ahead, the piece explores how the future of observability will be shaped by advancements in Artificial Intelligence and Machine Learning, leading to more proactive, predictive, and autonomous monitoring solutions. These future systems are expected to leverage AI for anomaly detection, root cause analysis, and automated remediation, transforming how organizations understand and maintain the health and performance of their critical applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Observability</span><span>Monitoring</span><span>Distributed Systems</span><span>Microservices</span><span>AIOps</span><span>Site Reliability Engineering</span><span>Cloud Native</span><span>Tracing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://blog.sherwoodcallaway.com/observability-s-past-present-and-future/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Utopian Scholastic</h2>
                <span class="published-time">Published: 2026-01-05 18:29:28</span>
                
                <p class="summary">Given the title 'Utopian Scholastic' and the minimal content provided, this Hacker News story likely delves into philosophical and aspirational perspectives regarding the future of academic pursuits and knowledge acquisition, potentially influenced by advanced technologies such as Artificial Intelligence. It could explore the vision of an ideal learning ecosystem where AI facilitates personalized education, universal access to information, and unbiased scholarly collaboration, moving beyond traditional educational constraints. The narrative might discuss the role of intelligent tutoring systems, advanced knowledge representation, and ethical frameworks necessary to realize a truly 'utopian' academic landscape. Such a piece would provoke contemplation on the societal implications and the research pathways required to achieve such a technologically enhanced scholarly world. While specific technical details are not provided, the discussion broadly centers on the intersection of innovative technology and educational ideals.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI in Education</span><span>Future Learning Systems</span><span>Knowledge Representation</span><span>Intelligent Tutoring Systems</span><span>AI Ethics</span><span>Personalized Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://wol.fm/blog/utopian-scholastic.html" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos</h2>
                <span class="published-time">Published: 2026-01-01T17:07:30.000Z</span>
                
                <p class="summary">In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>4D world model</span><span>4D reconstruction</span><span>Video generation</span><span>Monocular videos</span><span>Scalability</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Generative AI</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.00393" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization</h2>
                <span class="published-time">Published: 2025-12-31T04:17:36.000Z</span>
                
                <p class="summary">Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47%) and GAIA (72.8%) using open-weight models. Our automated generation pipeline achieves over 81% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7% and +5.4% respectively. Moreover, our Agent RL training achieves 40% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35% and 21% on Maths and general/multi-hop QA benchmarks.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLM Agents</span><span>Automated Generation</span><span>Hybrid Policy Optimization</span><span>Reinforcement Learning</span><span>Agent Frameworks</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24615" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</h2>
                <span class="published-time">Published: 2026-01-02T11:58:48.000Z</span>
                
                <p class="summary">Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>talking head generation</span><span>interactive avatars</span><span>diffusion forcing</span><span>real-time interaction</span><span>direct preference optimization</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.00664" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning</h2>
                <span class="published-time">Published: 2025-12-30T16:31:45.000Z</span>
                
                <p class="summary">While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multimodal Agentic Reasoning</span><span>Reinforcement Learning</span><span>Vision-Language Models</span><span>Tool-use</span><span>Multimodal Search</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>AI Agent</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24330" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</h2>
                <span class="published-time">Published: 2025-12-30T14:53:33.000Z</span>
                
                <p class="summary">Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise ell_1 advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multimodal Large Language Models</span><span>Video Understanding</span><span>Hallucinations</span><span>Counterfactual Video Generation</span><span>Diffusion-based Video Editing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Video Understanding</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.24271" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing</h2>
                <span class="published-time">Published: 2026-01-01T04:42:59.000Z</span>
                
                <p class="summary">3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>3D Morphing</span><span>Structured Latent</span><span>Generative AI</span><span>Attention Mechanisms</span><span>Cross-Category Morphing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Deep Learning</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2601.00204" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>