<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-08</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="." class="active">‰∏≠Êñá</a>
                <a href="en/" class="">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-12-08</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../home/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† ËøîÂõû‰∏ªÈ°µ</a>
            <a href="../../daily/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ ÊúÄÊñ∞Êó•Êä•</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ ÂÖ≥‰∫éÊàë‰ª¨</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Deep dive on Nvidia circular funding</h2>
                <span class="published-time">Published: 2025-12-08 18:48:27</span>
                
                <p class="summary">This deep dive article meticulously examines Nvidia's highly effective 'circular funding' model, a pivotal strategy underpinning its unparalleled dominance in the semiconductor and artificial intelligence industries. The analysis elucidates how Nvidia's superior GPU hardware, essential for AI training and high-performance computing, fosters a powerful synergistic ecosystem, particularly through its proprietary CUDA software platform. This comprehensive integration of hardware and software robustly attracts a vast network of developers, researchers, and enterprises, stimulating continuous innovation and solidifying the demand for Nvidia's cutting-edge products. The operational cycle involves the strategic reinvestment of substantial profits into advanced research and development, state-of-the-art manufacturing capabilities, and ongoing software enhancements. This continuous improvement strategy not only fortifies its market leadership but also enables Nvidia to progressively capture and expand its market share across an extensive range of AI-driven applications, spanning from sophisticated data centers to emerging autonomous systems and consumer electronics. The article underscores how this integrated, self-reinforcing strategy has been instrumental in securing Nvidia's long-term competitive advantage.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Nvidia</span><span>GPU Technology</span><span>AI Hardware</span><span>CUDA Platform</span><span>Semiconductor Industry</span><span>Market Dominance</span><span>Technology Ecosystem</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Launch HN: Nia (YC S25) ‚Äì Give better context to coding agents</h2>
                <span class="published-time">Published: 2025-12-08 17:10:14</span>
                
                <p class="summary">Nia (YC S25) has launched a new context layer specifically engineered to augment the capabilities of AI coding agents, including tools like Cursor and Claude Code. This innovation directly tackles a critical limitation of current AI models: their tendency to hallucinate or utilize outdated information due to a lack of real-time, specific context from private code repositories, internal wikis, or precise third-party SDK versions. Nia empowers these AI systems by providing a mechanism to index and query up-to-date codebases and documentation. By supplying grounded and accurate context, Nia prevents agents from confidently suggesting outdated APIs or incorrect framework versions, a common frustration that forces developers to spend considerable time verifying and correcting AI-generated code. This solution aims to drastically improve developer productivity and the reliability of AI assistance in coding. Furthermore, Nia's framework is designed to be applicable beyond coding agents, offering contextual grounding to any AI system that demands accurate, domain-specific information.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Coding Agents</span><span>Contextual AI</span><span>Codebase Management</span><span>Knowledge Retrieval</span><span>Software Development AI</span><span>API Integration</span><span>AI System Grounding</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.trynia.ai/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>We collected 10k hours of neuro-language data in our basement</h2>
                <span class="published-time">Published: 2025-12-08 17:33:13</span>
                
                <p class="summary">A team has announced the successful collection of an extensive neuro-language dataset, comprising 10,000 hours of data. This significant achievement, reportedly undertaken in a 'basement' setting, suggests a potentially independent or resource-efficient approach to large-scale data acquisition. The term 'neuro-language data' indicates a focus on capturing neural activity in conjunction with language processing, which is critical for advancing research in various fields. This includes understanding the neurological underpinnings of language, developing more sophisticated brain-computer interfaces, or training advanced artificial intelligence models that can better interpret or generate human language based on neural correlates. The sheer volume of data collected is noteworthy, as large datasets are often instrumental in driving breakthroughs in data-intensive scientific domains, particularly in deep learning and computational neuroscience. This collection effort could pave the way for novel insights into human cognition and language, offering a valuable resource for the broader scientific community to explore complex interactions between the brain and linguistic functions.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Neuro-language data</span><span>Data collection</span><span>Neuroscience</span><span>Natural Language Processing</span><span>Large datasets</span><span>AI research</span><span>Cognitive science</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://condu.it/thought/10k-hours" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Has the cost of building software just dropped 90%?</h2>
                <span class="published-time">Published: 2025-12-08 19:00:48</span>
                
                <p class="summary">The question of whether software development costs have plummeted by 90% is gaining traction, largely fueled by advancements in artificial intelligence and automation tools. This inquiry suggests a paradigm shift in the software industry, where new technologies, particularly large language models (LLMs) and sophisticated code generation platforms, are significantly streamlining the development lifecycle. Proponents of this drastic cost reduction point to the ability of AI to automate repetitive coding tasks, generate boilerplate code, assist with debugging, and even contribute to design and architecture. Such tools empower developers to achieve higher productivity, reduce the need for extensive manual coding, and potentially lower labor costs. While a 90% drop may be an ambitious figure, the underlying sentiment highlights a growing trend towards substantial efficiency gains and cost savings, transforming traditional software engineering practices and making development more accessible and cost-effective across various scales.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Software Development</span><span>Cost Efficiency</span><span>Artificial Intelligence</span><span>Large Language Models</span><span>Code Generation</span><span>Automation</span><span>Productivity</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>I successfully recreated the 1996 Space Jam website with Claude</h2>
                <span class="published-time">Published: 2025-12-08 15:33:01</span>
                
                <p class="summary">The article details the successful recreation of the iconic 1996 Space Jam website, a significant achievement accomplished with the assistance of the AI model, Claude. This project underscores the evolving capabilities of large language models in practical web development and design tasks, particularly those involving historical web elements and styling. The process likely involved Claude interpreting design specifications, legacy HTML structures, and potentially CSS from the original era, subsequently generating the necessary code to replicate the vintage aesthetic and functionality. This success follows a previously reported failed attempt, indicating either advancements in Claude's abilities or a refined methodology employed by the user. The recreation serves as a compelling case study for AI-assisted development, demonstrating how current AI tools can effectively understand and implement complex, era-specific web projects, offering potential benefits for digital preservation and modernizing legacy web content.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Claude</span><span>AI Assistant</span><span>Web Development</span><span>Website Recreation</span><span>Vintage Web</span><span>HTML</span><span>CSS</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://theahura.substack.com/p/i-successfully-recreated-the-1996" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Microsoft has a problem: lack of demand for its AI products</h2>
                <span class="published-time">Published: 2025-12-08 16:54:31</span>
                
                <p class="summary">Recent reports indicate that Microsoft is encountering a significant challenge with the commercial adoption of its artificial intelligence products, facing a notable lack of demand from potential customers. Despite the company's substantial investments in AI research, development, and strategic partnerships, including its high-profile collaboration with OpenAI, the market appears hesitant to embrace Microsoft's AI offerings at the expected rate. This situation poses a critical problem for the tech giant, which has positioned AI as a cornerstone of its future growth strategy across its cloud services, enterprise software, and consumer products. Analysts suggest potential factors contributing to this sluggish demand could include perceived gaps in product quality, issues with user experience, competitive pressures from a rapidly evolving AI landscape, or perhaps a misalignment between Microsoft's AI solutions and specific market needs. The struggle to convert internal innovations and external partnerships into widespread customer adoption could impact Microsoft's revenue projections and its standing in the increasingly competitive global AI market, prompting a reevaluation of its product development and go-to-market strategies.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Commercialization</span><span>Technology Adoption</span><span>Product Market Fit</span><span>Microsoft AI</span><span>Enterprise Software</span><span>Market Analysis</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows</h2>
                <span class="published-time">Published: 2025-12-03T07:45:46.000Z</span>
                
                <p class="summary">Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>TwinFlow</span><span>One-step generation</span><span>Large Generative Models</span><span>Self-adversarial flows</span><span>Inference efficiency</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.05150" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>EditThinker: Unlocking Iterative Reasoning for Any Image Editor</h2>
                <span class="published-time">Published: 2025-12-05T18:58:09.000Z</span>
                
                <p class="summary">Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Image Editing</span><span>Iterative Reasoning</span><span>Multimodal Large Language Model</span><span>Reinforcement Learning</span><span>AI Agent</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Computer Vision</span><span>Multimodal</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.05965" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</h2>
                <span class="published-time">Published: 2025-12-04T14:01:53.000Z</span>
                
                <p class="summary">We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multimodal Understanding</span><span>Multimodal Generation</span><span>Unified Architecture</span><span>Efficient Autoencoder</span><span>Mixture-of-Experts</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Deep Learning</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.04810" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</h2>
                <span class="published-time">Published: 2025-12-05T18:06:18.000Z</span>
                
                <p class="summary">Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Controllable Video Generation</span><span>Uncertainty Quantification</span><span>World Models</span><span>Robot Learning</span><span>Calibrated Uncertainty</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Computer Vision</span><span>Robotics</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.05927" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs</h2>
                <span class="published-time">Published: 2025-12-05T03:58:04.000Z</span>
                
                <p class="summary">Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>SQ-format</span><span>Post-training quantization</span><span>Large Language Models</span><span>AI accelerators</span><span>Hardware support</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.05409" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding</h2>
                <span class="published-time">Published: 2025-12-05T15:03:48.000Z</span>
                
                <p class="summary">Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Active Video Perception</span><span>Long Video Understanding</span><span>AI Agent</span><span>Multimodal Large Language Models</span><span>Iterative Evidence Seeking</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Video Understanding</span><span>AI Agent</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.05774" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>