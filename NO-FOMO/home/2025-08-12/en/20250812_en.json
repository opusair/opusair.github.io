[
  {
    "id": "twitter_Skywork_ai_1955237399912648842",
    "source": "Twitter",
    "url": "https://x.com/Skywork_ai/status/1955237399912648842",
    "title_en": "Skywork_ai_Launches Matrix-Game 2.0: First Open-Source Real-Time Long-Sequence World Model",
    "summary_en": "Skywork_ai has launched Matrix-Game 2.0, claiming it to be the first open-source, real-time, long-sequence interactive world model. This model achieves a performance of 25 frames per second and supports minutes-long interactions. It aims to challenge the non-open-source nature of DeepMind's Genie 3, providing a fully open solution for the AI world model domain.",
    "keywords_en": [
      "Matrix-Game 2.0",
      "World Model",
      "Open Source",
      "Real-time Interaction",
      "Skywork"
    ],
    "area_en": [
      "Large Language Model",
      "Open Source",
      "Tech News"
    ],
    "published_time": "2025-08-12T11:58:17.000Z",
    "download_time": "2025-08-13 01:06:48",
    "visual_resource": [
      "screenshot/twitter/Skywork_ai_1955237399912648842.png"
    ],
    "extra_info": "{\"username\": \"Skywork_ai\", \"tweet_id\": \"1955237399912648842\"}"
  },
  {
    "id": "twitter_Thom_Wolf_1955213381545132267",
    "source": "Twitter",
    "url": "https://x.com/Thom_Wolf/status/1955213381545132267",
    "title_en": "Thom_Wolf_Significant Progress in Open-Source Visual Reasoning with GLM-4.5V",
    "summary_en": "Thomas Wolf expresses excitement over significant progress in visual reasoning for open-source models, emphasizing its critical importance for Vision-Language Models (VLMs) even more than for text models. He highlights Z.ai's introduction of GLM-4.5V, noting its breakthrough in open-source visual reasoning, achieving state-of-the-art performance and dominating across 41 benchmarks, signaling serious advancements in open-source VLMs.",
    "keywords_en": [
      "Visual Reasoning",
      "Open-Source Models",
      "Multimodal Large Models",
      "GLM-4.5V",
      "Tech Progress"
    ],
    "area_en": [
      "Computer Vision",
      "Multimodal",
      "Open Source"
    ],
    "published_time": "2025-08-12T10:22:50.000Z",
    "download_time": "2025-08-13 01:06:39",
    "visual_resource": [
      "screenshot/twitter/Thom_Wolf_1955213381545132267.png"
    ],
    "extra_info": "{\"username\": \"Thom_Wolf\", \"tweet_id\": \"1955213381545132267\"}"
  },
  {
    "id": "twitter_Miles_Brundage_1955310600696824212",
    "source": "Twitter",
    "url": "https://x.com/Miles_Brundage/status/1955310600696824212",
    "title_en": "Miles_Brundage_Claude Sonnet 4 Context Window Significantly Expanded",
    "summary_en": "Anthropic's Claude Sonnet 4 model has received a significant context window upgrade on its API, now supporting 1 million tokens, a fivefold increase. This enhancement allows users to process over 75,000 lines of code or hundreds of documents in a single request, greatly improving the model's ability to handle long texts and complex tasks, providing developers with a more powerful tool.",
    "keywords_en": [
      "Claude Sonnet 4",
      "Context Window",
      "Large Language Model",
      "Anthropic",
      "API",
      "Long Text Processing"
    ],
    "area_en": [
      "Large Language Model",
      "Tech News",
      "Product Launch"
    ],
    "published_time": "2025-08-12T16:49:09.000Z",
    "download_time": "2025-08-13 01:03:40",
    "visual_resource": [
      "screenshot/twitter/Miles_Brundage_1955310600696824212.png"
    ],
    "extra_info": "{\"username\": \"Miles_Brundage\", \"tweet_id\": \"1955310600696824212\"}"
  },
  {
    "id": "twitter_allen_ai_1955253470962872350",
    "source": "Twitter",
    "url": "https://x.com/allen_ai/status/1955253470962872350",
    "title_en": "allen_ai_Launches MolmoAct: An Open Action Reasoning Model for Physical World Actions",
    "summary_en": "Allen AI has announced the release of MolmoAct, a new, fully open Action Reasoning Model (ARM). This model is designed to enable AI models that operate in the physical world to process and execute human instructions, facilitating more intelligent interactions and automation. The introduction of MolmoAct represents a significant advancement in the fields of embodied AI and robotics control, providing researchers with an open-source tool for further development.",
    "keywords_en": [
      "MolmoAct",
      "Action Reasoning Model",
      "Open Source",
      "Embodied AI",
      "Robotics",
      "Allen AI"
    ],
    "area_en": [
      "AI Agent",
      "Robotics",
      "Open Source"
    ],
    "published_time": "2025-08-12T13:02:08.000Z",
    "download_time": "2025-08-13 01:03:49",
    "visual_resource": [
      "screenshot/twitter/allen_ai_1955253470962872350.png"
    ],
    "extra_info": "{\"username\": \"allen_ai\", \"tweet_id\": \"1955253470962872350\"}"
  },
  {
    "id": "twitter_sama_1955077002945585333",
    "source": "Twitter",
    "url": "https://x.com/sama/status/1955077002945585333",
    "title_en": "sama_Sam Altman Details Compute Prioritization for GPT-5 and Future Expansion Plans",
    "summary_en": "OpenAI CEO Sam Altman outlined the compute prioritization strategy for the coming months due to increased demand from GPT-5. He stated that the company will first ensure more usage for current paying ChatGPT users, then prioritize existing API demand, followed by improving the free tier of ChatGPT, and finally addressing new API demand. Altman also revealed plans to double OpenAI's compute fleet within the next five months to alleviate current compute constraints.",
    "keywords_en": [
      "Sam Altman",
      "OpenAI",
      "GPT-5",
      "Compute Prioritization",
      "ChatGPT",
      "API"
    ],
    "area_en": [
      "Large Language Model",
      "Industry News",
      "Tech News"
    ],
    "published_time": "2025-08-12T01:20:55.000Z",
    "download_time": "2025-08-13 00:38:51",
    "visual_resource": [
      "screenshot/twitter/sama_1955077002945585333.png"
    ],
    "extra_info": "{\"username\": \"sama\", \"tweet_id\": \"1955077002945585333\"}"
  },
  {
    "id": "twitter_NaveenGRao_1955373941813547069",
    "source": "Twitter",
    "url": "https://x.com/NaveenGRao/status/1955373941813547069",
    "title_en": "NaveenGRao_Improving AI Agent Reliability and New Generative Model Evaluation Method",
    "summary_en": "Naveen Rao shared and commented on Jonathan Frankle's insights regarding generative model evaluation. Frankle highlights the limitations of current LLM judges, including their slow speed, high cost, non-determinism, and lack of calibration. He introduces PGRM as a novel evaluation method designed to overcome these drawbacks, ultimately contributing to the enhanced reliability of AI agents.",
    "keywords_en": [
      "AI Agents",
      "Reliability",
      "LLM Judges",
      "Generative Models",
      "Model Evaluation",
      "PGRM"
    ],
    "area_en": [
      "AI Agent",
      "Generative AI",
      "Research Progress"
    ],
    "published_time": "2025-08-12T21:00:51.000Z",
    "download_time": "2025-08-13 01:09:36",
    "visual_resource": [
      "screenshot/twitter/NaveenGRao_1955373941813547069.png"
    ],
    "extra_info": "{\"username\": \"NaveenGRao\", \"tweet_id\": \"1955373941813547069\"}"
  },
  {
    "id": "Mcm4SAJ1bfwES3o_RDxxQA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Mcm4SAJ1bfwES3o_RDxxQA",
    "title_en": "Online Reinforcement Learning + Flow Matching Model! Flow-GRPO: The First Online RL-Driven Flow Matching Generative Model",
    "summary_en": "Flow-GRPO marks a pioneering advancement by integrating online reinforcement learning into Flow Matching generative models. This innovative framework transforms deterministic Ordinary Differential Equation (ODE) samplers into equivalent Stochastic Differential Equation (SDE) samplers, thereby introducing the necessary stochasticity for online RL application and optimizing the denoising process. By incorporating the GRPO algorithm, Flow-GRPO effectively addresses the inherent challenges of Flow Models in handling complex scenarios such as multi-object composition, spatial relationships, and accurate text rendering. Experimental results demonstrate that Flow-GRPO significantly enhances the accuracy and human preference alignment of text-to-image (T2I) generation. Furthermore, its novel Denoising Reduction strategy substantially accelerates the training process without compromising output quality. This breakthrough opens new avenues for developing high-quality generative models capable of producing intricate and precise visual content.",
    "keywords_en": [
      "Flow-GRPO",
      "Flow Matching",
      "Online Reinforcement Learning",
      "Generative Model",
      "Text-to-Image",
      "SDE"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Machine Learning"
    ],
    "published_time": "2025-08-12T14:01:41.000Z",
    "download_time": "2025-08-13T15:11:29.206425",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Mcm4SAJ1bfwES3o_RDxxQA.png"
    ],
    "extra_info": null
  },
  {
    "id": "wY5vW-f66pbSWwpt_G_P_g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/wY5vW-f66pbSWwpt_G_P_g",
    "title_en": "Tencent Open-Sources Stand-In! A New Breakthrough in Lightweight Identity-Preserving Video Generation",
    "summary_en": "Tencent has open-sourced Stand-In, a lightweight, plug-and-play framework designed to address the challenges of large training parameters and insufficient compatibility in high-fidelity identity-preserving video generation. By integrating a conditional image branch and a restricted self-attention mechanism, Stand-In achieves precise identity control and cross-branch information interaction with minimal training samples and negligible additional parameters. This framework demonstrates state-of-the-art performance in identity-preserving text-to-video generation, excelling in both video quality and identity consistency. Furthermore, Stand-In boasts strong compatibility, seamlessly extending to various applications such as topic-based generation, pose-guided video creation, stylization, and face swapping, showcasing its immense potential in the generative AI domain.",
    "keywords_en": [
      "Stand-In",
      "Identity Preservation",
      "Video Generation",
      "Generative AI",
      "Lightweight",
      "Self-Attention"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-08-12T06:43:33.000Z",
    "download_time": "2025-08-13T15:11:39.327380",
    "visual_resource": [
      "screenshot/wechat/wechat_image_wY5vW-f66pbSWwpt_G_P_g.png"
    ],
    "extra_info": null
  },
  {
    "id": "ZOVfNYnb3sswD4dtNrSy6A",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ZOVfNYnb3sswD4dtNrSy6A",
    "title_en": "Physics' \"AlphaGo Moment\"? AI Solves 40-Year Unfinished Problem, Top Physicists Stunned",
    "summary_en": "Artificial intelligence has achieved groundbreaking advancements in physics, marking what is being called the \"AlphaGo moment for physics.\" AI successfully designed counter-intuitive and highly complex optical component layouts, significantly boosting the sensitivity of the LIGO gravitational wave detector by 10-15%, a remarkable feat that addresses a challenge puzzling physicists for decades. Furthermore, AI independently redesigned a quantum entanglement experiment, discovered a more accurate dark matter formula than those previously proposed by human scientists, and even independently rediscovered \"Lorentz symmetry,\" a fundamental cornerstone of Einstein's theory of relativity, without prior physical knowledge. These compelling cases demonstrate that AI is rapidly evolving from a mere computational tool into an indispensable and powerful scientific collaborator, signaling the imminent arrival of an era where AI profoundly assists in discovering entirely new physics.",
    "keywords_en": [
      "Artificial Intelligence",
      "Physics",
      "Gravitational Wave Detection",
      "Quantum Entanglement",
      "Scientific Discovery"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Deep Learning",
      "Generative AI"
    ],
    "published_time": "2025-08-12T04:16:31.000Z",
    "download_time": "2025-08-13T15:11:50.081734",
    "visual_resource": [
      "screenshot/wechat/wechat_image_ZOVfNYnb3sswD4dtNrSy6A.png"
    ],
    "extra_info": null
  },
  {
    "id": "hUNjk1O2clhj9B4BPnQLTA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/hUNjk1O2clhj9B4BPnQLTA",
    "title_en": "GitHub No Longer Independent: CEO Resigns, Microsoft Integrates into CoreAI, Signaling a Paradigm Shift for Developers",
    "summary_en": "GitHub CEO Thomas Dohmke has announced his resignation, and GitHub will no longer operate independently, instead being fully integrated into Microsoft's newly formed CoreAI engineering group. This strategic move signifies GitHub's transformation from a mere code hosting platform into Microsoft's \"AI agent factory\" and \"AI training ground,\" aiming to drive an \"AI-first\" software development paradigm through tools like Copilot. Microsoft's deep integration of GitHub into its AI division suggests a future where developers may increasingly supervise AI-generated code rather than writing it from scratch. This profound change is not merely a personnel shift but a complete reorientation of the software development model. Developers must adapt to GitHub's new role as a core component of Microsoft's AI arsenal. The article highlights GitHub's significant growth under Dohmke's leadership, with over 1.5 billion developers and 1 billion repositories, and Copilot's success as a leading AI coding assistant. Dohmke's departure to pursue new entrepreneurial ventures underscores the completion of his mission to lead GitHub into the AI era. This integration solidifies Microsoft's vision of embedding AI assistants across its product ecosystem, positioning GitHub at the forefront of this \"Copilotization\" strategy.",
    "keywords_en": [
      "GitHub",
      "Microsoft",
      "Artificial Intelligence",
      "Copilot",
      "AI Agent",
      "Software Development Paradigm"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Generative AI"
    ],
    "published_time": "2025-08-12T04:16:31.000Z",
    "download_time": "2025-08-13T15:11:54.334362",
    "visual_resource": [
      "screenshot/wechat/wechat_image_hUNjk1O2clhj9B4BPnQLTA.png"
    ],
    "extra_info": null
  },
  {
    "id": "DBCbTGlMQ-vRuwEPtko5Ew",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/DBCbTGlMQ-vRuwEPtko5Ew",
    "title_en": "LLMs Overcomplicate Simple Tasks, Karpathy Frustrated: Some Tasks Don't Need That Much Thought",
    "summary_en": "The article highlights a growing issue where large language models (LLMs), despite their advanced “deep thinking” capabilities enabled by reasoning and Chain of Thought, increasingly overcomplicate simple tasks. Experts like Andrej Karpathy observe that LLMs, by default, exhibit an excessive “agentic” tendency, engaging in lengthy reasoning processes for straightforward queries. This leads to slow responses and inefficiency, particularly evident in coding and image editing applications. The phenomenon is attributed to LLMs being heavily optimized for long-duration, complex task benchmarks, causing them to treat all tasks as high-stakes “exams.” The piece emphasizes the critical need for users to have a mechanism to explicitly specify the required depth of thought for a given task, thereby preventing unnecessary overthinking and enhancing the practical utility of LLMs. This adjustment is crucial for improving user experience and operational efficiency across various applications.",
    "keywords_en": [
      "Large Language Models",
      "Overthinking",
      "Chain of Thought",
      "AI Agent",
      "Andrej Karpathy"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-12T03:08:20.000Z",
    "download_time": "2025-08-13T15:11:52.770641",
    "visual_resource": [
      "screenshot/wechat/wechat_image_DBCbTGlMQ-vRuwEPtko5Ew.png"
    ],
    "extra_info": null
  },
  {
    "id": "SpfmMPU_fsRIzUcHC1Dasw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/SpfmMPU_fsRIzUcHC1Dasw",
    "title_en": "Zhipu Open-Sources GLM-4.5V, Unveiling Visual Reasoning Capabilities Rivaling OpenAI's",
    "summary_en": "Zhipu AI has open-sourced its flagship visual reasoning model, GLM-4.5V, demonstrating exceptional performance across various visual tasks. The model notably defeated 99.99% of human players in the \"GeoGuessr\" game, showcasing its powerful visual inference capabilities from subtle cues. GLM-4.5V excels in advanced image recognition, long video understanding, GUI Agent applications, and complex document and chart interpretation. With 106 billion total parameters, it employs an advanced architecture and a three-stage training strategy, achieving state-of-the-art open-source performance across 41 public visual multimodal benchmarks. This open-sourcing initiative aims to shift the focus of AI development from benchmark competition to practical application, providing developers with a robust multimodal foundation model to collaboratively shape the future of AI.",
    "keywords_en": [
      "Zhipu",
      "GLM-4.5V",
      "Visual Reasoning",
      "Open-Source",
      "Multimodal",
      "AI Agent"
    ],
    "area_en": [
      "Multimodal",
      "Computer Vision",
      "Large Language Model"
    ],
    "published_time": "2025-08-12T03:08:20.000Z",
    "download_time": "2025-08-13T15:12:12.591730",
    "visual_resource": [
      "screenshot/wechat/wechat_image_SpfmMPU_fsRIzUcHC1Dasw.png"
    ],
    "extra_info": null
  },
  {
    "id": "poml",
    "source": "GitHub",
    "url": "https://github.com/microsoft/poml",
    "title_en": "POML: Prompt Orchestration Markup Language",
    "summary_en": "POML (Prompt Orchestration Markup Language) is an innovative markup language specifically designed to bring structure, maintainability, and versatility to advanced prompt engineering for Large Language Models (LLMs). It effectively addresses prevalent challenges in prompt development, including the lack of inherent structure, complexities in data integration, sensitivity to output formats, and insufficient tooling. POML achieves this by providing a systematic approach that includes structured prompting with semantic components like <role> and <task>, comprehensive data handling for diverse types such as documents and images, decoupled presentation styling via a CSS-like system, and a robust integrated templating engine supporting variables and conditionals. This comprehensive framework, complemented by a rich development toolkit including a VS Code extension and SDKs, empowers developers to create more sophisticated, reliable, and easily maintainable LLM applications by organizing prompt components and managing variations efficiently.",
    "keywords_en": [
      "Prompt Engineering",
      "Large Language Models",
      "Markup Language",
      "Structured Prompting",
      "Data Integration",
      "Templating Engine",
      "Development Toolkit"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-13T05:02:40Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "https://i3.ytimg.com/vi/b9WDcFsKixo/maxresdefault.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "abogen",
    "source": "GitHub",
    "url": "https://github.com/denizsafak/abogen",
    "title_en": "abogen",
    "summary_en": "Abogen is a powerful and efficient text-to-speech conversion tool that seamlessly transforms ePub, PDF, or plain text files into high-quality audio, complete with perfectly synchronized subtitles, all within seconds. Built upon the advanced Kokoro-82M model, this application provides a comprehensive suite of features, including support for multiple audio and subtitle output formats, a sophisticated voice mixer for custom voice creation, and a robust queue mode for batch processing. Its capabilities extend to intelligent chapter marking and metadata tagging, making it exceptionally versatile for diverse applications. Abogen is an ideal solution for generating professional audiobooks, creating engaging voiceovers for social media platforms like Instagram, YouTube, and TikTok, or any project demanding natural-sounding speech synthesis. With straightforward cross-platform installation options and Docker deployment support, Abogen significantly streamlines and enhances content creation workflows for users across various operating environments.",
    "keywords_en": [
      "Text-to-Speech",
      "Audiobook Generation",
      "Voice Synthesis",
      "Subtitle Generation",
      "File Conversion",
      "Cross-platform"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Others"
    ],
    "published_time": "2025-08-12T13:49:56Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png",
      "https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif",
      "https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png"
    ],
    "extra_info": null
  },
  {
    "id": "gpt4all",
    "source": "GitHub",
    "url": "https://github.com/nomic-ai/gpt4all",
    "title_en": "GPT4All",
    "summary_en": "GPT4All is an open-source project designed to enable large language models (LLMs) to run privately on everyday desktops and laptops, eliminating the need for API calls or dedicated GPUs. It offers both a desktop application and a Python client, supporting DeepSeek R1 distilled models, GGUF format, and Nomic Vulkan for GPU inference. The project focuses on private data processing through local LLM deployment and integrates with tools like Langchain, providing a convenient and efficient local AI solution for individual users and developers.",
    "keywords_en": [
      "GPT4All",
      "Large Language Model",
      "Local Deployment",
      "Private AI",
      "LLM",
      "llama.cpp",
      "GGUF",
      "Vulkan"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-05-27T19:46:52Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "screenshot/github/gpt4all.png"
    ],
    "extra_info": null
  },
  {
    "id": "notebooks",
    "source": "GitHub",
    "url": "https://github.com/unslothai/notebooks",
    "title_en": "Fine-tuning Notebooks",
    "summary_en": "This GitHub repository, maintained by Unsloth AI, offers an extensive and practical collection of fine-tuning notebooks specifically designed to simplify the development and deployment of various advanced artificial intelligence models. It provides comprehensive, guided examples for working with Large Language Models (LLMs), sophisticated multimodal systems, Text-to-Speech (TTS) applications, and cutting-edge computer vision models. Users can seamlessly leverage these well-structured notebooks on popular cloud-based platforms such as Google Colab and Kaggle, enabling streamlined data preparation, highly efficient model training, thorough performance evaluation, and seamless model saving. The project actively fosters community engagement through clear and accessible contribution guidelines, continuously encouraging the expansion of its diverse model and use-case coverage. This makes it an invaluable and versatile resource for AI developers and researchers aiming to achieve high-performance model fine-tuning with ease.",
    "keywords_en": [
      "Model Fine-tuning",
      "Large Language Models",
      "Multimodal",
      "Text-to-Speech",
      "Computer Vision",
      "Natural Language Processing",
      "Deep Learning",
      "AI Development Tools"
    ],
    "area_en": [
      "Deep Learning",
      "Large Language Model",
      "Multimodal"
    ],
    "published_time": "2025-08-09T21:22:18Z",
    "download_time": "2024-07-30 12:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png",
      "https://raw.githubusercontent.com/unslothai/unsloth/main/images/start%20free%20finetune%20button.png",
      "https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png"
    ],
    "extra_info": null
  },
  {
    "id": "system-prompts-and-models-of-ai-tools",
    "source": "GitHub",
    "url": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
    "title_en": "FULL v0, Cursor, Manus, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent, VSCode Agent, Dia Browser, Trae AI, Cluely, Perplexity, Xcode, Spawn & Orchids.app (And other Open Sourced) System Prompts, Tools & AI Models",
    "summary_en": "This GitHub repository serves as a comprehensive collection of AI system prompts and models, encompassing system instructions and underlying model information from prominent AI tools such as Cursor, Devin, Replit Agent, and VSCode Agent. It offers over 9000 lines of in-depth insights into the structure and functionality of these AI systems. The project aims to provide early access to the latest instructions via its Discord community and emphasizes the critical importance of data security for AI startups, offering security audit services. This resource is highly valuable for researching the internal mechanisms of AI tools, conducting reverse engineering, and enhancing AI system security.",
    "keywords_en": [
      "System Prompts",
      "AI Models",
      "AI Agent",
      "Reverse Engineering",
      "AI Security",
      "Large Language Model",
      "Developer Tools"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-12T11:11:26Z",
    "download_time": "2024-05-15 10:30:00",
    "visual_resource": [
      "https://img.shields.io/discord/1402660735833604126?label=LeaksLab%20Discord&logo=discord&style=for-the-badge",
      "https://trendshift.io/api/badge/repositories/14084",
      "https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date"
    ],
    "extra_info": null
  },
  {
    "id": "buttercup",
    "source": "GitHub",
    "url": "https://github.com/trailofbits/buttercup",
    "title_en": "Buttercup Cyber Reasoning System (CRS)",
    "summary_en": "Buttercup is a sophisticated Cyber Reasoning System (CRS) developed by Trail of Bits for the DARPA AIxCC program, specifically designed to automate the identification and remediation of software vulnerabilities in open-source code repositories. The system leverages advanced AI/ML-assisted fuzzing campaigns, built on frameworks like oss-fuzz, to proactively discover security flaws. Once vulnerabilities are found, Buttercup conducts in-depth analysis and deploys a multi-agent AI-driven patcher to autonomously generate and apply effective repairs. Its modular architecture includes an Orchestrator for workflow management, a Seed Generator, a Fuzzer for vulnerability discovery, a Program Model for code analysis, and a Patcher for remediation. Supporting C and Java source code that is OSS-Fuzz compatible, Buttercup significantly enhances software supply chain security and streamlines automated vulnerability management processes.",
    "keywords_en": [
      "Cyber Reasoning System",
      "Vulnerability Patching",
      "Fuzzing",
      "Artificial Intelligence",
      "Software Security",
      "Open Source",
      "Automation",
      "Patch Generation"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "AI Agent"
    ],
    "published_time": "2025-08-09T18:13:28Z",
    "download_time": "2024-07-30 12:00:00",
    "visual_resource": [
      "screenshot/github/buttercup.png"
    ],
    "extra_info": null
  },
  {
    "id": "2508.08189",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.08189",
    "title_en": "Reinforcement Learning in Vision: A Survey",
    "summary_en": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
    "keywords_en": [
      "Reinforcement Learning",
      "Visual Intelligence",
      "Policy Optimization",
      "Large Language Models",
      "Visual Generation"
    ],
    "area_en": [
      "Computer Vision",
      "Machine Learning",
      "Multimodal"
    ],
    "published_time": "2025-08-11T17:08:55.000Z",
    "download_time": "2025-08-12 18:20:35",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08189.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.08189\", \"arxiv_url\": \"https://arxiv.org/abs/2508.08189\"}"
  },
  {
    "id": "2508.07407",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.07407",
    "title_en": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm\n  Bridging Foundation Models and Lifelong Agentic Systems",
    "summary_en": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems.",
    "keywords_en": [
      "Self-Evolving AI Agents",
      "Foundation Models",
      "Lifelong Agentic Systems",
      "Agent Evolution",
      "Survey"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-10T16:07:32.000Z",
    "download_time": "2025-08-12 18:20:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07407.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.07407\", \"arxiv_url\": \"https://arxiv.org/abs/2508.07407\"}"
  },
  {
    "id": "2508.07981",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.07981",
    "title_en": "Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation",
    "summary_en": "Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.",
    "keywords_en": [
      "Visual Effects",
      "Spatially-Controllable",
      "Unified Framework",
      "LoRA-MoE",
      "Spatial-Aware Prompt"
    ],
    "area_en": [
      "Computer Vision",
      "Generative AI",
      "Deep Learning"
    ],
    "published_time": "2025-08-11T13:41:24.000Z",
    "download_time": "2025-08-12 18:20:36",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07981.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.07981\", \"arxiv_url\": \"https://arxiv.org/abs/2508.07981\"}"
  },
  {
    "id": "2508.07999",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.07999",
    "title_en": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
    "summary_en": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/",
    "keywords_en": [
      "AI Agent",
      "Information Seeking",
      "Benchmark",
      "Large Language Models",
      "Automated Search"
    ],
    "area_en": [
      "AI Agent",
      "Large Language Model",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-11T14:03:09.000Z",
    "download_time": "2025-08-12 18:20:35",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07999.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.07999\", \"arxiv_url\": \"https://arxiv.org/abs/2508.07999\"}"
  },
  {
    "id": "2508.07917",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.07917",
    "title_en": "MolmoAct: Action Reasoning Models that can Reason in Space",
    "summary_en": "Reasoning is central to purposeful action, yet most robotic foundation models\nmap perception and instructions directly to control, which limits adaptability,\ngeneralization, and semantic grounding. We introduce Action Reasoning Models\n(ARMs), a class of vision-language-action models that integrate perception,\nplanning, and control through a structured three-stage pipeline. Our model,\nMolmoAct, encodes observations and instructions into depth-aware perception\ntokens, generates mid-level spatial plans as editable trajectory traces, and\npredicts precise low-level actions, enabling explainable and steerable\nbehavior. MolmoAct-7B-D achieves strong performance across simulation and\nreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching\ntasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on\nLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;\nand in real-world fine-tuning, an additional 10% (single-arm) and an additional\n22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines\nby an additional 23.3% on out-of-distribution generalization and achieves top\nhuman-preference scores for open-ended instruction following and trajectory\nsteering. Furthermore, we release, for the first time, the MolmoAct Dataset --\na mid-training robot dataset comprising over 10,000 high quality robot\ntrajectories across diverse scenarios and tasks. Training with this dataset\nyields an average 5.5% improvement in general performance over the base model.\nWe release all model weights, training code, our collected dataset, and our\naction reasoning dataset, establishing MolmoAct as both a state-of-the-art\nrobotics foundation model and an open blueprint for building ARMs that\ntransform perception into purposeful action through structured reasoning.\nBlogpost: https://allenai.org/blog/molmoact",
    "keywords_en": [
      "Action Reasoning Models",
      "Robotics Foundation Models",
      "Spatial Plans",
      "Vision-Language-Action Models",
      "MolmoAct Dataset"
    ],
    "area_en": [
      "Robotics",
      "Multimodal",
      "Large Language Model"
    ],
    "published_time": "2025-08-11T12:32:45.000Z",
    "download_time": "2025-08-12 18:20:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07917.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.07917\", \"arxiv_url\": \"https://arxiv.org/abs/2508.07917\"}"
  },
  {
    "id": "2508.07785",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.07785",
    "title_en": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts",
    "summary_en": "The Mixture of Experts (MoE) architecture is a cornerstone of modern\nstate-of-the-art (SOTA) large language models (LLMs). MoE models facilitate\nscalability by enabling sparse parameter activation. However, traditional MoE\narchitecture uses homogeneous experts of a uniform size, activating a fixed\nnumber of parameters irrespective of input complexity and thus limiting\ncomputational efficiency. To overcome this limitation, we introduce Grove MoE,\na novel architecture incorporating experts of varying sizes, inspired by the\nheterogeneous big.LITTLE CPU architecture. This architecture features novel\nadjugate experts with a dynamic activation mechanism, enabling model capacity\nexpansion while maintaining manageable computational overhead. Building on this\narchitecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs\ndeveloped by applying an upcycling strategy to the Qwen3-30B-A3B-Base model\nduring mid-training and post-training. GroveMoE models dynamically activate\n3.14-3.28B parameters based on token complexity and achieve performance\ncomparable to SOTA open-source models of similar or even larger size.",
    "keywords_en": [
      "Mixture of Experts",
      "Large Language Models",
      "Sparse Activation",
      "Adjugate Experts",
      "Dynamic Activation"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-08-11T09:15:36.000Z",
    "download_time": "2025-08-12 18:20:34",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07785.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.07785\", \"arxiv_url\": \"https://arxiv.org/abs/2508.07785\"}"
  }
]