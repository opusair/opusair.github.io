[
  {
    "id": "hackernews_46377597",
    "source": "Hacker News",
    "url": "https://github.com/VibiumDev/vibium",
    "title": "Show HN: Vibium â€“ Browser automation for AI and humans, by Selenium's creator",
    "summary": "Vibium, a new browser automation tool, has been launched by the original creator of Selenium, addressing modern development needs with a specific focus on AI agents. After initiating the Selenium project 21 years ago, the developer has reimagined browser automation from the ground up, integrating considerations for AI-driven workflows. Vibium operates with a Go binary under the hood, managing browser interactions, BiDi (Bidirectional Protocol), and MCP (Multi-Client Protocol), but simplifies the developer experience through a straightforward npm install command. Future support for Python and Java is planned. The project emphasizes ease of use for AI agents, as demonstrated by its integration with Claude code, allowing for direct interaction with the Vibium framework. This initial V1 release aims to provide a robust, AI-centric solution for automated browser interactions, offering a modern alternative for web automation tasks.",
    "keywords": [
      "Vibium",
      "Browser Automation",
      "AI Agents",
      "Selenium",
      "Web Automation",
      "Development Tools",
      "Go programming language"
    ],
    "area": [
      "AI Agent",
      "Artificial Intelligence",
      "Others"
    ],
    "published_time": "2025-12-24 17:49:02",
    "download_time": "2025-12-24 20:00:30",
    "extra_info": "{\"score\": 76, \"by\": \"hugs\", \"descendants\": 35, \"story_id\": 46377597}"
  },
  {
    "id": "hackernews_46377537",
    "source": "Hacker News",
    "url": "https://learn.microsoft.com/en-us/agent-framework/overview/agent-framework-overview",
    "title": "Microsoft Agent Framework",
    "summary": "The Microsoft Agent Framework emerges as a pivotal development toolkit, empowering developers to construct sophisticated artificial intelligence agents, predominantly by harnessing the capabilities of large language models (LLMs) within the expansive Azure cloud environment. This framework is meticulously engineered to streamline the creation of intelligent applications, addressing fundamental challenges inherent in agent development, including advanced prompt engineering, seamless integration with external tools and APIs, and effective management of conversational state and long-term memory. It empowers agents to autonomously execute complex workflows by decomposing problems into manageable sub-tasks, interacting dynamically with diverse external systems, and preserving crucial contextual information across interactions. Prioritizing modularity and extensibility, the framework allows for extensive customization of agent behaviors and the integration of a wide array of AI services. This strategic approach is poised to significantly accelerate the deployment of robust, enterprise-grade AI solutions, offering a structured and reliable methodology for orchestrating LLM-powered applications and enhancing overall agent performance and dependability in practical, real-world deployments.",
    "keywords": [
      "AI Agent",
      "Large Language Model",
      "Microsoft Azure",
      "Agent Framework",
      "Prompt Engineering",
      "Orchestration",
      "Generative AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-12-24 17:41:55",
    "download_time": "2025-12-24 20:00:45",
    "extra_info": "{\"score\": 29, \"by\": \"mooreds\", \"descendants\": 18, \"story_id\": 46377537}"
  },
  {
    "id": "hackernews_46372153",
    "source": "Hacker News",
    "url": "https://armeet.bearblog.dev/becoming-the-machine/",
    "title": "Don't Become the Machine",
    "summary": "The article, provocatively titled \"Don't Become the Machine,\" serves as a cautionary reflection on the potential erosion of human qualities in an increasingly automated and AI-driven world. Despite its minimalistic content, the title suggests an important discourse on maintaining human agency, critical thinking, and individuality amidst pervasive technological advancement. It implicitly urges individuals to resist blindly adopting machine-like efficiencies or allowing algorithms to dictate human thought and behavior entirely. The message highlights the significance of preserving creativity, emotional intelligence, and nuanced judgment, which are inherent human attributes distinct from artificial systems. This piece encourages a balanced and conscious interaction with technology, ensuring that tools enhance human capabilities without inadvertently diminishing our essential humanity. It aligns with broader discussions on AI ethics, responsible innovation, and the future of human-computer interaction.",
    "keywords": [
      "Artificial Intelligence",
      "Human Agency",
      "AI Ethics",
      "Automation",
      "Critical Thinking",
      "Human-Computer Interaction"
    ],
    "area": [
      "Artificial Intelligence",
      "AI Agent",
      "Others"
    ],
    "published_time": "2025-12-24 03:34:31",
    "download_time": "2025-12-24 20:00:53",
    "extra_info": "{\"score\": 197, \"by\": \"armeet\", \"descendants\": 116, \"story_id\": 46372153}"
  },
  {
    "id": "hackernews_46374018",
    "source": "Hacker News",
    "url": "https://blog.google/technology/ai/2025-research-breakthroughs/",
    "title": "Google 2025 recap: Research breakthroughs of the year",
    "summary": "This article presents a comprehensive recap of Google's significant research breakthroughs throughout 2025, detailing advancements that have shaped the landscape of artificial intelligence and related technological fields. The annual review highlights pivotal progress in foundational AI methodologies, encompassing innovations in large language models, sophisticated multimodal AI systems, and novel machine learning paradigms. It explores how Google's dedicated research efforts have yielded substantial improvements in areas such as contextual understanding, creative content generation, and efficient problem-solving across complex datasets. The recap is expected to showcase the most impactful scientific contributions from Google's global research teams, underscoring their role in pushing the boundaries of AI, robotics, and other emerging technologies, thereby influencing future applications in diverse sectors from personalized computing to scientific discovery and societal well-being.",
    "keywords": [
      "AI Research",
      "Google AI",
      "Machine Learning",
      "Technological Innovation",
      "Research Breakthroughs",
      "Annual Review",
      "Future of AI"
    ],
    "area": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-12-24 09:30:58",
    "download_time": "2025-12-24 20:01:01",
    "extra_info": "{\"score\": 148, \"by\": \"Anon84\", \"descendants\": 113, \"story_id\": 46374018}"
  },
  {
    "id": "2512.20619",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.20619",
    "title": "SemanticGen: Video Generation in Semantic Space",
    "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.",
    "keywords": [
      "Video Generation",
      "Semantic Space",
      "Diffusion Models",
      "Computational Efficiency",
      "Long Video Generation"
    ],
    "area": [
      "Generative AI",
      "Deep Learning",
      "Computer Vision"
    ],
    "published_time": "2025-12-23T18:59:56.000Z",
    "download_time": "2025-12-24 12:01:12",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.20619\", \"arxiv_url\": \"https://arxiv.org/abs/2512.20619\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png\", \"original_title\": \"SemanticGen: Video Generation in Semantic Space\"}"
  },
  {
    "id": "2512.20618",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.20618",
    "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
    "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
    "keywords": [
      "multi-agent reasoning",
      "long videos",
      "multimodal LLMs",
      "video understanding",
      "reinforcement learning"
    ],
    "area": [
      "AI Agent",
      "Multimodal",
      "Video Understanding"
    ],
    "published_time": "2025-12-23T18:59:49.000Z",
    "download_time": "2025-12-24 12:01:12",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.20618\", \"arxiv_url\": \"https://arxiv.org/abs/2512.20618\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20618.png\", \"original_title\": \"LongVideoAgent: Multi-Agent Reasoning with Long Videos\"}"
  },
  {
    "id": "2512.20617",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.20617",
    "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
    "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
    "keywords": [
      "Spatial Abilities",
      "Multimodal LLMs",
      "Cognitive Science",
      "Hierarchical Benchmark",
      "Reinforcement Learning"
    ],
    "area": [
      "Multimodal",
      "Large Language Model",
      "Deep Learning"
    ],
    "published_time": "2025-12-23T18:59:46.000Z",
    "download_time": "2025-12-24 12:01:15",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.20617\", \"arxiv_url\": \"https://arxiv.org/abs/2512.20617\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20617.png\", \"original_title\": \"SpatialTree: How Spatial Abilities Branch Out in MLLMs\"}"
  },
  {
    "id": "2512.20615",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.20615",
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "summary": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",
    "keywords": [
      "Video Avatars",
      "Active Intelligence",
      "World Modeling",
      "Goal-directed Planning",
      "Internal World Model"
    ],
    "area": [
      "AI Agent",
      "Generative AI",
      "Computer Vision"
    ],
    "published_time": "2025-12-23T18:59:16.000Z",
    "download_time": "2025-12-24 12:01:14",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.20615\", \"arxiv_url\": \"https://arxiv.org/abs/2512.20615\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20615.png\", \"original_title\": \"Active Intelligence in Video Avatars via Closed-loop World Modeling\"}"
  },
  {
    "id": "2512.20491",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.20491",
    "title": "Step-DeepResearch Technical Report",
    "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
    "keywords": [
      "Deep Research",
      "LLM Agents",
      "Data Synthesis",
      "Benchmarking",
      "AI Training"
    ],
    "area": [
      "Large Language Model",
      "AI Agent",
      "Natural Language Processing"
    ],
    "published_time": "2025-12-23T16:32:27.000Z",
    "download_time": "2025-12-24 12:01:14",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.20491\", \"arxiv_url\": \"https://arxiv.org/abs/2512.20491\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20491.png\", \"original_title\": \"Step-DeepResearch Technical Report\"}"
  },
  {
    "id": "2512.20182",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2512.20182",
    "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
    "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
    "keywords": [
      "Faithfulness Hallucination",
      "Large Language Models",
      "Hallucination Detection",
      "Retrieval-Augmented Generation",
      "Explainable AI"
    ],
    "area": [
      "Large Language Model",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-12-23T09:20:32.000Z",
    "download_time": "2025-12-24 12:01:13",
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2512.20182\", \"arxiv_url\": \"https://arxiv.org/abs/2512.20182\", \"thumbnail\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20182.png\", \"original_title\": \"FaithLens: Detecting and Explaining Faithfulness Hallucination\"}"
  }
]