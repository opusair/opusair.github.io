[
  {
    "id": "twitter_ammar__khairi_1960765201311010844",
    "source": "Twitter",
    "url": "https://x.com/ammar__khairi/status/1960765201311010844",
    "title_en": "ammar__khairi_New LLM Test-Time Compute Optimization Method to Appear at EMNLP2025",
    "summary_en": "Ammar Khairi's team announced their research on scaling test-time compute for Large Language Models (LLMs), dubbed the \"LLMonade recipe,\" has been accepted by the EMNLP 2025 Main Conference. This method combines strategic sampling with novel selection to significantly boost LLM inference performance without requiring extra training or special reward models, aiming for more efficient resource utilization and superior results.",
    "keywords_en": [
      "LLM",
      "Inference Optimization",
      "EMNLP",
      "Sampling",
      "Large Language Model",
      "Performance Enhancement"
    ],
    "area_en": [
      "Natural Language Processing",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-08-27T18:03:47.000Z",
    "download_time": "2025-08-28 02:02:18",
    "visual_resource": [
      "screenshot/twitter/ammar__khairi_1960765201311010844.png"
    ],
    "extra_info": "{\"username\": \"ammar__khairi\", \"tweet_id\": \"1960765201311010844\"}"
  },
  {
    "id": "twitter__KarenHao_1960685718621315482",
    "source": "Twitter",
    "url": "https://x.com/_KarenHao/status/1960685718621315482",
    "title_en": "_KarenHao_Teen Suicide Linked to ChatGPT Prompts Reflection",
    "summary_en": "Renowned journalist Karen Hao highlights the disturbing case of teenager Adam Raine, who died by suicide after prolonged engagement with ChatGPT. She expresses profound sorrow over the incident, recalling artist Hito Steyerl's insights that such tragedies are not inevitable. The tweet prompts deep reflection on AI ethics, the impact of AI technology on user mental health, and platform responsibility.",
    "keywords_en": [
      "ChatGPT",
      "Suicide",
      "AI Ethics",
      "Mental Health",
      "Teenager"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Industry News"
    ],
    "published_time": "2025-08-27T12:47:57.000Z",
    "download_time": "2025-08-28 02:01:42",
    "visual_resource": [
      "screenshot/twitter/_KarenHao_1960685718621315482.png"
    ],
    "extra_info": "{\"username\": \"_KarenHao\", \"tweet_id\": \"1960685718621315482\"}"
  },
  {
    "id": "twitter_AndrewYNg_1960731961494004077",
    "source": "Twitter",
    "url": "https://x.com/AndrewYNg/status/1960731961494004077",
    "title_en": "AndrewYNg_New Course on Agentic Knowledge Graph Construction",
    "summary_en": "Andrew Ng announced a new DeepLearning.AI short course, \"Agentic Knowledge Graph Construction,\" taught by a Neo4j expert. The course focuses on leveraging teams of AI agents to automate the building of knowledge graphs, extracting entities and relationships from unstructured data, performing deduplication, fact-checking, and committing them to a graph database. This significantly enhances the accuracy of RAG systems, transforming fragmented information into queryable business intelligence, particularly beneficial for high-stakes applications where precision is crucial.",
    "keywords_en": [
      "Knowledge Graph",
      "AI Agent",
      "RAG",
      "DeepLearning.AI",
      "Graph Database",
      "Information Retrieval"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Tech News"
    ],
    "published_time": "2025-08-27T15:51:42.000Z",
    "download_time": "2025-08-28 02:00:07",
    "visual_resource": [
      "screenshot/twitter/AndrewYNg_1960731961494004077.png"
    ],
    "extra_info": "{\"username\": \"AndrewYNg\", \"tweet_id\": \"1960731961494004077\"}"
  },
  {
    "id": "twitter_jaseweston_1960529697055355037",
    "source": "Twitter",
    "url": "https://x.com/jaseweston/status/1960529697055355037",
    "title_en": "jaseweston_Introduces StepWiser: Reframing Stepwise Reward Modeling as a Reasoning Task",
    "summary_en": "Jason Weston introduced StepWiser, a new research approach that reframes stepwise reward modeling as a reasoning task, generating Chain-of-Thought (CoT) and judgments. Trained by Reinforcement Learning using relative outcomes of rollouts, StepWiser achieves state-of-the-art performance on ProcessBench. It also demonstrates improvements in policy at train time and enhances inference-time search. This work offers a novel perspective on reward modeling and AI reasoning.",
    "keywords_en": [
      "StepWiser",
      "Reward Modeling",
      "Reasoning Task",
      "Reinforcement Learning",
      "ProcessBench",
      "Chain-of-Thought"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Research Progress"
    ],
    "published_time": "2025-08-27T02:27:59.000Z",
    "download_time": "2025-08-28 01:58:52",
    "visual_resource": [
      "screenshot/twitter/jaseweston_1960529697055355037.png"
    ],
    "extra_info": "{\"username\": \"jaseweston\", \"tweet_id\": \"1960529697055355037\"}"
  },
  {
    "id": "twitter_fchollet_1960808676262076629",
    "source": "Twitter",
    "url": "https://x.com/fchollet/status/1960808676262076629",
    "title_en": "fchollet_A New Method to Distinguish Model Memorization from Reasoning",
    "summary_en": "Prominent AI researcher François Chollet proposes a simple method to distinguish whether a model's correct answer stems from memorization or genuine reasoning. He suggests that when a model provides a correct answer to a reasoning question, one can test its true understanding by subtly tweaking the question. If the modified question requires the model to reason and adapt to produce a new answer, but the model still yields the original answer, it strongly indicates that its response was based on rote memorization rather than true reasoning ability. This practical method offers a valuable approach to evaluating the deeper cognitive capabilities of AI models.",
    "keywords_en": [
      "Model Reasoning",
      "Model Memorization",
      "AI Evaluation",
      "Machine Learning",
      "Deep Learning",
      "Chollet"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Research Progress"
    ],
    "published_time": "2025-08-27T20:56:33.000Z",
    "download_time": "2025-08-28 02:01:15",
    "visual_resource": [
      "screenshot/twitter/fchollet_1960808676262076629.png"
    ],
    "extra_info": "{\"username\": \"fchollet\", \"tweet_id\": \"1960808676262076629\"}"
  },
  {
    "id": "z_eUXxh-SXQyqx-999kSbQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/z_eUXxh-SXQyqx-999kSbQ",
    "title_en": "Google's \"Strongest Image Model\" Sweeps All! 3 Cents Per Image Stuns OpenAI, Photoshop May Cease to Exist",
    "summary_en": "Google has officially launched its top-tier image generation and editing model, Gemini 2.5 Flash Image, also known as nano-banana. This model has been hailed as the \"strongest image model\" after dominating LMArena blind tests with the largest historical lead. Its core capabilities include maintaining character consistency, advanced prompt-based image editing, leveraging native world knowledge, and multi-image fusion. Offering high-quality image generation and editing at an exceptionally low cost—approximately $0.039 per image—it significantly undercuts competitors like OpenAI. This breakthrough is poised to revolutionize traditional image processing software such as Adobe Photoshop, potentially rendering professional image editing roles obsolete. Furthermore, its advanced features and cost-effectiveness are expected to profoundly impact workflows in industries like advertising and film production, enabling new levels of creative control and efficiency.",
    "keywords_en": [
      "Gemini 2.5 Flash Image",
      "Image Generation",
      "Image Editing",
      "AI Model",
      "Cost-effectiveness",
      "Google"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Computer Vision",
      "Generative AI"
    ],
    "published_time": "2025-08-27T14:01:16.000Z",
    "download_time": "2025-08-28T13:31:09.990745",
    "visual_resource": [
      "screenshot/wechat/wechat_image_z_eUXxh-SXQyqx-999kSbQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "0SB5uhT4YE-vGSwm_FLFVw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/0SB5uhT4YE-vGSwm_FLFVw",
    "title_en": "Why PPO Consistently Outperforms Policy Gradient? The Answer Lies in This 'Golden Clipping' Strategy",
    "summary_en": "The Proximal Policy Optimization (PPO) algorithm has emerged as a highly effective solution in reinforcement learning, successfully overcoming the inherent instability of traditional Policy Gradient methods and the computational complexity associated with TRPO. PPO's core innovation lies in its unique \"Clipping\" mechanism, which judiciously limits the magnitude of policy updates, thereby guaranteeing a stable and secure learning process. Furthermore, PPO leverages advanced techniques such as Generalized Advantage Estimation (GAE) for more accurate value assessment, incorporates an entropy bonus to encourage exploration, and allows for multiple iterations of data utilization, all of which collectively boost its sample efficiency and overall robustness. This remarkable ability to strike an optimal balance among stability, ease of implementation, and sample efficiency has propelled PPO to become one of the most widely adopted and influential reinforcement learning algorithms across both academic research and industrial applications. It undeniably marks a pivotal advancement in the trajectory of policy gradient algorithms.",
    "keywords_en": [
      "PPO",
      "Reinforcement Learning",
      "Policy Gradient",
      "Clipping Mechanism",
      "Stability",
      "Sample Efficiency"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Deep Learning"
    ],
    "published_time": "2025-08-27T14:01:16.000Z",
    "download_time": "2025-08-28T13:31:15.644317",
    "visual_resource": [
      "screenshot/wechat/wechat_image_0SB5uhT4YE-vGSwm_FLFVw.png"
    ],
    "extra_info": null
  },
  {
    "id": "X4aCSgG3qhw_ApIPvR4NnQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/X4aCSgG3qhw_ApIPvR4NnQ",
    "title_en": "OmniHuman-1.5: A New Video Generation Paradigm for 'Thinking' Digital Humans",
    "summary_en": "Existing digital human models, while capable of generating fluid animations, often fall short in capturing the essence of characters due to a lack of deep understanding of emotions, intentions, and context. ByteDance's OmniHuman-1.5 framework addresses this limitation by aiming to produce physically plausible, semantically coherent, and highly expressive character animations. Its core innovations lie in two aspects: first, leveraging Multi-modal Large Language Models (MLLM) to generate structured conditional text representations, providing high-level semantic guidance that transcends simple rhythm synchronization; second, introducing a specialized Multi-modal DiT architecture with a novel Pseudo Last Frame design. This design facilitates efficient multi-modal signal fusion and mitigates cross-modal conflicts, enabling the model to accurately comprehend the joint semantics of audio, image, and text. The framework employs a dual-system simulation approach, combining a deliberative System 2 (planning via MLLM Agent) with a reactive System 1 (rendering via MMDiT network), ensuring high consistency across character, scene, and language content, thereby moving towards digital humans with 'thinking' capabilities.",
    "keywords_en": [
      "Digital Human",
      "Video Generation",
      "Multimodal Large Model",
      "AI Agent",
      "DiT Architecture",
      "Pseudo Last Frame"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Multimodal",
      "Generative AI"
    ],
    "published_time": "2025-08-27T12:07:04.000Z",
    "download_time": "2025-08-28T13:31:16.125271",
    "visual_resource": [
      "screenshot/wechat/wechat_image_X4aCSgG3qhw_ApIPvR4NnQ.png"
    ],
    "extra_info": null
  },
  {
    "id": "L8cv1IARvsIA0gSu8wlUFA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/L8cv1IARvsIA0gSu8wlUFA",
    "title_en": "Princeton, Tsinghua, and 20 Other Universities Jointly Release Survey on 'Self-Evolving' Agents",
    "summary_en": "A joint survey paper by 20 leading universities, including Princeton and Tsinghua, introduces the concept of \"self-evolving agents,\" emphasizing the critical role of adaptability and adjustment for AI systems in dynamic environments to achieve Artificial General Intelligence (AGI). The review proposes a four-dimensional analytical framework—What, When, How, and Where to evolve—systematically dissecting the evolvable components of agents, including model parameters, context, tools, and architecture. The paper elaborates on two evolution timings: in-task immediate evolution and post-task retrospective learning, alongside three driving forces: feedback, demonstration, and population-based evolution. Self-evolving agents show broad application prospects as general digital assistants and in specialized domains like coding, finance, and healthcare. Future evaluation metrics must focus on adaptability, knowledge retention, generalization, and safety efficiency. This research signals that the next generation of AI will be \"intelligent life\" capable of co-growth with users, moving beyond static tools.",
    "keywords_en": [
      "Self-Evolving Agents",
      "Large Language Models",
      "Adaptability",
      "Evolution Framework",
      "Reinforcement Learning",
      "Multi-Agent Systems"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-27T10:53:02.000Z",
    "download_time": "2025-08-28T13:31:11.416544",
    "visual_resource": [
      "screenshot/wechat/wechat_image_L8cv1IARvsIA0gSu8wlUFA.png"
    ],
    "extra_info": null
  },
  {
    "id": "hDBJ998nc9kO6vnTpx9exA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/hDBJ998nc9kO6vnTpx9exA",
    "title_en": "Breaking Bottlenecks, Enabling RAG to Reason: USTC, Zhipu AI, and Others Release Reasoning Retrieval Framework BGE-Reasoner",
    "summary_en": "A joint team from USTC, Zhipu AI, and other institutions has released BGE-Reasoner, an innovative reasoning retrieval framework designed to overcome the critical bottleneck of reasoning-intensive information retrieval in the advancement of RAG and AI Agents. This end-to-end solution significantly enhances search engine performance in complex query scenarios through a modular three-stage framework (Rewriter, Embedder, Reranker), leveraging large language models for high-quality synthetic training data, and empowering the Reranker with reinforcement learning. BGE-Reasoner has achieved a new state-of-the-art score on the authoritative BRIGHT benchmark, surpassing existing models. This breakthrough provides a novel paradigm for advancing Retrieval-Augmented Generation (RAG) in complex reasoning tasks and signals a crucial direction for future Agent Search development. The project plans to open-source its model weights and training code to further promote research and application in this field.",
    "keywords_en": [
      "Reasoning Retrieval",
      "RAG",
      "AI Agent",
      "BGE-Reasoner",
      "Reinforcement Learning",
      "Synthetic Data"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-27T06:24:35.000Z",
    "download_time": "2025-08-28T13:31:11.083716",
    "visual_resource": [
      "screenshot/wechat/wechat_image_hDBJ998nc9kO6vnTpx9exA.png"
    ],
    "extra_info": null
  },
  {
    "id": "_2xyfpJU60rx6-FhdXNCQA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/_2xyfpJU60rx6-FhdXNCQA",
    "title_en": "Claude for Chrome Arrives! Directly Usable as a Browser Extension",
    "summary_en": "Anthropic has launched Claude for Chrome, a new AI agent available as a browser extension. This extension allows users to interact with Claude in a side window, maintaining context from active browser tabs, and can perform tasks like calendar management, email replies, and information retrieval with user authorization. Currently, access is limited to a select group of Max subscribers, as Anthropic prioritizes security, particularly against \"prompt injection attacks.\" The company has implemented various safeguards, including restricting access to specific websites, blocking sensitive content by default, and requiring user permission for high-risk operations. The article highlights that AI-powered browsers are emerging as a new competitive arena for tech giants, citing examples such as Perplexity's Comet, Google's Gemini, and Microsoft's Copilot. It suggests two main development paths: integrating AI as an extension into existing browsers or developing entirely new AI-centric browsers.",
    "keywords_en": [
      "Claude for Chrome",
      "Browser Extension",
      "AI Agent",
      "Prompt Injection Attack",
      "AI Browser"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-27T05:47:09.000Z",
    "download_time": "2025-08-28T13:31:26.493723",
    "visual_resource": [
      "screenshot/wechat/wechat_image__2xyfpJU60rx6-FhdXNCQA.png"
    ],
    "extra_info": null
  },
  {
    "id": "system_prompts_leaks",
    "source": "GitHub",
    "url": "https://github.com/asgeirtj/system_prompts_leaks",
    "title_en": "System Prompts Leaks",
    "summary_en": "The GitHub repository \"System Prompts Leaks\" is a significant open-source project dedicated to the systematic collection and public dissemination of system message instructions employed by various publicly deployed AI chatbots. This initiative provides an invaluable and unique resource for researchers, AI ethicists, and developers who are keen to delve into the intricate internal workings, operational logic, and behavioral patterns of large language models and advanced conversational AI systems. By meticulously compiling these often-undisclosed system prompts, the project plays a pivotal role in advancing the understanding of AI transparency, facilitating the identification of inherent biases, and enabling a deeper analysis of the complex processes that govern AI response generation. Ultimately, this repository aims to significantly enhance the explainability, robustness, and safety of contemporary AI systems, thereby fostering a more transparent, responsible, and informed development ecosystem for artificial intelligence technologies across various domains.",
    "keywords_en": [
      "System Prompts",
      "Chatbots",
      "Large Language Models",
      "AI Ethics",
      "Prompt Engineering",
      "Data Collection"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-27T17:43:45Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://api.star-history.com/svg?repos=asgeirtj/system_prompts_leaks&type=Date"
    ],
    "extra_info": null
  },
  {
    "id": "WhisperLiveKit",
    "source": "GitHub",
    "url": "https://github.com/QuentinFuxa/WhisperLiveKit",
    "title_en": "WhisperLiveKit",
    "summary_en": "WhisperLiveKit is an open-source project offering real-time, fully local speech-to-text with integrated speaker identification. It leverages cutting-edge research like SimulStreaming, WhisperStreaming, and Sortformer to achieve ultra-low latency and high-accuracy speech processing. The tool includes a ready-to-use backend server and a simple frontend, supporting multiple concurrent users and optimizing resources through Voice Activity Detection. Its core advantage lies in intelligent buffering and incremental processing, effectively addressing context loss issues common with traditional Whisper models in real-time, small-batch processing. WhisperLiveKit is suitable for various applications such as meeting transcription, accessibility tools for the hearing-impaired, automated content creation for podcasts/videos, and customer service call transcription, offering easy integration and scalability with Docker support.",
    "keywords_en": [
      "Real-time Speech-to-Text",
      "Speaker Diarization",
      "Voice Activity Detection",
      "Streaming Speech Recognition",
      "Local Deployment",
      "Low Latency",
      "Speech Processing",
      "Natural Language Processing"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Deep Learning"
    ],
    "published_time": "2025-08-27T19:02:25Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png",
      "https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png"
    ],
    "extra_info": null
  },
  {
    "id": "nn-zero-to-hero",
    "source": "GitHub",
    "url": "https://github.com/karpathy/nn-zero-to-hero",
    "title_en": "Neural Networks: Zero to Hero",
    "summary_en": "This GitHub repository serves as the official companion for the \"Neural Networks: Zero to Hero\" course, providing comprehensive resources through a series of YouTube video lectures and corresponding Jupyter Notebooks. The course meticulously guides learners from the absolute basics of neural networks and backpropagation, exemplified by the `micrograd` project, to the intricate development of character-level language models with `makemore`, culminating in the construction of a Generative Pre-trained Transformer (GPT) from scratch. It delves into essential concepts such as PyTorch tensor operations, efficient neural network evaluation, model training methodologies, hyperparameter tuning, and advanced techniques like Batch Normalization and manual backpropagation. This educational initiative is designed to equip individuals with a deep, hands-on understanding of modern deep learning architectures, particularly focusing on their practical application in natural language processing and large language models.",
    "keywords_en": [
      "Neural Networks",
      "Deep Learning",
      "Natural Language Processing",
      "Large Language Models",
      "Backpropagation",
      "PyTorch",
      "GPT",
      "Machine Learning"
    ],
    "area_en": [
      "Deep Learning",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2024-02-20T17:19:51Z",
    "download_time": "2024-02-20 17:20:00",
    "visual_resource": [
      "screenshot/github/nn-zero-to-hero.png"
    ],
    "extra_info": null
  },
  {
    "id": "audiblez",
    "source": "GitHub",
    "url": "https://github.com/santinic/audiblez",
    "title_en": "Audiblez: Generate audiobooks from e-books",
    "summary_en": "Audiblez is an open-source tool designed to efficiently convert e-books, specifically in EPUB format, into high-quality M4B audiobooks. It utilizes the Kokoro-82M text-to-speech model, known for its natural-sounding output and compact size, supporting a wide array of languages including English, Spanish, French, and Chinese. The tool offers both a command-line interface and a user-friendly graphical interface, catering to different user preferences. A key feature is its support for CUDA acceleration, which dramatically speeds up the conversion process; for example, a 160,000-character book can be converted in about 5 minutes on a GPU. Users can also fine-tune the audiobook experience by adjusting playback speed and selecting from a diverse range of voices. This makes Audiblez a robust and versatile solution for anyone looking to create personalized audiobooks from their digital library.",
    "keywords_en": [
      "Audiobook Generation",
      "E-book Conversion",
      "Text-to-Speech",
      "Speech Synthesis",
      "Multilingual Support",
      "GPU Acceleration",
      "Open Source Tool",
      "EPUB"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-03-02T18:28:03Z",
    "download_time": "2024-05-15 10:00:00",
    "visual_resource": [
      "https://github.com/santinic/audiblez/raw/main/imgs/mac.png"
    ],
    "extra_info": null
  },
  {
    "id": "genai-toolbox",
    "source": "GitHub",
    "url": "https://github.com/googleapis/genai-toolbox",
    "title_en": "MCP Toolbox for Databases",
    "summary_en": "MCP Toolbox for Databases is an open-source MCP server specifically designed to streamline the development of Generative AI tools. It empowers AI agents to interact with and access database data with greater ease, speed, and security by abstracting away complexities like connection pooling and robust authentication mechanisms. This solution provides a simplified development experience, optimizes performance through best practices, and ensures enhanced data security via integrated authentication. Furthermore, it offers comprehensive end-to-end observability with built-in OpenTelemetry support, allowing for better monitoring. Developers can leverage this toolbox to create powerful AI database assistants, enabling natural language querying directly from their IDEs, automating database management tasks, and generating context-aware application code and tests. This significantly reduces manual setup, boilerplate, and error-prone schema migrations, ultimately boosting productivity and allowing developers to focus on core innovation.",
    "keywords_en": [
      "MCP",
      "Database",
      "Generative AI",
      "AI Agent",
      "Toolbox",
      "Connection Pooling",
      "Data Access",
      "OpenTelemetry"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Generative AI",
      "AI Agent"
    ],
    "published_time": "2025-08-28T02:07:20Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://github.com/googleapis/genai-toolbox/raw/main/logo.png",
      "https://github.com/googleapis/genai-toolbox/raw/main/docs/en/getting-started/introduction/architecture.png"
    ],
    "extra_info": null
  },
  {
    "id": "ART",
    "source": "GitHub",
    "url": "https://github.com/OpenPipe/ART",
    "title_en": "Agent Reinforcement Trainer",
    "summary_en": "ART (Agent Reinforcement Trainer) is an innovative open-source reinforcement learning framework designed to empower Large Language Models (LLMs) to learn from experience and effectively train multi-step agents for complex real-world tasks using GRPO. A standout feature is RULER (Relative Universal LLM-Elicited Rewards), which revolutionizes reward engineering by employing an LLM-as-judge to automatically score agent trajectories. This eliminates the traditional need for hand-crafted reward functions, labeled data, or expert feedback, significantly accelerating the development cycle by 2-3x. ART operates on a robust client-server architecture, ensuring flexible deployment and training from various environments, including local GPUs or ephemeral cloud instances. It supports a wide range of vLLM/HuggingFace-compatible models and offers seamless integrations with observability platforms like W&B and Langfuse, simplifying debugging. ART's capabilities are demonstrated across diverse applications, from email research and game playing (e.g., 2048, Tic Tac Toe) to mastering complex server tools, making it a versatile solution for advanced AI agent development.",
    "keywords_en": [
      "Reinforcement Learning",
      "Agent Training",
      "Large Language Model",
      "Reward Function",
      "GRPO",
      "RULER",
      "Multi-step Tasks",
      "Open-source Framework"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "AI Agent"
    ],
    "published_time": "2025-08-28T02:35:08Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://github.com/openpipe/art/raw/main/assets/ART_logo.png",
      "https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png",
      "https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg"
    ],
    "extra_info": null
  },
  {
    "id": "2508.19500",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.19500",
    "title_en": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H)\n  Agent Unlocks Adversarial Skills",
    "summary_en": "This paper identifies and analyzes a novel vulnerability class in Model\nContext Protocol (MCP) based agent systems. The attack chain describes and\ndemonstrates how benign, individually authorized tasks can be orchestrated to\nproduce harmful emergent behaviors. Through systematic analysis using the MITRE\nATLAS framework, we demonstrate how 95 agents tested with access to multiple\nservices-including browser automation, financial analysis, location tracking,\nand code deployment-can chain legitimate operations into sophisticated attack\nsequences that extend beyond the security boundaries of any individual service.\nThese red team exercises survey whether current MCP architectures lack\ncross-domain security measures necessary to detect or prevent a large category\nof compositional attacks. We present empirical evidence of specific attack\nchains that achieve targeted harm through service orchestration, including data\nexfiltration, financial manipulation, and infrastructure compromise. These\nfindings reveal that the fundamental security assumption of service isolation\nfails when agents can coordinate actions across multiple domains, creating an\nexponential attack surface that grows with each additional capability. This\nresearch provides a barebones experimental framework that evaluate not whether\nagents can complete MCP benchmark tasks, but what happens when they complete\nthem too well and optimize across multiple services in ways that violate human\nexpectations and safety constraints. We propose three concrete experimental\ndirections using the existing MCP benchmark suite.",
    "keywords_en": [
      "AI Agent Security",
      "Vulnerability Analysis",
      "Attack Chain",
      "Service Orchestration",
      "Cross-domain Security"
    ],
    "area_en": [
      "AI Agent",
      "Artificial Intelligence",
      "Large Language Model"
    ],
    "published_time": "2025-08-27T01:11:59.000Z",
    "download_time": "2025-08-27 20:36:24",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19500.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.19500\", \"arxiv_url\": \"https://arxiv.org/abs/2508.19500\"}"
  },
  {
    "id": "2508.17445",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.17445",
    "title_en": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
    "summary_en": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
    "keywords_en": [
      "Large Language Models",
      "Reinforcement Learning",
      "Policy Optimization",
      "Inference Efficiency",
      "Tree-based Modeling"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-24T16:52:37.000Z",
    "download_time": "2025-08-27 20:36:26",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17445.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.17445\", \"arxiv_url\": \"https://arxiv.org/abs/2508.17445\"}"
  },
  {
    "id": "2508.19209",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.19209",
    "title_en": "OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive\n  Simulation",
    "summary_en": "Existing video avatar models can produce fluid human animations, yet they\nstruggle to move beyond mere physical likeness to capture a character's\nauthentic essence. Their motions typically synchronize with low-level cues like\naudio rhythm, lacking a deeper semantic understanding of emotion, intent, or\ncontext. To bridge this gap, we propose a framework designed to\ngenerate character animations that are not only physically plausible but also\nsemantically coherent and expressive. Our model, OmniHuman-1.5, is\nbuilt upon two key technical contributions. First, we leverage Multimodal Large\nLanguage Models to synthesize a structured textual representation of conditions\nthat provides high-level semantic guidance. This guidance steers our motion\ngenerator beyond simplistic rhythmic synchronization, enabling the production\nof actions that are contextually and emotionally resonant. Second, to ensure\nthe effective fusion of these multimodal inputs and mitigate inter-modality\nconflicts, we introduce a specialized Multimodal DiT architecture with a novel\nPseudo Last Frame design. The synergy of these components allows our model to\naccurately interpret the joint semantics of audio, images, and text, thereby\ngenerating motions that are deeply coherent with the character, scene, and\nlinguistic content. Extensive experiments demonstrate that our model achieves\nleading performance across a comprehensive set of metrics, including lip-sync\naccuracy, video quality, motion naturalness and semantic consistency with\ntextual prompts. Furthermore, our approach shows remarkable extensibility to\ncomplex scenarios, such as those involving multi-person and non-human subjects.\nHomepage: https://omnihuman-lab.github.io/v1_5/",
    "keywords_en": [
      "Avatars",
      "Cognitive Simulation",
      "Multimodal Large Language Models",
      "Motion Generation",
      "Semantic Coherence"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "Generative AI"
    ],
    "published_time": "2025-08-26T17:15:26.000Z",
    "download_time": "2025-08-27 20:36:22",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19209.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.19209\", \"arxiv_url\": \"https://arxiv.org/abs/2508.19209\"}"
  },
  {
    "id": "2508.15774",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.15774",
    "title_en": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
    "summary_en": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. In this work, we propose CineScale, a novel inference\nparadigm to enable higher-resolution visual generation. To tackle the various\nissues introduced by the two types of video generation architectures, we\npropose dedicated variants tailored to each. Unlike existing baseline methods\nthat are confined to high-resolution T2I and T2V generation, CineScale broadens\nthe scope by enabling high-resolution I2V and V2V synthesis, built atop\nstate-of-the-art open-source video generation frameworks. Extensive experiments\nvalidate the superiority of our paradigm in extending the capabilities of\nhigher-resolution visual generation for both image and video models.\nRemarkably, our approach enables 8k image generation without any fine-tuning,\nand achieves 4k video generation with only minimal LoRA fine-tuning. Generated\nvideo samples are available at our website:\nhttps://eyeline-labs.github.io/CineScale/.",
    "keywords_en": [
      "High-resolution generation",
      "Visual diffusion models",
      "Video generation",
      "Tuning-free",
      "Inference paradigm"
    ],
    "area_en": [
      "Generative AI",
      "Computer Vision",
      "Deep Learning"
    ],
    "published_time": "2025-08-21T17:59:57.000Z",
    "download_time": "2025-08-27 20:36:27",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15774.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.15774\", \"arxiv_url\": \"https://arxiv.org/abs/2508.15774\"}"
  },
  {
    "id": "2508.18370",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.18370",
    "title_en": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo",
    "summary_en": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems.",
    "keywords_en": [
      "Large Language Models",
      "AI Agents",
      "Vulnerability Discovery",
      "CTF-Dojo",
      "Executable Environments"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Machine Learning"
    ],
    "published_time": "2025-08-25T18:02:23.000Z",
    "download_time": "2025-08-27 20:36:24",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18370.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.18370\", \"arxiv_url\": \"https://arxiv.org/abs/2508.18370\"}"
  },
  {
    "id": "2508.18756",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.18756",
    "title_en": "UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior\n  Long-Context Learning",
    "summary_en": "While Mixture of Experts (MoE) models achieve remarkable efficiency by\nactivating only subsets of parameters, they suffer from high memory access\ncosts during inference. Memory-layer architectures offer an appealing\nalternative with very few memory access, but previous attempts like UltraMem\nhave only matched the performance of 2-expert MoE models, falling significantly\nshort of state-of-the-art 8-expert configurations. We present UltraMemV2, a\nredesigned memory-layer architecture that closes this performance gap. Our\napproach introduces five key improvements: integrating memory layers into every\ntransformer block, simplifying value expansion with single linear projections,\nadopting FFN-based value processing from PEER, implementing principled\nparameter initialization, and rebalancing memory-to-FFN computation ratios.\nThrough extensive evaluation, we demonstrate that UltraMemV2 achieves\nperformance parity with 8-expert MoE models under same computation and\nparameters but significantly low memory access. Notably, UltraMemV2 shows\nsuperior performance on memory-intensive tasks, with improvements of +1.6\npoints on long-context memorization, +6.2 points on multi-round memorization,\nand +7.9 points on in-context learning. We validate our approach at scale with\nmodels up to 2.5B activated parameters from 120B total parameters, and\nestablish that activation density has greater impact on performance than total\nsparse parameter count. Our work brings memory-layer architectures to\nperformance parity with state-of-the-art MoE models, presenting a compelling\nalternative for efficient sparse computation.",
    "keywords_en": [
      "Memory Networks",
      "Mixture of Experts",
      "Long-Context Learning",
      "Large Models",
      "Sparse Computation"
    ],
    "area_en": [
      "Deep Learning",
      "Large Language Model",
      "Natural Language Processing"
    ],
    "published_time": "2025-08-26T07:33:11.000Z",
    "download_time": "2025-08-27 20:36:24",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18756.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.18756\", \"arxiv_url\": \"https://arxiv.org/abs/2508.18756\"}"
  }
]