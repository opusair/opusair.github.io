[
  {
    "id": "twitter_demishassabis_1960355658059891018",
    "source": "Twitter",
    "url": "https://x.com/demishassabis/status/1960355658059891018",
    "title_en": "demishassabis_Gemini 2.5 Image Model Excels in Editing and Character Consistency",
    "summary_en": "Demis Hassabis of Google DeepMind announced that the new Gemini 2.5 image model is the best in the industry, leading by 180 ELO points in image editing and excelling in character consistency. The model is now available for free in the Gemini App, allowing users to experience its advanced image generation, editing, and refinement capabilities with new levels of visual reasoning.",
    "keywords_en": [
      "Gemini 2.5",
      "Image Model",
      "Image Editing",
      "Character Consistency",
      "Google DeepMind",
      "Generative AI"
    ],
    "area_en": [
      "Generative AI",
      "Multimodal",
      "Product Launch"
    ],
    "published_time": "2025-08-26T14:56:25.000Z",
    "download_time": "2025-08-27 01:52:15",
    "visual_resource": [
      "screenshot/twitter/demishassabis_1960355658059891018.png"
    ],
    "extra_info": "{\"username\": \"demishassabis\", \"tweet_id\": \"1960355658059891018\"}"
  },
  {
    "id": "twitter_steeve_1960333418467664332",
    "source": "Twitter",
    "url": "https://x.com/steeve/status/1960333418467664332",
    "title_en": "steeve_zml/llmd Transparently Running on TPU with Paged Attention",
    "summary_en": "Steeve Morin announced that after a week of development, their zml/llmd project is now transparently running on TPUs, featuring full prefill/decode paged attention. This significant advancement means users can enable this functionality with just a single flag and no code changes, greatly simplifying the deployment and optimization of large language models on TPUs. This demonstrates efficient and seamless hardware acceleration capabilities.",
    "keywords_en": [
      "zml/llmd",
      "TPU",
      "Paged Attention",
      "Large Language Model",
      "Hardware Acceleration"
    ],
    "area_en": [
      "Large Language Model",
      "Tech News",
      "Research Progress"
    ],
    "published_time": "2025-08-26T13:28:02.000Z",
    "download_time": "2025-08-27 01:45:08",
    "visual_resource": [
      "screenshot/twitter/steeve_1960333418467664332.png"
    ],
    "extra_info": "{\"username\": \"steeve\", \"tweet_id\": \"1960333418467664332\"}"
  },
  {
    "id": "twitter_Muennighoff_1960391987917402509",
    "source": "Twitter",
    "url": "https://x.com/Muennighoff/status/1960391987917402509",
    "title_en": "Muennighoff_AI Large Models Solve Open Scientific Problems with Expert Validation",
    "summary_en": "Niklas Muennighoff's team is deeply exploring the capability of Artificial Intelligence, specifically frontier Large Language Models (LLMs), in solving open problems across fields like mathematics, physics, coding, and medical sciences. By collecting unsolved questions and testing them with LLMs, they found that some AI-generated solutions successfully passed expert validation. This indicates AI's significant potential in advancing scientific research and pushing the boundaries of existing knowledge.",
    "keywords_en": [
      "Artificial Intelligence",
      "Large Language Models",
      "Scientific Problems",
      "Expert Validation",
      "Research Progress"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Research Progress"
    ],
    "published_time": "2025-08-26T17:20:46.000Z",
    "download_time": "2025-08-27 01:46:10",
    "visual_resource": [
      "screenshot/twitter/Muennighoff_1960391987917402509.png"
    ],
    "extra_info": "{\"username\": \"Muennighoff\", \"tweet_id\": \"1960391987917402509\"}"
  },
  {
    "id": "twitter_Google_1960373032456757340",
    "source": "Twitter",
    "url": "https://x.com/Google/status/1960373032456757340",
    "title_en": "Google_Google Translate Launches Gemini-Powered Live Translate",
    "summary_en": "Google announced two significant updates to Google Translate, including the launch of a \"Live translate\" feature powered by Gemini models. Users can now engage in real-time audio conversations with on-screen translations directly within the Translate app, supporting over 70 languages. This feature is rolling out this week to users in the U.S., India, and Mexico, aiming to facilitate communication between speakers of different languages.",
    "keywords_en": [
      "Google Translate",
      "Gemini",
      "Live Translate",
      "Real-time Translation",
      "Product Update"
    ],
    "area_en": [
      "Natural Language Processing",
      "Multimodal",
      "Product Launch"
    ],
    "published_time": "2025-08-26T16:05:27.000Z",
    "download_time": "2025-08-27 01:51:32",
    "visual_resource": [
      "screenshot/twitter/Google_1960373032456757340.png"
    ],
    "extra_info": "{\"username\": \"Google\", \"tweet_id\": \"1960373032456757340\"}"
  },
  {
    "id": "twitter_CranQnow_1958527257632461022",
    "source": "Twitter",
    "url": "https://x.com/CranQnow/status/1958527257632461022/analytics",
    "title_en": "CranQnow_Google Chrome Browser Integrates AI Features",
    "summary_en": "Google Chrome has received a significant update, now fully integrating advanced AI capabilities. This groundbreaking AI functionality enables the browser to intelligently read screen content, understand its context, and provide comprehensive explanations for anything a user is viewing, regardless of the website. This major update introduces 10 innovative AI features, designed to significantly enhance user experience and streamline information retrieval.",
    "keywords_en": [
      "Google",
      "Chrome",
      "AI",
      "Artificial Intelligence",
      "Browser",
      "New Features"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Product Launch",
      "Tech News"
    ],
    "published_time": "2025-08-26T07:09:36.000Z",
    "download_time": "2025-08-27 01:45:06",
    "visual_resource": [
      "screenshot/twitter/CranQnow_1958527257632461022.png"
    ],
    "extra_info": "{\"username\": \"CranQnow\", \"tweet_id\": \"1958527257632461022\"}"
  },
  {
    "id": "twitter_Google_1960401639988220087",
    "source": "Twitter",
    "url": "https://x.com/Google/status/1960401639988220087",
    "title_en": "Google_Fitbit to Launch AI-Powered Personal Health Coach",
    "summary_en": "Google announced that a new AI-powered personal health coach will be available in public preview within the Fitbit app starting this October. This feature integrates the roles of a fitness trainer, sleep coach, and health and wellness advisor, aiming to comprehensively assist users in optimizing their health. This initiative marks a significant step for Google in combining artificial intelligence with wearable health devices, offering users more personalized and intelligent health management services.",
    "keywords_en": [
      "Google",
      "Fitbit",
      "AI Health Coach",
      "Artificial Intelligence",
      "Health Management",
      "Product Launch"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Product Launch",
      "Industry News"
    ],
    "published_time": "2025-08-26T17:59:08.000Z",
    "download_time": "2025-08-27 01:51:32",
    "visual_resource": [
      "screenshot/twitter/Google_1960401639988220087.png"
    ],
    "extra_info": "{\"username\": \"Google\", \"tweet_id\": \"1960401639988220087\"}"
  },
  {
    "id": "IbDiV9A0qDwnqMDkVolJRA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/IbDiV9A0qDwnqMDkVolJRA",
    "title_en": "Google Unveils nano-banana: Gemini 2.5 Flash Image!",
    "summary_en": "Google has officially unveiled Gemini 2.5 Flash Image, an upgraded image generation and editing model building upon Gemini 2.0 Flash. This new model has achieved the top rank on the LMArena image editing leaderboard and is now available for API access. Its key highlights include: the ability to seamlessly fuse multiple images, maintain character or object consistency across diverse scenarios, perform precise image transformations and localized edits using natural language prompts, and leverage Gemini's extensive world knowledge for advanced image creation and reasoning. The model underscores the future potential of native multimodal capabilities in the image generation domain, offering robust application templates that demonstrate its powerful abilities in complex editing tasks and semantic understanding. This advancement signifies a significant step towards more intelligent and versatile visual content creation.",
    "keywords_en": [
      "Gemini 2.5 Flash Image",
      "Image Generation",
      "Image Editing",
      "Character Consistency",
      "Natural Language Processing",
      "Multimodal"
    ],
    "area_en": [
      "Generative AI",
      "Large Language Model",
      "Computer Vision"
    ],
    "published_time": "2025-08-26T15:32:08.000Z",
    "download_time": "2025-08-27T14:28:23.066459",
    "visual_resource": [
      "screenshot/wechat/wechat_image_IbDiV9A0qDwnqMDkVolJRA.png"
    ],
    "extra_info": null
  },
  {
    "id": "SI_G7aAFLoeS0bX_yl4VZA",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/SI_G7aAFLoeS0bX_yl4VZA",
    "title_en": "Large Models Gain 'Eagle Eye': MiniCPM-V 4.5 Pioneers High-Refresh Video Understanding, Outperforming Google Gemini 2.5",
    "summary_en": "OpenBMB has recently open-sourced MiniCPM-V 4.5, an 8B parameter multimodal model that establishes a new benchmark in edge-side multimodal AI, particularly with its groundbreaking \"high-refresh video understanding\" capability. This innovative model achieves state-of-the-art (SOTA) performance across a diverse range of tasks, including single-image comprehension, long video analysis, optical character recognition (OCR), and complex document parsing. Remarkably, MiniCPM-V 4.5 not only matches but often surpasses the performance of larger, top-tier cloud-based large models such as Google Gemini 2.5 Pro and OpenAI's GPT-4o, showcasing its \"small but powerful\" efficiency. Key innovations include an advanced 3D-Resampler structure that enables high-density video compression, allowing it to process significantly more video frames than comparable models. Furthermore, it seamlessly integrates OCR with knowledge learning and supports a controllable hybrid inference mode, balancing deep analysis with rapid response. MiniCPM-V 4.5 demonstrates exceptional performance, energy efficiency, and deployment friendliness, providing a robust and highly efficient solution for a wide array of edge AI applications, from automotive systems to embodied intelligence.",
    "keywords_en": [
      "MiniCPM-V 4.5",
      "High-Refresh Video Understanding",
      "Multimodal Model",
      "Edge AI",
      "Video Understanding"
    ],
    "area_en": [
      "Multimodal",
      "Video Understanding",
      "Large Language Model"
    ],
    "published_time": "2025-08-26T15:01:25.000Z",
    "download_time": "2025-08-27T14:28:25.632595",
    "visual_resource": [
      "screenshot/wechat/wechat_image_SI_G7aAFLoeS0bX_yl4VZA.png"
    ],
    "extra_info": null
  },
  {
    "id": "-ria0vd2RxZ-rrFrq8hesw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/-ria0vd2RxZ-rrFrq8hesw",
    "title_en": "Musk's Trillion-Dollar AI Bet: 50 Million H100 GPUs in Five Years, Aiming for Billions in Compute Power",
    "summary_en": "Elon Musk, the world's richest man, has announced an \"all-in\" strategy for AI, planning to deploy 50 million H100 GPUs within five years, with an estimated total cost exceeding $2 trillion. This colossal investment, far surpassing the US annual military budget, underscores AI's emergence as a new strategic competitive frontier. Musk aims to build supercomputing clusters like Colossus 2 to provide massive computational power for his companies, including xAI and Tesla. This infrastructure will facilitate the training of next-generation large models (e.g., Grok 4), the development of multimodal agents, and video generation models, ultimately targeting billions of H100s to establish an overwhelming advantage in the AI domain. This ambitious endeavor also faces significant challenges, particularly regarding power supply.",
    "keywords_en": [
      "Elon Musk",
      "AI Compute Power",
      "H100 GPU",
      "Supercomputing Cluster",
      "Large Language Models"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Multimodal"
    ],
    "published_time": "2025-08-26T15:01:25.000Z",
    "download_time": "2025-08-27T14:28:23.431820",
    "visual_resource": [
      "screenshot/wechat/wechat_image_-ria0vd2RxZ-rrFrq8hesw.png"
    ],
    "extra_info": null
  },
  {
    "id": "JEBVLo00XZiAMKPdGXAjEg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/JEBVLo00XZiAMKPdGXAjEg",
    "title_en": "Vision-Language-Action Model + Reinforcement Learning! ReCogDrive: The First Open-Source RL-Driven Driving VLA Model",
    "summary_en": "Huazhong University of Science and Technology and Xiaomi Auto have jointly introduced ReCogDrive, the first open-source reinforcement learning-driven driving Vision-Language-Action (VLA) model. This novel framework integrates the world knowledge of Vision-Language Models (VLMs), a diffusion planner, and reinforcement learning fine-tuning to address the generalization challenges of end-to-end autonomous driving in long-tail scenarios. ReCogDrive employs a three-stage training paradigm: building a 3.1 million driving Q&A dataset, incorporating a diffusion model for continuous trajectory generation, and leveraging simulator-assisted reinforcement learning to optimize driving strategies. This approach enables the model to comprehend complex road conditions and generate safe, human-like driving trajectories. Achieving a new SOTA score of PDMS 90.5 on the NAVSIM benchmark, ReCogDrive significantly enhances autonomous driving's cognitive capabilities and generalization performance, advancing the application of VLMs in safe driving.",
    "keywords_en": [
      "ReCogDrive",
      "Vision-Language Model",
      "Reinforcement Learning",
      "Autonomous Driving",
      "Diffusion Model",
      "End-to-End"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "Machine Learning"
    ],
    "published_time": "2025-08-26T14:00:56.000Z",
    "download_time": "2025-08-27T14:28:21.268780",
    "visual_resource": [
      "screenshot/wechat/wechat_image_JEBVLo00XZiAMKPdGXAjEg.png"
    ],
    "extra_info": null
  },
  {
    "id": "Kc96mCYJBVHuhl7jBe-lHg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Kc96mCYJBVHuhl7jBe-lHg",
    "title_en": "NVIDIA's New Model Launched! 4B Inference Soars 53x, New Attention Architecture Surpasses Mamba 2",
    "summary_en": "NVIDIA has launched its new Jet-Nemotron small model series (2B/4B), developed by an all-Chinese team. Key innovations include Post Neural Architecture Search (PostNAS) for efficient architecture optimization and the novel linear attention module, JetBlock. These models surpass mainstream counterparts like Qwen3, Gemma3, and Llama3.2 in accuracy across various benchmarks including mathematics, code, common sense, retrieval, and long-context understanding. Furthermore, Jet-Nemotron achieves up to a 53-fold increase in inference throughput on H100 GPUs, demonstrating exceptional performance, particularly in long-context scenarios. JetBlock significantly outperforms previous designs like Mamba2, positioning Jet-Nemotron as a highly efficient small model that combines speed with accuracy, signaling NVIDIA's continued focus on the small model domain.",
    "keywords_en": [
      "NVIDIA",
      "Jet-Nemotron",
      "Small Models",
      "PostNAS",
      "JetBlock",
      "Inference Throughput"
    ],
    "area_en": [
      "Natural Language Processing",
      "Deep Learning",
      "Large Language Model"
    ],
    "published_time": "2025-08-26T14:00:56.000Z",
    "download_time": "2025-08-27T14:28:21.776422",
    "visual_resource": [
      "screenshot/wechat/wechat_image_Kc96mCYJBVHuhl7jBe-lHg.png"
    ],
    "extra_info": null
  },
  {
    "id": "WjouTIuSQ4leefCDbqD58Q",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/WjouTIuSQ4leefCDbqD58Q",
    "title_en": "NVIDIA's General-Purpose Robot Chip Arrives: AI Computing Power Boosted by 7.5x, Unitree and Galactic General Already Adopting",
    "summary_en": "NVIDIA has officially launched its next-generation robot-specific chip, Jetson Thor, featuring a Blackwell architecture GPU. This new chip boasts a 7.5x increase in AI computing power over its predecessor, reaching 2070 FP4 TFLOPS, and a 3.5x improvement in energy efficiency. Designed for embodied AI and physical AI agents, Jetson Thor supports large Transformer, VLM, and VLA models for real-time edge-side operation, minimizing cloud dependency. It is compatible with NVIDIA's full AI software stack and has already been adopted by leading Chinese robotics companies like Unitree Technology and Galactic General. NVIDIA is strategically positioning itself in the multi-trillion dollar robotics and autonomous driving markets, viewing them as key future growth drivers, aiming to provide foundational computing power and software support for the entire industry.",
    "keywords_en": [
      "NVIDIA",
      "Jetson Thor",
      "Robot Chip",
      "Embodied AI",
      "AI Computing Power"
    ],
    "area_en": [
      "Robotics",
      "Artificial Intelligence",
      "AI Agent"
    ],
    "published_time": "2025-08-26T04:09:14.000Z",
    "download_time": "2025-08-27T14:28:47.466200",
    "visual_resource": [
      "screenshot/wechat/wechat_image_WjouTIuSQ4leefCDbqD58Q.png"
    ],
    "extra_info": null
  },
  {
    "id": "system_prompts_leaks",
    "source": "GitHub",
    "url": "https://github.com/asgeirtj/system_prompts_leaks",
    "title_en": "System Prompts Leaks",
    "summary_en": "The \"System Prompts Leaks\" GitHub repository is a dedicated and growing collection of system message instructions obtained from various publicly deployed chatbots. This valuable resource offers a unique opportunity for researchers, developers, and AI enthusiasts to delve into the foundational prompt designs that dictate the behavior and responses of diverse AI conversational agents. By centralizing these \"leaked\" prompts, the project provides critical insights for understanding the nuances of large language model (LLM) interactions, advancing the field of prompt engineering, and addressing crucial aspects related to AI safety, ethics, and potential biases. The initiative actively encourages community contributions, aiming to foster greater transparency and facilitate a deeper, more comprehensive understanding of the intricate internal workings of contemporary AI systems, thereby serving as an essential reference for anyone studying chatbot architecture and prompt-driven AI development.",
    "keywords_en": [
      "System Prompts",
      "Chatbots",
      "Large Language Models",
      "Prompt Engineering",
      "AI Safety",
      "Open Source"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "published_time": "2025-08-26T15:45:44Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://api.star-history.com/svg?repos=asgeirtj/system_prompts_leaks&type=Date"
    ],
    "extra_info": null
  },
  {
    "id": "verifiers",
    "source": "GitHub",
    "url": "https://github.com/willccbb/verifiers",
    "title_en": "Verifiers",
    "summary_en": "Verifiers is a comprehensive, modular library specifically engineered for constructing sophisticated environments and training advanced agents within the domain of Large Language Model (LLM) Reinforcement Learning. It incorporates an asynchronous GRPO implementation, seamlessly integrating with the Hugging Face Transformers Trainer and leveraging `prime-rl` for efficient, large-scale FSDP training across multiple GPUs. Beyond core RL training, Verifiers serves as a versatile tool for conducting rigorous LLM evaluations, streamlining synthetic data generation pipelines, and developing robust agent harnesses. The library supports diverse interaction paradigms, including single-turn, multi-turn, and complex tool-calling environments, while maintaining compatibility with OpenAI-compatible inference clients and advanced vLLM sampling parameters. Its design philosophy emphasizes providing a reliable and extensible toolkit for RL infrastructure, actively working to mitigate the common issue of \"fork proliferation\" prevalent in the research ecosystem.",
    "keywords_en": [
      "LLM Reinforcement Learning",
      "Agent Training",
      "Environment Building",
      "GRPO",
      "vLLM",
      "FSDP",
      "Tool Calling",
      "Large Model Evaluation"
    ],
    "area_en": [
      "Large Language Model",
      "Machine Learning",
      "AI Agent"
    ],
    "published_time": "2025-08-26T11:55:03Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "screenshot/github/verifiers.png"
    ],
    "extra_info": null
  },
  {
    "id": "DeepCode",
    "source": "GitHub",
    "url": "https://github.com/HKUDS/DeepCode",
    "title_en": "DeepCode: Open Agentic Coding",
    "summary_en": "DeepCode, an innovative AI-powered open agentic coding platform developed by the Data Intelligence Lab at the University of Hong Kong, revolutionizes software development by automating complex code generation and implementation tasks. Leveraging a sophisticated multi-agent system, it efficiently translates diverse inputs, from intricate research papers to natural language descriptions, into high-quality, production-ready code. Its core capabilities, such as Paper2Code, streamline the implementation of complex algorithms; Text2Web facilitates rapid frontend web development; and Text2Backend automates robust server-side code generation. This comprehensive approach significantly accelerates the entire development lifecycle, from initial concept to deployable code, while boosting overall efficiency and enabling seamless research reproducibility. DeepCode provides flexible interaction through both command-line and intuitive web interfaces, and ensures broad tool compatibility through its adherence to the Model Context Protocol (MCP) standard, making it a powerful solution for modern coding challenges.",
    "keywords_en": [
      "Multi-Agent System",
      "Code Generation",
      "AI Agent",
      "Paper2Code",
      "Frontend Development",
      "Backend Development",
      "Retrieval-Augmented Generation"
    ],
    "area_en": [
      "AI Agent",
      "Generative AI",
      "Large Language Model"
    ],
    "published_time": "2025-08-23T14:44:18Z",
    "download_time": "2024-05-23 12:00:00",
    "visual_resource": [
      "https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif",
      "https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif",
      "https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg"
    ],
    "extra_info": null
  },
  {
    "id": "SurfSense",
    "source": "GitHub",
    "url": "https://github.com/MODSetter/SurfSense",
    "title_en": "SurfSense",
    "summary_en": "SurfSense is a highly customizable AI research agent designed to enhance research capabilities by integrating personal knowledge bases with external data sources such as search engines, Slack, Notion, and YouTube. It supports uploading over 50 file formats, offering powerful search, natural language interaction with saved content, and cited answers. The project emphasizes privacy with local LLM support and is self-hostable. Its core technologies include advanced RAG techniques (supporting over a hundred LLMs and thousands of embedding models, utilizing hierarchical indices and hybrid search) and rapid podcast generation. SurfSense also provides a cross-browser extension for saving web page content.",
    "keywords_en": [
      "AI Research Agent",
      "Knowledge Management",
      "Retrieval Augmented Generation",
      "Local LLM",
      "Podcast Generation",
      "Hybrid Search",
      "Multimodal",
      "External Integration"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-27T01:08:41Z",
    "download_time": "2024-07-29 07:00:00",
    "visual_resource": [
      "screenshot/github/SurfSense.png"
    ],
    "extra_info": null
  },
  {
    "id": "mcp-context-forge",
    "source": "GitHub",
    "url": "https://github.com/IBM/mcp-context-forge",
    "title_en": "MCP Gateway",
    "summary_en": "MCP Gateway is a feature-rich gateway, proxy, and MCP Registry designed to unify REST, MCP, and A2A services, providing a single endpoint for AI clients. It supports service discovery, authentication, rate-limiting, observability, virtual servers, and multi-transport protocols, along with an optional Admin UI. Deployable as a fully compliant MCP server via PyPI or Docker, it scales to multi-cluster Kubernetes environments with Redis-backed federation and caching. The gateway integrates external AI agents (e.g., OpenAI, Anthropic) and virtualizes them as MCP-compliant tools, offers OpenTelemetry observability, and supports various deployment options including PyPI, Docker, and IBM Cloud Code Engine.",
    "keywords_en": [
      "MCP Gateway",
      "Model Context Protocol",
      "AI Agent",
      "REST API",
      "Service Federation",
      "Observability",
      "Kubernetes",
      "Docker"
    ],
    "area_en": [
      "Artificial Intelligence",
      "AI Agent",
      "Large Language Model"
    ],
    "published_time": "2025-08-25T11:22:49Z",
    "download_time": "2024-07-09 08:00:00",
    "visual_resource": [
      "https://github.com/IBM/mcp-context-forge/raw/main/docs/docs/images/contextforge-banner.png",
      "https://ibm.github.io/mcp-context-forge/images/mcpgateway.gif",
      "https://ibm.github.io/mcp-context-forge/images/mcpgateway.svg"
    ],
    "extra_info": null
  },
  {
    "id": "onlook",
    "source": "GitHub",
    "url": "https://github.com/onlook-dev/onlook",
    "title_en": "An Open-Source, Visual-First Code Editor",
    "summary_en": "Onlook is an open-source, visual-first code editor designed for designers. It enables users to build websites, prototypes, and designs with AI in Next.js and TailwindCSS environments, supporting direct visual editing within the browser DOM for real-time synchronization between code and design. This project aims to provide a powerful alternative, integrating AI-assisted development, real-time code editing, and project deployment features. It utilizes an innovative web container architecture, ensuring an efficient and flexible development experience for crafting modern web applications.",
    "keywords_en": [
      "Code Editor",
      "Visual Development",
      "Artificial Intelligence",
      "Frontend Framework",
      "Web Development",
      "Design Tools",
      "Open Source"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Generative AI",
      "Others"
    ],
    "published_time": "2025-08-25T20:56:17Z",
    "download_time": "2024-07-29 08:00:00",
    "visual_resource": [
      "https://github.com/onlook-dev/onlook/blob/main/assets/architecture.png?raw=true",
      "https://github.com/onlook-dev/onlook/blob/main/assets/web-preview.png?raw=true"
    ],
    "extra_info": null
  },
  {
    "id": "2508.18265",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.18265",
    "title_en": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
    "summary_en": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05times inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
    "keywords_en": [
      "Multimodal Large Language Model",
      "Open-source Models",
      "Reasoning Capability",
      "Inference Efficiency",
      "Reinforcement Learning"
    ],
    "area_en": [
      "Multimodal",
      "Large Language Model",
      "AI Agent"
    ],
    "published_time": "2025-08-25T17:58:17.000Z",
    "download_time": "2025-08-26 18:58:18",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18265.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.18265\", \"arxiv_url\": \"https://arxiv.org/abs/2508.18265\"}"
  },
  {
    "id": "2508.18032",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.18032",
    "title_en": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation",
    "summary_en": "Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.",
    "keywords_en": [
      "Text-to-Image Generation",
      "Reinforcement Learning",
      "Chain of Guidance",
      "Stage-Aware",
      "Semantic Reasoning"
    ],
    "area_en": [
      "Generative AI",
      "Machine Learning",
      "Multimodal"
    ],
    "published_time": "2025-08-25T13:53:02.000Z",
    "download_time": "2025-08-26 18:58:18",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18032.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.18032\", \"arxiv_url\": \"https://arxiv.org/abs/2508.18032\"}"
  },
  {
    "id": "2508.16949",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.16949",
    "title_en": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
    "summary_en": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3.",
    "keywords_en": [
      "Large Language Models",
      "Reinforcement Learning",
      "LLM Reasoning",
      "Exploration Bottleneck",
      "Rubric-Scaffolding"
    ],
    "area_en": [
      "Large Language Model",
      "Natural Language Processing",
      "Machine Learning"
    ],
    "published_time": "2025-08-23T08:47:31.000Z",
    "download_time": "2025-08-26 18:58:22",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16949.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.16949\", \"arxiv_url\": \"https://arxiv.org/abs/2508.16949\"}"
  },
  {
    "id": "2508.17188",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.17188",
    "title_en": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs",
    "summary_en": "Multi-agent systems built upon large language models (LLMs) have demonstrated\nremarkable capabilities in tackling complex compositional tasks. In this work,\nwe apply this paradigm to the paper-to-poster generation problem, a practical\nyet time-consuming process faced by researchers preparing for conferences.\nWhile recent approaches have attempted to automate this task, most neglect core\ndesign and aesthetic principles, resulting in posters that require substantial\nmanual refinement. To address these design limitations, we propose PosterGen, a\nmulti-agent framework that mirrors the workflow of professional poster\ndesigners. It consists of four collaborative specialized agents: (1) Parser and\nCurator agents extract content from the paper and organize storyboard; (2)\nLayout agent maps the content into a coherent spatial layout; (3) Stylist\nagents apply visual design elements such as color and typography; and (4)\nRenderer composes the final poster. Together, these agents produce posters that\nare both semantically grounded and visually appealing. To evaluate design\nquality, we introduce a vision-language model (VLM)-based rubric that measures\nlayout balance, readability, and aesthetic coherence. Experimental results show\nthat PosterGen consistently matches in content fidelity, and significantly\noutperforms existing methods in visual designs, generating posters that are\npresentation-ready with minimal human refinements.",
    "keywords_en": [
      "Multi-agent systems",
      "Large Language Models",
      "Paper-to-Poster Generation",
      "Aesthetic Design",
      "Vision-Language Model"
    ],
    "area_en": [
      "Large Language Model",
      "AI Agent",
      "Generative AI"
    ],
    "published_time": "2025-08-24T02:25:45.000Z",
    "download_time": "2025-08-26 18:58:19",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17188.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.17188\", \"arxiv_url\": \"https://arxiv.org/abs/2508.17188\"}"
  },
  {
    "id": "2508.19201",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.19201",
    "title_en": "Understanding Tool-Integrated Reasoning",
    "summary_en": "We study why Tool-Integrated Reasoning (TIR) makes Large Language Models\n(LLMs) more capable. While LLMs integrated with tools like Python code\ninterpreters show great promise, a principled theory explaining why this\nparadigm is effective has been missing. This work provides the first formal\nproof that TIR fundamentally expands an LLM's capabilities. We demonstrate that\ntools enable a strict expansion of the model's empirical and feasible support,\nbreaking the capability ceiling of pure-text models by unlocking\nproblem-solving strategies that are otherwise impossible or intractably\nverbose. To guide model behavior without compromising training stability and\nperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), a\nnovel algorithm that directly modifies the advantage function to guide the\npolicy behavior. We conduct comprehensive experiments on challenging\nmathematical benchmarks, leveraging a Python interpreter as the external tool.\nOur results show that the TIR model decisively outperforms its pure-text\ncounterpart on the pass@k metric. Crucially, this advantage is not confined to\ncomputationally-intensive problems but extends to those requiring significant\nabstract insight. We further identify the emergent cognitive patterns that\nillustrate how models learn to think with tools. Finally, we report improved\ntool usage behavior with early code invocation and much more interactive turns\nwith ASPO. Overall, our work provides the first principled explanation for\nTIR's success, shifting the focus from the mere fact that tools work to why and\nhow they enable more powerful reasoning.",
    "keywords_en": [
      "Tool-Integrated Reasoning",
      "Large Language Models",
      "Capability Expansion",
      "Policy Optimization",
      "Tool Usage"
    ],
    "area_en": [
      "Artificial Intelligence",
      "Machine Learning",
      "Large Language Model"
    ],
    "published_time": "2025-08-26T17:03:46.000Z",
    "download_time": "2025-08-26 18:58:17",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19201.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.19201\", \"arxiv_url\": \"https://arxiv.org/abs/2508.19201\"}"
  },
  {
    "id": "2508.16790",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2508.16790",
    "title_en": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language\n  Modeling",
    "summary_en": "Speech tokenizers serve as foundational components for speech language\nmodels, yet current designs exhibit several limitations, including: 1)\ndependence on multi-layer residual vector quantization structures or high frame\nrates, 2) reliance on auxiliary pre-trained models for semantic distillation,\nand 3) requirements for complex two-stage training processes. In this work, we\nintroduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a\nnovel approach designed to overcome these challenges. TaDiCodec employs\nend-to-end optimization for quantization and reconstruction through a diffusion\nautoencoder, while integrating text guidance into the diffusion decoder to\nenhance reconstruction quality and achieve optimal compression. TaDiCodec\nachieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of\n0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining\nsuperior performance on critical speech generation evaluation metrics such as\nWord Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).\nNotably, TaDiCodec employs a single-stage, end-to-end training paradigm, and\nobviating the need for auxiliary pre-trained models. We also validate the\ncompatibility of TaDiCodec in language model based zero-shot text-to-speech\nwith both autoregressive modeling and masked generative modeling, demonstrating\nits effectiveness and efficiency for speech language modeling, as well as a\nsignificantly small reconstruction-generation gap. We will open source our code\nand model checkpoints. Audio samples are are available at\nhttps:/tadicodec.github.io/. We release code and model checkpoints at\nhttps:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.",
    "keywords_en": [
      "TaDiCodec",
      "Diffusion Model",
      "Speech Tokenizer",
      "Speech Language Modeling",
      "Low Bitrate"
    ],
    "area_en": [
      "Deep Learning",
      "Natural Language Processing",
      "Generative AI"
    ],
    "published_time": "2025-08-22T20:45:03.000Z",
    "download_time": "2025-08-26 18:58:17",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16790.png"
    ],
    "extra_info": "{\"url\": \"https://huggingface.co/papers/2508.16790\", \"arxiv_url\": \"https://arxiv.org/abs/2508.16790\"}"
  }
]