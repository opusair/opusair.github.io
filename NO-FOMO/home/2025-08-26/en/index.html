<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-08-26</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }

        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.3s ease, transform 0.2s ease;
            border: 2px solid transparent;
            font-size: 0.9em;
        }

        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }

        .language-switch a.active {
            background: var(--secondary-color);
            border-color: var(--border-color);
        }

        @media (max-width: 768px) {
            .language-switch {
                position: static;
                justify-content: center;
                margin-bottom: 20px;
            }
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../">‰∏≠Êñá</a>
                <a href="./" class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-08-26</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Twitter</h2>

            <article class="item-card">
                <h2>demishassabis_Gemini 2.5 Image Model Excels in Editing and Character Consistency</h2>
                <span class="published-time">Published: 2025-08-26T14:56:25.000Z</span>
                <img src="../screenshot/twitter/demishassabis_1960355658059891018.png" alt="demishassabis_Gemini 2.5 Image Model Excels in Editing and Character Consistency">
                <p class="summary">Demis Hassabis of Google DeepMind announced that the new Gemini 2.5 image model is the best in the industry, leading by 180 ELO points in image editing and excelling in character consistency. The model is now available for free in the Gemini App, allowing users to experience its advanced image generation, editing, and refinement capabilities with new levels of visual reasoning.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini 2.5</span><span>Image Model</span><span>Image Editing</span><span>Character Consistency</span><span>Google DeepMind</span><span>Generative AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Product Launch</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/demishassabis/status/1960355658059891018" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>steeve_zml/llmd Transparently Running on TPU with Paged Attention</h2>
                <span class="published-time">Published: 2025-08-26T13:28:02.000Z</span>
                <img src="../screenshot/twitter/steeve_1960333418467664332.png" alt="steeve_zml/llmd Transparently Running on TPU with Paged Attention">
                <p class="summary">Steeve Morin announced that after a week of development, their zml/llmd project is now transparently running on TPUs, featuring full prefill/decode paged attention. This significant advancement means users can enable this functionality with just a single flag and no code changes, greatly simplifying the deployment and optimization of large language models on TPUs. This demonstrates efficient and seamless hardware acceleration capabilities.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>zml/llmd</span><span>TPU</span><span>Paged Attention</span><span>Large Language Model</span><span>Hardware Acceleration</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Tech News</span><span>Research Progress</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/steeve/status/1960333418467664332" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Muennighoff_AI Large Models Solve Open Scientific Problems with Expert Validation</h2>
                <span class="published-time">Published: 2025-08-26T17:20:46.000Z</span>
                <img src="../screenshot/twitter/Muennighoff_1960391987917402509.png" alt="Muennighoff_AI Large Models Solve Open Scientific Problems with Expert Validation">
                <p class="summary">Niklas Muennighoff's team is deeply exploring the capability of Artificial Intelligence, specifically frontier Large Language Models (LLMs), in solving open problems across fields like mathematics, physics, coding, and medical sciences. By collecting unsolved questions and testing them with LLMs, they found that some AI-generated solutions successfully passed expert validation. This indicates AI's significant potential in advancing scientific research and pushing the boundaries of existing knowledge.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Artificial Intelligence</span><span>Large Language Models</span><span>Scientific Problems</span><span>Expert Validation</span><span>Research Progress</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Research Progress</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/Muennighoff/status/1960391987917402509" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Google_Google Translate Launches Gemini-Powered Live Translate</h2>
                <span class="published-time">Published: 2025-08-26T16:05:27.000Z</span>
                <img src="../screenshot/twitter/Google_1960373032456757340.png" alt="Google_Google Translate Launches Gemini-Powered Live Translate">
                <p class="summary">Google announced two significant updates to Google Translate, including the launch of a "Live translate" feature powered by Gemini models. Users can now engage in real-time audio conversations with on-screen translations directly within the Translate app, supporting over 70 languages. This feature is rolling out this week to users in the U.S., India, and Mexico, aiming to facilitate communication between speakers of different languages.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Google Translate</span><span>Gemini</span><span>Live Translate</span><span>Real-time Translation</span><span>Product Update</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Multimodal</span><span>Product Launch</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/Google/status/1960373032456757340" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>CranQnow_Google Chrome Browser Integrates AI Features</h2>
                <span class="published-time">Published: 2025-08-26T07:09:36.000Z</span>
                <img src="../screenshot/twitter/CranQnow_1958527257632461022.png" alt="CranQnow_Google Chrome Browser Integrates AI Features">
                <p class="summary">Google Chrome has received a significant update, now fully integrating advanced AI capabilities. This groundbreaking AI functionality enables the browser to intelligently read screen content, understand its context, and provide comprehensive explanations for anything a user is viewing, regardless of the website. This major update introduces 10 innovative AI features, designed to significantly enhance user experience and streamline information retrieval.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Google</span><span>Chrome</span><span>AI</span><span>Artificial Intelligence</span><span>Browser</span><span>New Features</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Product Launch</span><span>Tech News</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/CranQnow/status/1958527257632461022/analytics" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Google_Fitbit to Launch AI-Powered Personal Health Coach</h2>
                <span class="published-time">Published: 2025-08-26T17:59:08.000Z</span>
                <img src="../screenshot/twitter/Google_1960401639988220087.png" alt="Google_Fitbit to Launch AI-Powered Personal Health Coach">
                <p class="summary">Google announced that a new AI-powered personal health coach will be available in public preview within the Fitbit app starting this October. This feature integrates the roles of a fitness trainer, sleep coach, and health and wellness advisor, aiming to comprehensively assist users in optimizing their health. This initiative marks a significant step for Google in combining artificial intelligence with wearable health devices, offering users more personalized and intelligent health management services.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Google</span><span>Fitbit</span><span>AI Health Coach</span><span>Artificial Intelligence</span><span>Health Management</span><span>Product Launch</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Product Launch</span><span>Industry News</span></div>
                </div>
                <div class="read-more">
                    <a href="https://x.com/Google/status/1960401639988220087" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">wechat</h2>

            <article class="item-card">
                <h2>Google Unveils nano-banana: Gemini 2.5 Flash Image!</h2>
                <span class="published-time">Published: 2025-08-26T15:32:08.000Z</span>
                <img src="../screenshot/wechat/wechat_image_IbDiV9A0qDwnqMDkVolJRA.png" alt="Google Unveils nano-banana: Gemini 2.5 Flash Image!">
                <p class="summary">Google has officially unveiled Gemini 2.5 Flash Image, an upgraded image generation and editing model building upon Gemini 2.0 Flash. This new model has achieved the top rank on the LMArena image editing leaderboard and is now available for API access. Its key highlights include: the ability to seamlessly fuse multiple images, maintain character or object consistency across diverse scenarios, perform precise image transformations and localized edits using natural language prompts, and leverage Gemini's extensive world knowledge for advanced image creation and reasoning. The model underscores the future potential of native multimodal capabilities in the image generation domain, offering robust application templates that demonstrate its powerful abilities in complex editing tasks and semantic understanding. This advancement signifies a significant step towards more intelligent and versatile visual content creation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Gemini 2.5 Flash Image</span><span>Image Generation</span><span>Image Editing</span><span>Character Consistency</span><span>Natural Language Processing</span><span>Multimodal</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Large Language Model</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/IbDiV9A0qDwnqMDkVolJRA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Large Models Gain 'Eagle Eye': MiniCPM-V 4.5 Pioneers High-Refresh Video Understanding, Outperforming Google Gemini 2.5</h2>
                <span class="published-time">Published: 2025-08-26T15:01:25.000Z</span>
                <img src="../screenshot/wechat/wechat_image_SI_G7aAFLoeS0bX_yl4VZA.png" alt="Large Models Gain 'Eagle Eye': MiniCPM-V 4.5 Pioneers High-Refresh Video Understanding, Outperforming Google Gemini 2.5">
                <p class="summary">OpenBMB has recently open-sourced MiniCPM-V 4.5, an 8B parameter multimodal model that establishes a new benchmark in edge-side multimodal AI, particularly with its groundbreaking "high-refresh video understanding" capability. This innovative model achieves state-of-the-art (SOTA) performance across a diverse range of tasks, including single-image comprehension, long video analysis, optical character recognition (OCR), and complex document parsing. Remarkably, MiniCPM-V 4.5 not only matches but often surpasses the performance of larger, top-tier cloud-based large models such as Google Gemini 2.5 Pro and OpenAI's GPT-4o, showcasing its "small but powerful" efficiency. Key innovations include an advanced 3D-Resampler structure that enables high-density video compression, allowing it to process significantly more video frames than comparable models. Furthermore, it seamlessly integrates OCR with knowledge learning and supports a controllable hybrid inference mode, balancing deep analysis with rapid response. MiniCPM-V 4.5 demonstrates exceptional performance, energy efficiency, and deployment friendliness, providing a robust and highly efficient solution for a wide array of edge AI applications, from automotive systems to embodied intelligence.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>MiniCPM-V 4.5</span><span>High-Refresh Video Understanding</span><span>Multimodal Model</span><span>Edge AI</span><span>Video Understanding</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Video Understanding</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/SI_G7aAFLoeS0bX_yl4VZA" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Musk's Trillion-Dollar AI Bet: 50 Million H100 GPUs in Five Years, Aiming for Billions in Compute Power</h2>
                <span class="published-time">Published: 2025-08-26T15:01:25.000Z</span>
                <img src="../screenshot/wechat/wechat_image_-ria0vd2RxZ-rrFrq8hesw.png" alt="Musk's Trillion-Dollar AI Bet: 50 Million H100 GPUs in Five Years, Aiming for Billions in Compute Power">
                <p class="summary">Elon Musk, the world's richest man, has announced an "all-in" strategy for AI, planning to deploy 50 million H100 GPUs within five years, with an estimated total cost exceeding $2 trillion. This colossal investment, far surpassing the US annual military budget, underscores AI's emergence as a new strategic competitive frontier. Musk aims to build supercomputing clusters like Colossus 2 to provide massive computational power for his companies, including xAI and Tesla. This infrastructure will facilitate the training of next-generation large models (e.g., Grok 4), the development of multimodal agents, and video generation models, ultimately targeting billions of H100s to establish an overwhelming advantage in the AI domain. This ambitious endeavor also faces significant challenges, particularly regarding power supply.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Elon Musk</span><span>AI Compute Power</span><span>H100 GPU</span><span>Supercomputing Cluster</span><span>Large Language Models</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/-ria0vd2RxZ-rrFrq8hesw" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Vision-Language-Action Model + Reinforcement Learning! ReCogDrive: The First Open-Source RL-Driven Driving VLA Model</h2>
                <span class="published-time">Published: 2025-08-26T14:00:56.000Z</span>
                <img src="../screenshot/wechat/wechat_image_JEBVLo00XZiAMKPdGXAjEg.png" alt="Vision-Language-Action Model + Reinforcement Learning! ReCogDrive: The First Open-Source RL-Driven Driving VLA Model">
                <p class="summary">Huazhong University of Science and Technology and Xiaomi Auto have jointly introduced ReCogDrive, the first open-source reinforcement learning-driven driving Vision-Language-Action (VLA) model. This novel framework integrates the world knowledge of Vision-Language Models (VLMs), a diffusion planner, and reinforcement learning fine-tuning to address the generalization challenges of end-to-end autonomous driving in long-tail scenarios. ReCogDrive employs a three-stage training paradigm: building a 3.1 million driving Q&A dataset, incorporating a diffusion model for continuous trajectory generation, and leveraging simulator-assisted reinforcement learning to optimize driving strategies. This approach enables the model to comprehend complex road conditions and generate safe, human-like driving trajectories. Achieving a new SOTA score of PDMS 90.5 on the NAVSIM benchmark, ReCogDrive significantly enhances autonomous driving's cognitive capabilities and generalization performance, advancing the application of VLMs in safe driving.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>ReCogDrive</span><span>Vision-Language Model</span><span>Reinforcement Learning</span><span>Autonomous Driving</span><span>Diffusion Model</span><span>End-to-End</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/JEBVLo00XZiAMKPdGXAjEg" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>NVIDIA's New Model Launched! 4B Inference Soars 53x, New Attention Architecture Surpasses Mamba 2</h2>
                <span class="published-time">Published: 2025-08-26T14:00:56.000Z</span>
                <img src="../screenshot/wechat/wechat_image_Kc96mCYJBVHuhl7jBe-lHg.png" alt="NVIDIA's New Model Launched! 4B Inference Soars 53x, New Attention Architecture Surpasses Mamba 2">
                <p class="summary">NVIDIA has launched its new Jet-Nemotron small model series (2B/4B), developed by an all-Chinese team. Key innovations include Post Neural Architecture Search (PostNAS) for efficient architecture optimization and the novel linear attention module, JetBlock. These models surpass mainstream counterparts like Qwen3, Gemma3, and Llama3.2 in accuracy across various benchmarks including mathematics, code, common sense, retrieval, and long-context understanding. Furthermore, Jet-Nemotron achieves up to a 53-fold increase in inference throughput on H100 GPUs, demonstrating exceptional performance, particularly in long-context scenarios. JetBlock significantly outperforms previous designs like Mamba2, positioning Jet-Nemotron as a highly efficient small model that combines speed with accuracy, signaling NVIDIA's continued focus on the small model domain.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>NVIDIA</span><span>Jet-Nemotron</span><span>Small Models</span><span>PostNAS</span><span>JetBlock</span><span>Inference Throughput</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Deep Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/Kc96mCYJBVHuhl7jBe-lHg" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>NVIDIA's General-Purpose Robot Chip Arrives: AI Computing Power Boosted by 7.5x, Unitree and Galactic General Already Adopting</h2>
                <span class="published-time">Published: 2025-08-26T04:09:14.000Z</span>
                <img src="../screenshot/wechat/wechat_image_WjouTIuSQ4leefCDbqD58Q.png" alt="NVIDIA's General-Purpose Robot Chip Arrives: AI Computing Power Boosted by 7.5x, Unitree and Galactic General Already Adopting">
                <p class="summary">NVIDIA has officially launched its next-generation robot-specific chip, Jetson Thor, featuring a Blackwell architecture GPU. This new chip boasts a 7.5x increase in AI computing power over its predecessor, reaching 2070 FP4 TFLOPS, and a 3.5x improvement in energy efficiency. Designed for embodied AI and physical AI agents, Jetson Thor supports large Transformer, VLM, and VLA models for real-time edge-side operation, minimizing cloud dependency. It is compatible with NVIDIA's full AI software stack and has already been adopted by leading Chinese robotics companies like Unitree Technology and Galactic General. NVIDIA is strategically positioning itself in the multi-trillion dollar robotics and autonomous driving markets, viewing them as key future growth drivers, aiming to provide foundational computing power and software support for the entire industry.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>NVIDIA</span><span>Jetson Thor</span><span>Robot Chip</span><span>Embodied AI</span><span>AI Computing Power</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Robotics</span><span>Artificial Intelligence</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://mp.weixin.qq.com/s/WjouTIuSQ4leefCDbqD58Q" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">GitHub</h2>

            <article class="item-card">
                <h2>System Prompts Leaks</h2>
                <span class="published-time">Published: 2025-08-26T15:45:44Z</span>
                <img src="https://api.star-history.com/svg?repos=asgeirtj/system_prompts_leaks&type=Date" alt="System Prompts Leaks">
                <p class="summary">The "System Prompts Leaks" GitHub repository is a dedicated and growing collection of system message instructions obtained from various publicly deployed chatbots. This valuable resource offers a unique opportunity for researchers, developers, and AI enthusiasts to delve into the foundational prompt designs that dictate the behavior and responses of diverse AI conversational agents. By centralizing these "leaked" prompts, the project provides critical insights for understanding the nuances of large language model (LLM) interactions, advancing the field of prompt engineering, and addressing crucial aspects related to AI safety, ethics, and potential biases. The initiative actively encourages community contributions, aiming to foster greater transparency and facilitate a deeper, more comprehensive understanding of the intricate internal workings of contemporary AI systems, thereby serving as an essential reference for anyone studying chatbot architecture and prompt-driven AI development.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>System Prompts</span><span>Chatbots</span><span>Large Language Models</span><span>Prompt Engineering</span><span>AI Safety</span><span>Open Source</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Artificial Intelligence</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/asgeirtj/system_prompts_leaks" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Verifiers</h2>
                <span class="published-time">Published: 2025-08-26T11:55:03Z</span>
                <img src="../screenshot/github/verifiers.png" alt="Verifiers">
                <p class="summary">Verifiers is a comprehensive, modular library specifically engineered for constructing sophisticated environments and training advanced agents within the domain of Large Language Model (LLM) Reinforcement Learning. It incorporates an asynchronous GRPO implementation, seamlessly integrating with the Hugging Face Transformers Trainer and leveraging `prime-rl` for efficient, large-scale FSDP training across multiple GPUs. Beyond core RL training, Verifiers serves as a versatile tool for conducting rigorous LLM evaluations, streamlining synthetic data generation pipelines, and developing robust agent harnesses. The library supports diverse interaction paradigms, including single-turn, multi-turn, and complex tool-calling environments, while maintaining compatibility with OpenAI-compatible inference clients and advanced vLLM sampling parameters. Its design philosophy emphasizes providing a reliable and extensible toolkit for RL infrastructure, actively working to mitigate the common issue of "fork proliferation" prevalent in the research ecosystem.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>LLM Reinforcement Learning</span><span>Agent Training</span><span>Environment Building</span><span>GRPO</span><span>vLLM</span><span>FSDP</span><span>Tool Calling</span><span>Large Model Evaluation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/willccbb/verifiers" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DeepCode: Open Agentic Coding</h2>
                <span class="published-time">Published: 2025-08-23T14:44:18Z</span>
                <img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="DeepCode: Open Agentic Coding">
                <p class="summary">DeepCode, an innovative AI-powered open agentic coding platform developed by the Data Intelligence Lab at the University of Hong Kong, revolutionizes software development by automating complex code generation and implementation tasks. Leveraging a sophisticated multi-agent system, it efficiently translates diverse inputs, from intricate research papers to natural language descriptions, into high-quality, production-ready code. Its core capabilities, such as Paper2Code, streamline the implementation of complex algorithms; Text2Web facilitates rapid frontend web development; and Text2Backend automates robust server-side code generation. This comprehensive approach significantly accelerates the entire development lifecycle, from initial concept to deployable code, while boosting overall efficiency and enabling seamless research reproducibility. DeepCode provides flexible interaction through both command-line and intuitive web interfaces, and ensures broad tool compatibility through its adherence to the Model Context Protocol (MCP) standard, making it a powerful solution for modern coding challenges.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multi-Agent System</span><span>Code Generation</span><span>AI Agent</span><span>Paper2Code</span><span>Frontend Development</span><span>Backend Development</span><span>Retrieval-Augmented Generation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>AI Agent</span><span>Generative AI</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/HKUDS/DeepCode" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>SurfSense</h2>
                <span class="published-time">Published: 2025-08-27T01:08:41Z</span>
                <img src="../screenshot/github/SurfSense.png" alt="SurfSense">
                <p class="summary">SurfSense is a highly customizable AI research agent designed to enhance research capabilities by integrating personal knowledge bases with external data sources such as search engines, Slack, Notion, and YouTube. It supports uploading over 50 file formats, offering powerful search, natural language interaction with saved content, and cited answers. The project emphasizes privacy with local LLM support and is self-hostable. Its core technologies include advanced RAG techniques (supporting over a hundred LLMs and thousands of embedding models, utilizing hierarchical indices and hybrid search) and rapid podcast generation. SurfSense also provides a cross-browser extension for saving web page content.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Research Agent</span><span>Knowledge Management</span><span>Retrieval Augmented Generation</span><span>Local LLM</span><span>Podcast Generation</span><span>Hybrid Search</span><span>Multimodal</span><span>External Integration</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/MODSetter/SurfSense" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>MCP Gateway</h2>
                <span class="published-time">Published: 2025-08-25T11:22:49Z</span>
                <img src="https://github.com/IBM/mcp-context-forge/raw/main/docs/docs/images/contextforge-banner.png" alt="MCP Gateway">
                <p class="summary">MCP Gateway is a feature-rich gateway, proxy, and MCP Registry designed to unify REST, MCP, and A2A services, providing a single endpoint for AI clients. It supports service discovery, authentication, rate-limiting, observability, virtual servers, and multi-transport protocols, along with an optional Admin UI. Deployable as a fully compliant MCP server via PyPI or Docker, it scales to multi-cluster Kubernetes environments with Redis-backed federation and caching. The gateway integrates external AI agents (e.g., OpenAI, Anthropic) and virtualizes them as MCP-compliant tools, offers OpenTelemetry observability, and supports various deployment options including PyPI, Docker, and IBM Cloud Code Engine.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>MCP Gateway</span><span>Model Context Protocol</span><span>AI Agent</span><span>REST API</span><span>Service Federation</span><span>Observability</span><span>Kubernetes</span><span>Docker</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/IBM/mcp-context-forge" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>An Open-Source, Visual-First Code Editor</h2>
                <span class="published-time">Published: 2025-08-25T20:56:17Z</span>
                <img src="https://github.com/onlook-dev/onlook/blob/main/assets/architecture.png?raw=true" alt="An Open-Source, Visual-First Code Editor">
                <p class="summary">Onlook is an open-source, visual-first code editor designed for designers. It enables users to build websites, prototypes, and designs with AI in Next.js and TailwindCSS environments, supporting direct visual editing within the browser DOM for real-time synchronization between code and design. This project aims to provide a powerful alternative, integrating AI-assisted development, real-time code editing, and project deployment features. It utilizes an innovative web container architecture, ensuring an efficient and flexible development experience for crafting modern web applications.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Code Editor</span><span>Visual Development</span><span>Artificial Intelligence</span><span>Frontend Framework</span><span>Web Development</span><span>Design Tools</span><span>Open Source</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Generative AI</span><span>Others</span></div>
                </div>
                <div class="read-more">
                    <a href="https://github.com/onlook-dev/onlook" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,
  Reasoning, and Efficiency</h2>
                <span class="published-time">Published: 2025-08-25T17:58:17.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18265.png" alt="InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,
  Reasoning, and Efficiency">
                <p class="summary">We introduce InternVL 3.5, a new family of open-source multimodal models that
significantly advances versatility, reasoning capability, and inference
efficiency along the InternVL series. A key innovation is the Cascade
Reinforcement Learning (Cascade RL) framework, which enhances reasoning through
a two-stage process: offline RL for stable convergence and online RL for
refined alignment. This coarse-to-fine training strategy leads to substantial
improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To
optimize efficiency, we propose a Visual Resolution Router (ViR) that
dynamically adjusts the resolution of visual tokens without compromising
performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)
strategy separates the vision encoder and language model across different GPUs,
effectively balancing computational load. These contributions collectively
enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning
performance and a 4.05times inference speedup compared to its predecessor,
i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as
GUI interaction and embodied agency. Notably, our largest model, i.e.,
InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs
across general multimodal, reasoning, text, and agentic tasks -- narrowing the
performance gap with leading commercial models like GPT-5. All models and code
are publicly released.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multimodal Large Language Model</span><span>Open-source Models</span><span>Reasoning Capability</span><span>Inference Efficiency</span><span>Reinforcement Learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Multimodal</span><span>Large Language Model</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.18265" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance
  for Text-to-Image Generation</h2>
                <span class="published-time">Published: 2025-08-25T13:53:02.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18032.png" alt="Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance
  for Text-to-Image Generation">
                <p class="summary">Despite the promising progress of recent autoregressive models in
text-to-image (T2I) generation, their ability to handle multi-attribute and
ambiguous prompts remains limited. To address these limitations, existing works
have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and
employed reinforcement learning (RL) to improve reasoning capabilities.
However, most models provide reward signals only at the end of the generation
stage. This monolithic final-only guidance makes it difficult to identify which
stages contribute positively to the final outcome and may lead to suboptimal
policies. To tackle this issue, we propose a Visual-Chain of Guidance
(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process
refining, and outcome evaluation, with stage-aware rewards providing immediate
guidance throughout the image generation pipeline. We further construct a
visual cognition benchmark, VisCog-Bench, which comprises four subtasks to
evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on
GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,
5%, and 19%, respectively, demonstrating the superior performance of the
proposed Visual-CoG. We will release all the resources soon.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Text-to-Image Generation</span><span>Reinforcement Learning</span><span>Chain of Guidance</span><span>Stage-Aware</span><span>Semantic Reasoning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Machine Learning</span><span>Multimodal</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.18032" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement
  Learning for General LLM Reasoning</h2>
                <span class="published-time">Published: 2025-08-23T08:47:31.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16949.png" alt="Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement
  Learning for General LLM Reasoning">
                <p class="summary">Recent advances in Large Language Models (LLMs) have underscored the
potential of Reinforcement Learning (RL) to facilitate the emergence of
reasoning capabilities. Despite the encouraging results, a fundamental dilemma
persists as RL improvement relies on learning from high-quality samples, yet
the exploration for such samples remains bounded by the inherent limitations of
LLMs. This, in effect, creates an undesirable cycle in which what cannot be
explored cannot be learned. In this work, we propose Rubric-Scaffolded
Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework
designed to break the exploration bottleneck for general LLM reasoning.
Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit
scaffolding for exploration during rollout generation, where different rubrics
are provided as external guidance within task instructions to steer diverse
high-quality responses. This guidance is gradually decayed over time,
encouraging the model to internalize the underlying reasoning patterns; (2)
verifiable rewards for exploitation during model training, where we can obtain
robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL
on general reasoning tasks. Extensive experiments demonstrate the superiority
of the proposed RuscaRL across various benchmarks, effectively expanding
reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL
significantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,
surpassing GPT-4.1. Furthermore, our fine-tuned variant on
Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading
LLMs including OpenAI-o3.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Large Language Models</span><span>Reinforcement Learning</span><span>LLM Reasoning</span><span>Exploration Bottleneck</span><span>Rubric-Scaffolding</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.16949" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent
  LLMs</h2>
                <span class="published-time">Published: 2025-08-24T02:25:45.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17188.png" alt="PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent
  LLMs">
                <p class="summary">Multi-agent systems built upon large language models (LLMs) have demonstrated
remarkable capabilities in tackling complex compositional tasks. In this work,
we apply this paradigm to the paper-to-poster generation problem, a practical
yet time-consuming process faced by researchers preparing for conferences.
While recent approaches have attempted to automate this task, most neglect core
design and aesthetic principles, resulting in posters that require substantial
manual refinement. To address these design limitations, we propose PosterGen, a
multi-agent framework that mirrors the workflow of professional poster
designers. It consists of four collaborative specialized agents: (1) Parser and
Curator agents extract content from the paper and organize storyboard; (2)
Layout agent maps the content into a coherent spatial layout; (3) Stylist
agents apply visual design elements such as color and typography; and (4)
Renderer composes the final poster. Together, these agents produce posters that
are both semantically grounded and visually appealing. To evaluate design
quality, we introduce a vision-language model (VLM)-based rubric that measures
layout balance, readability, and aesthetic coherence. Experimental results show
that PosterGen consistently matches in content fidelity, and significantly
outperforms existing methods in visual designs, generating posters that are
presentation-ready with minimal human refinements.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Multi-agent systems</span><span>Large Language Models</span><span>Paper-to-Poster Generation</span><span>Aesthetic Design</span><span>Vision-Language Model</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>AI Agent</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.17188" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Understanding Tool-Integrated Reasoning</h2>
                <span class="published-time">Published: 2025-08-26T17:03:46.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19201.png" alt="Understanding Tool-Integrated Reasoning">
                <p class="summary">We study why Tool-Integrated Reasoning (TIR) makes Large Language Models
(LLMs) more capable. While LLMs integrated with tools like Python code
interpreters show great promise, a principled theory explaining why this
paradigm is effective has been missing. This work provides the first formal
proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that
tools enable a strict expansion of the model's empirical and feasible support,
breaking the capability ceiling of pure-text models by unlocking
problem-solving strategies that are otherwise impossible or intractably
verbose. To guide model behavior without compromising training stability and
performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a
novel algorithm that directly modifies the advantage function to guide the
policy behavior. We conduct comprehensive experiments on challenging
mathematical benchmarks, leveraging a Python interpreter as the external tool.
Our results show that the TIR model decisively outperforms its pure-text
counterpart on the pass@k metric. Crucially, this advantage is not confined to
computationally-intensive problems but extends to those requiring significant
abstract insight. We further identify the emergent cognitive patterns that
illustrate how models learn to think with tools. Finally, we report improved
tool usage behavior with early code invocation and much more interactive turns
with ASPO. Overall, our work provides the first principled explanation for
TIR's success, shifting the focus from the mere fact that tools work to why and
how they enable more powerful reasoning.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Tool-Integrated Reasoning</span><span>Large Language Models</span><span>Capability Expansion</span><span>Policy Optimization</span><span>Tool Usage</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.19201" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language
  Modeling</h2>
                <span class="published-time">Published: 2025-08-22T20:45:03.000Z</span>
                <img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16790.png" alt="TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language
  Modeling">
                <p class="summary">Speech tokenizers serve as foundational components for speech language
models, yet current designs exhibit several limitations, including: 1)
dependence on multi-layer residual vector quantization structures or high frame
rates, 2) reliance on auxiliary pre-trained models for semantic distillation,
and 3) requirements for complex two-stage training processes. In this work, we
introduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a
novel approach designed to overcome these challenges. TaDiCodec employs
end-to-end optimization for quantization and reconstruction through a diffusion
autoencoder, while integrating text guidance into the diffusion decoder to
enhance reconstruction quality and achieve optimal compression. TaDiCodec
achieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of
0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining
superior performance on critical speech generation evaluation metrics such as
Word Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).
Notably, TaDiCodec employs a single-stage, end-to-end training paradigm, and
obviating the need for auxiliary pre-trained models. We also validate the
compatibility of TaDiCodec in language model based zero-shot text-to-speech
with both autoregressive modeling and masked generative modeling, demonstrating
its effectiveness and efficiency for speech language modeling, as well as a
significantly small reconstruction-generation gap. We will open source our code
and model checkpoints. Audio samples are are available at
https:/tadicodec.github.io/. We release code and model checkpoints at
https:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>TaDiCodec</span><span>Diffusion Model</span><span>Speech Tokenizer</span><span>Speech Language Modeling</span><span>Low Bitrate</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Deep Learning</span><span>Natural Language Processing</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2508.16790" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>