<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Daily Report - 2025-12-31</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --background-color: #f8f9fa;
            --card-background-color: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --shadow-color: rgba(0, 0, 0, 0.075);
            --tag-keyword-bg: #e7f3ff;
            --tag-keyword-text: #0056b3;
            --tag-area-bg: #e6f7f0;
            --tag-area-text: #198754;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            font-weight: 400;
        }

        .container {
            max-width: 1100px;
            margin: 30px auto;
            padding: 25px 30px;
            background: var(--card-background-color);
            border-radius: 12px;
            box-shadow: 0 4px 20px var(--shadow-color);
        }

        .report-header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            position: relative;
        }

        .report-header h1 {
            font-size: 2.2em;
            color: #333;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .report-header .theme-info {
            font-size: 1em;
            color: var(--secondary-color);
        }

        .source-group {
            margin-bottom: 40px;
        }

        .source-group-title {
            font-size: 1.8em;
            font-weight: 600;
            color: #444;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
            display: inline-block;
        }

        .item-card {
            background-color: var(--card-background-color);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 10px var(--shadow-color);
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }

        .item-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .item-card h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 8px;
        }

        .item-card .published-time {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 12px;
            display: block;
        }

        .item-card .summary {
            margin-bottom: 15px;
            font-size: 0.95em;
            color: #555;
        }

        .item-card img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            margin: 15px auto;
            display: block;
            border: 1px solid var(--border-color);
            background-color: #fdfdfd;
        }

        .item-card .meta-tags {
            margin-top: 15px;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .item-card .keywords,
        .item-card .area {
            font-size: 0.85em;
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            align-items: center;
        }

        .item-card .keywords .label,
        .item-card .area .label {
            font-weight: 600;
            color: var(--text-color);
            margin-right: 8px;
        }

        .item-card .keywords span,
        .item-card .area span {
            padding: 5px 12px;
            border-radius: 15px;
            display: inline-block;
            font-weight: 500;
            line-height: 1.3;
        }

        .item-card .keywords span {
            background-color: var(--tag-keyword-bg);
            color: var(--tag-keyword-text);
        }

        .item-card .area span {
            background-color: var(--tag-area-bg);
            color: var(--tag-area-text);
        }

        .item-card .read-more a {
            display: inline-block;
            margin-top: 15px;
            font-weight: 600;
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        .item-card .read-more a:hover {
            text-decoration: underline;
            color: #0056b3;
        }

        .report-footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border-color);
            font-size: 0.9em;
            color: var(--secondary-color);
        }
    
        .language-switch {
            position: absolute;
            top: 0;
            right: 0;
            display: flex;
            gap: 10px;
        }
        .language-switch a {
            background: var(--primary-color);
            color: white;
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .language-switch a:hover {
            background: #0056b3;
            transform: translateY(-2px);
        }
        .language-switch a.active {
            background: var(--secondary-color);
        }
    </style>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-008T4WC27P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-008T4WC27P');
    </script>

</head>
<body>
    <div class="container">
        <header class="report-header">
            <div class="language-switch">
                <a href="../" class="">‰∏≠Êñá</a>
                <a href="." class="active">English</a>
            </div>

            <h1>AI Daily Report</h1>
            <p class="date">2025-12-31</p>
            <p class="theme-info">About us: <a href="https://opusair.github.io/" target="_blank" rel="noopener noreferrer">https://opusair.github.io/</a></p>
        </header>

        <nav style="text-align: center; margin-bottom: 30px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <a href="../../../home/en/" style="margin: 0 15px; padding: 10px 20px; background: #007bff; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üè† Back to Homepage</a>
            <a href="../../../daily/en/" style="margin: 0 15px; padding: 10px 20px; background: #28a745; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üìÖ Latest Daily</a>
            <a href="https://opusair.github.io/" style="margin: 0 15px; padding: 10px 20px; background: #ff9800; color: white; text-decoration: none; border-radius: 5px; font-weight: 500;">üë§ About Us</a>
        </nav>


        <section class="source-group">
            <h2 class="source-group-title">Hacker News</h2>

            <article class="item-card">
                <h2>Scaffolding to Superhuman: How Curriculum Learning Solved 2048 and Tetris</h2>
                <span class="published-time">Published: 2025-12-31 15:52:51</span>
                
                <p class="summary">A recent blog post titled 'Scaffolding to Superhuman: How Curriculum Learning Solved 2048 and Tetris' delves into the innovative application of curriculum learning to achieve superhuman performance in classic games, specifically 2048 and Tetris. The research showcases how a meticulously structured learning process, analogous to human educational scaffolding, can dramatically improve an AI agent's capacity to master intricate tasks. This methodology involves progressively increasing the complexity of the learning environment, enabling the AI to build foundational skills before tackling more challenging scenarios. This incremental approach effectively mitigates issues like premature convergence to suboptimal strategies, fostering the development of robust and adaptive game-playing intelligence. The successful resolution of these well-known games underscores curriculum learning's efficacy as a potent training paradigm, offering significant implications for advancing AI development not only in game artificial intelligence but also in broader applications requiring sophisticated strategic planning and problem-solving abilities.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Curriculum Learning</span><span>Reinforcement Learning</span><span>Game AI</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Tetris</span><span>2048</span><span>Game Solving</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>AI Agent</span></div>
                </div>
                <div class="read-more">
                    <a href="https://kywch.github.io/blog/2025/12/curriculum-learning-2048-tetris/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Show HN: Use Claude Code to Query 600 GB Indexes over Hacker News, ArXiv, etc.</h2>
                <span class="published-time">Published: 2025-12-31 07:47:44</span>
                
                <p class="summary">A new Show HN project, 'Exopriors Scry', introduces an advanced research tool leveraging Claude Code to query vast 600 GB indexes across prominent high-quality public commons sites such as Hacker News, arXiv, and LessWrong. This innovative platform provides a public, read-only SQL+vector database that allows users to embed their API key and utilize Claude's AI capabilities to generate complex SQL queries for nuanced information retrieval. Beyond direct querying, the service features an 'Alerts' functionality, enabling users to set specific, highly detailed criteria for email notifications when new content matches their interests, such as "estrogen in a psychoactive context" or specific biological metaphors in infrastructure discussions. The system currently indexes a significant portion of posts (1.4M out of 4.6M) and comments (15.6M out of 38M) using Voyage-3.5-lite, supporting sophisticated compositional vector search. This tool aims to revolutionize information discovery and monitoring over large, diverse datasets.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Claude Code</span><span>Vector Database</span><span>SQL Queries</span><span>Information Retrieval</span><span>Semantic Search</span><span>AI Agent</span><span>Data Indexing</span><span>Alerts System</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://exopriors.com/scry" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>How AI labs are solving the power problem</h2>
                <span class="published-time">Published: 2025-12-31 13:50:41</span>
                
                <p class="summary">AI labs are increasingly confronting substantial challenges related to the escalating power consumption of their computational infrastructure, particularly driven by the training and inference requirements of large-scale AI models. The 'power problem' encompasses not only the direct electricity demand of high-performance GPUs and specialized AI accelerators but also the extensive energy needed for cooling systems and the overall operation of vast data centers. To address these critical issues, AI research institutions are actively exploring multifaceted solutions. These strategies include significant advancements in algorithmic efficiency, such as developing sparse models or more optimized neural network architectures that demand less computational power. Concurrently, there is a strong focus on hardware innovation, aiming to create more energy-efficient processors and specialized AI chips. Furthermore, labs are investigating the integration of sustainable and renewable energy sources to power their operations and implementing advanced cooling technologies to minimize energy waste. These concerted efforts are vital for ensuring the sustainable growth, economic viability, and environmental responsibility of the rapidly evolving AI landscape.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>AI Energy Consumption</span><span>Sustainable AI</span><span>AI Hardware</span><span>Algorithmic Efficiency</span><span>Data Center Energy</span><span>Green AI</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Machine Learning</span><span>Deep Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://newsletter.semianalysis.com/p/how-ai-labs-are-solving-the-power" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Claude wrote a functional NES emulator using my engine's API</h2>
                <span class="published-time">Published: 2025-12-31 13:07:22</span>
                
                <p class="summary">A notable advancement in AI-driven software development has been observed, with the AI model Claude successfully engineering a functional Nintendo Entertainment System (NES) emulator. This project exemplifies Claude's sophisticated capability to generate complex code, specifically designed to interact with an existing game engine's Application Programming Interface (API). The development involved Claude interpreting specific requirements for hardware emulation and translating them into functional, API-compliant software. This achievement underscores the increasing proficiency of large language models as advanced coding assistants, capable of not only generating substantial amounts of code but also ensuring its operational integrity and integration within established software ecosystems. The successful creation of an emulator, a task traditionally requiring deep human expertise in systems programming, showcases AI's potential to accelerate development cycles and innovate in specialized domains like retro gaming and custom software solutions. This case serves as a compelling illustration of AI agents' practical application in overcoming intricate technical challenges and expanding the scope of automated code generation.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Claude</span><span>NES Emulator</span><span>API</span><span>Code Generation</span><span>AI Agent</span><span>Game Development</span><span>Software Engineering</span><span>Hardware Emulation</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>AI Agent</span><span>Generative AI</span></div>
                </div>
                <div class="read-more">
                    <a href="https://carimbo.games/games/nintendo/" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Court report detailing ChatGPT's involvement with a recent murder suicide [pdf]</h2>
                <span class="published-time">Published: 2025-12-31 18:25:16</span>
                
                <p class="summary">A significant court report has emerged, detailing the alleged involvement of OpenAI's ChatGPT in a recent murder-suicide case. The publicly available PDF document, filed within a legal proceeding, aims to outline the specific circumstances and the extent to which the large language model played a role in the tragic events. This development underscores a critical examination of the ethical implications and potential misuse of powerful AI technologies in real-world scenarios. Legal experts and AI ethicists are expected to analyze the report closely to understand the nature of ChatGPT's interaction, whether it involved providing instructions, influencing behavior, or being used in a manner that contributed to the outcome. The case highlights the pressing need for comprehensive regulatory frameworks and robust safety protocols to address the societal impact and potential liabilities associated with advanced artificial intelligence systems, particularly concerning human well-being and legal accountability.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>ChatGPT</span><span>Artificial Intelligence</span><span>Legal Implications</span><span>AI Ethics</span><span>Large Language Model</span><span>AI Misuse</span><span>Court Document</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Artificial Intelligence</span><span>Large Language Model</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.461878/gov.uscourts.cand.461878.1.0.pdf" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <section class="source-group">
            <h2 class="source-group-title">huggingface</h2>

            <article class="item-card">
                <h2>UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement</h2>
                <span class="published-time">Published: 2025-12-24T14:08:38.000Z</span>
                
                <p class="summary">In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>3D Shape Generation</span><span>Geometric Refinement</span><span>Diffusion Framework</span><span>High-Fidelity Geometry</span><span>Data Processing</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Deep Learning</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.21185" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>DreamOmni3: Scribble-based Editing and Generation</h2>
                <span class="published-time">Published: 2025-12-27T09:07:12.000Z</span>
                
                <p class="summary">Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Scribble-based Editing</span><span>Scribble-based Generation</span><span>Unified Generation and Editing</span><span>Multimodal Instruction</span><span>Graphical User Interface</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Generative AI</span><span>Multimodal</span><span>Computer Vision</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.22525" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>End-to-End Test-Time Training for Long Context</h2>
                <span class="published-time">Published: 2025-12-29T18:30:14.000Z</span>
                
                <p class="summary">We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Test-Time Training</span><span>Long Context</span><span>Continual Learning</span><span>Transformer</span><span>Meta-learning</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Natural Language Processing</span><span>Deep Learning</span><span>Large Language Model</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.23675" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>Evaluating Parameter Efficient Methods for RLVR</h2>
                <span class="published-time">Published: 2025-12-29T03:13:08.000Z</span>
                
                <p class="summary">We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (e.g., PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (e.g., VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Parameter-Efficient Fine-Tuning</span><span>Reinforcement Learning</span><span>Large Language Models</span><span>Mathematical Reasoning</span><span>LoRA</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Natural Language Processing</span><span>Machine Learning</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.23165" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>

            <article class="item-card">
                <h2>GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs</h2>
                <span class="published-time">Published: 2025-12-24T07:13:24.000Z</span>
                
                <p class="summary">Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness. In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.</p>
                <div class="meta-tags">
                    <div class="keywords"><span class="label">KeywordsÔºö</span><span>Mixture-of-Experts</span><span>Large Language Models</span><span>Safety Alignment</span><span>Inference-time attacks</span><span>GateBreaker</span></div>
                    <div class="area"><span class="label">AreasÔºö</span><span>Large Language Model</span><span>Deep Learning</span><span>Natural Language Processing</span></div>
                </div>
                <div class="read-more">
                    <a href="https://huggingface.co/papers/2512.21008" target="_blank" rel="noopener noreferrer">Read more...</a>
                </div>
            </article>
        </section>
        <footer class="report-footer">
            Generated by AI Assistant
        </footer>
    </div>
</body>
</html>