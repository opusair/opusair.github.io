[
  {
    "id": "Twitter75b81d63-fb23-4cb1-ae4c-5a5cddf5b8b3",
    "source": "Twitter",
    "url": "https://x.com/AnimaAnandkumar/status/1926781853630930946",
    "title": "In a recent interview I talk about what it takes for AI to make new scientific discoveries. tldr: it won’t be just LLMs.",
    "content": "In a recent interview I talk about what it takes for AI to make new scientific discoveries. tldr: it won’t be just LLMs.",
    "summary": "在最近的采访中，Anima Anandkumar教授讨论了AI在进行科学发现方面的必要条件，强调不仅仅依赖于大型语言模型（LLMs）。",
    "keywords": "AI,科学发现,LLMs,AnimaAnandkumar,研究",
    "area": "人工智能,自然语言处理,其他",
    "published_time": "2025-05-26T07:26:00Z",
    "download_time": "2025-05-26 07:26:00",
    "visual_resource": [
      "screenshot/twitter_Twitter75b81d63-fb23-4cb1-ae4c-5a5cddf5b8b3.png"
    ]
  },
  {
    "id": "Twitter3fff1a58-9324-4b88-bcea-e7a629c5bd86",
    "source": "Twitter",
    "url": "https://x.com/Kyle_L_Wiggers/status/1926743728598724914",
    "title": "From LLMs to hallucinations, here’s a simple guide to common AI terms",
    "content": "From LLMs to hallucinations, here’s a simple guide to common AI terms",
    "summary": "这篇文章提供了一份关于大语言模型（LLM）到幻觉（hallucinations）等常见人工智能术语的指南。这些术语涵盖了一系列与人工智能和机器学习相关的概念，包括模型训练、数据集处理、推理与解释以及应用实例。文章还引用了TechCrunch作为信息来源，并提供了一些实例来展示其实际应用。",
    "keywords": "大语言模型, 幻觉, 人工智能, 指南, TechCrunch",
    "area": "人工智能, 自然语言处理, 其他",
    "published_time": "2025-05-26T04:54:00Z",
    "download_time": "2023-10-14 11:22:00",
    "visual_resource": [
      "screenshot/twitter_Twitter3fff1a58-9324-4b88-bcea-e7a629c5bd86.png"
    ]
  },
  {
    "id": "Twitterd5c1274e-47c6-4a1f-bbe2-dbcbb4750432",
    "source": "Twitter",
    "url": "https://x.com/GaryMarcus/status/1926851942917054485",
    "title": "“We want to do the AI thing, with the computers” - The President of The United States",
    "content": "“We want to do the AI thing, with the computers” - The President of The United States",
    "summary": "美国总统的一句话显示出对人工智能的兴趣，引发了大家对AI技术发展的关注。",
    "keywords": "美国总统,人工智能,AI技术,发展,关注",
    "area": "人工智能,自然语言处理,其他",
    "published_time": "12:04 PM · May 26, 2025",
    "download_time": "2025-05-14 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitterd5c1274e-47c6-4a1f-bbe2-dbcbb4750432.png"
    ]
  },
  {
    "id": "Twitter9c2d0fba-6039-4416-a120-57877d6d2c96",
    "source": "Twitter",
    "url": "https://x.com/Goran_Majic/status/1926673373750288424",
    "title": "AI-generated Video Content: Limitations and Human Influence",
    "content": "It’s interesting: AI often positions the camera as if it’s held by a professional cameraman using a gimbal, or mounted on a physical surface. This reveals something important: all AI-generated video content is still heavily based/scraped on footage created by humans, with all the limitations of real-world environments and hardware. But AI shouldn’t have those limitations, right? It shouldn’t be constrained by the physical boundaries of lenses, tripods, or gravity. So why does it still mimic them? 😉 @GaryMarcus",
    "summary": "这篇文章探讨了AI生成视频内容的限制，指出这些内容往往基于人类创造的素材，反映出AI技术仍受制于真实环境的限制，尽管理论上AI不应受此约束。",
    "keywords": "AI视频,限制,人类影响,内容生成,物理约束",
    "area": "人工智能,多模态,视频理解",
    "published_time": "12:15 AM · May 26, 2025",
    "download_time": "2023-10-11 12:34:56",
    "visual_resource": [
      "screenshot/twitter_Twitter9c2d0fba-6039-4416-a120-57877d6d2c96.png"
    ]
  },
  {
    "id": "Twitter328ebcf8-a504-4af1-af71-d88d0b87a296",
    "source": "Twitter",
    "url": "https://x.com/_KarenHao/status/1926997388327035208",
    "title": "EMPIRE OF AI is the @NPR book of the day",
    "content": "EMPIRE OF AI is the book of the day. 😍😍 Order my book on OpenAI and Silicon Valley’s extraordinary seizure of power to build so-called AGI here: empireofai.com. npr.org Karen Hao's new book is a skeptical look at Sam Altman and Elon Musk's AI empire: NPR's Book of... From npr.org",
    "summary": "《人工智能的帝国》成为@NPR当天推荐书籍。这本书探讨了OpenAI及硅谷在建构被称为AGI的人造智能方面的异常权力攫取，提出了对Sam Altman和Elon Musk的AI帝国的批判性看法。",
    "keywords": "EMPIRE OF AI, OpenAI, AGI, Sam Altman, Elon Musk",
    "area": "人工智能,其他",
    "published_time": "2025-05-26T21:42:00Z",
    "download_time": "2023-10-04 14:15:00",
    "visual_resource": [
      "screenshot/twitter_Twitter328ebcf8-a504-4af1-af71-d88d0b87a296.png"
    ]
  },
  {
    "id": "Twitter0c0f51a4-42fb-4066-b365-0cb105e49fa8",
    "source": "Twitter",
    "url": "https://x.com/nitashatiku/status/1926843487279817138",
    "title": "I just finished @_KarenHao book & it’s incredible. A reporting triumph.",
    "content": "I just finished @_KarenHao book & it’s incredible. A reporting triumph. How company books should be done. Even if you really know OpenAI, it’s still riveting. Smooth, gripping, with many new, rich, telling details. San Francisco, come see me interview her Wed 5/28 @ Commonwealth Club",
    "summary": "我刚读完@_KarenHao的书，简直难以置信。这是一场报道的胜利。即使你已经了解OpenAI，这本书依然引人入胜，内容流畅、引人入胜，且包含了许多新的、丰富而发人深省的细节。5月28日周三，于旧金山来见证我对她的采访。",
    "keywords": "KarenHao,OpenAI,旧金山,采访,书籍",
    "area": "自然语言处理,多模态,其他",
    "published_time": "2025-05-26T11:31:00Z",
    "download_time": "2023-10-17 15:23:56",
    "visual_resource": [
      "screenshot/twitter_Twitter0c0f51a4-42fb-4066-b365-0cb105e49fa8.png"
    ]
  },
  {
    "id": "Twitter89f5e433-c81a-4a56-ad10-bce6422730eb",
    "source": "Twitter",
    "url": "https://x.com/oomdamar/status/1926919640581611849",
    "title": "Menghabiskan waktu di tokbuk Kinokuniya dan Periplus isi waktu, sambil mikir gimana cara agar bisa kebeli dan kebaca buku2 keren ini.",
    "content": "Menghabiskan waktu di tokbuk Kinokuniya dan Periplus isi waktu, sambil mikir gimana cara agar bisa kebeli dan kebaca buku2 keren ini. 1. Empire of AI",
    "summary": "在Kinokuniya和Periplus书店消磨时间，并思考如何购买和阅读这些很棒的书籍。",
    "keywords": "书店,购买,阅读,书籍,Empire of AI",
    "area": "人工智能,多模态,其他",
    "published_time": "2025-05-26T16:33:00Z",
    "download_time": "2023-11-01 12:00:00",
    "visual_resource": [
      "screenshot/twitter_Twitter89f5e433-c81a-4a56-ad10-bce6422730eb.png"
    ]
  },
  {
    "id": "6gtSZ_9-jyORBi5BVFUlvw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/6gtSZ_9-jyORBi5BVFUlvw",
    "title": "实测惊艳全球的Veo3！音画同步无敌，贵是有原因的",
    "summary": "谷歌最新发布的Veo3文生视频模型，在近期开发者大会上惊艳亮相，突破性地实现了视频与音频的同步生成，涵盖画面、环境音效、背景音乐及人物对白，并能精准匹配口型。通过实测，Veo3在生成脱口秀、新闻播报、情感演唱乃至Twitch风格游戏直播等场景时，展现出极高的逼真度和音画一致性，尤其在处理单一场景与简单动作时效果卓越。然而，其在生成复杂体操动作或篮球比赛等高速、多变场景时，仍存在局部扭曲或逻辑不符的局限。尽管如此，Veo3的问世标志着生成式AI在视频创作领域迈出重要一步，开启了内容生成的新可能。",
    "keywords": [
      "Veo3",
      "文生视频",
      "音画同步",
      "生成式AI",
      "大模型",
      "视频生成"
    ],
    "area": [
      "生成式AI",
      "多模态",
      "大模型"
    ],
    "content": "标题：实测惊艳全球的Veo3！音画同步无敌，贵是有原因的\n公众号：AI好好用\n--------------------------------------------------\n\n编辑：杨文、+0\n\n好莱坞要完蛋了。\n\n「你大爷永远是你大爷」这句话的含金量还在上升。\n\n上周谷歌举办了一场开发者大会，祭出一堆好东西，其中最让人震撼的就是 Veo3。\n\n该模型具备强大的文本和图像转视频能力，并首次实现了视频与音频的同步生成。\n\n换句话说，视频画面和环境音效、背景音乐、人物对白终于可以一锅出了，而且口型还能对得上。\n\n视频加载失败，请刷新页面再试\n\n不少网友心甘情愿为其氪金，并在社交平台放出了诸多 Veo3 生成的视频，我看完后的第一反应就是刘晓艳「附体」：\n\n没演技的流量明星们，回家吧。\n\n咱不说别的，当初看《演员请就位》第一期的时候，就被这群选手们的烂演技炸得脑瓜子嗡嗡的。\n\n杨子为了演出西门庆的放荡，不是对着于佩尔夸赞「龙睛凤眼，唇红齿白」，就是追着章子怡「死锤烂打」：\n\n刘梓晨版的九妖之王相柳，来一个导师他就「死」一次，演个倒地都一股子喜感：\n\n再对比下 Veo3 生成的「演技」。一位美国士兵在战火纷飞的战场上踉跄行走，表情木然，双眼空洞，突然他停下脚步，在泥泞中跪下，低声呢喃：「为什么我还活着？」\n\n瞅瞅这细微的小表情，这流畅的肢体动作，这充满绝望的台词，你觉得流量明星们赶得上吗？\n\nPrompt：Handheld medium shot tracking an American soldier walking through a ruined Normandy battlefield at dusk. Heavy rain falls. The camera moves backward, facing him directly. His muddy face is blank, eyes hollow. Explosions flash behind him. He stops, kneels in the mud, and whispers: ‘Why am I still here?’ A slow, somber orchestral score swells.\n\n这个 Veo3 生成的车展视频，也逼真的让人分不清现实还是虚拟。\n\n还有下面这个 ASMR 视频，也是出自 Veo3 之手。整个过程该博主就用了一句提示词：asmr creator typing on a noisy keyboard and then looking up and blowing into the microphone as she talks。\n\n更离谱的是，X 网友 Hashem Al-Ghaili 拿 Veo3 探讨了一个非常魔幻的哲学问题：如果 AI 生成的角色不相信自己是 AI 生成的，会发生什么？\n\nVeo 3 生成的视频在视觉和音频上都达到了极高的逼真度，角色动作、表情、口型同步以及环境音效足够以假乱真。\n\n虽然我们不是尊贵的 Ultra 会员，但前段时间谷歌突然卡 bug，普通用户只需登录 Google 个人账户，且 IP 为美国，就可以免费领取 Google One 会员到 2026 年底，我们正好薅到了羊毛。今天一试发现有了这个会员也可以在 Gemini 官网和 Flow 中使用 Veo3。\n\n接下来，我们就亲自实测一波，看看它是否真的有两把刷子。（温馨提示：以下实测均一次生成，无抽卡。）\n\nGemini：https://gemini.google.com/\n\nFlow：https://labs.google/flow/about\n\n一手实测\n\nGemini 官网已更新换代，下方聊天框中除了 Deep Research 和 Canvas 功能外，又新增了 Video 按钮，我们只需输入提示词即可生成 Veo3 视频。\n\n值得注意的是，谷歌官网显示，Google AI Pro 用户可使用主要的 Flow 功能和每月 100 次生成，而 Google AI Ultra 用户则获得最高的使用限制以及 Veo 3 的早期访问权限。\n\n刚开始，我们本想用 Veo3 生成「泰勒・斯威夫特唱 rap」的视频，但尝试几次它总是「罢工」。\n\n扒了下 Gemini 的政策指南，发现它拒绝生成会在现实世界中造成伤害和冒犯的内容，例如儿童安全威胁、危险活动、暴力血腥、露骨色情内容或者拿现实中的名人整活。\n\n那我们就先来个脱口秀。\n\n提示词：一个脱口秀演员在台上说了一个笑话，内容是「别整天说自己是单身狗，狗在你这个年纪，早 die 了」，观众爆笑。\n\n视频中，脱口秀演员讲笑话的节奏感掌握得很好，观众的反应也很真实、自然，这不比春晚的尬相声好多了？\n\n说到做假新闻，Veo3 更是一绝。\n\n提示词：A news anchor with a serious tone reporting an obviously fake news story about aliens landing in New York City, complete with stock footage overlays, dramatic music, and animated graphics behind them — newsroom background, 16:9 aspect ratio.\n\nAI 主播坐在演播室，操着一口纯正的美式播音腔一本正经地胡说八道，就是眼神稍微凶狠了些。\n\nVeo3 多少有点刻板印象，比如让它生成一个唱 rap 的歌手，它大概率输出的是黑人。\n\n提示词：A male singer in a cozy recording studio singing into a microphone with headphones on, surrounded by acoustic panels and warm lighting — close-up on emotional facial expressions, intimate mood.\n\n但不管怎样，这视频生成效果确实没得说，无论是歌手的深情演唱，还是歌曲旋律，都真实得没边了。\n\n最让机器之心编辑部看傻了的，是这个 Veo3 生成的游戏直播视频。\n\n提示词：Streamer-style Minecraft gameplay footage with a facecam overlay in the corner, showing a male gamer reacting excitedly while battling mobs in a cave — Twitch stream layout, live chat visible, dynamic lighting.\n\n角落里的主播，占据屏幕大部分的《我的世界》动态游戏画面，还有观众聊天框，简直就是 Twitch 直播标配。\n\n尤其是游戏主播的反应，瞪大双眼，嘴里喊着「Oh my god」，太真实了！不过唯一的瑕疵就是观众实时聊天框静止不动。\n\n我们再回到这次 Veo 3 强调的「音画同步」上来，让它生成一段简单的对白。\n\n尽管字幕慢了一拍，但 Veo3 这口型对得太丝滑了。\n\n翻车合集\n\nVeo3 的生成效果确实惊艳，但也有翻车的时候。\n\n比如曾让一众视频生成模型「闹笑话」的体操类视频，Veo3 还是搞不定。\n\n提示词：一位体操运动员在明亮的体操房内，身着鲜艳的体操服，在高低杠上优雅地旋转、跳跃、翻腾，动作行云流水，镜头从不同角度捕捉她的精彩表现，背景音乐是激昂的交响乐，旁白详细讲解着她的动作技巧和训练历程。\n\n这个视频乍一看挺像那么回事，但你一帧帧拎出来瞅，好多邪门的细节：在单杠上旋转时要骨折的胳膊、原地跳跃时 360 度旋转的手臂……\n\n提示词：体操馆内，一位气质儒雅的女体操运动员，身着浅粉色体操服，正在高低杠上比赛。她稳稳地抓住高杠，开始一系列复杂的动作，如后摆上、换杠、空翻抓杠等，动作衔接行云流水，展现出高超的技巧和优雅的姿态。镜头切换多样，包括正面、侧面和俯视角度，记录下她在高低杠上的每一个优美弧度，同时捕捉到她在完成动作后的轻松微笑和对观众的挥手致意。\n\n眼尖的小伙伴应该发现了视频的诡异之处，在旋转过程中，运动员的的身体从「正面」丝滑变成了「背面」，不禁幻视之前的波士顿动力 Atlas 翻跟头。\n\n提示词：一位身穿红色 23 号球衣的高大篮球运动员，肌肉线条分明，正站在篮球场三分线外，阳光从场馆高窗洒下，照亮他专注的面庞和紧握篮球的双手。他深吸一口气，做出标准的投篮姿势，双脚微微分开，膝盖微屈，右手托球，左手轻扶球侧，手腕轻抖，将球高高抛向空中，篮球在空中划出一道优美的弧线，最终投入篮筐，镜头跟随篮球的轨迹，捕捉篮筐、篮板和观众席的反应，背景是热闹的篮球馆，观众们或站或坐，欢呼雀跃。\n\n这个视频画面错乱到已无力吐槽，擦着自家篮筐往对手篮筐里投篮，这操作乔丹看了也得沉默，看来 AI 也不太懂篮球。\n\n提示词：在一个宁静的海底峡谷中，阳光温柔地洒下。一群美人鱼正与她们的海洋朋友们亲密互动。一个红发美人鱼轻轻抚摸着一只海龟布满纹路的脖颈，另一位金发美人鱼则与一群顽皮的海豚分享着发光的海藻。她们的歌声在水中回荡，充满了爱与和谐，吸引了各种各样的海洋生物前来倾听，包括优雅的海马、好奇的章鱼和色彩斑斓的热带鱼。她们的脸上洋溢着纯真快乐的笑容，形成一幅温馨动人的画面。\n\n这画面，这质感，是不是很像小时候的劣质拼贴广告？\n\n另外，谷歌官方还贴心地整理了一份提示词指南，帮助大家更好生成自己想要的画面。\n\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/video/video-gen-prompt-guide?hl=zh-cn\n\n以下是基于这份文档整理的实用提示词编写结构与优化方法：\n\n1. 核心场景描述\n\n首先明确视频的主要场景和主题，清晰传达视频的核心内容。例如：\n\n「一个现代化的城市咖啡馆内部，阳光透过大窗户照射进来，照亮了木质桌椅和绿色植物。」\n\n2. 视觉细节描述\n\n补充颜色、材质、光线、氛围等视觉细节。例如：\n\n「咖啡馆装饰着工业风格的金属吊灯，墙上挂有抽象画作。两位顾客坐在窗边的高脚凳上，面前摆放着冒着热气的咖啡杯，杯中拉花清晰可见。」\n\n3. 运动和镜头指令\n\n描述镜头运动、拍摄角度和视角变化。例如：\n\n「镜头从咖啡馆门口缓慢推进，然后平滑地向右平移，展示整个空间，最后停留在窗边的顾客身上，进行特写拍摄。」\n\n4. 音频和音效描述\n\nVeo 3 支持音频生成，可在提示词中指定背景音乐、环境音、对话等。例如：\n\n「背景中可以听到轻柔的爵士乐，咖啡机的嗡嗡声，以及顾客低声交谈的声音。女顾客说道：\" 这是我喝过的最好的拿铁。」\n\n5. 风格与技术参数\n\n补充期望的色调、风格、帧率、分辨率等。例如：\n\n「整体氛围温暖而放松，色调以暖棕色和淡绿色为主，拍摄风格类似电影《爱在黎明破晓前》的质感和光线处理。以电影 24fps、浅景深拍摄，确保高清画质，保持自然的色彩饱和度。」\n\n理论结束，我们来实践一下。根据上述提示词结构，让 Veo3 复刻《肖申克的救赎》中的名场面！\n\n提示词：在一片阴郁的夜幕下，一条通向自由的下水道出口位于树林边缘的土壤中，泥泞湿滑。安迪・杜佛兰（着囚服，浑身污泥）从出口中奋力爬出，全身沾满污水与污泥。他踉跄爬起，走到空旷草地中央，天空忽然下起滂沱大雨。闪电划过夜空，在雨中泛出银白的光。 安迪仰望天空，张开双臂，头仰向天，任雨水冲刷全身，脸上显露出一种崩溃后的解脱与重生的神情。 镜头从安迪背后缓慢升起，采用低机位仰拍逐渐转为鸟瞰俯拍，随着雨水从空中泼洒而下，镜头旋转轻微环绕他，营造出史诗感和敬畏感。地面泥泞中留下的是他艰难爬行的痕迹。 背景音中雷声轰鸣，雨声密集而真实，伴随着低沉的管弦乐情绪逐渐上扬，烘托出破茧成蝶般的胜利与自由感。 整个画面以冷蓝色调为主，突出夜雨肃穆庄严的氛围。光影处理上以闪电和月光微弱照亮安迪湿漉漉的身影和表情，手臂上的水珠闪动微光。 画面风格类似电影《肖申克的救赎》原片，注重写实布光与戏剧化构图，帧率 24fps，使用电影级浅景深虚化周围景物，强调人物的孤独与灵魂的觉醒。 特写镜头捕捉雨水从他脸颊缓缓滑落，他的双眼微闭，嘴角略微颤动，传递出不可言说的复杂情感。\n\n对比原版，质量还是有待提升，但内容相对完整。\n\n测试过程中还发现，英文提示词会比中文提示词效果好一点。\n\n总体来说，Veo3 的音画同步非常惊艳，在生成场景单一、动作简单的画面时效果很真实，但涉及到多种场景转换和复杂的交互时，就略显乏力了。\n\n从 GPT-4o、即梦那些以假乱真的图像，到可灵、Veo 3 让人惊叹的视频效果，科技的进步让人目不暇接，甚至有点喘不过气。\n\n面对这一切，简单地唱衰或叫好没有意义，我们更期待的是， 这些强大的技术能够实实在在地为我们每个人的生活增添一些便利，或者解决一些我们真正头疼的问题。 毕竟，科技的真谛不是让人类跪着喊「牛 X」，而是让我们能躺着喊「舒服了」。\n\n参考链接：\n\nhttps://x.com/HashemGhaili/status/1925616536791760987\n\nhttps://x.com/MayorKingAI/status/1926046987884908848\n\nhttps://x.com/laszlogaal_/status/1925094336200573225\n\nhttps://x.com/venturetwins/status/1925046014689608146\n\n© THE END\n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com",
    "published_time": "2025-05-26T09:50:36.000Z",
    "download_time": "2025-05-26T23:48:54.546483",
    "visual_resource": [
      "https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MXaibAM9XERwicygL7ye5W9AtRkQ4Aic10Dwh7SxKLexwYGl98fLY8rcmarldeEIe6T4RFKLnVdbKQ/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T09:50:36.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MXaibAM9XERwicygL7ye5W9AtRkQ4Aic10Dwh7SxKLexwYGl98fLY8rcmarldeEIe6T4RFKLnVdbKQ/0?wx_fmt=jpeg\", \"id\": \"6gtSZ_9-jyORBi5BVFUlvw\"}, \"extraction_info\": {\"account\": \"AI好好用\", \"file_path\": \"./database/content/wechat/6gtSZ_9-jyORBi5BVFUlvw.txt\"}}"
  },
  {
    "id": "_z-shpkM0N6w_ueS1UF7uQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/_z-shpkM0N6w_ueS1UF7uQ",
    "title": "机器人杭州上演格斗赛！拳拳到肉，宇树CEO王兴兴：创造了人类历史新时刻",
    "summary": "全球首个人形机器人格斗赛近期在杭州圆满落幕，标志着机器人技术发展的一个里程碑时刻。比赛中，宇树科技G1人形机器人进行了激烈对抗，展现了灵活的格斗技巧。赛事规则涵盖击打得分、倒地扣分，并结合了人类遥控和语音控制。此次格斗赛不仅是娱乐表演，更在于在高压、快节奏环境中，充分检验人形机器人的抗冲击性、多模态感知以及全身协调能力。通过强化学习和先进算法，机器人实现了对专业选手动作的模仿和优化，体现了AI与机器人硬件在极端条件下的协同能力，为人形机器人未来的性能提升提供了宝贵经验。",
    "keywords": [
      "人形机器人",
      "格斗赛",
      "宇树科技",
      "人工智能",
      "强化学习",
      "运动控制",
      "抗冲击性"
    ],
    "area": [
      "机器人",
      "人工智能",
      "多模态"
    ],
    "content": "标题：机器人杭州上演格斗赛！拳拳到肉，宇树CEO王兴兴：创造了人类历史新时刻\n公众号：量子位\n--------------------------------------------------\n\n激烈，着实激烈！\n\n全球首个人形机器人格斗赛刚刚在杭州落下帷幕，战况那叫一个精彩纷呈。\n\n贴身肉搏、侧身闪避、倒地后快速爬起…格斗技巧的十八般武艺通通拿出来了：\n\n当然也不乏搞笑画风，有选手直接对着空气一通乱挥：\n\n本次公开表演赛一共有四支参赛队伍，均使用了宇树科技G1人形机器人。\n\n在经过两两PK后，最终由名为“AI测算师”的机器人夺冠：\n\n对于整场比赛，有现场解说员激动表示，“对比两三个月前，进步天差地别”。\n\n宇树科技CEO王兴兴更是发朋友圈声称：\n\n创造了人类历史新时刻！\n\n那么，首个人形机器人格斗赛都有哪些看点呢？\n\n先简单介绍下比赛规则。\n\n机器人对战过程中，击中头部、躯干为有效击打，手部动作有效击打计1分，腿部动作有效击打计3分。倒地一次扣5分，被击倒8秒内无法起身则扣10分，本回合结束。\n\n全程机器人可以由人类手动遥控以及语音控制。\n\n话不多说，我们直接请出四位参赛选手，为便于区分，以下直接称呼为：小红、小粉、小绿和小黑。\n\n比赛分为表演赛和竞技赛，前者主要是热热场，由其他非参赛机器人给大家打打样。\n\n这里还发生了一件小插曲，小紫机器人不慎跌出拳台，结果一个“鲤鱼打挺”自己起来了（不过关键画面被导播切走了）。\n\n调动起全场气氛后，重头戏竞技赛正式开始，它采用标准的三回合赛制，每回合两分钟，分组如下：\n\n第一场比赛，由“算无遗测”的小黑和“全场最粉”的小粉打响。\n\n第一回合一开始，双方都毫不“腿软”，默契使用了利落的侧踢。\n\n不过打着打着，二位就开始背对背拥抱了，有点找不准对手位置。\n\n在一番主动出击后，小黑华丽丽迎来倒下（还是被自己绊倒的），不过幸好在5s左右就爬起来了。\n\n注意看，此时另一边的小粉还在赤裸裸挑衅（doge）。\n\n一番胶着后，第一回合计时结束。\n\n进入第二回合，小粉的攻势开始加强，还上演了惊魂一幕——\n\n同样由于主动出击导致下盘不稳后，小粉一个趔趄差点倒地。\n\n在小粉的持续挑衅和小黑的数次反击中，第二回合仍在纠缠中结束。\n\n最后一回合，面对小粉施展的抱腰战术，小黑一个侧踢带动了小粉的身体，导致小粉倒地。\n\n不过搞笑的是，明明胜负即将揭晓，小黑自己又被小粉绊倒了。\n\n好在计时过程中，小黑最终缓缓起身，最终拿下第一局（原直播中小粉获胜为口误）。\n\n接下来上场的是小绿和小红。\n\n第一回合，双方火药味直接溢出屏幕，各种直拳、勾拳、侧踢轮番上演。\n\n随着一个失误，小绿一秒倒地，不过幸好快速起来了。\n\n进入第二回合后，小绿一上来就拿下3分，出腿动作相当利落。\n\n还开启上头模式，组合拳就跟不要钱一样往外扔：\n\n甚至，它还直接嘲讽挑衅对手：\n\n对此，小红也不惯着，一边利落反击一边反嘲：\n\n伴随着双方的激烈打斗，比赛进入第三回合。\n\n本以为二位还会缠斗一会儿，结果令人出乎意料的是——\n\n在大家都没反应过来的时候，小绿一上来就是一个侧踢，一秒钟杀死比赛。\n\n终于来到决胜局。\n\n比赛开始前，官方特意展示了待解锁的“终极大招”——飓风冲撞，机器人一阵快速奔跑积蓄能量后，挥出充满力量的一拳（不过最后未使用）。\n\n第一回合，战斗节奏越来越快，小绿一个不稳直接迎面倒下。\n\n令人惊险的是，它卡在计时结束前站了起来：\n\n类似的情况也延续到了最后两个回合，小绿又倒下了两次（规定时间内起来了）：\n\n虽然小绿的坚强令人感动，但根据规则，失误更少的小黑最终拿下了比赛。\n\n而这位“算无遗测”的选手，也获得了《世界机器人大赛·系列赛》的邀请函。\n\n事实上，除了科普展示，本次比赛作为史上第一个人形机器人格斗锦标赛，更充分考验了机器人在高压、快节奏的极端环境中的抗冲击性、多模态感知和全身协调能力。\n\n以参加本次比赛的宇树机器人G1为例，身高约130cm，体重约35kg，在MMA中比最轻的蝇量级（57kg）还要低，但配备有先进的计算能力和流畅的运动控制。\n\n根据介绍，这些机器人在比赛前还要参加大量“培训”。\n\n通过多个传感器，技术人员对专业格斗选手进行动作捕捉，并将关键运动轨迹映射到机器人身上。\n\n然后利用AI强化学习，让机器人不断训练调整后，四位参赛选手已初步可以比肩职业选手——配备8套基础格斗动作和多个组合动作，包括直拳、勾拳、侧踢和空中旋转踢等。\n\n视频加载失败，请刷新页面再试\n\n等到实际上场后，操作员主要通过语音控制、遥感控制两种方式协同完成对机器人的实时操纵。\n\n具体玩法是，利用遥控器摇杆控制前进方向，按键组合进行招式出击。\n\n由于融合了动作控制、智能决策等多种算法模型，当操作员发出指令时，机器人的感知系统就会迅速定位对手并构建地图数据，决策系统将进行实时评估，生成可执行策略路径，并交由控制执行系统完成动作。\n\n这有点像咱们小时候玩的拳皇，只不过现在是3D版。\n\n除了算法方面，本次使用的机器人在硬件上也有特殊设计。\n\n当中最值得一提的是，相较于真人拳击比赛中出于对选手的保护，规定禁用腿部动作，那么本次格斗比赛，机器人选手们则是手脚并用，十八般武艺轮番上场。\n\n这就对机器人的灵活性以及平衡性提出了更高的要求。\n\n因此，技术人员在赛前反复对其进行“抗击打”、“防过热”训练，就是为了保证人形机器人在极端条件下依旧能够完成比赛，并在摔倒后模拟人类形态自主站立。\n\n在训练中也能明显发现，在遭受猛烈击打下，机器人的金属外壳出现了多处“皮外伤”，但运动功能却丝毫不受其影响。\n\n据现场测试员介绍，这是因为额外为参赛选手的骨骼和关键结构件进行了抗干扰设计和材料升级，也通过特定的动态平衡补偿算法优化机器人的抗冲击性能，在软硬件的协调配合下，确保动作足够平稳和精确。\n\n而比赛期间所暴露出的问题，例如朝向错误方向挥拳、主动攻击但失去平衡，都将在未来的针对性算法优化中，进一步促进人形机器人性能全方位提升。\n\n赛场内拳拳到肉、热火朝天的同时，赛场外的网友们也唇枪舌战、热议连连。\n\n有网友惊呼，足球已经pass了，现在机器人MMA才是王道！\n\n也有人将它和前不久的北京机器人马拉松比赛对比，为技术的进步欢呼雀跃。\n\n当然也有网友已经开始思考机器人的权利……\n\n那么你对此有什么看法吗？欢迎在评论区留言讨论～\n\n参考链接：[1]https://x.com/kimmonismus/status/1926688772768284710?s=46[2]https://x.com/cixliv/status/1926624239940981247[3]https://x.com/CGTNOfficial/status/1926616679288615213[4]https://news.cctv.com/2025/05/24/ARTIVutCzf3xCnCxgqwtsBMS250524.shtml[5]https://news.cctv.com/2025/05/25/ARTIanejqVfCNeeekKA8FsHg250525.shtml[6]https://www.yicai.com/news/102634073.html\n\n— 完 —\n\n📪 量子位AI主题策划正在征集中！欢迎参与专题365行AI落地方案，一千零一个AI应用，或与我们分享你在寻找的AI产品，或发现的AI新动向。\n\n💬 也欢迎你加入量子位每日AI交流群，一起来畅聊AI吧～\n\n一键关注 👇 点亮星标\n\n一键三连「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！",
    "published_time": "2025-05-26T05:28:35.000Z",
    "download_time": "2025-05-26T23:47:49.619707",
    "visual_resource": [
      "https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBHYQjMiappg4LGkN8wRtPFmaF4ibpwp2y2wXsHQJyjPgGtH6lM8hgAZtYmxKAu0BVoBmC1UANktDDg/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T05:28:35.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBHYQjMiappg4LGkN8wRtPFmaF4ibpwp2y2wXsHQJyjPgGtH6lM8hgAZtYmxKAu0BVoBmC1UANktDDg/0?wx_fmt=jpeg\", \"id\": \"_z-shpkM0N6w_ueS1UF7uQ\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/_z-shpkM0N6w_ueS1UF7uQ.txt\"}}"
  },
  {
    "id": "-gKPx3nXSk-RwU8CCcLm4g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/-gKPx3nXSk-RwU8CCcLm4g",
    "title": "4000亿国产算力航母：芯片巨头合并超算巨头",
    "summary": "中国算力领域迎来重大整合。芯片巨头海光信息与中科曙光宣布筹划重组，拟由海光信息换股吸收合并中科曙光，并募集配套资金。此次强强联合旨在构建国产算力“航母”，应对蓬勃发展的AI计算需求。海光信息专注于CPU和GPU研发，掌握核心技术，其产品广泛应用于AI计算、大模型等领域；中科曙光则深耕高性能计算机和服务器市场。双方营收均保持增长，此次合并有望协同优势，提升中国算力产业链的自主创新与发展，增强国产芯片和超算在人工智能时代的竞争力。",
    "keywords": [
      "算力",
      "芯片",
      "超算",
      "重组",
      "海光信息",
      "中科曙光",
      "人工智能",
      "大模型"
    ],
    "area": [
      "人工智能",
      "大模型",
      "深度学习"
    ],
    "content": "标题：4000亿国产算力航母：芯片巨头合并超算巨头\n公众号：量子位\n--------------------------------------------------\n\n中国算力巨震，海光信息、中科曙光两大巨头官宣拟合并！\n\n就在昨晚，上交所披露双方“关于筹划重大资产重组的停牌公告”：\n\n正在筹划由海光信息通过向公司全体A股换股股东发行A股股票的方式换股吸收合并中科曙光，并发行A股股票募集配套资金。\n\n根据公告，为保证公平信息披露，维护投资者利益，避免造成公司股价异常波动，两家公司A股股票自5月26日开市时起开始停牌。\n\n预计停牌时间不超过10个交易日。\n\n这一消息如同一颗深水炸弹，瞬间引爆资本市场与科技行业。\n\n吸并方海光信息，公司全称海光信息技术股份有限公司，2014年成立。\n\n2022年08月12日，海光信息在上交所科创板上市，公司证券代码为688041，发行价格36元/股。\n\n公司聚焦高端CPU、通用GPU等计算机芯片产品和系统的研发，包含3000、5000、7000三大产品系列：\n\n早在2016年，海光信息与AMD共同成立合资子公司并获得AMD技术授权，处理器兼容市场主流x86指令集。\n\n据其官网介绍，在高性能处理器核心技术自主研发方面，海光信息已拥有全球授权专利891项，累计申请专利1821项，已登记软件著作权259项，拥有集成电路布图专有权250项。\n\n其产品已广泛应用于企业计算、云数据中心、大数据分析，边缘计算等众多领域，并在AI浪潮中抢占先机。\n\n比如自主研发异构计算平台，提供全面的开发者工具，包括编译器、训练框架、推理框架、大模型套件等全栈能力。海光信息还是首批宣布适配DeepSeek的芯片企业之一。\n\n根据海光信息公开的2025年第一季度报告，其Q1营业收入24亿元，同比增长50.76%；归母净利润5.06亿元，同比增长75.33%。\n\n主要因通用计算与AI计算市场需求增加，产品竞争力提升。\n\n同时，报告中还显示，海光信息第一大股东即为中科曙光，持股比例27.96%。\n\n截至上周五休市，海光信息股价为136.13元/股。\n\n总市值为3164.12亿元，在上交所科创板股票市价总值排名第一。\n\n被吸并方中科曙光，公司全称曙光信息产业股份有限公司，2006年成立。\n\n2014年，中科曙光在上交所主板上市。\n\n中科曙光一直专注于服务器领域的研发、生产与应用，在高性能计算机领域有非常深厚的积累。\n\n其在全国各地拥有3大智能制造生产基地、5大研发中心，全国50多个城市都有其部署的城市云计算中心。\n\n根据中科曙光公开的2025年第一季度报告，其Q1营业收入达25.86亿元，同比增长4.34%；归母净利润1.86亿元，同比增长30.79%；经营活动现金流净额-11.18 亿元，同比净流出扩大（上年同期-4.94亿元）。\n\n截至上周五休市，中科曙光股价为61.90元/股，总市值为905.72亿元。\n\n另外，关于这次两家合体，公告中还声明：\n\n本次重组的具体方案以双方进一步签署的交易文件为准。本次重组尚需履行必要的内部决策程序并需经有权监管机构批准后方可正式实施，能否实施尚存在不确定性。\n\n参考链接：[1]https://www.hygon.cn/investor[2]https://www.sugon.com/investor/affiche#[3]https://www.sse.com.cn/disclosure/listedinfo/announcement/\n\n— 完 —\n\n📪 量子位AI主题策划正在征集中！欢迎参与专题365行AI落地方案，一千零一个AI应用，或与我们分享你在寻找的AI产品，或发现的AI新动向。\n\n💬 也欢迎你加入量子位每日AI交流群，一起来畅聊AI吧～\n\n一键关注 👇 点亮星标\n\n一键三连「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！",
    "published_time": "2025-05-26T05:28:35.000Z",
    "download_time": "2025-05-26T23:48:09.953880",
    "visual_resource": [
      "https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBHYQjMiappg4LGkN8wRtPFmOcvsosR0MUyc2icc2plPqfUa2nouD4E5YA1Dvb9zkDqnzy2hrjcjZNQ/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T05:28:35.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBHYQjMiappg4LGkN8wRtPFmOcvsosR0MUyc2icc2plPqfUa2nouD4E5YA1Dvb9zkDqnzy2hrjcjZNQ/0?wx_fmt=jpeg\", \"id\": \"-gKPx3nXSk-RwU8CCcLm4g\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/-gKPx3nXSk-RwU8CCcLm4g.txt\"}}"
  },
  {
    "id": "aavYxTWWHeUhm2jID5s9bg",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/aavYxTWWHeUhm2jID5s9bg",
    "title": "说句话就能飞！北航发布语言交互的无人机控制模型",
    "summary": "北京航空航天大学团队发布一种创新的语言交互无人机控制模型，旨在通过自然语言指令实现无人机精细化飞行控制。该团队利用模仿学习方法，基于人类飞行员操作数据开发出视觉语言动作（VLA）模型，使无人机能准确执行原子化语言指令。为填补语言引导无人机低层控制的空白，北航构建了大规模真实世界数据集与仿真评估基准。研究还提出地面站-无人机协作策略及轨迹对齐算法，以应对机载资源限制和通信延迟。目前，该模型已在北航国际创新研究院开放场景成功部署，首次通过自然语言对话实时控制无人机完成任务，显著降低了操作门槛，展现了无人机作为“智能助手”的巨大潜力。",
    "keywords": [
      "无人机",
      "语言交互",
      "模仿学习",
      "视觉语言动作",
      "飞行控制",
      "多模态",
      "智能体",
      "数据集"
    ],
    "area": [
      "人工智能",
      "自然语言处理",
      "多模态"
    ],
    "content": "标题：说句话就能飞！北航发布语言交互的无人机控制模型\n公众号：量子位\n--------------------------------------------------\n\n论文链接：https://arxiv.org/abs/2505.15725项目主页：https://prince687028.github.io/UAV-Flow/\n\n一句话就能让无人机起飞？\n\n就像人类飞行员一样，听懂指令后立刻照做。\n\n该团队采用模仿学习方法，使无人机通过习得人类飞行员在真实环境中的操作策略来响应原子化语言指令。\n\n随后，视觉语言动作（VLA）模型被成功部署至真实无人机平台，并在北京航空航天大学国际创新研究院的开放场景中验证了其基于自然语言对话实现飞行控制的可行性。\n\n近年来，无人机（UAV）凭借其灵活的空中机动能力，已成为视觉感知与任务执行领域的重要平台。\n\n随着自动化技术的普及，无人机操作的门槛大幅降低，但如何让它像“智能助手”一样理解人类语言，例如只需说一句“环绕着我飞”，无人机就能理解并执行相应的动作，仍是亟待突破的前沿课题。\n\n当前研究主要将地面机器人的视觉语言导航（VLN）任务迁移至无人机平台，重点解决基于语言指令的目标搜索与远程导航等高层次推理问题。\n\n然而，语言引导的无人机低层控制（如执行短距离原子动作或响应简单指令）尚不完备，成为实现智能无人机系统的关键但尚未充分探索的方向。\n\nFlying-on-a-Word (Flow) 任务致力于实现自然语言指令与无人机精细飞行控制的高效对齐。\n\n在该任务框架中，无人机代理整合三种输入模态：自然语言指令、六自由度状态信息和第一视角视觉观测，生成符合指令语义的动作序列，以模仿人类飞行员的操作。\n\n为支持Flow任务的研究，北航刘偲教授团队构建了一个大规模的真实世界语言引导的无人机模仿学习数据集。该数据采集工作在三所高校校园内展开，覆盖总面积达5.02平方公里。\n\n为建立统一的评估基准，研究团队构建了UAV-Flow-Sim仿真数据集，并在仿真闭环测试环境下对多个模型进行了系统评估，采用成功率(SR)和归一化动态时间规整(NDTW)等指标对飞行轨迹质量进行量化分析，具体测试结果如下。\n\n针对无人机机载计算资源受限的挑战，研究团队提出了一套地面站-无人机协作策略，并为缓解通信和推理延迟带来的控制滞后问题，提出具有前瞻机制的全局轨迹对齐算法，确保运动控制的连续性。\n\n团队基于UAV-Flow真实世界数据集训练了Pi-0-UAV模型，在北航国新院的开放场景中成功实现了视觉语言动作（VLA）系统的真机部署，首次通过自然语言对话实时控制无人机完成指令任务。\n\n更多真机飞行精彩视频，详见项目主页。\n\n一键三连「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n🌟 点亮星标 🌟",
    "published_time": "2025-05-26T05:28:35.000Z",
    "download_time": "2025-05-26T23:48:20.625050",
    "visual_resource": [
      "https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBHYQjMiappg4LGkN8wRtPFmBCgoK9BRwLZq4hFggD2Q0gAZP2qXobuHZ3wCLHue7COD0wEXltdeRw/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T05:28:35.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBHYQjMiappg4LGkN8wRtPFmBCgoK9BRwLZq4hFggD2Q0gAZP2qXobuHZ3wCLHue7COD0wEXltdeRw/0?wx_fmt=jpeg\", \"id\": \"aavYxTWWHeUhm2jID5s9bg\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/aavYxTWWHeUhm2jID5s9bg.txt\"}}"
  },
  {
    "id": "bnoUz1Sdy0XTjnSnTLmHXw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/bnoUz1Sdy0XTjnSnTLmHXw",
    "title": "这届国产AI真的可以！20分钟生成万字报告，附带可视化网页，可直接下载食用",
    "summary": "国产AI工具“问小白”凭借其创新的“小白研报”和AI搜索功能，正在变革传统办公与科研方式。其中，“小白研报”能将繁冗的财报解读、行业研究等工作在20分钟内转化为详尽且附带可视化网页的万字报告，通过模拟人类思维、多轮思考与工具调用，深度整合多源信息，提供专业分析和决策支持。其AI搜索功能则支持多模型切换和多种搜索模式，覆盖全球权威学术数据库，实现精准高效的信息检索。问小白以其卓越的效率、智能化的信息处理能力和“输入问题即获得决策支持”的Agent产品特质，显著提升了用户的工作价值和时间复利，成功印证了AI从“卖工具”向“卖收益”的价值转化理念。",
    "keywords": [
      "问小白",
      "小白研报",
      "AI搜索",
      "研报生成",
      "智能体",
      "大模型",
      "效率工具",
      "信息处理"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "content": "标题：这届国产AI真的可以！20分钟生成万字报告，附带可视化网页，可直接下载食用\n公众号：量子位\n--------------------------------------------------\n\n大上午的，还没到10点呢，量子位负责财报分析的同学就已经在我背后的工位上发出了第N次哀嚎。\n\n不怪她，换我我也得喊救命——最近一季度财报频发，这已经是本月解读的不知道第几家Q1财报内容了。\n\n每天，真的是每一天，她一会儿回头问我这个数据算得对不对，一会儿微信上问我这个文章立意角度切入得妙不妙。终于，我不耐烦了，头也不抬地回了一句：\n\n求你别问我了，你去问问小白吧！\n\n我真的不是打哈哈敷衍她。之前就有做行研和一级市场的朋友们跟我安利过问小白AI，听说最近又出了个类似Deep Research的「小白研报」功能，在工作中很能帮得上忙。\n\n我自己浅浅试过一波，真的顶。\n\n果不其然，自打让小白研报做了她的每日财报伴侣，她再也没有像之前那样在苦海里挣扎。\n\n太好了，这个世界上又有一位打工人被问小白（wenxiaobai.com）拯救了。\n\nAI发展迅速，应用和功能都层出不穷，为什么给同事推荐的是小白研报？\n\n先看它的官方定义，初步感知一下——\n\n小白研报基于元石自研模型打造，让小白模拟人类思维，像真人一样研究问题，通过多轮思考与工具调用，自动生成论文、行业研究与趋势报告，并呈现精美可视化网页。\n\n它能做的，就是输出各类行业报告、学术论文、调研报告、资讯盘点、在线吃瓜、旅游攻略、舆情监测、投资决策等等。\n\n发现了没？它擅长做的事情，恰好就是所有打工人闻之胆寒的那一类：\n\n工作时间紧，资料多，数据碎，分析难，写了半天还容易出错……总之就是debuff叠满的那种。\n\n就像最近各个公司唰唰往外发Q1财报，虽然拆解分析它们已经是固定常规操作，但由于内容长、数据多，压根不是能速通的活。\n\n但如果交给小白研报，你只需要“吩咐”它一句话。\n\n此处根据需要，你可选择是否附带上传额外的文档。\n\n然后最多等待20分钟。\n\n乍一听，和一般的AI Chatbot瞬时返回结果相比，16分钟是有点久。\n\n但其实仔细想想，再熟练的人想要认真地把一份财报解读清楚，并且撰写出结果，也要花小半天时间。\n\n这16分钟里，小白研报不只是在慢慢啃财报本身，它还在全网搜罗了零跑的相关资料，甚至整个汽车行业的诸多内容，包括财报原文、券商研报、行业对比数据和市场反馈。\n\n毕竟想要形成一份扎实的研报，光用Q1财报内容进行简单堆砌信息可不行。\n\n大家看到的最终呈现结果，都是小白研报搜索、整理了几十甚至上百份靠谱资料得出的。\n\n搜索整理过后，小白研报是真的在呈现结果中把搜罗来的资料用上了。\n\n比如，在战略分析处，小白研报引用了这样一句话：“零跑建供应链的原则是，尽可能少出钱”。\n\n这是零跑董事长去年在接受媒体采访时说过的话，并不属于财报内容。\n\n这就是小白研报的特别之处，你能明显感觉到它在思考，在分析数据和信息之间的关联性。\n\n最后我们来整体看一下，小白研报花费16分钟，返回了怎样一份报告：\n\n不是干巴巴的纯文字分析，而是做了重点提炼，有清晰表格、对比柱状图的富媒体网页报告，有逻辑、有层次、有重点的专业分析。\n\n详实，美观，直接可用——\n\n还可以点击导出PDF或者DOCX文档，就可以下载完整小白研报产出的内容。\n\n注意，下载下来的不是对话框里展示的富媒体网页，而是随随便便万字起步的调研报告。\n\n小白研报处理工作的专业能力无可挑剔，自从同事实践证明了它用来解读财报很给力过后，编辑部里让它整理AI发展趋势、挖掘名人故事的活，都不必再100%亲力亲为。\n\n作为AI狂热爱好者，既然国产AI界有如此神器，同事立马和我合计，想开发它更多的玩法来“为我所用”——花的还是小白研报帮忙省下来的时间，笑死。\n\n我们发现，在日常生活中，小白研报也能发挥同样的作用，帮助解决实际问题。\n\n最近不是已经开启618的预售活动了吗，又有国补政策，买买买的剁手党们已经坐不住了。\n\n我的目标就一个，拿下一台万元内的单反相机。\n\n没啥意外，这活儿同样交给了小白研报。\n\n首先一定要给大家看的一个对比。每次给小白研报提需求的时候，我们给的prompt都非常简单，往往就是一句话的事儿。\n\n但就是短短一句话，小白研报都会尽可能地去拆解有效信息和关键词。\n\n输入“618了，推荐一台万元内适合女性新手的单反相机”和“推荐一台万元内适合新手的单反相机”，会有什么不同？\n\n小白研报能抓住前者“618”“女性”等词，去做更多的市场调研。\n\n联网搜索12分钟后，小白研报整理出了一份攻略报告。\n\n它很明白自己的推荐对象是新手，所以遵循下面几个原则：\n\n在技术规格与性能分析中，研报内容专门在“操作体验”中考虑了机身重量这一项，也符合我们需求中“女性”使用的通常考虑项。\n\n然后，它横向调研了佳能、尼康、富士等五大品牌，不仅对比了参数和价格，还详细说明了每款相机的优缺点和适用场景。\n\n比如哪款适合拍人像，哪款风景更出色，哪款性价比最高，都分析得明明白白。\n\n最贴心的是，它甚至写出了“二手市场性价比高，但需仔细甄别”这样的话。\n\n最后，小白研报给出的推荐清单是这样的：\n\n如此惊艳又实用的小白研报，只是问小白诸多AI功能中的一个，问小白还有很多其它绝活，帮你捋清思路、拆解重点、挽救输出。\n\n深入体验过后，最想跟大家分享的就是AI搜索。\n\n说实话，AI搜索大战打得很激烈，国内外很多AI公司都出了这个功能，但问小白的AI搜索有它自己独到的优势。\n\n首先是模型选择。\n\n问小白目前搭载了最前沿的大模型，用户可随意切换，界面还标明了各个模型推荐的实用场景。\n\n除了模型可以选择，用问小白搜索时，还能选择不同的搜索模式，分别是：\n\n日常搜索可以即时获取最新信息，最高能搜索100+网页；而在专业搜索模式下，能同时处理200多个网页，相同时间内的阅读量是其他AI搜索的3到5倍。\n\n而且问小白会优先选择权威网站，让答案更可靠。\n\n另外每一种搜索模式都没有传统搜索引擎的广告哦（doge保命）。\n\n一番尝试下来，感觉学术搜索这个模式蛮特别，简直是为科研党量身定制的，倾情推荐大家食用。\n\n主要原因之一，就是它免费啊！就问科研党谁看了不馋？\n\n当然，免费这个赤裸裸的诱惑，还是建立在它正儿八经很好用的基础上。\n\n问小白背后使用的自研模型，可根据语义理解，将问答query转化为中英文检索词，提高检索结果质量。\n\n也就是说，它会先根据具体问题，罗列相关的关键词，让AI搜索不跑偏，然后再进一步精准地“大浪淘金”。\n\n与此同时，问小白背后的自研模型会增加相关性模型进行相关性处理，提高检索文献相关性。\n\n不得不提的是，问小白的学术搜索接入了全球最大的学术数据库，覆盖的学术领域极广，包括自然科学、社会科学、人文科学、工程技术、医学、生命科学等等前沿领域，SCI、SSCI、EI等权威数据库收录的论文也悉数覆盖。\n\n专业来源，如arXiv、IEEE官网，其它来源，如微博、B站，都被它作为来源一网打尽。\n\n并且全部列举在右侧展示框，并包含资料年份、被引次数、作者等信息，方便你做文献内容筛选。\n\n此外，点击相应的原文，即可以PDF的形式查看原始资料。\n\n整个AI搜索的体验中，问小白有一个细节特别打动人，就是它具备「追问功能」。\n\n大家都是普通人，有时候提问题不够准确。这个时候，问小白会引导你补充信息，帮你学会如何提出一个好问题。\n\n这种交互体验，真的像在和一个专业顾问对话。\n\n需要注意，作为一款好用的AI产品，小白研报和AI搜索，都只是问小白十八般武艺的一角而已。\n\n各式写作、推理生图、拍照答疑……几乎所有AI能干的事，用户都可以在问小白一站式解决。\n\n最近问小白还在PC端上线了隐私模式，点击“开启新对话”右侧的方框按钮即可开启。\n\n这样一来，你可以放心大胆和AI畅所欲言。\n\n不用刻意删除历史记录，也不必担心和AI说的心里话被留痕了……\n\n无论你是写稿、做PPT、看财报、填志愿，甚至是选志愿、搜资料、聊心事——\n\n只要是个AI能完成的任务，交给问小白准没错，有了问小白，你就有了最强大的信息处理外援。\n\n在这个AI重新定义效率的时代，会选择和会使用好的AI工具的人，永远比只会蛮干的人多出8小时创造价值。\n\n根据量子位智库统计的数据，4月问小白AI网页端使用规模超600万。这份成绩单是用户亲身体验后的选择。\n\n前段时间，第三届红杉资本AI峰会在洛杉矶落幕，长达6个小时的闭门会传递出一个共识：AI不再卖工具，而是卖收益。\n\nAI的价值在于解决问题、创造结果，要把AI的价值和客户的实际收益绑在一起。\n\n这恰恰是问小白正在用实际行动印证的，尤其是小白研报——当一款Agent产品能把传统需数小时的数据清洗、行业分析、图表生成压缩至分钟级，实现「输入问题即获得决策支持」的无缝体验时，你获得的已经是实实在在的时间复利。\n\n省时省力，效率拉满。\n\n最后，Attention Please！\n\n问小白的部分功能需要消耗金币，不过不要紧张，最近官方开启了一波送金币活动：\n\n什么叫心动不如行动～\n\n问小白官网（或点击文末“阅读原文”直达）：https://www.wenxiaobai.com\n\n#问小白 #问小白研报 #问小白下载 #问小白学术搜索 #AI搜索 #AI\n\n一键三连「点赞」「转发」「小心心」\n\n欢迎在评论区留下你的想法！\n\n— 完 —\n\n🌟 点亮星标 🌟",
    "published_time": "2025-05-26T05:28:35.000Z",
    "download_time": "2025-05-26T23:48:33.855324",
    "visual_resource": [
      "https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBHYQjMiappg4LGkN8wRtPFmWLmoEYJiaMS9EC41ucwplzl0jN7hYdl8khekMR9wLRFiao0EE7wia5xsg/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T05:28:35.000Z\", \"image\": \"https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBHYQjMiappg4LGkN8wRtPFmWLmoEYJiaMS9EC41ucwplzl0jN7hYdl8khekMR9wLRFiao0EE7wia5xsg/0?wx_fmt=jpeg\", \"id\": \"bnoUz1Sdy0XTjnSnTLmHXw\"}, \"extraction_info\": {\"account\": \"量子位\", \"file_path\": \"./database/content/wechat/bnoUz1Sdy0XTjnSnTLmHXw.txt\"}}"
  },
  {
    "id": "9hvxN0wvr38ahYAEhS_54g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/9hvxN0wvr38ahYAEhS_54g",
    "title": "新智元十年，ASI降临，诚邀你加入！",
    "summary": "新智元迎来成立十周年里程碑，此篇报道恰逢其时，并同步提及人工超级智能（ASI）这一前沿概念的潜在降临。文章暗示，新智元将借此契机，深化在人工智能领域的布局与影响力。标题中“诚邀你加入”的表述，强烈预示着新智元可能围绕十周年庆典举办一场引人瞩目的行业盛会，或启动重大人才招募计划，旨在汇聚全球AI精英，共同探索并引领ASI时代的未来发展方向。这篇报道凸显了新智元作为资深AI媒体，在行业发展关键节点所扮演的连接与推动角色，展望AI的下一个黄金十年。",
    "keywords": [
      "新智元",
      "ASI",
      "人工智能",
      "超级智能",
      "十周年",
      "AI发展",
      "行业盛会",
      "未来AI"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "content": "标题：新智元十年，ASI降临，诚邀你加入！\n公众号：新智元\n--------------------------------------------------\n\n新智元报道",
    "published_time": "2025-05-26T01:33:59.000Z",
    "download_time": "2025-05-26T23:46:31.910493",
    "visual_resource": [
      "https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb0UW7sJXoiaeUGtFg6Lh1gfPW4NkZleTEWtKw8MBOwSrm8icTWfPGd9Fia5Iwf1m7GcckHIKBGQ1nC6g/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T01:33:59.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb0UW7sJXoiaeUGtFg6Lh1gfPW4NkZleTEWtKw8MBOwSrm8icTWfPGd9Fia5Iwf1m7GcckHIKBGQ1nC6g/0?wx_fmt=jpeg\", \"id\": \"9hvxN0wvr38ahYAEhS_54g\"}, \"extraction_info\": {\"account\": \"新智元\", \"file_path\": \"./database/content/wechat/9hvxN0wvr38ahYAEhS_54g.txt\"}}"
  },
  {
    "id": "DeEY7V1WLFI7ehoXr6HymQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/DeEY7V1WLFI7ehoXr6HymQ",
    "title": "硅谷顶级AI天才成「团宠」：布林请吃饭，奥特曼约打牌",
    "summary": "硅谷正掀起一场前所未有的AI人才争夺战。OpenAI、谷歌DeepMind等科技巨头为吸引和留住“超级明星研究员”，不惜开出千万美元级年薪和股权激励，甚至提供私人飞机接送等特殊待遇。文章指出，AI热潮使得能推动大模型发展的“10,000倍研究员”成为行业核心，其稀缺性达到新高。研究员在选择公司时，除了薪酬，研究资源与愿景同样是关键考量。这场人才战正重塑AI行业格局，吸引着各领域顶尖人才涌入。",
    "keywords": [
      "AI人才争夺",
      "硅谷",
      "顶级研究员",
      "薪酬激励",
      "大模型",
      "人才稀缺",
      "行业格局",
      "AI创新"
    ],
    "area": [
      "人工智能",
      "大模型",
      "生成式AI"
    ],
    "content": "标题：硅谷顶级AI天才成「团宠」：布林请吃饭，奥特曼约打牌\n公众号：新智元\n--------------------------------------------------\n\n新智元报道\n\n硅谷巨头们，正掀起一场前所未有的人才争夺战。\n\n如今，OpenAI、谷歌、xAI等公司不惜重金，争夺那些能够改变游戏规则的「超级明星研究员」。\n\n路透爆料称，顶级AI研究人员的年薪已突破天际。\n\nOpenAI的顶尖人才年收入可轻松超过1000万美元。\n\n谷歌DeepMind更是开出高达2000万美元的年薪，并通过缩短股权归属期（从4年降至3年），以及额外的股权激励来吸引人才。\n\n更夸张的是，派私人飞机接送，都成了招人的「常规操作」。\n\n不得不说，硅谷大厂真是下足了血本！\n\n在科技行业，顶级人才一直是稀缺资源。但AI热潮让这种稀缺性达到了前所未有的高度。\n\n自ChatGPT在2022年底横空出世以来，AI行业的招聘竞争已升级到「职业运动员」级别。\n\n为了抢夺这些被称为「个体贡献者」（IC）的顶尖人才，公司甚至抛出数百万美元的奖金和股权激励。\n\nOpenAI为留住有意加入Ilya新公司的研究人员，还开出了200万美元的留任奖金，以及2000万美元以上的股权激励。\n\n而面对来自Eleven Labs的挖角，OpenAI也提供了至少100万美元的留任奖金。\n\n有些人只需留任一年，即可获得全部奖金。\n\n相较之下，Comprehensive.io数据显示，科技巨头的顶级工程师平均年薪为28.1万美元，股权为26.1万美元。\n\n12位参与AI研究人员招聘的人士比喻道，「AI实验室的招聘方式就像下棋」。\n\n前OpenAI研究员，现RunSybil CEO Ariel Herbert-Voss表示：\n\n他们会像在棋盘上布子一样，希望尽可能快地行动，因此愿意为拥有专业且互补专长的候选人支付高额费用。\n\n这些科技公司会考虑：我有足够的车吗？足够的马吗？\n\n一些顶级人才的争夺，甚至惊动了公司创始人。\n\nOpenAI研究员、德扑之父Noam Brown，在2023年探索工作机会时，谷歌创始人谢尔盖·布林亲自请他共进午餐。\n\n奥特曼邀他到家中玩扑克，还有投资者派私人飞机接送。甚至，就连马斯克也亲自致电，力求为xAI争取人才。\n\n尽管薪酬诱人，但对许多研究员来说，资源支持和研究愿景同样重要。\n\nNoam Brown最终选择OpenAI，并非因为薪资最高，而是因为OpenAI愿意为他感兴趣的项目投入人力和计算资源。\n\n全球真正能够推动LLM发展的顶尖IC，可能仅有数百人。\n\n这些人才对AI模型的成败至关重要，被称为「10,000倍研究员」，远超传统「10倍工程师」的影响力。\n\n奥特曼曾在2023年底发帖感叹，「10倍工程师很酷，但那些10,000倍的工程师/研究人员才真是厉害......」\n\n2024年9月，OpenAI首席技术官Mira Murati的离职，点燃了人才争夺战的又一把火。\n\n离职后，Murati创立了一家AI初创公司Thinking Machine，迅速从OpenAI挖走20名员工，并吸引了其他实验室的研究员。\n\n如今，她的团队规模已达60人。\n\n虽然她的公司尚未推出产品，但凭借强大团队实力，已在进行一轮破纪录的种子轮融资。\n\nMurati的离职不仅让OpenAI感受到压力，也让整个行业意识到人才流动的巨大影响力。\n\nAI人才的稀缺迫使一些公司另辟蹊径，采取创意招聘策略。\n\n专注于识别顶级AI人才的数据公司Zeki Data表示，它正在采用体育行业的数据分析技术，来发现有潜力但尚未被发掘的人才。\n\n他们发现，Anthropic倾向于招聘理论物理学背景的研究员，而其他AI公司则瞄准了量子计算领域的专家。\n\n在这份「2025年AI人才报告」中，一张图表指出美国科技巨头每月招聘软件工程师的曲线图，从2023年之后，整体趋势有所下降。\n\n前微软GenAI研究副总裁、现OpenAI研究员Sébastien Bubeck表示，「我的团队中有非常有才华的数学家，如果不是AI领域的快速发展，他们不会进入这个行业。\n\n现在，各领域的人才正涌入AI，这些聪明人正在改变行业」。",
    "published_time": "2025-05-26T01:33:59.000Z",
    "download_time": "2025-05-26T23:46:42.006248",
    "visual_resource": [
      "https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb0UW7sJXoiaeUGtFg6Lh1gfPUACELjvkDY2uzFxWXJhGdfCH9zWow8icSbRXfSTysJZh3V0nHEibgY2g/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T01:33:59.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb0UW7sJXoiaeUGtFg6Lh1gfPUACELjvkDY2uzFxWXJhGdfCH9zWow8icSbRXfSTysJZh3V0nHEibgY2g/0?wx_fmt=jpeg\", \"id\": \"DeEY7V1WLFI7ehoXr6HymQ\"}, \"extraction_info\": {\"account\": \"新智元\", \"file_path\": \"./database/content/wechat/DeEY7V1WLFI7ehoXr6HymQ.txt\"}}"
  },
  {
    "id": "KvryEjpY02GL7pGQ6eUjdQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/KvryEjpY02GL7pGQ6eUjdQ",
    "title": "刷新世界记录！40B模型+20万亿token，散户组团挑战算力霸权",
    "summary": "Nous Research成功推出Psyche网络，利用Solana区块链技术汇聚全球闲散计算资源，首次实现大规模去中心化AI训练。该网络已启动40B参数大语言模型Consilience的预训练，总计20万亿token，创下互联网最大规模预训练纪录。Psyche网络通过DisTrO优化器显著降低带宽需求，结合强化学习激励机制，旨在打破少数科技巨头对AI算力的垄断，大幅降低训练成本，推动AI模型的民主化发展。此举为开源社区和小型团队提供了与AI巨头抗衡的机遇，有望成为AI普惠发展的关键里程碑。",
    "keywords": [
      "Psyche网络",
      "去中心化AI",
      "大语言模型",
      "区块链",
      "算力",
      "预训练",
      "DisTrO优化器"
    ],
    "area": [
      "人工智能",
      "大模型",
      "机器学习"
    ],
    "content": "标题：刷新世界记录！40B模型+20万亿token，散户组团挑战算力霸权\n公众号：新智元\n--------------------------------------------------\n\n新智元报道\n\n互联网上最大规模的预训练来了！\n\nNous Research宣布正式推出Psyche网络（Psyche Network），通过去中心化方式革新人工智能（AI）训练。\n\nPsyche网络利用区块链技术，汇聚全球计算资源，成功启动了40B参数大语言模型Consilience的预训练任务，总计20万亿token，创下了迄今为止互联网上最大规模的预训练纪录。\n\n大语言模型Consilience采用DeepSeek V3的多头潜在注意力（MLA）架构，相较于Llama使用的GQA架构更具表达力，同时通过优化QKV投影矩阵减少计算开销。\n\n三种注意力的对比\n\nPsyche利用全球闲置的计算资源（如4090、A100和H100等消费级GPU），大幅降低训练成本。\n\n通过并行实验，Psyche鼓励开源社区提出新的模型架构和训练方法，未来可能催生更多创新。\n\nPsyche网络技术原理图，核心在于DisTrO优化器与Solana区块链\n\n过去，人们总觉得「AI模型的去中心化训练」不过是一种幻想，尤其在那些超越了爱好者规模的语言模型面前更是如此。\n\n但几项关键技术突破——尤其是并行化和强化学习——正在逐渐打破这种局限，让除了OpenAI、Anthropic这类大公司之外的小型团队也开始进入这个赛道。\n\n现在看来，聪明的算法可以弥补基础设施的不足，而像Nous Research这样的去中心化参与者正希望抓住这个机会。\n\n近年来，AI模型的训练逐渐被大型科技公司垄断。\n\n训练一个前沿模型需要数千个高性能GPU和超高带宽的集群，这使得普通研究者或小型团队几乎无法参与。\n\n这种集中化趋势不仅限制了创新，还可能导致少数科技去投垄断甚至控制AI模型。\n\n集中式AI，可能会少数科技巨头「比你更了解你自己」。\n\nHermes系列中规模最大的模型——Hermes 3 405B，是在基础的Llama 3.1模型上进行微调完成的。\n\n整个训练过程动用了128块H100 GPU，耗时约16小时（总计约2,086GPU小时）。\n\n从成本上看其实并不离谱——目前租用8块H100的计算节点每小时大约在16到24美元之间，因此一次完整训练的开销大约在5,000美元左右。\n\n作为Nous Research Hermes系列的最新迭代，Hermes 3 405B自Llama-3.1 405B的全参数微调模型，\n\n但如果我们想更进一步，想得更大呢？\n\n毕竟，Hermes目前还是依赖Llama作为基础模型。\n\n如果我们不再依赖已有的模型，而是从零开始构建自己的基础模型，那我们就需要更庞大的“船”了。\n\n要以更大规模、低成本地实现类似的训练成果，确实面临不少挑战，尤其是当训练从集中化的GPU集群转向基于互联网的去中心化网络时。\n\nNous Research提出了Psyche网络的解决方案：通过去中心化的方式，让全球的计算资源参与AI模型训练，降低进入门槛，推动AI发展的民主化。\n\nNous Research的Psyche网络成功实现了去中心化的AI训练，开创了一个全新的模式。\n\nPsyche不仅降低了AI开发的门槛，还推动了全球协作和创新。\n\nConsilience模型的预训练只是起点，未来Psyche网络有望成为AI民主化的重要基石，为开源社区和小型团队提供与科技巨头抗衡的机会。\n\n在去中心化训练中，网络带宽一直是最令人担忧的问题之一。\n\n在传统的数据中心里，GPU之间通过极高带宽的连接（如NVLink或InfiniBand）相连，带宽可达每秒几百Gb（千兆位）。\n\n而相比之下，互联网上的志愿者节点，往往只有几十甚至几百Mb（兆位）每秒的带宽。\n\n质疑者认为，这种高达100倍甚至1,000倍的带宽差距，会让跨互联网的AI训练变得无比缓慢、几乎不可能。\n\n毕竟，传统的训练方式需要GPU之间持续地交换更新信息，而如果试图用普通家用网络来完成这些通信，很可能会陷入「灾难级」的训练体验。\n\n在此前对DeMo（Decoupled Momentum Optimization）的研究基础上，Nous推出的DisTrO技术，能够让所有训练节点保持高度同步，同时将所需带宽降低1,000到10,000倍。\n\n2024年12月，Nous与多位合作伙伴一起，在封闭测试网中，训练了一个150亿参数的基础模型，并成功验证了多项理论设想：\n\n首次将DisTrO优化器系列大规模应用于训练任务\n\n验证了节点中途掉线和新增节点时的容错能力\n\n证明了增加训练节点确实能提升整体训练速度\n\n这次实验标志着分布式、去中心化训练迈出了从理论走向现实的关键一步。\n\n在硅谷的一些圈子里，「加密」这个词几乎成了贬义词，而Nous一直努力保持与AI开发者之间的开放交流桥梁不被切断。\n\n也正因如此，他们这次将Psyche搭建在区块链上，是一个值得关注的重要转变。\n\nPsyche将成为Nous用于预训练、微调和部署下一代模型的平台。\n\n通过将技术栈迁移到Solana区块链，Nous希望释放区块链的以下三大优势：\n\n无需许可：任何人都可以贡献计算资源\n\n弹性与高可用性：不再依赖中心化基础设施\n\n激励机制：协调并奖励为网络作出贡献的参与者\n\n将这一协议向整个市场开放，意味着任何人都能拥有其中的一部分。而其潜在的扩展性之大，显然已经让不少极客兴奋不已。\n\nNous的初期目标是先上线一个封闭测试网（Phase 0），验证是否能在Solana上运行一个更大规模、分布式、具备容错能力的DisTrO系统。后续阶段会逐步引入更高级的功能。\n\n在Phase 0阶段，贡献者可以携带自己的GPU加入进来（明确提到支持4090、A100和H100等型号），并开始获得奖励。此阶段会对参与者进行筛选，以防止恶意行为者加入。\n\n一旦系统稳定运行，权限将逐步开放，允许不同类型的计算资源（无论是专业的还是消费级的）自由接入网络，协助训练Llama、Diffusion等不同类型的模型架构。\n\n强化学习（Reinforcement Learning，RL）不依赖于预先准备好的数据集，而是通过模型与环境直接互动来学习。\n\n每个节点如果做出有助于模型进化的行为，就会获得正反馈，反之则获得负反馈。\n\n由于这些节点可以异步运行，分布式训练在强化学习框架下反而运行良好。\n\n每个节点可以独立行动，收集经验，并定期与其他节点分享进展。\n\n这极大缓解了传统训练中常见的「同步难题」，特别是在硬件能力和网络延迟差异大的情况下。\n\n通过RL，Psyche上的预训练模型可以进一步学会推理能力和领域知识。\n\n而每个Psyche节点在训练过程中的表现都将影响它的奖励：计算能力更强或使用了更先进训练方法的节点，可能会获得更多代币激励。\n\n在常常被斥为「过度炒作又频频令人失望」的区块链生态中，能看到真正的创新成果，确实令人欣慰——简直让人「冷漠的灵魂也重新燃起了热情」。\n\n这一切，真的令人感到振奋。\n\nNous并不是一开始就拥抱区块链技术的，相反，他们几乎是被「拖着、踢着、喊着」走上了这条路——\n\n但原因很简单：区块链确实是解决他们问题最合适的工具。\n\n他们需要一种方式，不论对方来自哪里，都能吸引计算资源与人才并进行公平支付；区块链，在这一点上表现得无比出色。\n\n他们需要一种手段，能够协调并扩展大规模训练任务；而协调与扩展，正是区块链技术的「第二天性」。\n\n他们还需要一种不受停电、封禁、宕机等影响的托管机制，能让项目「打不死」、无法被关闭；在这方面，区块链（这次不再是讽刺）也的确提供了最可靠的保障。\n\n而最值得欣慰的是：这一次，人们选择区块链，不是出于投机炒作，而是出于对实际问题的认真思考与真实需求的回应。\n\n如果Psyche成功了，它不仅将证明去中心化训练是切实可行的，更是回归初心：为取代的集中化计算，提供了强有力的工具。",
    "published_time": "2025-05-26T01:33:59.000Z",
    "download_time": "2025-05-26T23:46:50.969386",
    "visual_resource": [
      "https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb0UW7sJXoiaeUGtFg6Lh1gfPv9yzGIzeCyHz19uI8vjiaenaMKYiaKl1Rx0Lx52QZuYefEeEwk4mHNYA/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T01:33:59.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb0UW7sJXoiaeUGtFg6Lh1gfPv9yzGIzeCyHz19uI8vjiaenaMKYiaKl1Rx0Lx52QZuYefEeEwk4mHNYA/0?wx_fmt=jpeg\", \"id\": \"KvryEjpY02GL7pGQ6eUjdQ\"}, \"extraction_info\": {\"account\": \"新智元\", \"file_path\": \"./database/content/wechat/KvryEjpY02GL7pGQ6eUjdQ.txt\"}}"
  },
  {
    "id": "ZC0MgvEhlSfZzOV0dRiIsQ",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/ZC0MgvEhlSfZzOV0dRiIsQ",
    "title": "华为中科大联创大模型低比特量化算法，1‰数据实现昇腾无损压缩7倍",
    "summary": "华为诺亚方舟实验室联合中科大发布创新性CBQ（Cross-Block Quantization）算法，显著解决大模型低比特量化面临的性能下降与依赖挑战。该算法通过跨块重建、自适应LoRA-Rounding和粗细粒度预处理，仅用0.1%校准数据实现大模型7倍压缩，同时保持浮点模型99%的性能，成功入选ICLR 2025 Spotlight。CBQ已集成至昇腾模型压缩工具包ModelSlim，有效推动华为盘古大模型等在国产算力及端侧设备上的高效部署，标志着大模型轻量化与普及应用进入新阶段。",
    "keywords": [
      "大模型",
      "低比特量化",
      "CBQ算法",
      "昇腾",
      "模型压缩",
      "华为",
      "中科大",
      "后训练量化"
    ],
    "area": [
      "大模型",
      "机器学习",
      "人工智能"
    ],
    "content": "标题：华为中科大联创大模型低比特量化算法，1‰数据实现昇腾无损压缩7倍\n公众号：新智元\n--------------------------------------------------\n\n新智元报道\n\n大模型，如今堪称AI界的「吞金巨兽」。\n\n从写诗到解题，从对话到编程，它们几乎无所不能，但动辄千亿甚至万亿参数的规模，让部署成本高得离谱。\n\n以FP16精度部署的DeepSeek-R1 671B为例，推理时大概需要1342GB的显存，如果是32GB 5090显卡，需要整整42张！\n\n为了降低成本，天才工程师们想出了后训练量化（Post-training Quantization，PTQ）的方法，它能够在有限的校准数据和计算资源下对模型进行高效压缩。\n\n但是PTQ依然带来新的问题——在极低比特精度（如W2A16、W4A4）时往往会出现明显的性能下降，规模是降了，但是不好用了！\n\n就在这关键时刻，华为诺亚方舟实验室联合中科大亮出了「杀手锏」——CBQ（Cross-Block Quantization），一种基于跨块重建的后训练量化方案。\n\n论文地址：https://openreview.net/pdf?id=eW4yh6HKz4\n\n相比量化感知训练（QAT）所需数据量，CBQ仅用0.1%的训练数据，一键压缩大模型至1/7体积——浮点模型性能保留99%，真正实现「轻量不降智」。\n\n值得一提的是，这项成果已荣登ICLR 2025 Spotlight（录取率仅5%）。\n\n它不仅展现了大模型压缩领域的创新性和实用性，更像一颗信号弹，宣告大模型在国产算力上的普及时代已然来临！\n\n目前，CBQ已作为可调用的算法之一，正式加入昇腾模型压缩工具包ModelSlim，帮助开发者在昇腾芯片上实现LLM的高效部署。\n\n长期以来，后训练量化（PTQ）一直是压缩大语言模型的「黑科技」——通过解决异常值和采用layer-wise或block-wise的loss优化技术取得了比较不错的结果。\n\n但是当把参数比特「压得特别低」的时候，模型性能会严重下降。\n\n为什么极低比特量化，如此困难？其实，答案隐藏在大模型的复杂结构中。\n\n研究者们对LLM在低比特量化场景下的量化误差进行了深入分析，发现了问题的关键所在：\n\n随着模型参数数量的增加和量化bit数的减少，模型内部的层间依赖（inter-layer dependencies）和层内依赖（intra-layer dependencies）会显著增强，这严重影响了量化精度。\n\n如下实验所示，清晰展示了LLAMA-7B层间与层内的依赖关系。\n\n图1：Llama-7B内部权重和层之间依赖关系的变化，以及层间缩放因子（scale）对误差的影响\n\n图1（a）为LLAMA-7B单一层中权重的Hessian矩阵绝对值可视化，2-bit图比4-bit更模糊，非对角线噪声增多，表示在低比特下权重间的「干扰」增强了。\n\n图1（b）为LLAMA-7B 32层中损失相对于scale的Hessian矩阵可视化，在2-bit量化中，非对角线明显比4-bit更亮，说明层间依赖增强，模型更容易因为一层的误差影响到另一层。\n\n以及图1（c）LLAMA-7B前两个Transformer块的平均scale与相应损失之间的关系，4-bit情况下，误差平稳区域大，模型对 scale 不敏感。2-bit情况下，误差对scale非常敏感，选择不当误差急剧上升，黑色区域更集中、易出错。\n\n总结来说，将模型参数从高精度压缩到低精度，这一过程主要面临三大核心挑战：\n\n1. 层间依赖的「雪球效应」\n\n大模型由多个Transformer层组成，各层参数之间存在复杂的相互依赖。\n\n在极低比特量化时，量化误差会在层间不断累积放大，就像「滚雪球」一样，导致整体性能严重下降。\n\n然而，传统逐层量化的方法，无法有效捕捉这些层间依赖，进而造成了精度损失。\n\n2. 层内依赖的复杂性\n\n同一层内的参数并非独立存在，而是存在紧密的关联性。\n\n极低比特量化会破坏这些精细的层内依赖，导致模型在处理复杂任务时「力不从心」。\n\n比如，大模型语义理解或推理能力，可能因参数精度的降低而显著退化。\n\n3. 权重和激活的异常值\n\n模型的权重和激活值中的异常值，在低比特量化时会引发较大的误差。\n\n传统的方法无法精确识别和处理这些异常值，进一步加剧了量化误差。\n\n可见，这些挑战让低比特量化，成为大模型压缩的「拦路虎」。\n\n那么，华为的CBQ方案，是如何突破这些瓶颈？让我们一探究竟！\n\nCBQ的核心思想是，通过跨块依赖（Cross-Block Dependency, CBD）机制和自适应LoRA-Rounding技术，同时优化多个Transformer块的量化参数，从而更好地保留模型内部的依赖关系。\n\n具体来说，它通过三大技术创新，为极低比特量化注入了全新活力。\n\n刚刚也提到，传统量化方法采用逐层优化，却忽视了层间依赖的复杂性。\n\nCBQ引入了CBD机制，通过滑动窗口的方式，同时优化多个Transformer块，并且相邻窗口之间会有重叠的块，以确保块之间的连接性和协作性。\n\n这种方法，可以有效地捕捉到模型内部的长距离依赖关系，使得相邻的块能够共同参与到量化过程中，从而提高整体的量化性能。\n\n在实验中，随着滑动窗口中块的数量增加，模型的性能也得到了显著提升。\n\n为了应对层内依赖的复杂性，CBQ提出了自适应LoRA-Rounding技术，通过两个低秩矩阵来学习量化权重的自适应补偿值。\n\n与传统的AdaRound方法相比，LoRA-Rounding通过低秩分解大大减少了可学习参数，训练速度更快，GPU内存消耗更低。\n\n这种方法能够在训练过程中动态调整权重的量化精度，从而更好地适应模型的内部结构和数据分布。\n\n针对异常值问题，CBQ采用了粗到细的预处理策略（Coarse-to-Fine Preprocessing, CFP）。\n\nCFP策略从统计学的角度出发，通过分阶段检测和处理权重和激活中的异常值。\n\n在粗粒度检测阶段，通过计算四分位数和四分位距来初步估计异常值的范围；在细粒度检测阶段，通过最小化异常值子集与正常值子集之间的距离，同时最大化子集内部的方差，来精确识别异常值的位置。\n\n这种分阶段策略，有效减少了量化误差，确保模型在低比特场景下依然「稳如泰山」。\n\n那么，CBQ在场景中的真实表现又如何呢？\n\n一系列研究结果显示，CBQ在华为盘古模型和开源模型的表现上，大放异彩。\n\nCBQ量化技术已成功应用于华为盘古大模型PanGu-7B和PanGu-1.5B的端侧部署，凭借其高精度的量化性能，有效支撑了盘古大模型在多个业务场景的落地应用。\n\n如下表所示，在W8A8/W4A16精度下，PanGu-1.5B模型在中文（C-Eval/CMMLU）、多任务语言理解（MMLU）基准中的表现，毫不逊色于全精度模型的性能。\n\n在中文、多语言理解、数学基准中，PanGu-7B的表现同样如此。\n\n这些成果，足以让盘古模型在手机等终端设备上，轻松运行。\n\n此外，CBQ在多个开源LLM（如OPT、LLaMA）上也取得了SOTA。\n\n例如，在W4A16、W2A16和W4A8等低比特量化设置下，CBQ的性能均优于现有的最先进方法，并且与全精度模型的性能差距缩小到了1%以内。\n\n更令人惊叹的是，CBQ仅需4.3小时即可完成对4位权重的LLaMA1-65B模型的量化，展现了压缩率与精度之间的完美平衡（trade-off）。\n\n华为的CBQ方案，以跨块依赖机制、自适应LoRA-Rounding技术，以及粗到细的预处理策略，成功征服了极低比特量化的「三大高峰」。\n\n这项创新有效地解决了，大模型在低比特量化场景下所面临的层间依赖和层内依赖难题。\n\n它不仅在多种大语言模型和数据集上展现出了显著的性能提升，成功缩小了与全精度模型之间的差距，还以高效的量化效率实现了复杂模型的快速压缩。\n\n最终，让盘古和各类开源模型，成功实现了在昇腾硬件上的高效部署，并为更加广泛的应用铺就坦途。",
    "published_time": "2025-05-26T01:33:59.000Z",
    "download_time": "2025-05-26T23:46:59.050918",
    "visual_resource": [
      "https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb3oAhwne1dEic3FiaHhHmoR1wLlYGicJhaxXBicsyS2RF3raqB8YJJLh7h6grU7wDthibx6djpnwwyaiaQg/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T01:33:59.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/UicQ7HgWiaUb3oAhwne1dEic3FiaHhHmoR1wLlYGicJhaxXBicsyS2RF3raqB8YJJLh7h6grU7wDthibx6djpnwwyaiaQg/0?wx_fmt=jpeg\", \"id\": \"ZC0MgvEhlSfZzOV0dRiIsQ\"}, \"extraction_info\": {\"account\": \"新智元\", \"file_path\": \"./database/content/wechat/ZC0MgvEhlSfZzOV0dRiIsQ.txt\"}}"
  },
  {
    "id": "g-nm0s2eIN73emjcj2oP6g",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/g-nm0s2eIN73emjcj2oP6g",
    "title": "ACL 2025 高分接收 | 高感情语音技术：逻辑智能小语种TTS破局之道",
    "summary": "北京深度逻辑智能科技有限公司与宁波东方理工EIT-NLP实验室联合攻克小语种语音合成（TTS）难题，其针对泰语TTS的创新方案被ACL 2025 Industry track接收。该方案通过数据优化驱动的声学建模框架，系统化构建泰语数据集，结合LLM增强的文本处理、音素-声调序列转换，以及Phone-Tone BERT与GAN解码器等先进技术，实现了低资源环境下高质量TTS合成和零样本声音克隆，为全球小语种TTS的落地与普及提供了可复制的工程化路径，突破了数据稀缺和语言复杂性双重瓶颈。",
    "keywords": [
      "语音合成",
      "小语种",
      "泰语TTS",
      "数据优化",
      "声学建模",
      "零样本克隆",
      "BERT",
      "GAN"
    ],
    "area": [
      "人工智能",
      "自然语言处理",
      "深度学习"
    ],
    "content": "标题：ACL 2025 高分接收 | 高感情语音技术：逻辑智能小语种TTS破局之道\n公众号：机器之心\n--------------------------------------------------\n\n该工作由北京深度逻辑智能科技有限公司×宁波东方理工EIT-NLP实验室联合完成。\n\n语音合成（TTS）技术近十年来突飞猛进，从早期的拼接式合成和统计参数模型，发展到如今的深度神经网络与扩散、GAN 等先进架构，实现了接近真人的自然度与情感表达，广泛赋能智能助手、无障碍阅读、沉浸式娱乐等场景。\n\n然而，这一繁荣几乎局限于英语、普通话等资源充沛的大语种；全球一千多种小语种由于语料稀缺、文字无空格或多音调等复杂语言学特性，在数据收集、文本前端处理和声学建模上都面临巨大挑战，导致高质量 TTS 迟迟无法落地。破解「小语种困境」既是学术前沿课题，也是实现数字包容与多语文化传播的关键。\n\n面对这一挑战，逻辑智能团队提出了一种针对低资源语言 TTS 的解决方案并应用于泰语 TTS 合成，该工作已经被 ACL 2025 Industry track 正式接收！\n\n论文标题：Scaling Under-Resourced TTS: A Data-Optimized Framework with Advanced Acoustic Modeling for Thai\n\n论文地址：https://arxiv.org/abs/2504.07858\n\n效果试听：https://luoji.cn/static/thai/demo.html\n\n这项工作提出了一种数据优化驱动的声学建模框架的创新方案，通过从语音、文本、音素、语法等多个维度构建系统化的泰语数据集，并结合先进的声学建模技术，成功实现了在有限资源下的高质量 TTS 合成效果。\n\n此外，该框架还具备 zero-shot 声音克隆的能力，展示了优异的跨场景适用性，为行业提供了一种在数据稀少环境下高效构建小语种 TTS 系统的有效范式，对推动全球小语种 TTS 技术的落地与普及具有重要的启示和借鉴意义。\n\n该工作遵循数据驱动模型能力的整体思路：\n\n首先从源头切入，系统化采集并标注跨领域语音、文本与语言学信息，构建覆盖广、颗粒度细的多维泰语语料库；\n\n随后通过 LLM 增强的停顿预测、词切分与混合式 G2P，将原始文本稳健转换为结构化的「音素-声调」序列；\n\n最后在此精炼输入之上，引入声调感知的 Phoneme-Tone BERT 与多源特征驱动的 GAN 解码器，实现高保真、低延迟的语音合成，并支持零样本声音克隆。\n\n整套框架以数据质量为核心抓手、以模块化设计保障可扩展性，为解决小语种 TTS「数据稀缺 + 语言复杂」双重瓶颈提供了一条可复制、可落地的工程化路径。\n\n该工作构建了一套专为低资源泰语 TTS 设计的多维数据集，涵盖语音、文本和注释三大类：\n\n该工作设计了一套强大的预处理流程。预处理流水线最大的亮点在于「三步一体、逐层解耦」地化解泰语文本的无标点、无空格、声调复杂三重难题：\n\n首先通过 SFT 微调的 Typhoon2 LLM，对 1.5 万句人工标注语料学习停顿规律，在原始文本中智能插入停顿标签以更好地建模口语韵律；\n\n随后在扩充至 10 万词的分词词典支撑下，改进版 pythainlp Tokenizer 将连续书写的泰文字流精准切分，为领域专有词提供稳健支持；\n\n最后利用 4 万词的音素-声调注释库，结合规则+Transformer 混合式 G2P，把每个词映射成带五声调标记的 IPA 音素序列。\n\n该流水线不仅输出结构化的「音素-声调」序列，大幅降低后续声学模型学习难度，也为其他低资源音调语言提供了可复用的文本前端范式。\n\n该工作的 TTS 模型集成了「多源特征 × 声调感知 × 零样本克隆」的组合设计：\n\n首先利用多语种预训练模型提取时长、音高、能量等强鲁棒特征，并以风格编码器压缩说话人/情感信息，为后续零样本克隆奠定基础；\n\n其次，通过 Phoneme-Tone BERT 在音素序列中显式融入五声调，精准捕捉泰语语义-韵律关联；\n\n最后以 GAN 解码器直接从音素与预测特征合成波形，联合时域、频域与感知损失实现高保真、低延迟合成。\n\n整体采取「先独立训练预测器，再与解码器联合微调」的策略，兼顾稳定性与音质，使模型达到 SOTA 表现并支持零样本声音克隆。\n\n© THE END\n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com",
    "published_time": "2025-05-26T01:29:10.000Z",
    "download_time": "2025-05-26T23:45:26.315626",
    "visual_resource": [
      "https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW87gf5WTBYFmy1xmgymvicsgiahmfr8SKkoxlNiaUsUH1GlzQ25bb5F4iaUjicGsR5oiaQ42o5478k4rmaw/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T01:29:10.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW87gf5WTBYFmy1xmgymvicsgiahmfr8SKkoxlNiaUsUH1GlzQ25bb5F4iaUjicGsR5oiaQ42o5478k4rmaw/0?wx_fmt=jpeg\", \"id\": \"g-nm0s2eIN73emjcj2oP6g\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/g-nm0s2eIN73emjcj2oP6g.txt\"}}"
  },
  {
    "id": "tREq8EM2AMpy4UWTKvuU-w",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/tREq8EM2AMpy4UWTKvuU-w",
    "title": "微软副总裁X上「开课」，连更关于RL的一切，LLM从业者必读",
    "summary": "微软副总裁Nando de Freitas在社交平台X上启动“开课”系列，深入讲解强化学习（RL）及其在大语言模型（LLM）中的应用。文章指出，监督学习侧重模仿，而强化学习强调选择性模仿与自我提升，能从次优数据中学习并超越专家。内容详细介绍了分布式强化学习系统中的Actor-Learner架构、离线强化学习、单步与多步RL问题。Freitas强调，当前主流LLM的RL后训练，核心在于单步RL，并阐释了策略梯度、基线减法、KL散度、重要性采样及PPO等关键算法与技巧。文章总结，理解这些理论对于当前LLM的强化学习应用至关重要。",
    "keywords": [
      "强化学习",
      "大语言模型",
      "策略梯度",
      "分布式强化学习",
      "单步强化学习",
      "智能体",
      "PPO",
      "重要性采样"
    ],
    "area": [
      "机器学习",
      "大模型",
      "人工智能"
    ],
    "content": "标题：微软副总裁X上「开课」，连更关于RL的一切，LLM从业者必读\n公众号：机器之心\n--------------------------------------------------\n\n作者：Nando de Freitas\n\n机器之心编译\n\n别人都在用 X 发帖子，分享新鲜事物，微软副总裁 Nando de Freitas 却有自己的想法：他要在 X 上「开课」，发布一些关于人工智能教育的帖子。该系列会从 LLM 的强化学习开始，然后逐步讲解扩散、流匹配，以及看看这些技术接下来会如何发展。\n\n话说回来，Freitas 有这个想法时还是 4 月 24 日，到今天为止，他已经更新了多篇帖子，每篇都干货满满。\n\n由于涉及的内容需要费点脑细胞来思考，在更新了几篇后，Freitas 抱怨道：「随着数学知识的增多，自己 X 上的读者人数正在下降。」\n\n或许，太硬核的东西，浏览量确实不会太高。\n\n不过，遗憾归遗憾，这些帖子对于那些想学习 RL、从事大模型的人非常有帮助。\n\nFreitas 也表示，他会不断更新内容，感兴趣的读者可以随时关注。\n\n接下来，我们看看最近几篇帖子内容。\n\n无监督学习、监督学习、强化学习终极定论尚未形成\n\n监督学习对应于最基础的模仿形式：简单的行为复制。它通过最大似然估计，将世界状态（如文本问题）映射到行动（如文本答案）。我们将这种映射关系称为策略。监督学习需要高质量的专家数据，学生只是机械地模仿教师行为，因此需要教师本身必须足够优秀。教师仅示范操作方式，并不进行评分反馈。\n\n另外，目前存在一些非常强大的监督学习方法，它们在通用性极强的专家指导下进行下一步预测（关联学习）和重构学习。这正是大语言模型预训练的核心原理，也是扩散模型、流匹配和自编码器在多模态感知与生成中运作的基础。从本质上看，预测下一个 bit 的过程实则是一种自由能（熵）最小化的过程，简而言之：在趋于无序的世界中创造有序。这正是细胞和生命运作的基本原理 —— 埃尔温・薛定谔和保罗・纳斯各自撰写的同名著作《生命是什么》对此有深入阐述。既然生命遵循这样的规律，那么智能系统采用类似机制运作也就不足为奇了。\n\n另一方面，强化学习 (RL) 则侧重于选择性模仿（selective imitation），这对于优化特定任务的性能非常有效。RL 可以从智能体或其他智能体先前生成的大量次优经验数据中进行训练。RL 可以利用价值函数或其他工具（通过奖励学习）来识别和选择有用的信号。这种选择过程使模型能够利用大量廉价的次优数据进行学习，并最终超越最优秀的老师。\n\n也就是说，在 RL 中，智能体可以识别哪些数据对学习有用，哪些数据应该忽略。\n\n就像我们不会模仿父母的每一个行为，而是选择模仿部分，以及哪些部分应该忽略。\n\nRL 的核心在于自我提高。智能体会生成数据，因此，他们可以从自身数据（成功和错误）以及来自其他智能体的混合数据中学习。\n\n当我们使用奖励信号构建选择机制（例如，对数据进行排序并只挑选最佳的那一半）时，智能体就可以开始从自身数据中学习并自我提升，这种方式非常强大。\n\n此外，智能体会利用其获得的知识来决定在环境中采取哪些行动，从而获得介入性因果知识。\n\n在《An Invitation to Imitation 》一书中，CMU 教授 Drew Bagnell 探讨了一种名为 Dagger 的强化学习替代方案，其中智能体采取行动，老师来纠正学生。\n\n对于智能体来说，从自身行动和自身经验中学习至关重要，这样它才能学会保持鲁棒性。\n\n例如，如果智能体使用专业驾驶员提供的数据学习驾驶，有一天发现自己偏离了道路（这种情况即使是完美的老师也从未发生过），那么学生将不知所措。为了让学生学会回到道路上，它需要老师在那时提供建议。\n\n一项重要的研究启示在于：生成模型对强化学习的作用与任何强化学习算法创新一样重要。这或许存在争议，但我认为过去十年间强化学习的进步，本质上是生成模型发展的结果。从算法演进来看（下文将详细展开），当前 AI 界普遍采用的基础算法思想 —— 如期望最大化算法（EM 算法）和策略梯度 —— 实际上已存在超过 50 年。真正的变革力量来自强化学习基础设施的规模扩张。\n\n希望读者能通过本文认识到：关于无监督学习、监督学习与强化学习的终极定论尚未形成。虽然我质疑这种分类法的有效性，但在未来的教学实践中仍将沿用该框架以辅助知识传递。\n\n分布式强化学习系统\n\n智能体是一种能够感知环境、自主采取行动从而实现目标，并可能通过强化学习或教学来提升自身性能的实体。\n\n智能体可以是一个多模态神经网络，它通过与环境的交互，为用户提供个性化目标。智能体观测得越多，就越容易为用户定制个性化的学习方案。\n\n基于工业级大语言模型（LLM）的强化学习（RL），可能涉及数百万次并行交互，使用数十亿参数的模型，甚至需要调动整个数据中心 —— 成本极其高昂！\n\n如何构建能在如此庞大尺度下高效运行的强化学习系统，绝非易事。\n\n根据文章《IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures》、《acme: A library of reinforcement learning》，现代分布式强化学习系统可以分为两个部分：Actors 和 Learners。\n\n每个 actor 通过使用称为策略的网络生成动作来与环境交互。actor 还从环境中收集奖励和观测结果。收集到的数据被添加到一个公共记忆中。\n\nLearner 从记忆中采样数据并使用它来更新策略网络。更新网络后，需要将权重检查点发送给每个 actor。在设计此类系统时，测量每个操作的持续时间、每个通信链路的带宽等非常重要。这需要精确的工程设计以及全面的测量和消融。\n\n在语言中，actors 是聊天机器人，环境是人。每次聊天的数据随后被发送到重放内存进行学习。通常，learner 可能比 actors 需要更多的存储空间和计算资源，因为 learner 需要跟踪梯度和大规模统计数据。\n\n此外，了解 actors 的推理成本、通信成本和学习成本也至关重要。\n\n另一方面，若数据采集速度不足，learner 可能需要重复利用历史经验来更新策略 —— 这正是 off-policy 场景的核心特征。此时必须解决一个关键问题：陈旧数据导致的偏差修正。在前面推文中的驾驶示例表明，过度依赖 off-policy 数据可能引发严重后果！\n\n值得庆幸的是，研究者已提出多种解决方案：\n\n重要性加权（Importance Weights）：对历史数据赋予不同权重以修正分布偏差；\n\n近端策略优化（PPO）：通过剪裁机制控制策略更新幅度；\n\nDeepSeek-R1 论文提出的加权方案：动态调整新旧数据贡献度。\n\n当系统仅依赖大型历史经验库进行策略学习时，便进入离线强化学习（Off-line RL）或批量强化学习（Batch RL）范式。相较于监督学习，其优势在于继承了前文讨论的生成 - 选择机制；但相比在线强化学习，又因缺失环境实时交互而存在局限。\n\n不过，离线强化学习在以下关键场景中具有不可替代价值：\n\n高成本交互（如机器人物理训练）；\n\n高风险环境（如自动驾驶事故数据学习）。\n\n用于后训练 LLM 的 RL\n\n目前看来，RL 有多种形式。\n\n其中一种是单步（one-step ）RL 问题（上图左上角）。在这里，给定一个提示，模型会生成一个动作并得到一个评估。\n\n这个动作可以是文本答案、CoT 推理序列、语音或任何其他行为信号，即任何 Token 序列。\n\n评估通常是一个单一的结果奖励，例如答案是否正确。\n\n与单步对应的是多步（multi-step）RL 问题，这种情况大多存在于与聊天机器人对话中，用户是环境，聊天机器人是智能体。\n\n在用户不提供任何输入的情况下，智能体会思考下一步做什么，这是一个单步强化学习问题。这在我们的图中（左上角）清晰可见，因为这三个动作可以轻松地合并为一个动作，而不会破坏决策图的结构。\n\n然而，规划整个对话以最终实现目标，在此期间用户和聊天智能体都会变化，这是一个多步强化学习问题（我们的图中，左下角）。\n\n在这种设置下，智能体可以使用工具（例如 Web 浏览器、编译器等）来建模聊天机器人，从而收集信息。\n\n当强化学习涉及多步时，可能每一步都会对结果有影响。就像当一个人获得奖励时，他并不知道是众多决策中的哪一个导致了奖励。这种现象人们称之为信用分配问题。\n\n由于多步问题，强化学习通常是组合型的，而且维度非常高。在这些情况下，我们必须解决跨维度推理问题。\n\n简而言之，强化学习真的很难，解的方差可能非常大。虽然研究人员发明了一系列概念来控制方差，但代价是引入了偏差，其中包括价值函数。这些概念在多步决策问题中很有用，但对于单步强化学习来说并非总是必需的。虽然其中一些想法在电脑游戏中很有效，但在 LLM 中却行不通。\n\n在控制领域，普遍存在这些问题，如 T 步决策、二次奖励函数。这些被称为线性二次高斯控制器或调节器，构成了最普遍的控制类型之一 —— 模型预测控制 (MPC) 的基础。\n\n然而，盲目地将为电脑游戏或控制开发的强化学习方法的理论和软件引入语言模型领域是危险的。\n\n为了实现工具使用和多步辅助，我们需要为 LLM 提供多步强化学习。然而，要实现像 DeepSeek-R1 或测试时强化学习 TTRL 这样的方法，需要先解决单步强化学习问题，因为这个问题稍微简单一些。\n\n所有 RL 智能体都能够自我学习和自我改进。如果设计得当，它们可以构建质量不断提升的数据集，从而生成更好的策略。RL 智能体的这一特性对于性能和安全性都至关重要。\n\n可能存在一些更难的强化学习案例。比如有时决策范围是未知的或无限的，时间步长可能是连续的，也可能是中断驱动的，动作和观测可以是离散的、也可以是连续的，这些都增加了推理的复杂性。\n\n出于教学原因，明天我们将首先介绍最简单的案例：单步强化学习。\n\n单步强化学习与策略梯度\n\n包括 DeepSeek-R1 在内的许多团队，当他们声称在进行 RL 时，会最大化单步目标函数，如下所示：\n\n这些数学符号代表了以下概念：\n\n也就是说，我们正在针对所有的数据字符串 (a,o) 对 LLM 进行微调。在处理文本时，通常使用积分符号来表示非常庞大的离散求和。\n\n因此，如果我们有一个包含 N 对观测值和动作 (o, a) 的数据集，那么在实际操作中，我们会按如下方式评估目标函数：\n\n环境（用户）也会为我们提供观测值（提示或指令 o）。所以不需要知道分布 P (o)。由于不知道最优动作（大语言模型生成的内容，也就是动作 a），将对这些动作进行积分。这是在概率中处理未知量的标准方法。我们对所有可能的值按照出现的概率进行加权求和。在这种情况下，动作的概率就是大语言模型所生成的结果。大语言模型是一个概率模型。\n\n这种在对大语言模型的权重进行最大化的同时对其输出进行求和的方法，被称为最大期望效用，这也是博弈论中理性主体所采取的做法。通过最大化期望效用（奖励的另一种说法），人们可以收敛到纳什均衡。在统计学中，人们把这个过程称为边缘化，而当它还涉及到对某个量进行最大化时，它就被称为实验设计。\n\n总之，在单步强化学习中，我们通过调整大语言模型的策略来最大化期望回报 R，也就是说，对于目前大多数的大语言模型而言（见上一篇文章），在单次结果中进行奖励 R=r (a,o)。\n\n策略梯度：就是人们所说的 on policy RL 或 Reinforce 算法。这种方法被称为 on-policy，是因为生成样本（动作）的策略（大语言模型）与正在被学习的策略是同一个。\n\n当生成样本的成本低于学习成本时，这种方法是有意义的。也就是说，当 learner 可以按需轻松获取新样本时适用。\n\n但对于成本高昂的游戏模拟引擎而言并非如此，在这类场景中，必须引入缓冲区和回放记忆来缓存数据。随着数据变得陈旧，就需要使用 off-policy 方法。\n\n那如何计算单步损失的梯度，答案是只需沿着梯度方向更新参数即可。\n\n从理论上讲，策略梯度可以使用微积分按如下方式得到它：\n\n策略梯度常用技巧\n\n前文重点介绍了策略梯度算法，不过大家还会使用一些技巧来提高性能。\n\n现在，我们从一个常用的技巧开始，即从奖励中减去奖励的均值。得到的表达式被称为优势（advantage）。这项技术本身被称为基线减法（baseline subtraction）。\n\n在策略梯度中，如果我们从奖励 r 中减去其均值，然后用下面的奖励来替代原来的奖励：\n\n这样做并没有改变最大值的位置，但降低了方差。\n\n此外，当奖励是二元的，而我们又需要一个更连续、渐进的反馈信号时，这种方法格外有用。\n\n下面是证明过程：\n\nKL 散度\n\nKL 散度是一种用于衡量两个分布之间「距离」的方法，从数学角度来说，KL 散度定义如下：\n\n如果在强化学习的损失函数中加入 KL 散度项，本质上是在鼓励后训练（post-training）过程中学习到的 LLM 策略保持接近监督微调（SFT）阶段的策略。\n\n如果我们根据最新的策略 p(a|o) 采样 N 个动作，我们可以再次使用蒙特卡罗方法来近似计算 KL 散度：\n\n此外，John Schulman 有一篇很棒的关于如何高效近似 KL 散度的博客。他提出了以下替代方法：\n\n采样、PPO 以及 GRPO 的重要性\n\n在强化学习系统中，有时会有多个 actors 来收集数据并将数据添加到记忆系统中。然后，learner 从这个记忆中提取样本进行学习。\n\n在这种异步设置中，有些样本会变得过时。生成样本的机制（actors）与更新参数的机制（learner）不同，因此这种方法被称为 off-policy。\n\n重要性采样（Importance Sampling, IS）提供了一种校正 off-policy 样本偏差的解决方案，其核心操作如下：\n\n我们通过在单步目标函数的被积项中乘以并除以旧策略 π_old (a|o) 实现修正。系统将基于该旧策略采取动作，但实际学习的却是新策略 —— 这正是 off-policy 学习的本质特征。数学表达上，通过引入行为策略进行乘除变换后，单步强化学习目标函数转化为：\n\n如果我们观察到一个提示 o^i ，并从行为策略中采样出一个动作 a^i，可以再次用以下蒙特卡罗近似来替代积分，这种近似被称为 IS 估计：\n\n分布的比率被称为重要性权重：\n\n这个权重可能会增大并导致不稳定性，尤其是因为我们计算这个比率所涉及的所有字符串的空间是非常高维的。\n\nPPO：为了防范高方差和不稳定性，我们必须巧妙地截断（裁剪）重要性权重。让我们再次来考虑一下我们的 off-policy 目标：\n\n近端策略优化（PPO）修改了这个目标函数，对那些使 w (theta) 偏离 1 的策略变化进行惩罚，具体如下：\n\nPPO 的内容远不止这些，所以我鼓励大家都去读一读这篇有影响力的论文《Proximal Policy Optimization Algorithms》。\n\nDeepSeek-R1 将裁剪后的重要性采样、基线减法以及与参考策略的 KL（相对熵）接近度相结合，以此来训练其推理模型。（PPO 也做了所有这些事情，但方式略有不同。）\n\n现在我们已经介绍了 DeepSeek 强化学习算法（GRPO）的所有要素，所以接下来就只是把它们整合起来的问题了。\n\n当然，真正的挑战在于解决实现过程中基础设施和数据方面的问题。\n\n为了得到第一个蒙特卡罗估计值，我们使用来自行为策略 pi_old 的样本 a^i ，但是如果我们想要保持估计的无偏性，对于第二项（即 KL 散度项）的蒙特卡罗估计应该使用来自 pi_theta 的样本 a^i ，而不是来自 pi_old 的样本。\n\n正如在之前的文章中所提到的，我们已经从奖励中减去了平均基线值：\n\n但与 DeepSeek-R1 不同的是，我们没有除以标准差。这一点值得通过实证来检验。\n\n注意：在这个版本中，我们针对每个观测值采样一个动作。也可以针对每个观测值采样多个动作来减少方差。DeepSeek-R1 基本上就是这么做的，其梯度更新包含了针对单个问题的多个动作样本。这种技术在随机近似中被称为公共随机数。\n\n如果你对 PPO 和 GRPO 的这些公式感到熟悉了，那么你现在几乎已经了解了如今所有公司在 LLM 中使用的强化学习（RL）所需的全部理论知识。\n\n接下来，Freitas 想从单步强化学习拓展到多步强化学习，从而进行更深入的研究。感兴趣的小伙伴，可以随时关注 Freitas 动态。\n\n参考链接\n\nhttps://x.com/NandoDF/status/1919728246821634205\n\nhttps://x.com/NandoDF/status/1918324866979184874\n\nhttps://x.com/NandoDF/status/1917865356829618645\n\nhttps://x.com/NandoDF/status/1917575545673417069\n\nhttps://x.com/NandoDF/status/1917270302666678614\n\nhttps://x.com/NandoDF/status/1916835195992277281\n\nhttps://x.com/NandoDF/status/1915548697548464359\n\nhttps://x.com/NandoDF/status/1915351835105169534\n\n© THE END\n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com",
    "published_time": "2025-05-26T01:29:10.000Z",
    "download_time": "2025-05-26T23:45:32.973354",
    "visual_resource": [
      "https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic9XyDIFDk5o9xkXDxmdyIv0qYqicLP3rpbymZF9icSzB7wTE5CnAuKx9TwpDCE6bWviaGjItLVib6stw/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T01:29:10.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic9XyDIFDk5o9xkXDxmdyIv0qYqicLP3rpbymZF9icSzB7wTE5CnAuKx9TwpDCE6bWviaGjItLVib6stw/0?wx_fmt=jpeg\", \"id\": \"tREq8EM2AMpy4UWTKvuU-w\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/tREq8EM2AMpy4UWTKvuU-w.txt\"}}"
  },
  {
    "id": "Z90cTpV6t7cwSm7ILdcj_w",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/Z90cTpV6t7cwSm7ILdcj_w",
    "title": "惊了，我的电脑在自动打工！花不到1块钱雇个「AI超人」，Office三件套被卷死",
    "summary": "昆仑万维近日发布天工超级智能体（Skywork Super Agents），标志着智能体技术迈入新阶段。该产品集成了专家级Agent，可一键实现Office文档、多模态内容（音乐、MV、播客等）的智能生成，并在GAIA等基准测试中表现卓越，超越国际竞品。Skywork以“场景全、能力强、框架开源”为核心优势，并创新性地提供信源可追溯、个人知识库、低成本使用等差异化功能。它打通了从信息发现到内容输出的全链路流程，有望革新传统办公方式，预示着中国AI企业在智能体领域具备国际竞争力与战略先机。此举推动AI Agent从技术展示走向场景落地，重塑生产力工具格局。",
    "keywords": [
      "智能体",
      "大模型",
      "昆仑万维",
      "Skywork",
      "多模态",
      "Office",
      "生产力",
      "开源"
    ],
    "area": [
      "智能体",
      "大模型",
      "多模态"
    ],
    "content": "标题：惊了，我的电脑在自动打工！花不到1块钱雇个「AI超人」，Office三件套被卷死\n公众号：机器之心\n--------------------------------------------------\n\n编辑：杜伟\n\n国产智能体，这次真封神了。\n\n过去这段时间，「智能体」简直杀疯了。\n\n无论是初创公司还是互联网大厂，主流 AI 玩家们都开始围着它转，说它是大模型的下一站也毫不夸张。\n\n作为人工智能的一种高级实现，智能体比大模型更具实体化、自主性、交互性，已经进化成「会思考、动手强、能串联一切工具的全能战士」。\n\n就在 5 月 22 日，这条 AI 赛道迎来了一个足以傲视群雄的「巨无霸」产品，其背后站着的正是一家国产大模型厂商 —— 昆仑万维。\n\n这家位居国内大模型第一梯队的选手，面向全球正式发布了天工超级智能体（Skywork Super Agents）（下文简称 Skywork），直接卷出了新高度！\n\n与 Manus、OpenAI deep research、Genspark 等其他智能体相比，Skywork 有三大必杀技：「场景全、能力强与框架开源。」\n\n首先是「全」，Skywork 远不是一两个小工具拼凑出来的智能体，而是系统打包了 5 个专家级 AI Agent，可以一键生成专业文档、数据表格、PPT、播客、网页五件套，称得上是内容创作者眼中的黄金搭子。\n\n更炸裂的是，Skywork 还提供了 1 个通用 AI Agent，可以一站式地输出音乐、MV、宣传片、绘本、有声书等多模态内容。\n\n其次是「智商」爆表，跑分成绩给了我们很大惊喜。\n\nSkywork 在多个 AI Agent 基准测试榜单中登顶，比如在 GAIA（最困难、最全面的智能体基准测试）中，面对从初级到高级、从易到难的不同任务（Level 1 到 Level 3），它全面超越了 Manus、OpenAI deep research。\n\n这还没完，Skywork 在 SimpleQA（评测智能体回答事实性问题准确性的基准测试）中的得分（94.5）同样超越了 OpenAI 以及当前 SOTA，解决大模型「胡言乱语」更给力了。\n\n最后是让开发者沸腾的 —— 全球首个开源的 deep research agent 框架，这意味着每个人都可以参与到智能体的定义中来了。\n\n同时，直接开放三大 MCP 接口，供开发者调用文档生成、数据分析、PPT 演示文档三大能力，形成以智能体为核心的「AI 操作系统」，成为开发者们的新基建。\n\n框架开源地址：https://github.com/SkyworkAI/DeepResearchAgent\n\nMCP 地址：https://mcp.so/server/skywork-super-agents/Skywork-ai\n\n还有一点特别值得称道，Skywork 不搞现在 AI 圈讨厌的饥饿营销那套，不排队、不抢码、不内测申请，上线即可用。性价比还贼高，单个通用任务成本仅需 0.96 元。\n\n今日，昆仑万维重磅宣布天工超级智能体（Skywork Super Agents）APP 正式上线，这也是全球首款基于 AI Agent 架构的 Office 智能体手机 App。\n\n看起来，想要体验智能体的小伙伴终于有了一个实力更强、价格又便宜的选择。\n\n已关注 关注 重播 分享 赞 关闭观看更多更多退出全屏视频加载失败，请刷新页面再试 刷新 视频详情\n\n视频加载失败，请刷新页面再试\n\n第一手实测：智能体界的「全能型选手」\n\n从 OpenAI 的 deep research 到 Manus、Genspark 等专精型 Agent，市面上的产品在功能上可以说大同小异，而「谁真正能落地、谁真正好用」成为普通用户最关心的问题。\n\n接下来，我们就搞个一手实测，看看 Skywork 这个「新秀」的实力究竟如何。\n\n全球官网：https://skywork.ai\n\n中国官网：https://tiangong.cn\n\n多场景写作\n\n最近 AI 率检测的问题频频登上各大平台的热搜榜。有大学生发帖称，熬秃了头写的毕业论文 AI 率被判了 80%，测试一番后发现朱自清的《荷塘月色》AI 率竟超 60%。为了去 AI 味儿，学生们绞尽脑汁，要么疯狂改标点，要么短句改长句……\n\n对此，我们让 Skywork 生成一份适用于 B 站 3 分钟科技短视频的脚本， 吐槽一下当前 AI 技术的发展带来的魔幻现实。\n\n不得不说，Skywork 设计的「UP 主台词」很有 B 站特色，如「屏幕前的各位『肝帝们』」、「亿点点变化」、「以前是怕抄袭，现在是怕被 AI」 等语句非常贴合年轻受众，既有梗又有深度。而且它还在合适的位置穿插着柱状图、专家观点的引用，更增强了脚本的传达力与可信度。\n\n可视化数据分析\n\nSkywork 的表格模式类似于 Microsoft Excel 和 Google Sheets，输入主题、需求或者上传原始数据后，它就能智能分析并生成表格、图标等。\n\n我们输入指令：「生成一份图灵奖 2015-2024 年的获奖统计」。\n\nSkywork 并不急于执行，而是通过与用户交互确认任务细节，包括统计维度（如获奖人数、国籍、研究领域、所属机构）和展示方式（如表格、趋势图、占比图等）。这种前置的任务拆解能力，使得整个分析过程具有极高的准确性和可控性。\n\n紧接着，它会自动生成待办清单，调用各种 MCP 工具依次执行任务，整个过程无需用户参与。\n\nSkywork 准确完成了数据整理和分析任务，最终生成四个结构完整、信息翔实的 sheet。在可视化呈现方面，它生成的饼图、柱状图和折线图不仅美观，还极具数据表达力。\n\n精美 PPT\n\n打工人最怕三件事：加班没有加班费、老板突然 @你、顺便做个 PPT。别的顺便是顺手，这个「顺便」是要命：字体怎么调都不对，图表怎么看都很土，辛辛苦苦熬夜干到凌晨三点，结果老板瞅了一眼：「感觉不太对，你再打磨下」。\n\n现在，Skywork 给每个被 PPT 逼疯的打工人开了张救命的「药方」。只要一句简单的 Prompt：制作《日本镰仓的旅游攻略》PPT，它立马就能生成一份结构清晰、颜值在线的 PPT。\n\n从最终生成效果来看，该 PPT 不仅有实用信息点，还配了高质量的场景图、路线图和日程表，我们完全可以直接拿来用。\n\n创意网页\n\n今年 2 月份，Andrej Karpathy 提出了 Vibe Coding（氛围编程）的概念，码农们不用逐行写代码，只要用自然语言描述目标功能，专用大模型即可生成对应代码。\n\nSkywork 目前也能实现这一功能，输入需求就能快速生成可溯源、可编辑、可应用的高质量多模态 html 成果。举个例子，我们让它「生成一个开心消消乐的网页小游戏」，几分钟后它啪地甩来一个水果卡通风的成品。\n\n这个网页小游戏不仅界面设计的好看，更重要的是，它真实可玩。在规定的一分钟内，玩家让三个或以上相同的水果图标连成一条直线即可消除，并且它还能根据得分情况增加等级。\n\n智能音频播客\n\n去年谷歌 NotebookLM 曾掀起一股 AI 播客热潮，仅需一个链接或文档，几分钟就能转成接地气的男女对谈。当时，不少 AI 圈的大咖为它「站台」，甚至连「死对头」OpenAI CEO Altman 都认为它很酷。\n\nSkywork 这次上线的播客模式与之有异曲同工之妙。我们输入 Prompt：《面纱》读书对谈播客，它随即就去找资料写稿「录制」，然后一键生成音频播客。\n\nAI 男女主播吐字清晰，语音语调自然真实，语气词、说话的气口都把握得相当到位。从内容来看，他俩也不是照本宣科，而是在真正读懂了小说后，针对播客这一媒介形式进行的自我创作。\n\n与谷歌 NotebookLM 不同的是，如果我们对成品有任何调整需求，可以直接在聊天框里提，Skywork 随时修改。比如，我们让播客增加一部分，介绍小说中的男主沃尔特・费恩是个怎样的人，Skywork 立马在原有播客基础上更新了相应的内容。\n\n音乐与视频生成\n\n此前，昆仑万维发布了全球首个音乐推理大模型 Mureka O1，不输 Suno 的惊艳效果让所有人看到了其在音乐生成领域的深厚造诣。\n\nSkywork 的音乐实力同样不俗，比如「生成一段适合清晨独自散步时听的轻音乐」，它以钢琴为主旋律，整体节奏舒缓匀称，给人一种在晨光中漫步的惬意感觉。\n\n视频生成也不在话下，比如「城市天台夜晚，一群会发光的小鸟从霓虹灯上起飞，汇聚成一个漂浮的时间钟表」，这种复杂 Prompt 的目标场景都能 hold 住，你就说赞不赞吧？！\n\nSkywork 还能生成带背景音乐的视频，如下「generate a video with music: a man raps to the camera」。下一步的升级方向可能就是直接生成指定对白、语气的音画同步视频了，就像谷歌 Veo 3 所能做到的那样。\n\n这一波实测下来，我们切身的感受是：强，太强了！\n\n无论是生成内容的丰富性、专业性、准确性，还是界面设计的美观程度和布局的合理性，Skywork 都展现出了一个「全能型」智能体该有的样子，甩开了现有竞品。\n\n人无我有，打造差异化竞争优势\n\n在体验过程中，我们发现，Skywork 在任务协同、多模态生成、结果可信度和个人知识库上，具备了真正的「差异化实力」，克服了 Manus、OpenAI deep research 等竞品的痛点，实现了「人无我有」的后发优势。\n\n超能 Office 三件套 —— 高效内容创作与生产力输出\n\n当代打工人，谁没被文档、表格和 PPT 这工作「三件套」逼疯过？\n\n如今，Skywork 把文档、表格、PPT 这三大办公工具整合在了一起，生成的内容不仅更详细、更条理，还能做出各种清晰好看的图表，甚至还能插入 Youtube 视频。\n\n当然，如果你想对生成的内容进一步细化调整，Skywork 提供了在线编辑功能，通过「编辑」按钮直接在界面上修改文字、调整结构，像使用在线协作工具一样自然流畅。\n\n导出格式也非常灵活，包括 PPTX、PDF、HTML、Google Slides 等多种格式，满足我们在决策讨论、版本迭代、二次创作中的不同需求。\n\n生成酷炫内容 —— 多模态内容融合\n\n在日常创作中，我们往往需要在文字、图片、音频、视频等多个工具之间来回切换，才能把一个想法完整呈现。\n\nSkywork 在通用对话任务上打破传统 Agent 任务执行的边界，接入网页搜索、思考分析、图片生成、图片理解、语音生成、音乐生成、视频生成等十余个 MCP，让创作者无需奔波于不同平台，就能一键生成宣传片、MV、有声书、绘本等多种形式的内容。\n\n比如我们仅用一个 Prompt，就让它混搭出一个小猫的旅行 vlog，不仅准确生成出各大地标，还让小猫在每一个场景中自然入镜，整个过程无需人工干预。\n\n提示词：帮我生成一个小猫的旅行 vlog，内容分别是小猫到法国埃菲尔铁塔、美国自由女神像、中国长城、澳大利亚悉尼歌剧院、埃及金字塔、印度泰姬陵、日本富士山等地旅游并与这些著名景点自拍合照，配乐轻松欢快。\n\n这种「Agent+MCP 多工具融合」的架构，有望引领下一代内容生产的范式革命。\n\n信源可追溯 —— 向可验证内容创作演进\n\n在如今这个内容泛滥的时代，信息可靠性反倒成了稀缺资源。大模型虽然擅长高效生成，但它们一本正经地胡说八道早就不是什么新闻。\n\nSkywork 试图解决的正是这个痛点。它生成的每一段文字、每一张图片都不是凭空捏造，而是能清晰追溯到具体出处。输出文本可以关联原文段落，图片也能标注出溯源网页或知识库来源，甚至还附上完整的信源列表。\n\n这种将信息溯源融入创作流程，让用户在生成内容的同时随时验证，大大降低了大模型「满嘴跑火车」的风险，真正让每一次产出都有据可查。\n\n个人知识库 —— 打造私有化智能内容循环\n\n如今，市面上的智能体普遍存在的一大痛点在于：素材零散、成果不可持续，缺乏系统性积累机制。\n\n为了解决这些挑战，Skywork 上线了个人知识库。我们可以上传 pdf、doc、ppt、xls 等多种格式的文件，也可以上传录音、url 和 youtube 视频播放地址。每个知识库支持上传最多 50 个文档，并可根据不同主题创建多个知识库，实现清晰有序的知识管理。\n\n更重要的是，Skywork 不只是一个信息存储工具，更是一个智能创作引擎。基于知识库内容，我们可以一键生成 多模态内容，它们又能反向存入知识库，形成「素材 - 创作 - 再积累」的正向循环，打造真正可生长的个人知识系统。\n\n以上这些差异化功能，构成了 Skywork 的核心竞争力，使之成为真正「有用、敢用、好用」且更具性价比的 AI 智能打工人。\n\n从信息发现到结构化内容输出\n\n全链路流程被打通\n\n为了实现通用化、性能更强的智能体，Skywork 在底层技术上祭出了多项自研，打造全链路智能内容引擎，从深度搜索到高效生成，一站式解决复杂任务。\n\n首先自研一个 deep research 模型，通过依托「深度思考 + 推理」的信息检索，不仅查得更广与更准，还能更快找到高质量源信息；强化学习能力的加持又进一步增强模型面向各种搜索任务的泛化性，性能上全面对标 OpenAI 竞品。\n\n接下来是一套自研的 agent workflow 框架，在高效完成传递信息、拆解任务之外，还能灵活调用基座大模型，使智能体能力得以延伸。效果也非常显著，在开源的 deep research 排行榜上拿下了 SOTA 成绩。\n\n此外还自研一个生成物模型，实现高质量数据的生成、收集和训练，使生成内容更丰富、更真实且可读性更强；配合自研的在线编辑系统，无缝兼容常见办公软件，实现一站式内容生成与修改，并能一键导出成稿，效率与友好性绝对是拉满了。\n\n最后，面对特别复杂的任务也有诀窍 —— 「化整为零、各个击破」，即将复杂任务拆分为多个小任务，每个小任务单独进行深度研究、互不干扰，有效突破了模型上下文长度的限制，支持超复杂任务协同解决。\n\n正是技术上的一系列突破，Skywork 才有了如今敢于叫板一切对手的实力。\n\nOffice 的下一次革命来了？\n\n自大模型技术爆发以来，人们一直在寻找应用的突破方向，最先开启自动化革命的恰恰是写代码本身。\n\n现在，很多人都知道自然语言驱动的编程工具 Cursor，它正在吸引越来越多的程序员。人们写代码的方式已经发生了变化：先让 AI 写一个 readme 列出项目设计思路、功能逻辑，然后再让 AI 一步一步地实现就可以了。\n\nCursor 也让编程门槛降到了一个前所未有的低点，号称让非程序员也能参与开发。只要你能描述清楚需求，Cursor 就能帮你生成专业级的代码。\n\n同样地，超级智能体带来的能力，就像是 Office 版本的 Cursor。无论是制作文档、表格、PPT，还是生成网页或播客，它都可以根据你提出的需求快速进行生成，节省你大量的工作时间。\n\n随着智能体成为 AI 产业界的核心关键词，它已过了秀概念的阶段，并开始了从技术展示向场景落地、从工具层向系统层的过渡。此次，Skywork 的推出不仅印证了中国 AI 企业在智能体领域具备了与国际对手抗衡乃至超越的实力，而且预示了接下来在该 AI 方向上「技术 + 场景 + 生态」全面交锋的趋势。\n\n对于昆仑万维来说，这是一次具有战略意义的突破。在未来更大的应用前景铺开之前，它用一款全栈自研的超级智能体为自己在市场上赢得了先机。从上手体验来看，这款产品已经越过了实用化的门槛。\n\n或许过不了多久，大量的工作就会由智能体接手，这何尝不是办公全家桶的一次进化？你只需要提出需求，投喂文件资料，确认好细节后，坐等 AI 交作业！\n\n© THE END\n\n转载请联系本公众号获得授权\n\n投稿或寻求报道：liyazhou@jiqizhixin.com",
    "published_time": "2025-05-26T01:29:10.000Z",
    "download_time": "2025-05-26T23:45:43.243091",
    "visual_resource": [
      "https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicC4VJaqEHxJ03AzvOfKsWEWjDYkR8PTVdDb8OTjcZZfVHiab5pnibYVgxTStJ60zMnthNkyrCjB40A/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T01:29:10.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicC4VJaqEHxJ03AzvOfKsWEWjDYkR8PTVdDb8OTjcZZfVHiab5pnibYVgxTStJ60zMnthNkyrCjB40A/0?wx_fmt=jpeg\", \"id\": \"Z90cTpV6t7cwSm7ILdcj_w\"}, \"extraction_info\": {\"account\": \"机器之心\", \"file_path\": \"./database/content/wechat/Z90cTpV6t7cwSm7ILdcj_w.txt\"}}"
  },
  {
    "id": "t6SC84o6633kKLciQkh0qw",
    "source": "wechat",
    "url": "https://mp.weixin.qq.com/s/t6SC84o6633kKLciQkh0qw",
    "title": "今日大厂员工事：字节“飞书之父”离职；阿里国际期权回购；腾讯社交线GM张浩离职；高德产研搬杭州；快手端午节发被子；岱宗入职高合",
    "summary": "本期聚焦科技巨头内部人事变动与公司动态。字节跳动旗下“飞书之父”被曝离职，引发业界关注；阿里巴巴国际数字商业集团启动期权回购计划，以激励员工并优化股权结构；腾讯社交平台业务部总经理张浩亦已离职，进一步引起市场对核心高管流动的讨论。此外，高德地图产研部门宣布将搬迁至杭州，快手在端午节期间发放被子作为员工福利，以及汽车领域岱宗入职高合汽车等消息，共同勾勒出当前大厂在业务调整、人才流动及企业管理方面的新动向。",
    "keywords": [
      "大厂",
      "员工",
      "离职",
      "期权回购",
      "人事变动",
      "企业管理"
    ],
    "area": [
      "人工智能",
      "大模型",
      "其他"
    ],
    "content": "标题：今日大厂员工事：字节“飞书之父”离职；阿里国际期权回购；腾讯社交线GM张浩离职；高德产研搬杭州；快手端午节发被子；岱宗入职高合\n公众号：大厂日爆\n--------------------------------------------------\n\n本周五（5月30号）晚24点之前参与爆料的朋友，可随意挑选。\n\n预祝节日快乐~\n\n（爆料得礼物入口）",
    "published_time": "2025-05-26T01:50:34.000Z",
    "download_time": "2025-05-26T19:07:22.581873",
    "visual_resource": [
      "https://mmbiz.qpic.cn/sz_mmbiz_jpg/a1K5vT5T4lCh8IHkp27ibJJKyvQbibLjKl68DQGkOIQ7WGn5grxo84CXNLhHgeKWgTTBtf1ABClucGTsRnAXXw2g/0?wx_fmt=jpeg"
    ],
    "meta-data": "{\"original_metadata\": {\"date_modified\": \"2025-05-26T01:50:34.000Z\", \"image\": \"https://mmbiz.qpic.cn/sz_mmbiz_jpg/a1K5vT5T4lCh8IHkp27ibJJKyvQbibLjKl68DQGkOIQ7WGn5grxo84CXNLhHgeKWgTTBtf1ABClucGTsRnAXXw2g/0?wx_fmt=jpeg\", \"id\": \"t6SC84o6633kKLciQkh0qw\"}, \"extraction_info\": {\"account\": \"大厂日爆\", \"file_path\": \"./database/content/wechat/t6SC84o6633kKLciQkh0qw.txt\"}}"
  },
  {
    "id": "agenticSeek",
    "source": "GitHub",
    "url": "https://github.com/Fosowl/agenticSeek",
    "title": "AgenticSeek: Private, Local Manus Alternative.",
    "content": "# AgenticSeek: Private, Local Manus Alternative.\n\n<p align=\"center\">\n<img align=\"center\" src=\"./media/agentic_seek_logo.png\" width=\"300\" height=\"300\" alt=\"Agentic Seek Logo\">\n<p>\n\n  English | [中文](./README_CHS.md) | [繁體中文](./README_CHT.md) | [Français](./README_FR.md) | [日本語](./README_JP.md)\n\n*A **100% local alternative to Manus AI**, this voice-enabled AI assistant autonomously browses the web, writes code, and plans tasks while keeping all data on your device. Tailored for local reasoning models, it runs entirely on your hardware, ensuring complete privacy and zero cloud dependency.*\n\n[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)\n\n### Why AgenticSeek ?\n\n* 🔒 Fully Local & Private - Everything runs on your machine — no cloud, no data sharing. Your files, conversations, and searches stay private.\n\n* 🌐 Smart Web Browsing - AgenticSeek can browse the internet by itself — search, read, extract info, fill web form — all hands-free.\n\n* 💻 Autonomous Coding Assistant - Need code? It can write, debug, and run programs in Python, C, Go, Java, and more — all without supervision.\n\n* 🧠 Smart Agent Selection - You ask, it figures out the best agent for the job automatically. Like having a team of experts ready to help.\n\n* 📋 Plans & Executes Complex Tasks - From trip planning to complex projects — it can split big tasks into steps and get things done using multiple AI agents.\n\n* 🎙️ Voice-Enabled - Clean, fast, futuristic voice and speech to text allowing you to talk to it like it's your personal AI from a sci-fi movie\n\n### **Demo**\n\n> *Can you search for the agenticSeek project, learn what skills are required, then open the CV_candidates.zip and then tell me which match best the project*\n\nhttps://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316\n\nDisclaimer: This demo, including all the files that appear (e.g: CV_candidates.zip), are entirely fictional. We are not a corporation, we seek open-source contributors not candidates.\n\n> 🛠️ **Work in Progress** – Looking for contributors!\n\n## Installation\n\nMake sure you have chrome driver, docker and python3.10 installed.\n\nWe highly advice you use exactly python3.10 for the setup. Dependencies error might happen otherwise.\n\nFor issues related to chrome driver, see the **Chromedriver** section.\n\n### 1️⃣ **Clone the repository and setup**\n\n```sh\ngit clone https://github.com/Fosowl/agenticSeek.git\ncd agenticSeek\nmv .env.example .env\n```\n\n### 2️ **Create a virtual env**\n\n```sh\npython3 -m venv agentic_seek_env\nsource agentic_seek_env/bin/activate\n# On Windows: agentic_seek_env\\Scripts\\activate\n```\n\n### 3️⃣ **Install package**\n\nEnsure Python, Docker and docker compose, and Google chrome are installed.\n\nWe recommand Python 3.10.0.\n\n**Automatic Installation (Recommanded):**\n\nFor Linux/Macos:\n```sh\n./install.sh\n```\n\nFor windows:\n\n```sh\n./install.bat\n```\n\n**Manually:**\n\n**Note: For any OS, ensure the ChromeDriver you install matches your installed Chrome version. Run `google-chrome --version`. See known issues if you have chrome >135**\n\n- *Linux*: \n\nUpdate Package List: `sudo apt update`\n\nInstall Dependencies: `sudo apt install -y alsa-utils portaudio19-dev python3-pyaudio libgtk-3-dev libnotify-dev libgconf-2-4 libnss3 libxss1`\n\nInstall ChromeDriver matching your Chrome browser version:\n`sudo apt install -y chromium-chromedriver`\n\nInstall requirements: `pip3 install -r requirements.txt`\n\n- *Macos*:\n\nUpdate brew : `brew update`\n\nInstall chromedriver : `brew install --cask chromedriver`\n\nInstall portaudio: `brew install portaudio`\n\nUpgrade pip : `python3 -m pip install --upgrade pip`\n\nUpgrade wheel : : `pip3 install --upgrade setuptools wheel`\n\nInstall requirements: `pip3 install -r requirements.txt`\n\n- *Windows*:\n\nInstall pyreadline3 `pip install pyreadline3`\n\nInstall portaudio manually (e.g., via vcpkg or prebuilt binaries) and then run: `pip install pyaudio`\n\nDownload and install chromedriver manually from: https://sites.google.com/chromium.org/driver/getting-started\n\nPlace chromedriver in a directory included in your PATH.\n\nInstall requirements: `pip3 install -r requirements.txt`\n\n---\n\n## Setup for running LLM locally on your machine\n\n**Hardware Requirements:**\n\nTo run LLMs locally, you'll need sufficient hardware. At a minimum, a GPU capable of running Qwen/Deepseek 14B is required. See the FAQ for detailed model/performance recommendations.\n\n**Setup your local provider**  \n\nStart your local provider, for example with ollama:\n\n```sh\nollama serve\n```\n\nSee below for a list of local supported provider.\n\n**Update the config.ini**\n\nChange the config.ini file to set the provider_name to a supported provider and provider_model to a LLM supported by your provider. We recommand reasoning model such as *Qwen* or *Deepseek*.\n\nSee the **FAQ** at the end of the README for required hardware.\n\n```sh\n[MAIN]\nis_local = True # Whenever you are running locally or with remote provider.\nprovider_name = ollama # or lm-studio, openai, etc..\nprovider_model = deepseek-r1:14b # choose a model that fit your hardware\nprovider_server_address = 127.0.0.1:11434\nagent_name = Jarvis # name of your AI\nrecover_last_session = True # whenever to recover the previous session\nsave_session = True # whenever to remember the current session\nspeak = True # text to speech\nlisten = False # Speech to text, only for CLI\nwork_dir =  /Users/mlg/Documents/workspace # The workspace for AgenticSeek.\njarvis_personality = False # Whenever to use a more \"Jarvis\" like personality (experimental)\nlanguages = en zh # The list of languages, Text to speech will default to the first language on the list\n[BROWSER]\nheadless_browser = True # Whenever to use headless browser, recommanded only if you use web interface.\nstealth_mode = True # Use undetected selenium to reduce browser detection\n```\n\nWarning: Do *NOT* set provider_name to `openai` if using LM-studio for running LLMs. Set it to `lm-studio`.\n\nNote: Some provider (eg: lm-studio) require you to have `http://` in front of the IP. For example `http://127.0.0.1:1234`\n\n**List of local providers**\n\n| Provider  | Local? | Description                                               |\n|-----------|--------|-----------------------------------------------------------|\n| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |\n| lm-studio  | Yes    | Run LLM locally with LM studio (set `provider_name` to `lm-studio`)|\n| openai    | Yes     |  Use openai compatible API (eg: llama.cpp server)  |\n\nNext step: [Start services and run AgenticSeek](#Start-services-and-Run)  \n\n*See the **Known issues** section if you are having issues*\n\n*See the **Run with an API** section if your hardware can't run deepseek locally*\n\n*See the **Config** section for detailled config file explanation.*\n\n---\n\n## Setup to run with an API\n\nSet the desired provider in the `config.ini`. See below for a list of API providers.\n\n```sh\n[MAIN]\nis_local = False\nprovider_name = google\nprovider_model = gemini-2.0-flash\nprovider_server_address = 127.0.0.1:5000 # doesn't matter\n```\nWarning: Make sure there is not trailing space in the config.\n\nExport your API key: `export <<PROVIDER>>_API_KEY=\"xxx\"`\n\nExample: export `TOGETHER_API_KEY=\"xxxxx\"`\n\n**List of API providers**\n  \n| Provider  | Local? | Description                                               |\n|-----------|--------|-----------------------------------------------------------|\n| openai    | Depends  | Use ChatGPT API  |\n| deepseek  | No     | Deepseek API (non-private)                            |\n| huggingface| No    | Hugging-Face API (non-private)                            |\n| togetherAI | No    | Use together AI API (non-private)                         |\n| google | No    | Use google gemini API (non-private)                         |\n\n*We advice against using gpt-4o or other closedAI models*, performance are poor for web browsing and task planning.\n\nPlease also note that coding/bash might fail with gemini, it seem to ignore our prompt for format to respect, which are optimized for deepseek r1.\n\nNext step: [Start services and run AgenticSeek](#Start-services-and-Run)\n\n*See the **Known issues** section if you are having issues*\n\n*See the **Config** section for detailled config file explanation.*\n\n---\n\n## Start services and Run\n\nActivate your python env if needed.\n```sh\nsource agentic_seek_env/bin/activate\n```\n\nStart required services. This will start all services from the docker-compose.yml, including:\n    - searxng\n    - redis (required by searxng)\n    - frontend\n\n```sh\nsudo ./start_services.sh # MacOS\nstart ./start_services.cmd # Window\n```\n\n**Options 1:** Run with the CLI interface.\n\n```sh\npython3 cli.py\n```\n\nWe advice you set `headless_browser` to False in the config.ini for CLI mode.\n\n**Options 2:** Run with the Web interface.\n\nStart the backend.\n\n```sh\npython3 api.py\n```\n\nGo to `http://localhost:3000/` and you should see the web interface.\n\n---\n\n## Usage\n\nMake sure the services are up and running with `./start_services.sh` and run the AgenticSeek with `python3 cli.py` for CLI mode or `python3 api.py` then go to `localhost:3000` for web interface.\n\nYou can also use speech to text by setting `listen = True` in the config. Only for CLI mode.\n\nTo exit, simply say/type `goodbye`.\n\nHere are some example usage:\n\n> *Make a snake game in python!*\n\n> *Search the web for top cafes in Rennes, France, and save a list of three with their addresses in rennes_cafes.txt.*\n\n> *Write a Go program to calculate the factorial of a number, save it as factorial.go in your workspace*\n\n> *Search my summer_pictures folder for all JPG files, rename them with today’s date, and save a list of renamed files in photos_list.txt*\n\n> *Search online for popular sci-fi movies from 2024 and pick three to watch tonight. Save the list in movie_night.txt.*\n\n> *Search the web for the latest AI news articles from 2025, select three, and write a Python script to scrape their titles and summaries. Save the script as news_scraper.py and the summaries in ai_news.txt in /home/projects*\n\n> *Friday, search the web for a free stock price API, register with supersuper7434567@gmail.com then write a Python script to fetch using the API daily prices for Tesla, and save the results in stock_prices.csv*\n\n*Note that form filling capabilities are still experimental and might fail.*\n\n\n\nAfter you type your query, AgenticSeek will allocate the best agent for the task.\n\nBecause this is an early prototype, the agent routing system might not always allocate the right agent based on your query.\n\nTherefore, you should be very explicit in what you want and how the AI might proceed for example if you want it to conduct a web search, do not say:\n\n`Do you know some good countries for solo-travel?`\n\nInstead, ask:\n\n`Do a web search and find out which are the best country for solo-travel`\n\n---\n\n## **Setup to run the LLM on your own server**  \n\nIf you have a powerful computer or a server that you can use, but you want to use it from your laptop you have the options to run the LLM on a remote server using our custom llm server. \n\nOn your \"server\" that will run the AI model, get the ip address\n\n```sh\nip a | grep \"inet \" | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # local ip\ncurl https://ipinfo.io/ip # public ip\n```\n\nNote: For Windows or macOS, use ipconfig or ifconfig respectively to find the IP address.\n\nClone the repository and enter the `server/`folder.\n\n\n```sh\ngit clone --depth 1 https://github.com/Fosowl/agenticSeek.git\ncd agenticSeek/llm_server/\n```\n\nInstall server specific requirements:\n\n```sh\npip3 install -r requirements.txt\n```\n\nRun the server script.\n\n```sh\npython3 app.py --provider ollama --port 3333\n```\n\nYou have the choice between using `ollama` and `llamacpp` as a LLM service.\n\n\nNow on your personal computer:\n\nChange the `config.ini` file to set the `provider_name` to `server` and `provider_model` to `deepseek-r1:xxb`.\nSet the `provider_server_address` to the ip address of the machine that will run the model.\n\n```sh\n[MAIN]\nis_local = False\nprovider_name = server\nprovider_model = deepseek-r1:70b\nprovider_server_address = x.x.x.x:3333\n```\n\n\nNext step: [Start services and run AgenticSeek](#Start-services-and-Run)  \n\n---\n\n## Speech to Text\n\nPlease note that currently speech to text only work in english.\n\nThe speech-to-text functionality is disabled by default. To enable it, set the listen option to True in the config.ini file:\n\n```\nlisten = True\n```\n\nWhen enabled, the speech-to-text feature listens for a trigger keyword, which is the agent's name, before it begins processing your input. You can customize the agent's name by updating the `agent_name` value in the *config.ini* file:\n\n```\nagent_name = Friday\n```\n\nFor optimal recognition, we recommend using a common English name like \"John\" or \"Emma\" as the agent name\n\nOnce you see the transcript start to appear, say the agent's name aloud to wake it up (e.g., \"Friday\").\n\nSpeak your query clearly.\n\nEnd your request with a confirmation phrase to signal the system to proceed. Examples of confirmation phrases include:\n```\n\"do it\", \"go ahead\", \"execute\", \"run\", \"start\", \"thanks\", \"would ya\", \"please\", \"okay?\", \"proceed\", \"continue\", \"go on\", \"do that\", \"go it\", \"do you understand?\"\n```\n\n## Config\n\nExample config:\n```\n[MAIN]\nis_local = True\nprovider_name = ollama\nprovider_model = deepseek-r1:32b\nprovider_server_address = 127.0.0.1:11434\nagent_name = Friday\nrecover_last_session = False\nsave_session = False\nspeak = False\nlisten = False\nwork_dir =  /Users/mlg/Documents/ai_folder\njarvis_personality = False\nlanguages = en zh\n[BROWSER]\nheadless_browser = False\nstealth_mode = False\n```\n\n**Explanation**:\n\n- is_local -> Runs the agent locally (True) or on a remote server (False).\n\n- provider_name -> The provider to use (one of: `ollama`, `server`, `lm-studio`, `deepseek-api`)\n\n- provider_model -> The model used, e.g., deepseek-r1:32b.\n\n- provider_server_address -> Server address, e.g., 127.0.0.1:11434 for local. Set to anything for non-local API.\n\n- agent_name -> Name of the agent, e.g., Friday. Used as a trigger word for TTS.\n\n- recover_last_session -> Restarts from last session (True) or not (False).\n\n- save_session -> Saves session data (True) or not (False).\n\n- speak -> Enables voice output (True) or not (False).\n\n- listen -> listen to voice input (True) or not (False).\n\n- work_dir -> Folder the AI will have access to. eg: /Users/user/Documents/.\n\n- jarvis_personality -> Uses a JARVIS-like personality (True) or not (False). This simply change the prompt file.\n\n- languages -> The list of supported language, needed for the llm router to work properly, avoid putting too many or too similar languages.\n\n- headless_browser -> Runs browser without a visible window (True) or not (False).\n\n- stealth_mode -> Make bot detector time harder. Only downside is you have to manually install the anticaptcha extension.\n\n- languages -> List of supported languages. Required for agent routing system. The longer the languages list the more model will be downloaded.\n\n## Providers\n\nThe table below show the available providers:\n\n| Provider  | Local? | Description                                               |\n|-----------|--------|-----------------------------------------------------------|\n| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |\n| server    | Yes    | Host the model on another machine, run your local machine |\n| lm-studio  | Yes    | Run LLM locally with LM studio (`lm-studio`)             |\n| openai    | Depends  | Use ChatGPT API (non-private) or openai compatible API  |\n| deepseek-api  | No     | Deepseek API (non-private)                            |\n| huggingface| No    | Hugging-Face API (non-private)                            |\n| togetherAI | No    | Use together AI API (non-private)                         |\n| google | No    | Use google gemini API (non-private)                         |\n\nTo select a provider change the config.ini:\n\n```\nis_local = True\nprovider_name = ollama\nprovider_model = deepseek-r1:32b\nprovider_server_address = 127.0.0.1:5000\n```\n`is_local`: should be True for any locally running LLM, otherwise False.\n\n`provider_name`: Select the provider to use by it's name, see the provider list above.\n\n`provider_model`: Set the model to use by the agent.\n\n`provider_server_address`: can be set to anything if you are not using the server provider.\n\n# Known issues\n\n## Chromedriver Issues\n\n**Known error #1:** *chromedriver mismatch*\n\n`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113\nCurrent browser version is 134.0.6998.89 with binary path`\n\nThis happen if there is a mismatch between your browser and chromedriver version.\n\nYou need to navigate to download the latest version:\n\nhttps://developer.chrome.com/docs/chromedriver/downloads\n\nIf you're using Chrome version 115 or newer go to:\n\nhttps://googlechromelabs.github.io/chrome-for-testing/\n\nAnd download the chromedriver version matching your OS.\n\n![alt text](./media/chromedriver_readme.png)\n\nIf this section is incomplete please raise an issue.\n\n##  connection adapters Issues\n\n```\nException: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:11434/v1/chat/completions'\n```\n\nMake sure you have `http://` in front of the provider IP address :\n\n`provider_server_address = http://127.0.0.1:11434`\n\n## SearxNG base URL must be provided\n\n```\nraise ValueError(\"SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.\")\nValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.\n```\n\nMaybe you didn't move `.env.example` as `.env` ? You can also export SEARXNG_BASE_URL:\n\n`export  SEARXNG_BASE_URL=\"http://127.0.0.1:8080\"`\n\n## FAQ\n\n**Q: What hardware do I need?**  \n\n| Model Size  | GPU  | Comment                                               |\n|-----------|--------|-----------------------------------------------------------|\n| 7B        | 8GB Vram | ⚠️ Not recommended. Performance is poor, frequent hallucinations, and planner agents will likely fail. |\n| 14B        | 12 GB VRAM (e.g. RTX 3060) | ✅ Usable for simple tasks. May struggle with web browsing and planning tasks. |\n| 32B        | 24+ GB VRAM (e.g. RTX 4090) | 🚀 Success with most tasks, might still struggle with task planning |\n| 70B+        | 48+ GB Vram (eg. mac studio) | 💪 Excellent. Recommended for advanced use cases. |\n\n**Q: Why Deepseek R1 over other models?**  \n\nDeepseek R1 excels at reasoning and tool use for its size. We think it’s a solid fit for our needs other models work fine, but Deepseek is our primary pick.\n\n**Q: I get an error running `cli.py`. What do I do?**  \n\nEnsure local is running (`ollama serve`), your `config.ini` matches your provider, and dependencies are installed. If none work feel free to raise an issue.\n\n**Q: Can it really run 100% locally?**  \n\nYes with Ollama, lm-studio or server providers, all speech to text, LLM and text to speech model run locally. Non-local options (OpenAI or others API) are optional.\n\n**Q: Why should I use AgenticSeek when I have Manus?**\n\nThis started as Side-Project we did out of interest about AI agents. What’s special about it is that we want to use local model and avoid APIs.\nWe draw inspiration from Jarvis and Friday (Iron man movies) to make it \"cool\" but for functionnality we take more inspiration from Manus, because that's what people want in the first place: a local manus alternative.\nUnlike Manus, AgenticSeek prioritizes independence from external systems, giving you more control, privacy and avoid api cost.\n\n## Contribute\n\nWe’re looking for developers to improve AgenticSeek! Check out open issues or discussion.\n\n[Contribution guide](./docs/CONTRIBUTING.md)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)\n\n## Maintainers:\n\n > [Fosowl](https://github.com/Fosowl) | Paris Time | (Sometime busy)\n\n > [https://github.com/antoineVIVIES](antoineVIVIES) | Taipei Time | (Often busy)\n\n > [steveh8758](https://github.com/steveh8758) | Taipei Time | (Always busy)",
    "summary": "AgenticSeek是一个完全本地化、注重隐私的AI助手，旨在作为Manus AI的替代方案。它无需云端依赖，所有数据和处理均在用户设备上进行，确保完全隐私。该助手具备自主网页浏览、代码编写、复杂任务规划与执行以及语音交互能力，支持多种本地LLM提供商，是构建私密AI智能体的理想选择。",
    "keywords": [
      "本地化AI",
      "隐私保护",
      "智能体",
      "大语言模型",
      "自主浏览",
      "代码生成",
      "任务规划",
      "语音交互"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-05-25T13:18:57Z",
    "download_time": "2024-07-28 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png"
    ],
    "extra_info": null
  },
  {
    "id": "telegram-search",
    "source": "GitHub",
    "url": "https://github.com/groupultra/telegram-search",
    "title": "Telegram Search",
    "content": "# Telegram Search\n\n[English](./README_EN.md) | [快速开始](./getting-started.md)\n\n[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white)](https://t.me/+Gs3SH2qAPeFhYmU9)\n[![Discord](https://dcbadge.limes.pink/api/server/NzYsmJSgCT)](https://discord.gg/NzYsmJSgCT)\n\n<a href=\"https://trendshift.io/repositories/13868\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/13868\" alt=\"groupultra%2Ftelegram-search | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n一个功能强大的 Telegram 聊天记录搜索工具，支持向量搜索和语义匹配。基于 OpenAI 的语义向量技术，让你的 Telegram 消息检索更智能、更精准。\n\n- 欢迎 PR 贡献！\n- 由于项目处于快速迭代阶段，可能会出现数据库不兼容的情况，建议定期备份数据。\n\n![preview](./docs/assets/preview.png)\n\n## 💖 赞助者\n\n![Sponsors](https://github.com/luoling8192/luoling8192/raw/master/sponsorkit/sponsors.svg)\n\n## 🚀 快速开始\n\n### 安装步骤\n\n1. 克隆仓库：\n\n```bash\ngit clone https://github.com/GramSearch/telegram-search.git\ncd telegram-search\n```\n\n2. 安装依赖：\n\n```bash\npnpm install\n```\n\n3. 配置环境：\n\n```bash\ncp config/config.example.yaml config/config.yaml\n```\n\n4. 启动数据库容器：\n\n```bash\ndocker compose up -d\n```\n\n5. 同步数据库表结构：\n\n```bash\npnpm run db:migrate\n```\n\n6. 启动服务：\n\n```bash\n# 启动后端服务\npnpm run dev:server\n\n# 启动前端界面\npnpm run dev:frontend\n```\n\n访问 `http://localhost:3333` 即可打开搜索界面。\n\n## 🚀 Activity\n\n[![Star History Chart](https://api.star-history.com/svg?repos=luoling8192/telegram-search&type=Date)](https://star-history.com/#luoling8192/telegram-search&Date)\n\n![Alt](https://repobeats.axiom.co/api/embed/c0fe5f057a33ce830a632c6ae421433f50e9083f.svg \"Repobeats analytics image\")\n\n## 📝 License\n\nMIT License © 2025",
    "summary": "Telegram Search是一款功能强大的Telegram聊天记录搜索工具，核心特点是支持向量搜索和语义匹配。该工具基于OpenAI的语义向量技术，旨在提供更智能、更精准的Telegram消息检索体验。项目处于快速迭代阶段，建议用户定期备份数据。",
    "keywords": [
      "Telegram搜索",
      "聊天记录搜索",
      "向量搜索",
      "语义匹配",
      "OpenAI",
      "自然语言处理"
    ],
    "area": [
      "人工智能",
      "自然语言处理",
      "机器学习"
    ],
    "published_time": "2025-05-26T14:46:33+00:00",
    "download_time": "2024-05-27 10:00:00",
    "visual_resource": [
      "https://github.com/groupultra/telegram-search/raw/main/docs/assets/preview.png"
    ],
    "extra_info": null
  },
  {
    "id": "qlib",
    "source": "GitHub",
    "url": "https://github.com/microsoft/qlib",
    "title": "Qlib",
    "content": "[!PyPI - Python Version](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)\n[!PyPI - Supported OS](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)\n[!PyPI - Version](https://img.shields.io/pypi/v/pyqlib)\n[!Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)\n[!Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)\n[!Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)\n[!PyPI - License](https://img.shields.io/pypi/l/pyqlib)\n[!Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)\n\n## :newspaper: **What's NEW!** \n\nRecent released features\n\n### Introducing <a href=\"https://github.com/microsoft/RD-Agent\"><img src=\"docs/_static/img/rdagent_logo.png\" alt=\"RD_Agent\" style=\"height: 2em\"></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your star🌟!\n\nTo learn more, please visit our [♾️Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (中文) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | 📈Coming soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |\n| 🔥LLM-driven Auto Quant Factory🔥 | 🚀 Released in [♾️RD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |\n| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |\n| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |\n| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|\n| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |\n| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | 📖 [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | \n| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |\n| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |\n| Arctic Provider Backend & Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |\n| Meta-Learning-based framework & DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | \n| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | \n| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |\n| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |\n| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |\n| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |\n| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |\n| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |\n| Transformer & Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |\n| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |\n| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |\n| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | \n| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | \n| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |\n| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | \n| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |\n| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |\n\nFeatures released before 2021 are not listed here.\n\n<p align=\"center\">\n  <img src=\"docs/_static/img/logo/1.png\" />\n</p>\n\nQlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.\n\nAn increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.\n\nIt contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. \nFor more details, please refer to our paper [\"Qlib: An AI-oriented Quantitative Investment Platform\"](https://arxiv.org/abs/2009.11189).\n\n\n<table>\n  <tbody>\n    <tr>\n      <th>Frameworks, Tutorial, Data & DevOps</th>\n      <th>Main Challenges & Solutions in Quant Research</th>\n    </tr>\n    <tr>\n      <td>\n        <li><a href=\"#plans\">**Plans**</a></li>\n        <li><a href=\"#framework-of-qlib\">Framework of Qlib</a></li>\n        <li><a href=\"#quick-start\">Quick Start</a></li>\n          <ul dir=\"auto\">\n            <li type=\"circle\"><a href=\"#installation\">Installation</a> </li>\n            <li type=\"circle\"><a href=\"#data-preparation\">Data Preparation</a></li>\n            <li type=\"circle\"><a href=\"#auto-quant-research-workflow\">Auto Quant Research Workflow</a></li>\n            <li type=\"circle\"><a href=\"#building-customized-quant-research-workflow-by-code\">Building Customized Quant Research Workflow by Code</a></li></ul>\n        <li><a href=\"#quant-dataset-zoo\">**Quant Dataset Zoo**</a></li>\n        <li><a href=\"#learning-framework\">Learning Framework</a></li>\n        <li><a href=\"#more-about-qlib\">More About Qlib</a></li>\n        <li><a href=\"#offline-mode-and-online-mode\">Offline Mode and Online Mode</a>\n        <ul>\n          <li type=\"circle\"><a href=\"#performance-of-qlib-data-server\">Performance of Qlib Data Server</a></li></ul>\n        <li><a href=\"#related-reports\">Related Reports</a></li>\n        <li><a href=\"#contact-us\">Contact Us</a></li>\n        <li><a href=\"#contributing\">Contributing</a></li>\n      </td>\n      <td valign=\"baseline\">\n        <li><a href=\"#main-challenges--solutions-in-quant-research\">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type=\"circle\"><a href=\"#forecasting-finding-valuable-signalspatterns\">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type=\"disc\"><a href=\"#quant-model-paper-zoo\">**Quant Model (Paper) Zoo**</a>\n                  <ul>\n                    <li type=\"circle\"><a href=\"#run-a-single-model\">Run a Single Model</a></li>\n                    <li type=\"circle\"><a href=\"#run-multiple-models\">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type=\"circle\"><a href=\"#adapting-to-market-dynamics\">Adapting to Market Dynamics</a></li>\n          <li type=\"circle\"><a href=\"#reinforcement-learning-modeling-continuous-decisions\">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style=\"align: center\">\n<img src=\"docs/_static/img/framework-abstract.jpg\" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib's design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.\n\n\n# Quick Start\n\nThis quick start guide tries to demonstrate\n1. It's very easy to build a complete Quant research workflow and try your ideas with _Qlib_.\n2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.\n\nHere is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).\n\n\n## Installation\n\nThis table demonstrates the supported Python version of `Qlib`:\n|               | install with pip      | install from source  |        plot        |\n| ------------- |:---------------------:|:--------------------:|:------------------:|\n| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n\n**Note**: \n1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.\n2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`'s Python to install ``Qlib`` from source.\n\n### Install with pip\nUsers can easily install ``Qlib`` by pip according to the following command.\n\n```bash\n  pip install pyqlib\n```\n\n**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.\n\n### Install from source\nAlso, users can install the latest dev version ``Qlib`` by the source code according to the following steps:\n\n* Before installing ``Qlib`` from source, users need to install some dependencies:\n\n  ```bash\n  pip install numpy\n  pip install --upgrade cython\n  ```\n\n* Clone the repository and install ``Qlib`` as follows.\n    ```bash\n    git clone https://github.com/microsoft/qlib.git && cd qlib\n    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst\n    ```\n\n**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.\n\n**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. \n\n## Data Preparation\n❗ Due to more restrict data security policy. The offical dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  > \n  > **NOTE**: Users can't incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the \"qlib\" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)\n\n### Checking the health of the data\n  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data\n    ```\n  * Of course, you can also add some parameters to adjust the test results, such as this.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20\n    ```\n  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).\n\n<!-- \n- Run the initialization code and get stock data:\n\n  ```python\n  import qlib\n  from qlib.data import D\n  from qlib.constant import REG_CN\n\n  # Initialization\n  mount_path = \"~/.qlib/qlib_data/cn_data\"  # target_dir\n  qlib.init(mount_path=mount_path, region=REG_CN)\n\n  # Get stock data by Qlib\n  # Load trading calendar with the given time range and frequency\n  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])\n\n  # Parse a given market name into a stockpool config\n  instruments = D.instruments('csi500')\n  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])\n\n  # Load features of certain instruments in given time range\n  instruments = ['SH600000']\n  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']\n  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())\n  ```\n -->\n\n## Docker images\n1. Pulling a docker image from a docker hub repository\n    ```bash\n    docker pull pyqlib/qlib_image_stable:stable\n    ```\n2. Start a new Docker container\n    ```bash\n    docker run -it --name <container name> -v <Mounted local directory>:/app qlib_image_stable\n    ```\n3. At this point you are in the docker environment and can run the qlib scripts. An example:\n    ```bash\n    >>> python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n    >>> python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n4. Exit the container\n    ```bash\n    >>> exit\n    ```\n5. Restart the container\n    ```bash\n    docker start -i -a <container name>\n    ```\n6. Stop the container\n    ```bash\n    docker stop <container name>\n    ```\n7. Delete the container\n    ```bash\n    docker rm <container name>\n    ```\n8. If you want to know more information, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html).\n\n## Auto Quant Research Workflow\nQlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: \n\n1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.\n    ```bash\n      cd examples  # Avoid running program under the directory contains `qlib`\n      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    If users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results\n\n## Building Customized Quant Research Workflow by Code\nThe automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. [Here](examples/workflow_by_code.ipynb) is a demo for customized Quant research workflow by code.\n\n# Main Challenges & Solutions in Quant Research\nQuant investment is a very unique scenario with lots of key challenges to be solved.\nCurrently, Qlib provides some solutions for several of them.\n\n## Forecasting: Finding Valuable Signals/Patterns\nAccurate forecasting of the stock price trend is a very important part to construct profitable portfolios.\nHowever, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.\n\nAn increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in `Qlib`\n\n\n### [Quant Model (Paper) Zoo](examples/benchmarks)\n\nHere is a list of models built on `Qlib`.\n- [GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)](examples/benchmarks/XGBoost/)\n- [GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)](examples/benchmarks/LightGBM/)\n- [GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)](examples/benchmarks/CatBoost/)\n- [MLP based on pytorch](examples/benchmarks/MLP/)\n- [LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)](examples/benchmarks/LSTM/)\n- [GRU based on pytorch (Kyunghyun Cho, et al. 2014)](examples/benchmarks/GRU/)\n- [ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)](examples/benchmarks/ALSTM)\n- [GATs based on pytorch (Petar Velickovic, et al. 2017)](examples/benchmarks/GATs/)\n- [SFM based on pytorch (Liheng Zhang, et al. KDD 2017)](examples/benchmarks/SFM/)\n- [TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)](examples/benchmarks/TFT/)\n- [TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)](examples/benchmarks/TabNet/)\n- [DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)](examples/benchmarks/DoubleEnsemble/)\n- [TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)](examples/benchmarks/TCTS/)\n- [Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)](examples/benchmarks/Transformer/)\n- [Localformer based on pytorch (Juyong Jiang, et al.)](examples/benchmarks/Localformer/)\n- [TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)](examples/benchmarks/TRA/)\n- [TCN based on pytorch (Shaojie Bai, et al. 2018)](examples/benchmarks/TCN/)\n- [ADARNN based on pytorch (YunTao Du, et al. 2021)](examples/benchmarks/ADARNN/)\n- [ADD based on pytorch (Hongshun Tang, et al.2020)](examples/benchmarks/ADD/)\n- [IGMTF based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/IGMTF/)\n- [HIST based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/HIST/)\n- [KRNN based on pytorch](examples/benchmarks/KRNN/)\n- [Sandwich based on pytorch](examples/benchmarks/Sandwich/)\n\nYour PR of new Quant models is highly welcomed.\n\nThe performance of each model on the `Alpha158` and `Alpha360` datasets can be found [here](examples/benchmarks/README.md).\n\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)\n\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: \"An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization\", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: \"Universal Trading for Order Execution with Oracle Policy Distillation\", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib's RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It's worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).\n\n\n\n# Offline Mode and Online Mode\nThe data server of Qlib can either deployed as `Offline` mode or `Online` mode. The default mode is offline mode.\n\nUnder `Offline` mode, the data will be deployed locally. \n\nUnder `Online` mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in [Qlib-Server](https://qlib-server.readthedocs.io/). The online mode can be deployed automatically with [Azure CLI based scripts](https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure). The source code of online data server can be found in [Qlib-Server repository](https://github.com/microsoft/qlib-server).\n\n## Performance of Qlib Data Server\nThe performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we\ncompare it with several other data storage solutions. \n\nWe evaluate the performance of several storage solutions by finishing the same task,\nwhich creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.\n\n|                         | HDF5      | MySQL     | MongoDB   | InfluxDB  | Qlib -E -D  | Qlib +E -D   | Qlib +E +D  |\n| --                      | ------    | ------    | --------  | --------- | ----------- | ------------ | ----------- |\n| Total (1CPU) (seconds)  | 184.4±3.7 | 365.3±7.5 | 253.6±6.7 | 368.2±3.6 | 147.0±8.8   | 47.6±1.0     | **7.4±0.3** |\n| Total (64CPU) (seconds) |           |           |           |           | 8.8±0.6     | **4.2±0.2**  |             |\n* `+(-)E` indicates with (out) `ExpressionCache`\n* `+(-)D` indicates with (out) `DatasetCache`\n\nMost general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions.\nSuch overheads greatly slow down the data loading process.\nQlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.\n\n# Related Reports\n- [Guide To Qlib: Microsoft’s AI Investment Platform](https://analyticsindiamag.com/qlib/)\n- [微软也搞AI量化平台？还是开源的！](https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ)\n- [微矿Qlib：业内首个AI量化投资开源平台](https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ)\n\n# Contact Us\n- If you have any issues, please create issue [here](https://github.com/microsoft/qlib/issues/new/choose) or send messages in [gitter](https://gitter.im/Microsoft/qlib).\n- If you want to make contributions to `Qlib`, please [create pull requests](https://github.com/microsoft/qlib/compare). \n- For other reasons, you are welcome to contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).\n  - We are recruiting new members(both FTEs and interns), your resumes are welcome!\n\nJoin IM discussion groups:\n|[Gitter](https://gitter.im/Microsoft/qlib)|\n|----|\n|![image](https://github.com/microsoft/qlib/blob/main/docs/_static/img/qrcode/gitter_qr.png)|\n\n# Contributing\nWe appreciate all contributions and thank all the contributors!\n<a href=\"https://github.com/microsoft/qlib/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=microsoft/qlib\" /></a>\n\nBefore we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and [Dong Zhou](https://github.com/evanzd/evanzd). Especially thanks to [Dong Zhou](https://github.com/evanzd/evanzd) due to his initial version of Qlib.\n\n## Guidance\n\nThis project welcomes contributions and suggestions.  \n**Here are some \n[code standards and development guidance](docs/developer/code_standard_and_dev_guide.rst) for submiting a pull request.**\n\nMaking contributions is not a hard thing. Solving an issue(maybe just answering a question raised in [issues list](https://github.com/microsoft/qlib/issues) or [gitter](https://gitter.im/Microsoft/qlib)), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.\n\nFor example, if you want to contribute to Qlib's document/code, you can follow the steps in the figure below.\n<p align=\"center\">\n  <img src=\"https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif\" />\n</p>\n\nIf you don't know how to start to contribute, you can refer to the following examples.\n| Type | Examples |\n| -- | -- |\n| Solving issues | [Answer a question](https://github.com/microsoft/qlib/issues/749);  [issuing](https://github.com/microsoft/qlib/issues/765) or [fixing](https://github.com/microsoft/qlib/pull/792) a bug |\n| Docs | [Improve docs quality](https://github.com/microsoft/qlib/pull/797/files) ;  [Fix a typo](https://github.com/microsoft/qlib/pull/774) | \n| Feature |  Implement a [requested feature](https://github.com/microsoft/qlib/projects) like [this](https://github.com/microsoft/qlib/pull/754); [Refactor interfaces](https://github.com/microsoft/qlib/pull/539/files) |\n| Dataset | [Add a dataset](https://github.com/microsoft/qlib/pull/733) | \n| Models |  [Implement a new model](https://github.com/microsoft/qlib/pull/689), [some instructions to contribute models](https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing) |\n\n[Good first issues](https://github.com/microsoft/qlib/labels/good%20first%20issue) are labelled to indicate that they are easy to start your contributions.\n\nYou can find some impefect implementation in Qlib by  `rg 'TODO|FIXME' qlib`\n \nIf you would like to become one of Qlib's maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).  We are glad to help to upgrade your permission.\n\n## Licence\nMost contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe right to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n",
    "summary": "Qlib是由微软开源的面向AI的量化投资平台，旨在利用人工智能技术赋能量化投资研究与生产。平台提供了完整的数据处理、模型训练、回测等机器学习流程，覆盖alpha挖掘、风险建模、投资组合优化和订单执行等量化投资全链条。Qlib支持监督学习、市场动态建模、强化学习等多种机器学习范式，并提供高效的数据服务。近期，平台引入了基于大型语言模型的自主进化智能体RD-Agent，用于自动化因子挖掘和模型优化。",
    "keywords": [
      "量化投资",
      "人工智能",
      "机器学习",
      "深度学习",
      "强化学习",
      "数据处理",
      "智能体",
      "回测"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "智能体"
    ],
    "published_time": "2025-05-26T14:08:43+00:00",
    "download_time": "2024-05-27 08:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/analysis/report.png",
      "https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/analysis/analysis_model_cumulative_return.png"
    ],
    "extra_info": null
  },
  {
    "id": "llm-course",
    "source": "GitHub",
    "url": "https://github.com/mlabonne/llm-course",
    "title": "LLM Course",
    "content": "<div align=\"center\">\n<img src=\"img/banner.png\" alt=\"LLM Course\">\n  <p align=\"center\">\n    𝕏 <a href=\"https://twitter.com/maximelabonne\">Follow me on X</a> • \n    🤗 <a href=\"https://huggingface.co/mlabonne\">Hugging Face</a> • \n    💻 <a href=\"https://mlabonne.github.io/blog\">Blog</a> • \n    📙 <a href=\"https://packt.link/a/9781836200079\">LLM Engineer's Handbook</a>\n  </p>\n</div>\n<br/>\n\n<a href=\"https://a.co/d/a2M67rE\"><img align=\"right\" width=\"25%\" src=\"https://i.imgur.com/7iNjEq2.png\" alt=\"LLM Engineer's Handbook Cover\"/></a>The LLM course is divided into three parts:\n\n1. 🧩 **LLM Fundamentals** is optional and covers fundamental knowledge about mathematics, Python, and neural networks.\n2. 🧑‍🔬 **The LLM Scientist** focuses on building the best possible LLMs using the latest techniques.\n3. 👷 **The LLM Engineer** focuses on creating LLM-based applications and deploying them.\n\n> [!NOTE]\n> Based on this course, I wrote the [LLM Engineer's Handbook](https://packt.link/a/9781836200079) with Paul Iuzstin. It's a hands-on and detailed book that covers an end-to-end LLM application from design to deployment. The LLM course will always stay free but feel free to support my work by purchasing the book.\n\nFor an interactive version of this course, I created an LLM assistant that will answer questions and test your knowledge in a personalized way on [**HuggingChat**](https://hf.co/chat/assistant/66029d2e5f4a884f7aabc9d1) or [**ChatGPT**](https://chat.openai.com/g/g-yviLuLqvI-llm-course).\n\n## 📝 Notebooks\n\nA list of notebooks and articles I wrote about LLMs.\n\n### Tools\n\n| Notebook | Description | Notebook |\n|----------|-------------|----------|\n| 🧐 [LLM AutoEval](https://github.com/mlabonne/llm-autoeval) | Automatically evaluate your LLMs using RunPod | <a href=\"https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| 🥱 LazyMergekit | Easily merge models using MergeKit in one click. | <a href=\"https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| 🦎 LazyAxolotl | Fine-tune models in the cloud using Axolotl in one click. | <a href=\"https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| ⚡ AutoQuant | Quantize LLMs in GGUF, GPTQ, EXL2, AWQ, and HQQ formats in one click. | <a href=\"https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| 🌳 Model Family Tree | Visualize the family tree of merged models. | <a href=\"https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| 🚀 ZeroSpace | Automatically create a Gradio chat interface using a free ZeroGPU. | <a href=\"https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n\n### Fine-tuning\n\n| Notebook | Description | Article | Notebook |\n|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Fine-tune Llama 3.1 with Unsloth | Ultra-efficient supervised fine-tuning in Google Colab. | [Article](https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html) | <a href=\"https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune Llama 3 with ORPO | Cheaper and faster fine-tuning in a single stage with ORPO. | [Article](https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html) | <a href=\"https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune Mistral-7b with DPO | Boost the performance of supervised fine-tuned models with DPO. | [Article](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html) | <a href=\"https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune Mistral-7b with QLoRA | Supervised fine-tune Mistral-7b in a free-tier Google Colab with TRL. |  | <a href=\"https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune CodeLlama using Axolotl | End-to-end guide to the state-of-the-art tool for fine-tuning. | [Article](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html) | <a href=\"https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune Llama 2 with QLoRA | Step-by-step guide to supervised fine-tune Llama 2 in Google Colab. | [Article](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) | <a href=\"https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n\n### Quantization\n\n| Notebook | Description | Article | Notebook |\n|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Introduction to Quantization | Large language model optimization using 8-bit quantization. | [Article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) | <a href=\"https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| 4-bit Quantization using GPTQ | Quantize your own open-source LLMs to run them on consumer hardware. | [Article](https://mlabonne.github.io/blog/4bit_quantization/) | <a href=\"https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Quantization with GGUF and llama.cpp | Quantize Llama 2 models with llama.cpp and upload GGUF versions to the HF Hub. | [Article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | <a href=\"https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| ExLlamaV2: The Fastest Library to Run\nLLMs | Quantize and run EXL2\nmodels and upload them to the HF Hub. | [Article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) | <a href=\"https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n\n### Other\n\n| Notebook | Description | Article | Notebook |\n|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Merge LLMs with MergeKit | Create your own models easily, no GPU required! | [Article](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit%20copy.html) | <a href=\"https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Create MoEs with MergeKit | Combine multiple experts into a single frankenMoE | [Article](https://mlabonne.github.io/blog/posts/2024-03-28_Create_Mixture_of_Experts_with_MergeKit.html) | <a href=\"https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Uncensor any LLM with abliteration | Fine-tuning without retraining | [Article](https://mlabonne.github.io/blog/posts/2024-06-04_Uncensor_any_LLM_with_abliteration.html) | <a href=\"https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Improve ChatGPT with Knowledge Graphs | Augment ChatGPT's answers with knowledge graphs. | [Article](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html) | <a href=\"https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Decoding Strategies in Large Language Models | A guide to text generation from beam search to nucleus sampling | [Article](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html) | <a href=\"https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n\n## 🧩 LLM Fundamentals\n\nThis section introduces essential knowledge about mathematics, Python, and neural networks. You might not want to start here but refer to it as needed.\n\n<details>\n<summary>Toggle section (optional)</summary>\n  \n![](img/roadmap_fundamentals.png)\n\n### 1. Mathematics for Machine Learning\n\nBefore mastering machine learning, it is important to understand the fundamental mathematical concepts that power these algorithms.\n\n- **Linear Algebra**: This is crucial for understanding many algorithms, especially those used in deep learning. Key concepts include vectors, matrices, determinants, eigenvalues and eigenvectors, vector spaces, and linear transformations.\n- **Calculus**: Many machine learning algorithms involve the optimization of continuous functions, which requires an understanding of derivatives, integrals, limits, and series. Multivariable calculus and the concept of gradients are also important.\n- **Probability and Statistics**: These are crucial for understanding how models learn from data and make predictions. Key concepts include probability theory, random variables, probability distributions, expectations, variance, covariance, correlation, hypothesis testing, confidence intervals, maximum likelihood estimation, and Bayesian inference.\n\n📚 Resources:\n\n- [3Blue1Brown - The Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab): Series of videos that give a geometric intuition to these concepts.\n- [StatQuest with Josh Starmer - Statistics Fundamentals](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9): Offers simple and clear explanations for many statistical concepts.\n- [AP Statistics Intuition by Ms Aerin](https://automata88.medium.com/list/cacc224d5e7d): List of Medium articles that provide the intuition behind every probability distribution.\n- [Immersive Linear Algebra](https://immersivemath.com/ila/learnmore.html): Another visual interpretation of linear algebra.\n- [Khan Academy - Linear Algebra](https://www.khanacademy.org/math/linear-algebra): Great for beginners as it explains the concepts in a very intuitive way.\n- [Khan Academy - Calculus](https://www.khanacademy.org/math/calculus-1): An interactive course that covers all the basics of calculus.\n- [Khan Academy - Probability and Statistics](https://www.khanacademy.org/math/statistics-probability): Delivers the material in an easy-to-understand format.\n\n---\n\n### 2. Python for Machine Learning\n\nPython is a powerful and flexible programming language that's particularly good for machine learning, thanks to its readability, consistency, and robust ecosystem of data science libraries.\n\n- **Python Basics**: Python programming requires a good understanding of the basic syntax, data types, error handling, and object-oriented programming.\n- **Data Science Libraries**: It includes familiarity with NumPy for numerical operations, Pandas for data manipulation and analysis, Matplotlib and Seaborn for data visualization.\n- **Data Preprocessing**: This involves feature scaling and normalization, handling missing data, outlier detection, categorical data encoding, and splitting data into training, validation, and test sets.\n- **Machine Learning Libraries**: Proficiency with Scikit-learn, a library providing a wide selection of supervised and unsupervised learning algorithms, is vital. Understanding how to implement algorithms like linear regression, logistic regression, decision trees, random forests, k-nearest neighbors (K-NN), and K-means clustering is important. Dimensionality reduction techniques like PCA and t-SNE are also helpful for visualizing high-dimensional data.\n\n📚 Resources:\n\n- [Real Python](https://realpython.com/): A comprehensive resource with articles and tutorials for both beginner and advanced Python concepts.\n- [freeCodeCamp - Learn Python](https://www.youtube.com/watch?v=rfscVS0vtbw): Long video that provides a full introduction into all of the core concepts in Python.\n- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): Free digital book that is a great resource for learning pandas, NumPy, Matplotlib, and Seaborn.\n- [freeCodeCamp - Machine Learning for Everybody](https://youtu.be/i_LwzRVP7bg): Practical introduction to different machine learning algorithms for beginners.\n- [Udacity - Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120): Free course that covers PCA and several other machine learning concepts.\n\n---\n\n### 3. Neural Networks\n\nNeural networks are a fundamental part of many machine learning models, particularly in the realm of deep learning. To utilize them effectively, a comprehensive understanding of their design and mechanics is essential.\n\n- **Fundamentals**: This includes understanding the structure of a neural network, such as layers, weights, biases, and activation functions (sigmoid, tanh, ReLU, etc.)\n- **Training and Optimization**: Familiarize yourself with backpropagation and different types of loss functions, like Mean Squared Error (MSE) and Cross-Entropy. Understand various optimization algorithms like Gradient Descent, Stochastic Gradient Descent, RMSprop, and Adam.\n- **Overfitting**: Understand the concept of overfitting (where a model performs well on training data but poorly on unseen data) and learn various regularization techniques (dropout, L1/L2 regularization, early stopping, data augmentation) to prevent it.\n- **Implement a Multilayer Perceptron (MLP)**: Build an MLP, also known as a fully connected network, using PyTorch.\n\n📚 Resources:\n\n- [3Blue1Brown - But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk): This video gives an intuitive explanation of neural networks and their inner workings.\n- [freeCodeCamp - Deep Learning Crash Course](https://www.youtube.com/watch?v=VyWAvY2CF9c): This video efficiently introduces all the most important concepts in deep learning.\n- [Fast.ai - Practical Deep Learning](https://course.fast.ai/): Free course designed for people with coding experience who want to learn about deep learning.\n- [Patrick Loeber - PyTorch Tutorials](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4): Series of videos for complete beginners to learn about PyTorch.\n\n---\n\n### 4. Natural Language Processing (NLP)\n\nNLP is a fascinating branch of artificial intelligence that bridges the gap between human language and machine understanding. From simple text processing to understanding linguistic nuances, NLP plays a crucial role in many applications like translation, sentiment analysis, chatbots, and much more.\n\n- **Text Preprocessing**: Learn various text preprocessing steps like tokenization (splitting text into words or sentences), stemming (reducing words to their root form), lemmatization (similar to stemming but considers the context), stop word removal, etc.\n- **Feature Extraction Techniques**: Become familiar with techniques to convert text data into a format that can be understood by machine learning algorithms. Key methods include Bag-of-words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and n-grams.\n- **Word Embeddings**: Word embeddings are a type of word representation that allows words with similar meanings to have similar representations. Key methods include Word2Vec, GloVe, and FastText.\n- **Recurrent Neural Networks (RNNs)**: Understand the working of RNNs, a type of neural network designed to work with sequence data. Explore LSTMs and GRUs, two RNN variants that are capable of learning long-term dependencies.\n\n📚 Resources:\n\n- [Lena Voita - Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html): Beginner-friendly course about concepts related to word embeddings.\n- [RealPython - NLP with spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/): Exhaustive guide about the spaCy library for NLP tasks in Python.\n- [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing): A few notebooks and resources for a hands-on explanation of NLP in Python.\n- [Jay Alammar - The Illustration Word2Vec](https://jalammar.github.io/illustrated-word2vec/): A good reference to understand the famous Word2Vec architecture.\n- [Jake Tae - PyTorch RNN from Scratch](https://jaketae.github.io/study/pytorch-rnn/): Practical and simple implementation of RNN, LSTM, and GRU models in PyTorch.\n- [colah's blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/): A more theoretical article about the LSTM network.\n</details>\n\n## 🧑‍🔬 The LLM Scientist\n\nThis section of the course focuses on learning how to build the best possible LLMs using the latest techniques.\n\n![](img/roadmap_scientist.png)\n\n### 1. The LLM architecture\n\nAn in-depth knowledge of the Transformer architecture is not required, but it's important to understand the main steps of modern LLMs: converting text into numbers through tokenization, processing these tokens through layers including attention mechanisms, and finally generating new text through various sampling strategies.\n\n- **Architectural Overview**: Understand the evolution from encoder-decoder Transformers to decoder-only architectures like GPT, which form the basis of modern LLMs. Focus on how these models process and generate text at a high level.\n- **Tokenization**: Learn the principles of tokenization - how text is converted into numerical representations that LLMs can process. Explore different tokenization strategies and their impact on model performance and output quality.\n- **Attention mechanisms**: Master the core concepts of attention mechanisms, particularly self-attention and its variants. Understand how these mechanisms enable LLMs to process long-range dependencies and maintain context throughout sequences.\n- **Sampling techniques**: Explore various text generation approaches and their tradeoffs. Compare deterministic methods like greedy search and beam search with probabilistic approaches like temperature sampling and nucleus sampling.\n\n📚 **References**:\n* [Visual intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M) by 3Blue1Brown: Visual introduction to Transformers for complete beginners.\n* [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft: Interactive 3D visualization of LLM internals.\n* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy: A 2h-long YouTube video to reimplement GPT from scratch (for programmers). He also made a video about [tokenization](https://www.youtube.com/watch?v=zduSFxRajkE).\n* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng: Historical overview to introduce the need for attention mechanisms.\n* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) by Maxime Labonne: Provide code and a visual introduction to the different decoding strategies to generate text.\n\n---\n### 2. Pre-training models\n\nPre-training is a computationally intensive and expensive process. While it's not the focus of this course, it's important to have a solid understanding of how models are pre-trained, especially in terms of data and parameters. Pre-training can also be performed by hobbyists at a small scale with <1B models.\n\n* **Data preparation**: Pre-training requires massive datasets (e.g., [Llama 3.1](https://arxiv.org/abs/2307.09288) was trained on 15 trillion tokens) that need careful curation, cleaning, deduplication, and tokenization. Modern pre-training pipelines implement sophisticated filtering to remove low-quality or problematic content.\n* **Distributed training**: Combine different parallelization strategies: data parallel (batch distribution), pipeline parallel (layer distribution), and tensor parallel (operation splitting). These strategies require optimized network communication and memory management across GPU clusters.\n* **Training optimization**: Use adaptive learning rates with warm-up, gradient clipping, and normalization to prevent explosions, mixed-precision training for memory efficiency, and modern optimizers (AdamW, Lion) with tuned hyperparameters.\n* **Monitoring**: Track key metrics (loss, gradients, GPU stats) using dashboards, implement targeted logging for distributed training issues, and set up performance profiling to identify bottlenecks in computation and communication across devices.\n\n📚 **References**:\n* [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) by Penedo et al.: Article to recreate a large-scale dataset for LLM pretraining (15T), including FineWeb-Edu, a high-quality subset.\n* [RedPajama v2](https://www.together.ai/blog/redpajama-data-v2) by Weber et al.: Another article and paper about a large-scale pre-training dataset with a lot of interesting quality filters.\n* [nanotron](https://github.com/huggingface/nanotron) by Hugging Face: Minimalistic LLM training codebase used to make [SmolLM2](https://github.com/huggingface/smollm).\n* [Parallel training](https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf) by Chenyan Xiong: Overview of optimization and parallelism techniques.\n* [Distributed training](https://arxiv.org/abs/2407.20018) by Duan et al.: A survey about efficient training of LLM on distributed architectures.\n* [OLMo 2](https://allenai.org/olmo) by AI2: Open-source language model with model, data, training, and evaluation code.\n* [LLM360](https://www.llm360.ai/) by LLM360: A framework for open-source LLMs with training and data preparation code, data, metrics, and models.\n\n---\n### 3. Post-training datasets\n\nPost-training datasets have a precise structure with instructions and answers (supervised fine-tuning) or instructions and chosen/rejected answers (preference alignment). Conversational structures are a lot rarer than the raw text used for pre-training, which is why we often need to process seed data and refine it to improve the accuracy, diversity, and complexity of the samples. More information and examples are available in my repo [💾 LLM Datasets](https://github.com/mlabonne/llm-datasets).\n\n* **Storage & chat templates**: Because of the conversational structure, post-training datasets are stored in a specific format like ShareGPT or OpenAI/HF. Then, these formats are mapped to a chat template like ChatML or Alpaca to produce the final samples the model is trained on.\n* **Synthetic data generation**: Create instruction-response pairs based on seed data using frontier models like GPT-4o. This approach allows for flexible and scalable dataset creation with high-quality answers. Key considerations include designing diverse seed tasks and effective system prompts.\n* **Data enhancement**: Enhance existing samples using techniques like verified outputs (using unit tests or solvers), multiple answers with rejection sampling, [Auto-Evol](https://arxiv.org/abs/2406.00770), Chain-of-Thought, Branch-Solve-Merge, personas, etc.\n* **Quality filtering**: Traditional techniques involve rule-based filtering, removing duplicates or near-duplicates (with MinHash or embeddings), and n-gram decontamination. Reward models and judge LLMs complement this step with fine-grained and customizable quality control.\n\n📚 **References**:\n* [Synthetic Data Generator](https://huggingface.co/spaces/argilla/synthetic-data-generator) by Argilla: Beginner-friendly way of building datasets using natural language in a Hugging Face space.\n* [LLM Datasets](https://github.com/mlabonne/llm-datasets) by Maxime Labonne: Curated list of datasets and tools for post-training.\n* [NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator) by Nvidia: Dataset preparation and curation framework for pre- and post-training data.\n* [Distilabel](https://distilabel.argilla.io/dev/sections/pipeline_samples/) by Argilla: Framework to generate synthetic data. It also includes interesting reproductions of papers like UltraFeedback.\n* [Semhash](https://github.com/MinishLab/semhash) by MinishLab: Minimalistic library for near-deduplication and decontamination with a distilled embedding model.\n* [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating) by Hugging Face: Hugging Face's documentation about chat templates.\n\n---\n### 4. Supervised Fine-Tuning\n\nSFT turns base models into helpful assistants, capable of answering questions and following instructions. During this process, they learn how to structure answers and reactivate a subset of knowledge learned during pre-training. Instilling new knowledge is possible but superficial: it cannot be used to learn a completely new language. Always prioritize data quality over parameter optimization.\n\n- **Training techniques**: Full fine-tuning updates all model parameters but requires significant compute. Parameter-efficient fine-tuning techniques like LoRA and QLoRA reduce memory requirements by training a small number of adapter parameters while keeping base weights frozen. QLoRA combines 4-bit quantization with LoRA to reduce VRAM usage. These techniques are all implemented in the most popular fine-tuning frameworks: [TRL](https://huggingface.co/docs/trl/en/index), [Unsloth](https://docs.unsloth.ai/), and [Axolotl](https://axolotl.ai/).\n- **Training parameters**: Key parameters include learning rate with schedulers, batch size, gradient accumulation, number of epochs, optimizer (like 8-bit AdamW), weight decay for regularization, and warmup steps for training stability. LoRA also adds three parameters: rank (typically 16-128), alpha (1-2x rank), and target modules.\n- **Distributed training**: Scale training across multiple GPUs using DeepSpeed or FSDP. DeepSpeed provides three ZeRO optimization stages with increasing levels of memory efficiency through state partitioning. Both methods support gradient checkpointing for memory efficiency.\n- **Monitoring**: Track training metrics including loss curves, learning rate schedules, and gradient norms. Monitor for common issues like loss spikes, gradient explosions, or performance degradation.\n\n📚 **References**:\n* [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3) by Maxime Labonne: Hands-on tutorial on how to fine-tune a Llama 3.1 model using Unsloth.\n* [Axolotl - Documentation](https://axolotl-ai-cloud.github.io/axolotl/) by Wing Lian: Lots of interesting information related to distributed training and dataset formats.\n* [Mastering LLMs](https://parlance-labs.com/education/) by Hamel Husain: Collection of educational resources about fine-tuning (but also RAG, evaluation, applications, and prompt engineering).\n* [LoRA insights](https://lightning.ai/pages/community/lora-insights/) by Sebastian Raschka: Practical insights about LoRA and how to select the best parameters.\n\n---\n### 5. Preference Alignment\n\nPreference alignment is a second stage in the post-training pipeline, focused on aligning generated answers with human preferences. This stage was designed to tune the tone of LLMs and reduce toxicity and hallucinations. However, it has become increasingly important to also boost their performance and improve usefulness. Unlike SFT, there are many preference alignment algorithms. Here, we'll focus on the three most important ones: DPO, GRPO, and PPO.\n\n- **Rejection sampling**: For each prompt, use the trained model to generate multiple responses, and score them to infer chosen/rejected answers. This creates on-policy data, where both responses come from the model being trained, improving alignment stability.\n- **[Direct Preference Optimization](https://arxiv.org/abs/2305.18290)** Directly optimizes the policy to maximize the likelihood of chosen responses over rejected ones. It doesn't require reward modeling, which makes it more computationally efficient than RL techniques but slightly worse in terms of quality. Great for creating chat models.\n- **Reward model**: Train a reward model with human feedback to predict metrics like human preferences. It can leverage frameworks like [TRL](https://huggingface.co/docs/trl/en/index), [verl](https://github.com/volcengine/verl), and [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) for scalable training.\n- **Reinforcement Learning**: RL techniques like [GRPO](https://arxiv.org/abs/2402.03300) and [PPO](https://arxiv.org/abs/1707.06347) iteratively update a policy to maximize rewards while staying close to the initial behavior. They can use a reward model or reward functions to score responses. They tend to be computationally expensive and require careful tuning of hyperparameters, including learning rate, batch size, and clip range. Ideal for creating reasoning models.\n\n📚 **References**:\n* [Illustrating RLHF](https://huggingface.co/blog/rlhf) by Hugging Face: Introduction to RLHF with reward model training and fine-tuning with reinforcement learning.\n* [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) by Sebastian Raschka: Overview of the RLHF process and alternatives like RLAIF.\n* [Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning) by Hugging Face: Comparison of the DPO, IPO, and KTO algorithms to perform preference alignment.\n* [Fine-tune with DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html) by Maxime Labonne: Tutorial to fine-tune a Mistral-7b model with DPO and reproduce [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B).\n* [Fine-tune with GRPO](https://huggingface.co/learn/llm-course/en/chapter12/5) by Maxime Labonne: Practical exercise to fine-tune a small model with GRPO.\n* [DPO Wandb logs](https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4) by Alexander Vishnevskiy: It shows you the main DPO metrics to track and the trends you should expect.\n\n---\n### 6. Evaluation\n\nReliably evaluating LLMs is a complex but essential task guiding data generation and training. It provides invaluable feedback about areas of improvement, which can be leveraged to modify the data mixture, quality, and training parameters. However, it's always good to remember Goodhart's law: \"When a measure becomes a target, it ceases to be a good measure.\"\n\n- **Automated benchmarks**: Evaluate models on specific tasks using curated datasets and metrics, like MMLU. It works well for concrete tasks but struggles with abstract and creative capabilities. It is also prone to data contamination.\n- **Human evaluation**: It involves humans prompting models and grading responses. Methods range from vibe checks to systematic annotations with specific guidelines and large-scale community voting (arena). It is more suited for subjective tasks and less reliable for factual accuracy.\n- **Model-based evaluation**: Use judge and reward models to evaluate model outputs. It highly correlates with human preferences but suffers from bias toward their own outputs and inconsistent scoring.\n- **Feedback signal**: Analyze error patterns to identify specific weaknesses, such as limitations in following complex instructions, lack of specific knowledge, or susceptibility to adversarial prompts. This can be improved with better data generation and training parameters.\n\n📚 **References**:\n* [Evaluation guidebook](https://github.com/huggingface/evaluation-guidebook) by Clémentine Fourrier: Practical insights and theoretical knowledge about LLM evaluation.\n* [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) by Hugging Face: Main leaderboard to compare LLMs in an open and reproducible way (automated benchmarks).\n* [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) by EleutherAI: A popular framework for evaluating LLMs using automated benchmarks.\n* [Lighteval](https://github.com/huggingface/lighteval) by Hugging Face: Alternative evaluation framework that also includes model-based evaluations.\n* [Chatbot Arena](https://lmarena.ai/) by LMSYS: Elo rating of general-purpose LLMs, based on comparisons made by humans (human evaluation).\n\n---\n### 7. Quantization\n\nQuantization is the process of converting the parameters and activations of a model using a lower precision. For example, weights stored using 16 bits can be converted into a 4-bit representation. This technique has become increasingly important to reduce the computational and memory costs associated with LLMs.\n\n* **Base techniques**: Learn the different levels of precision (FP32, FP16, INT8, etc.) and how to perform naïve quantization with absmax and zero-point techniques.\n* **GGUF & llama.cpp**: Originally designed to run on CPUs, [llama.cpp](https://github.com/ggerganov/llama.cpp) and the GGUF format have become the most popular tools to run LLMs on consumer-grade hardware. It supports storing special tokens, vocabulary, and metadata in a single file.\n* **GPTQ & AWQ**: Techniques like [GPTQ](https://arxiv.org/abs/2210.17323)/[EXL2](https://github.com/turboderp/exllamav2) and [AWQ](https://arxiv.org/abs/2306.00978) introduce layer-by-layer calibration that retains performance at extremely low bitwidths. They reduce catastrophic outliers using dynamic scaling, selectively skipping or re-centering the heaviest parameters.\n* **SmoothQuant & ZeroQuant**: New quantization-friendly transformations (SmoothQuant) and compiler-based optimizations (ZeroQuant) help mitigate outliers before quantization. They also reduce hardware overhead by fusing certain ops and optimizing dataflow.\n\n📚 **References**:\n* [Introduction to quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) by Maxime Labonne: Overview of quantization, absmax and zero-point quantization, and LLM.int8() with code.\n* [Quantize Llama models with llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) by Maxime Labonne: Tutorial on how to quantize a Llama 2 model using llama.cpp and the GGUF format.\n* [4-bit LLM Quantization with GPTQ](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html) by Maxime Labonne: Tutorial on how to quantize an LLM using the GPTQ algorithm with AutoGPTQ.\n* [Understanding Activation-Aware Weight Quantization](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8) by FriendliAI: Overview of the AWQ technique and its benefits.\n* [SmoothQuant on Llama 2 7B](https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_llama_demo.ipynb) by MIT HAN Lab: Tutorial on how to use SmoothQuant with a Llama 2 model in 8-bit precision.\n* [DeepSpeed Model Compression](https://www.deepspeed.ai/tutorials/model-compression/) by DeepSpeed: Tutorial on how to use ZeroQuant and extreme compression (XTC) with DeepSpeed Compression.\n\n---\n### 8. New Trends\n\nHere are notable topics that didn't fit into other categories. Some are established (model merging, multimodal) techniques, but others are more experimental (interpretability, test-time compute scaling) and the focus of numerous research papers.\n\n* **Model merging**: Merging trained models has become a popular way of creating performant models without any fine-tuning. The popular [mergekit](https://github.com/cg123/mergekit) library implements the most popular merging methods, like SLERP, [DARE](https://arxiv.org/abs/2311.03099), and [TIES](https://arxiv.org/abs/2311.03099).\n* **Multimodal models**: These models (like [CLIP](https://openai.com/research/clip), [Stable Diffusion](https://stability.ai/stable-image), or [LLaVA](https://llava-vl.github.io/)) process multiple types of inputs (text, images, audio, etc.) with a unified embedding space, which unlocks powerful applications like text-to-image.\n* **Interpretability**: Mechanistic interpretability techniques like Sparse Autoencoders (SAEs) have made remarkable progress to provide insights about the inner workings of LLMs. This has also been applied with techniques such as abliteration, which allow you to modify the behavior of models without training.\n* **Test-time compute**: Reasoning models trained with RL techniques can be further improved by scaling the compute budget during test time. It can involve multiple calls, MCTS, or specialized models like a Process Reward Model (PRM). Iterative steps with precise scoring significantly improve performance for complex reasoning tasks.\n\n📚 **References**:\n* [Merge LLMs with mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html) by Maxime Labonne: Tutorial about model merging using mergekit.\n* [Smol Vision](https://github.com/merveenoyan/smol-vision) by Merve Noyan: Collection of notebooks and scripts dedicated to small multimodal models.\n* [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html) by Chip Huyen: Overview of multimodal systems and the recent history of this field.\n* [Unsensor any LLM with abliteration](https://huggingface.co/blog/mlabonne/abliteration) by Maxime Labonne: Direct application of interpretability techniques to modify the style of a model.\n* [Intuitive Explanation of SAEs](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) by Adam Karvonen: Article about how SAEs work and why they make sense for interpretability.\n* [Scaling test-time compute](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) by Beeching et al.: Tutorial and experiments to outperform Llama 3.1 70B on MATH-500 with a 3B model.\n\n## 👷 The LLM Engineer\n\nThis section of the course focuses on learning how to build LLM-powered applications that can be used in production, with a focus on augmenting models and deploying them.\n\n![](img/roadmap_engineer.png)\n\n### 1. Running LLMs\n\nRunning LLMs can be difficult due to high hardware requirements. Depending on your use case, you might want to simply consume a model through an API (like GPT-4) or run it locally. In any case, additional prompting and guidance techniques can improve and constrain the output for your applications.\n\n* **LLM APIs**: APIs are a convenient way to deploy LLMs. This space is divided between private LLMs ([OpenAI](https://platform.openai.com/), [Google](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview), [Anthropic](https://docs.anthropic.com/ Claude/reference/getting-started-with-the-api), etc.) and open-source LLMs ([OpenRouter](https://openrouter.ai/), [Hugging Face](https://huggingface.co/inference-api), [Together AI](https://www.together.ai/), etc.).\n* **Open-source LLMs**: The [Hugging Face Hub](https://huggingface.co/models) is a great place to find LLMs. You can directly run some of them in [Hugging Face Spaces](https://huggingface.co/spaces), or download and run them locally in apps like [LM Studio](https://lmstudio.ai/) or through the CLI with [llama.cpp](https://github.com/ggerganov/llama.cpp) or [ollama](https://ollama.ai/).\n* **Prompt engineering**: Common techniques include zero-shot prompting, few-shot prompting, chain of thought, and ReAct. They work better with bigger models, but can be adapted to smaller ones.\n* **Structuring outputs**: Many tasks require a structured output, like a strict template or a JSON format. Libraries like [Outlines](https://github.com/outlines-dev/outlines) can be used to guide the generation and respect a given structure. Some APIs also support structured output generation natively using JSON schemas.\n\n📚 **References**:\n* [Run an LLM locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio) by Nisha Arya: Short guide on how to use LM Studio.\n* [Prompt engineering guide](https://www.promptingguide.ai/) by DAIR.AI: Exhaustive list of prompt techniques with examples\n* [Outlines - Quickstart](https://dottxt-ai.github.io/outlines/latest/quickstart/): List of guided generation techniques enabled by Outlines.\n* [LMQL - Overview](https://lmql.ai/docs/language/overview.html): Introduction to the LMQL language.\n\n---\n### 2. Building a Vector Storage\n\nCreating a vector storage is the first step to building a Retrieval Augmented Generation (RAG) pipeline. Documents are loaded, split, and relevant chunks are used to produce vector representations (embeddings) that are stored for future use during inference.\n\n* **Ingesting documents**: Document loaders are convenient wrappers that can handle many formats: PDF, JSON, HTML, Markdown, etc. They can also directly retrieve data from some databases and APIs (GitHub, Reddit, Google Drive, etc.).\n* **Splitting documents**: Text splitters break down documents into smaller, semantically meaningful chunks. Instead of splitting text after *n* characters, it's often better to split by header or recursively, with some additional metadata.\n* **Embedding models**: Embedding models convert text into vector representations. Picking task-specific models significantly improves performance for semantic search and RAG.\n* **Vector databases**: Vector databases (like [Chroma](https://www.trychroma.com/), [Pinecone](https://www.pinecone.io/), [Milvus](https://milvus.io/), [FAISS](https://faiss.ai/), [Annoy](https://github.com/spotify/annoy), etc.) are designed to store embedding vectors. They enable efficient retrieval of data that is 'most similar' to a query based on vector similarity.\n\n📚 **References**:\n* [LangChain - Text splitters](https://python.langchain.com/docs/how_to/#text-splitters): List of different text splitters implemented in LangChain.\n* [Sentence Transformers library](https://www.sbert.net/): Popular library for embedding models.\n* [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard): Leaderboard for embedding models.\n* [The Top 7 Vector Databases](https://www.datacamp.com/blog/the-top-5-vector-databases) by Moez Ali: A comparison of the best and most popular vector databases.\n\n---\n### 3. Retrieval Augmented Generation\n\nWith RAG, LLMs retrieve contextual documents from a database to improve the accuracy of their answers. RAG is a popular way of augmenting the model's knowledge without any fine-tuning.\n\n* **Orchestrators**: Orchestrators like [LangChain](https://python.langchain.com/docs/get_started/introduction) and [LlamaIndex](https://docs.llamaindex.ai/en/stable/) are popular frameworks to connect your LLMs with tools and databases. The Model Context Protocol (MCP) introduces a new standard to pass data and context to models across providers.\n* **Retrievers**: Query rewriters and generative retrievers like CoRAG and HyDE enhance search by transforming user queries. Multi-vector and hybrid retrieval methods combine embeddings with keyword signals to improve recall and precision.\n* **Memory**: To remember previous instructions and answers, LLMs and chatbots like ChatGPT add this history to their context window. This buffer can be improved with summarization (e.g., using a smaller LLM), a vector store + RAG, etc.\n* **Evaluation**: We need to evaluate both the document retrieval (context precision and recall) and generation stages (faithfulness and answer relevancy). It can be simplified with tools [Ragas](https://github.com/explodinggradients/ragas/tree/main) and [DeepEval](https://github.com/confident-ai/deepeval) (assessing quality).\n\n📚 **References**:\n* [Llamaindex - High-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html): Main concepts to know when building RAG pipelines.\n* [Model Context Protocol](https://modelcontextprotocol.io/introduction): Introduction to MCP with motivate, architecture, and quick starts.\n* [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/): Overview of the retrieval augmentation process.\n* [LangChain - Q&A with RAG](https://python.langchain.com/docs/tutorials/rag/): Step-by-step tutorial to build a typical RAG pipeline.\n* [LangChain - Memory types](https://python.langchain.com/docs/how_to/chatbots_memory/): List of different types of memories with relevant usage.\n* [RAG pipeline - Metrics](https://docs.ragas.io/en/stable/concepts/metrics/index.html): Overview of the main metrics used to evaluate RAG pipelines.\n\n---\n### 4. Advanced RAG\n\nReal-life applications can require complex pipelines, including SQL or graph databases, as well as automatically selecting relevant tools and APIs. These advanced techniques can improve a baseline solution and provide additional features.\n\n* **Query construction**: Structured data stored in traditional databases requires a specific query language like SQL, Cypher, metadata, etc. We can directly translate the user instruction into a query to access the data with query construction.\n* **Tools**: Agents augment LLMs by automatically selecting the most relevant tools to provide an answer. These tools can be as simple as using Google or Wikipedia, or more complex like a Python interpreter or Jira.\n* **Post-processing**: Final step that processes the inputs that are fed to the LLM. It enhances the relevance and diversity of documents retrieved with re-ranking, [RAG-fusion](https://github.com/Raudaschl/rag-fusion), and classification.\n* **Program LLMs**: Frameworks like [DSPy](https://github.com/stanfordnlp/dspy) allow you to optimize prompts and weights based on automated evaluations in a programmatic way.\n\n📚 **References**:\n* [LangChain - Query Construction](https://blog.langchain.dev/query-construction/): Blog post about different types of query construction.\n* [LangChain - SQL](https://python.langchain.com/docs/tutorials/sql_qa/): Tutorial on how to interact with SQL databases with LLMs, involving Text-to-SQL and an optional SQL agent.\n* [Pinecone - LLM agents](https://www.pinecone.io/learn/series/langchain/langchain-agents/): Introduction to agents and tools with different types.\n* [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) by Lilian Weng: A more theoretical article about LLM agents.\n* [LangChain - OpenAI's RAG](https://blog.langchain.dev/applying-openai-rag/): Overview of the RAG strategies employed by OpenAI, including post-processing.\n* [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task): General-purpose guide to DSPy introducing modules, signatures, and optimizers.\n\n---\n### 5. Agents\n\nAn LLM agent can autonomously perform tasks by taking actions based on reasoning about its environment, typically through the use of tools or functions to interact with external systems.\n\n* **Agent fundamentals**: Agents operate using thoughts (internal reasoning to decide what to do next), action (executing tasks, often by interacting with external tools), and observation (analyzing feedback or results to refine the next step).\n* **Agent frameworks**: Agent development can be streamlined using different frameworks like [LangGraph](https://www.langchain.com/langgraph) (design and visualization of workflows), [LlamaIndex](https://docs.llamaindex.ai/en/stable/use_cases/agents/) (data-augmented agents with RAG), or [smolagents](https://github.com/huggingface/smolagents) (beginner-friendly, lightweight option).\n* **Multi-agents**: More experimental frameworks include collaboration between different agents, such as [CrewAI](https://docs.crewai.com/introduction) (role-based team orchestration), [AutoGen](https://github.com/microsoft/autogen) (conversation-driven multi-agent systems), and [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) (production-ready with strong OpenAI model integration).\n\n📚 **References**:\n* [Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction): Popular course about AI agents made by Hugging Face.\n* [AI Agents Comparison](https://langfuse.com/blog/2025-03-19-ai-agent-comparison) by Jannik Maierhöfer: Comparison of features across different open-source AI agent frameworks.\n* [LangGraph](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/): Overview of how to build AI agents with LangGraph.\n* [LlamaIndex Agents](https://docs.llamaindex.ai/en/stable/use_cases/agents/): Uses cases and resources to build agents with LlamaIndex.\n* [smolagents](https://huggingface.co/docs/smolagents/index): Documentation with a guided tour, how-to guides, and more conceptual articles.\n\n---\n### 6. Inference optimization\n\nText generation is a costly process that requires expensive hardware. In addition to quantization, various techniques have been proposed to maximize throughput and reduce inference costs.\n\n* **Flash Attention**: Optimization of the attention mechanism to transform its complexity from quadratic to linear, speeding up both training and inference.\n* **Key-value cache**: Understand the key-value cache and the improvements introduced in [Multi-Query Attention](https://arxiv.org/abs/1911.02150) (MQA) and [Grouped-Query Attention](https://arxiv.org/abs/2305.13245) (GQA).\n* **Speculative decoding**: Use a small model to produce drafts that are then reviewed by a larger model to speed up text generation.\n\n📚 **References**:\n* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face: Explain how to optimize inference on GPUs.\n* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks: Best practices for how to optimize LLM inference in production.\n* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face: Explain three main techniques to optimize speed and memory, namely quantization, Flash Attention, and architectural innovations.\n* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face: HF's version of speculative decoding, it's an interesting blog post about how it works with code to implement it.\n\n---\n### 7. Deploying LLMs\n\nDeploying LLMs at scale is an engineering feat that can require multiple clusters of GPUs. In other scenarios, demos and local apps can be achieved with a much lower complexity.\n\n* **Local deployment**: Privacy is an important advantage that open-source LLMs have over private ones. Local LLM servers ([LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.ai/), [oobabooga](https://github.com/oobabooga/text-generation-webui), [kobold.cpp](https://github.com/LostRuins/koboldcpp), etc.) capitalize on this advantage to power local apps.\n* **Demo deployment**: Frameworks like [Gradio](https://www.gradio.app/) and [Streamlit](https://docs.streamlit.io/) are helpful to prototype applications and share demos. You can also easily host them online, for example using [Hugging Face Spaces](https://huggingface.co/spaces).\n* **Server deployment**: Deploy LLMs at scale requires cloud (see also [SkyPilot](https://skypilot.readthedocs.io/en/latest/)) or on-prem infrastructure and often leverage optimized text generation frameworks like [TGI](https://github.com/huggingface/text-generation-inference), [vLLM](https://github.com/vllm-project/vllm/tree/main), etc.\n* **Edge deployment**: In constrained environments, high-performance frameworks like [MLC LLM](https://github.com/mlc-ai/mlc-llm) and [mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md) can deploy LLM in web browsers, Android, and iOS.\n\n📚 **References**:\n* [Streamlit - Build a basic LLM app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps): Tutorial to make a basic ChatGPT-like app using Streamlit.\n* [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm): Deploy LLMs on Amazon SageMaker using Hugging Face's inference container.\n* [Philschmid\nblog](https://www.philschmid.de/) by Philipp Schmid: Collection of high-quality articles about LLM deployment using Amazon SageMaker.\n* [Optimizing latence](https://hamel.dev/notes/llm/inference/03_inference.html) by Hamel Husain: Comparison of TGI, vLLM, CTranslate2, and mlc in terms of throughput and latency.\n\n---\n### 8. Securing LLMs\n\nIn addition to traditional security problems associated with software, LLMs have unique weaknesses due to the way they are trained and prompted.\n\n* **Prompt hacking**: Different techniques related to prompt engineering, including prompt injection (additional instruction to hijack the model's answer), data/prompt leaking (retrieve its original data/prompt), and jailbreaking (craft prompts to bypass safety features).\n* **Backdoors**: Attack vectors can target the training data itself, by poisoning the training data (e.g., with false information) or creating backdoors (secret triggers to change the model's behavior during inference).\n* **Defensive measures**: The best way to protect your LLM applications is to test them against these vulnerabilities (e.g., using red teaming and checks like [garak](https://github.com/leondz/garak/)) and observe them in production (with a framework like [langfuse](https://github.com/langfuse/langfuse)).\n\n📚 **References**:\n* [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) by HEGO Wiki: List of the 10 most critical vulnerabilities seen in LLM applications.\n* [Prompt Injection Primer](https://github.com/jthack/PIPE) by Joseph Thacker: Short guide dedicated to prompt injection for engineers.\n* [LLM Security](https://llmsecurity.net/) by [@llm_sec](https://twitter.com/llm_sec): Extensive list of resources related to LLM security.\n* [Red teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming) by Microsoft: Guide on how to perform red teaming with LLMs.\n\n---\n## Acknowledgements\n\nThis roadmap was inspired by the excellent [DevOps Roadmap](https://github.com/milanm/DevOps-Roadmap) from Milan Milanović and Romano Roth.\n\nSpecial thanks to:\n\n* Thomas Thelen for motivating me to create a roadmap\n* André Frade for his input and review of the first draft\n* Dino Dunn for providing resources about LLM security\n* Magdalena Kuhn for improving the \"human evaluation\" part\n* Odoverdose for suggesting 3Blue1Brown's video about Transformers\n* Everyone who contributed to the educational references in this course :)\n\n*Disclaimer: I am not affiliated with any sources listed here.*\n\n---\n\n[![Star History Chart](https://api.star-history.com/svg?repos=mlabonne/llm-course&type=Date)](https://www.star-history.com/#mlabonne/llm-course&Date)\n",
    "summary": "该GitHub仓库提供了一个全面的大语言模型（LLM）学习路线图，分为基础、科学家和工程师三个部分。内容涵盖LLM架构、预训练、后训练（如SFT、偏好对齐）、评估、量化、最新趋势、模型运行、检索增强生成（RAG）、智能体、推理优化、模型部署及安全。仓库提供了丰富的笔记本、文章和资源链接，旨在帮助学习者掌握构建和部署LLM应用的最新技术和实践。",
    "keywords": [
      "大语言模型",
      "自然语言处理",
      "模型微调",
      "模型量化",
      "检索增强生成",
      "智能体",
      "模型部署",
      "机器学习"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "机器学习"
    ],
    "published_time": "2025-05-12T08:34:35Z",
    "download_time": "2024-07-30 10:00:00",
    "visual_resource": [
      "https://github.com/mlabonne/llm-course/raw/main/img/banner.png",
      "https://github.com/mlabonne/llm-course/raw/main/img/roadmap_fundamentals.png"
    ],
    "extra_info": null
  },
  {
    "id": "jellyfin",
    "source": "GitHub",
    "url": "https://github.com/jellyfin/jellyfin",
    "title": "Jellyfin",
    "content": "<h1 align=\"center\">Jellyfin</h1>\n<h3 align=\"center\">The Free Software Media System</h3>\n\n---\n\n<p align=\"center\">\n<img alt=\"Logo Banner\" src=\"https://raw.githubusercontent.com/jellyfin/jellyfin-ux/master/branding/SVG/banner-logo-solid.svg?sanitize=true\"/>\n<br/>\n<br/>\n<a href=\"https://github.com/jellyfin/jellyfin\">\n<img alt=\"GPL 2.0 License\" src=\"https://img.shields.io/github/license/jellyfin/jellyfin.svg\"/>\n</a>\n<a href=\"https://github.com/jellyfin/jellyfin/releases\">\n<img alt=\"Current Release\" src=\"https://img.shields.io/github/release/jellyfin/jellyfin.svg\"/>\n</a>\n<a href=\"https://translate.jellyfin.org/projects/jellyfin/jellyfin-core/?utm_source=widget\">\n<img alt=\"Translation Status\" src=\"https://translate.jellyfin.org/widgets/jellyfin/-/jellyfin-core/svg-badge.svg\"/>\n</a>\n<a href=\"https://hub.docker.com/r/jellyfin/jellyfin\">\n<img alt=\"Docker Pull Count\" src=\"https://img.shields.io/docker/pulls/jellyfin/jellyfin.svg\"/>\n</a>\n<br/>\n<a href=\"https://opencollective.com/jellyfin\">\n<img alt=\"Donate\" src=\"https://img.shields.io/opencollective/all/jellyfin.svg?label=backers\"/>\n</a>\n<a href=\"https://features.jellyfin.org\">\n<img alt=\"Submit Feature Requests\" src=\"https://img.shields.io/badge/fider-vote%20on%20features-success.svg\"/>\n</a>\n<a href=\"https://matrix.to/#/#jellyfinorg:matrix.org\">\n<img alt=\"Chat on Matrix\" src=\"https://img.shields.io/matrix/jellyfinorg:matrix.org.svg?logo=matrix\"/>\n</a>\n<a href=\"https://github.com/jellyfin/jellyfin/releases.atom\">\n<img alt=\"Release RSS Feed\" src=\"https://img.shields.io/badge/rss-releases-ffa500?logo=rss\" />\n</a>\n<a href=\"https://github.com/jellyfin/jellyfin/commits/master.atom\">\n<img alt=\"Master Commits RSS Feed\" src=\"https://img.shields.io/badge/rss-commits-ffa500?logo=rss\" />\n</a>\n</p>\n\n---\n\nJellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it. We welcome anyone who is interested in joining us in our quest!\n\nFor further details, please see [our documentation page](https://jellyfin.org/docs/). To receive the latest updates, get help with Jellyfin, and join the community, please visit [one of our communication channels](https://jellyfin.org/docs/general/getting-help). For more information about the project, please see our [about page](https://jellyfin.org/docs/general/about).\n\n<strong>Want to get started?</strong><br/>\nCheck out our <a href=\"https://jellyfin.org/downloads\">downloads page</a> or our <a href=\"https://jellyfin.org/docs/general/installation/\">installation guide</a>, then see our <a href=\"https://jellyfin.org/docs/general/quick-start\">quick start guide</a>. You can also <a href=\"https://jellyfin.org/docs/general/installation/source\">build from source</a>.<br/>\n\n<strong>Something not working right?</strong><br/>\nOpen an <a href=\"https://jellyfin.org/docs/general/contributing/issues\">Issue</a> on GitHub.<br/>\n\n<strong>Want to contribute?</strong><br/>\nCheck out our <a href=\"https://jellyfin.org/contribute\">contributing choose-your-own-adventure</a> to see where you can help, then see our <a href=\"https://jellyfin.org/docs/general/contributing/\">contributing guide</a> and our <a href=\"https://jellyfin.org/docs/general/community-standards\">community standards</a>.<br/>\n\n<strong>New idea or improvement?</strong><br/>\nCheck out our <a href=\"https://features.jellyfin.org/?view=most-wanted\">feature request hub</a>.<br/>\n\n<strong>Don't see Jellyfin in your language?</strong><br/>\nCheck out our <a href=\"https://translate.jellyfin.org\">Weblate instance</a> to help translate Jellyfin and its subprojects.<br/>\n\n<a href=\"https://translate.jellyfin.org/engage/jellyfin/?utm_source=widget\">\n<img src=\"https://translate.jellyfin.org/widgets/jellyfin/-/jellyfin-web/multi-auto.svg\" alt=\"Detailed Translation Status\"/>\n</a>\n\n---\n\n## Jellyfin Server\n\nThis repository contains the code for Jellyfin's backend server. Note that this is only one of many projects under the Jellyfin GitHub [organization](https://github.com/jellyfin/) on GitHub. If you want to contribute, you can start by checking out our [documentation](https://jellyfin.org/docs/general/contributing/index.html) to see what to work on.\n\n## Server Development\n\nThese instructions will help you get set up with a local development environment in order to contribute to this repository. Before you start, please be sure to completely read our [guidelines on development contributions](https://jellyfin.org/docs/general/contributing/development.html). Note that this project is supported on all major operating systems except FreeBSD, which is still incompatible.\n\n### Prerequisites\n\nBefore the project can be built, you must first install the [.NET 9.0 SDK](https://dotnet.microsoft.com/download/dotnet) on your system.\n\nInstructions to run this project from the command line are included here, but you will also need to install an IDE if you want to debug the server while it is running. Any IDE that supports .NET 6 development will work, but two options are recent versions of [Visual Studio](https://visualstudio.microsoft.com/downloads/) (at least 2022) and [Visual Studio Code](https://code.visualstudio.com/Download).\n\n[ffmpeg](https://github.com/jellyfin/jellyfin-ffmpeg) will also need to be installed.\n\n### Cloning the Repository\n\nAfter dependencies have been installed you will need to clone a local copy of this repository. If you just want to run the server from source you can clone this repository directly, but if you are intending to contribute code changes to the project, you should [set up your own fork](https://jellyfin.org/docs/general/contributing/development.html#set-up-your-copy-of-the-repo) of the repository. The following example shows how you can clone the repository directly over HTTPS.\n\n```bash\ngit clone https://github.com/jellyfin/jellyfin.git\n```\n\n### Installing the Web Client\n\nThe server is configured to host the static files required for the [web client](https://github.com/jellyfin/jellyfin-web) in addition to serving the backend by default. Before you can run the server, you will need to get a copy of the web client since they are not included in this repository directly.\n\nNote that it is also possible to [host the web client separately](#hosting-the-web-client-separately) from the web server with some additional configuration, in which case you can skip this step.\n\nThere are three options to get the files for the web client.\n\n1. Download one of the finished builds from the [Azure DevOps pipeline](https://dev.azure.com/jellyfin-project/jellyfin/_build?definitionId=27). You can download the build for a specific release by looking at the [branches tab](https://dev.azure.com/jellyfin-project/jellyfin/_build?definitionId=27&_a=summary&repositoryFilter=6&view=branches) of the pipelines page.\n2. Build them from source following the instructions on the [jellyfin-web repository](https://github.com/jellyfin/jellyfin-web)\n3. Get the pre-built files from an existing installation of the server. For example, with a Windows server installation the client files are located at `C:\\Program Files\\Jellyfin\\Server\\jellyfin-web`\n\n### Running The Server\n\nThe following instructions will help you get the project up and running via the command line, or your preferred IDE.\n\n#### Running With Visual Studio\n\nTo run the project with Visual Studio you can open the Solution (`.sln`) file and then press `F5` to run the server.\n\n#### Running With Visual Studio Code\n\nTo run the project with Visual Studio Code you will first need to open the repository directory with Visual Studio Code using the `Open Folder...` option.\n\nSecond, you need to [install the recommended extensions for the workspace](https://code.visualstudio.com/docs/editor/extension-gallery#_recommended-extensions). Note that extension recommendations are classified as either \"Workspace Recommendations\" or \"Other Recommendations\", but only the \"Workspace Recommendations\" are required.\n\nAfter the required extensions are installed, you can run the server by pressing `F5`.\n\n#### Running From the Command Line\n\nTo run the server from the command line you can use the `dotnet run` command. The example below shows how to do this if you have cloned the repository into a directory named `jellyfin` (the default directory name) and should work on all operating systems.\n\n```bash\ncd jellyfin                          # Move into the repository directory\ndotnet run --project Jellyfin.Server --webdir /absolute/path/to/jellyfin-web/dist # Run the server startup project\n```\n\nA second option is to build the project and then run the resulting executable file directly. When running the executable directly you can easily add command line options. Add the `--help` flag to list details on all the supported command line options.\n\n1. Build the project\n\n```bash\ndotnet build                       # Build the project\ncd Jellyfin.Server/bin/Debug/net9.0 # Change into the build output directory\n```\n\n2. Execute the build output. On Linux, Mac, etc. use `./jellyfin` and on Windows use `jellyfin.exe`.\n\n#### Accessing the Hosted Web Client\n\nIf the Server is configured to host the Web Client, and the Server is running, the Web Client can be accessed at `http://localhost:8096` by default.\n\nAPI documentation can be viewed at `http://localhost:8096/api-docs/swagger/index.html`\n\n\n### Running from GitHub Codespaces\n\nAs Jellyfin will run on a container on a GitHub hosted server, JF needs to handle some things differently.\n\n**NOTE:** Depending on the selected configuration (if you just click 'create codespace' it will create a default configuration one) it might take 20-30 seconds to load all extensions and prepare the environment while VS Code is already open. Just give it some time and wait until you see `Downloading .NET version(s) 7.0.15~x64 ...... Done!` in the output tab.\n\n**NOTE:** If you want to access the JF instance from outside, like with a WebClient on another PC, remember to set the \"ports\" in the lower VS Code window to public.\n\n**NOTE:** When first opening the server instance with any WebUI, you will be sent to the login instead of the setup page. Refresh the login page once and you should be redirected to the Setup.\n\nThere are two configurations for you to choose from.\n#### Default - Development Jellyfin Server\nThis creates a container that has everything to run and debug the Jellyfin Media server but does not setup anything else. Each time you create a new container you have to run through the whole setup again. There is also no ffmpeg, webclient or media preloaded. Use the `.NET Launch (nowebclient)` launch config to start the server.\n\n> Keep in mind that as this has no web client you have to connect to it via an external client. This can be just another codespace container running the WebUI. vuejs does not work from the get-go as it does not support the setup steps.\n\n#### Development Jellyfin Server ffmpeg\nthis extends the default server with a default installation of ffmpeg6 though the means described here: https://jellyfin.org/docs/general/installation/linux#repository-manual\nIf you want to install a specific ffmpeg version, follow the comments embedded in the `.devcontainer/Dev - Server Ffmpeg/install.ffmpeg.sh` file.\n\nUse the `ghcs .NET Launch (nowebclient, ffmpeg)` launch config to run with the jellyfin-ffmpeg enabled.\n\n\n### Running The Tests\n\nThis repository also includes unit tests that are used to validate functionality as part of a CI pipeline on Azure. There are several ways to run these tests.\n\n1. Run tests from the command line using `dotnet test`\n2. Run tests in Visual Studio using the [Test Explorer](https://docs.microsoft.com/en-us/visualstudio/test/run-unit-tests-with-test-explorer)\n3. Run individual tests in Visual Studio Code using the associated [CodeLens annotation](https://github.com/OmniSharp/omnisharp-vscode/wiki/How-to-run-and-debug-unit-tests)\n\n### Advanced Configuration\n\nThe following sections describe some more advanced scenarios for running the server from source that build upon the standard instructions above.\n\n#### Hosting The Web Client Separately\n\nIt is not necessary to host the frontend web client as part of the backend server. Hosting these two components separately may be useful for frontend developers who would prefer to host the client in a separate webpack development server for a tighter development loop. See the [jellyfin-web](https://github.com/jellyfin/jellyfin-web#getting-started) repo for instructions on how to do this.\n\nTo instruct the server not to host the web content, there is a `nowebclient` configuration flag that must be set. This can be specified using the command line\nswitch `--nowebclient` or the environment variable `JELLYFIN_NOWEBCONTENT=true`.\n\nSince this is a common scenario, there is also a separate launch profile defined for Visual Studio called `Jellyfin.Server (nowebcontent)` that can be selected from the 'Start Debugging' dropdown in the main toolbar.\n\n**NOTE:** The setup wizard cannot be run if the web client is hosted separately.\n\n---\n<p align=\"center\">\nThis project is supported by:\n<br/>\n<br/>\n<a href=\"https://www.digitalocean.com\"><img src=\"https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg\" height=\"50px\" alt=\"DigitalOcean\"></a>\n    &nbsp;\n<a href=\"https://www.jetbrains.com\"><img src=\"https://gist.githubusercontent.com/anthonylavado/e8b2403deee9581e0b4cb8cd675af7db/raw/fa104b7d73f759d7262794b94569f1b89df41c0b/jetbrains.svg\" height=\"50px\" alt=\"JetBrains logo\"></a>\n</p>",
    "summary": "Jellyfin是一个自由开源的媒体系统，用于管理和流式传输媒体内容。作为Emby和Plex的替代品，它提供了一个专用服务器，通过多种应用向终端用户设备提供媒体服务。Jellyfin基于Emby 3.5.2版本，并移植到.NET Core框架，实现跨平台支持。项目无付费许可或隐藏功能，由社区驱动开发，致力于构建更好的媒体管理解决方案。",
    "keywords": [
      "媒体系统",
      "流媒体",
      "开源软件",
      ".NET Core",
      "跨平台",
      "媒体服务器",
      "家庭影院",
      "Emby替代品"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-25T12:02:38+00:00",
    "download_time": "2024-05-18 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/jellyfin/jellyfin-ux/master/branding/SVG/banner-logo-solid.svg?sanitize=true"
    ],
    "extra_info": null
  },
  {
    "id": "iptv",
    "source": "GitHub",
    "url": "https://github.com/iptv-org/iptv",
    "title": "IPTV",
    "content": "# IPTV [![update](https://github.com/iptv-org/iptv/actions/workflows/update.yml/badge.svg)](https://github.com/iptv-org/iptv/actions/workflows/update.yml)\n\nCollection of publicly available IPTV (Internet Protocol television) channels from all over the world.\n\n## Table of contents\n\n- 🚀 [How to use?](#how-to-use)\n- 📺 [Playlists](#playlists)\n- 🗓 [EPG](#epg)\n- 🗄 [Database](#database)\n- 👨‍💻 [API](#api)\n- 📚 [Resources](#resources)\n- 💬 [Discussions](#discussions)\n- ❓ [FAQ](#faq)\n- 🛠 [Contribution](#contribution)\n- ⚖ [Legal](#legal)\n- © [License](#license)\n\n## How to use?\n\nSimply insert one of the links below into [any video player](https://github.com/iptv-org/awesome-iptv#apps) that supports live streaming and press _Open_.\n\n![VLC Network Panel](https://github.com/iptv-org/iptv/raw/master/.readme/preview.png)\n\n## Playlists\n\nThere are several versions of playlists that differ in the way they are grouped. As of January 30th, 2024, we have stopped distributing NSFW channels. For more information, please look at [this issue](https://github.com/iptv-org/iptv/issues/15723).\n\n### Main playlist\n\nThis playlist includes all known channels available in this repository.\n\n```\nhttps://iptv-org.github.io/iptv/index.m3u\n```\n\n### Grouped by category\n\nPlaylists in which channels are grouped by category. A list of all supported categories with descriptions can be found [here](.readme/supported-categories.md).\n\n<details>\n<summary>Expand</summary>\n<br>\n\n```\nhttps://iptv-org.github.io/iptv/index.category.m3u\n```\n\nSame thing, but split up into separate files:\n\n<!-- prettier-ignore -->\n<table>\n  <thead>\n    <tr><th align=\"left\">Category</th><th align=\"left\">Channels</th><th align=\"left\">Playlist</th></tr>\n  </thead>\n  <tbody>\n    <tr><td>Animation</td><td align=\"right\">48</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/animation.m3u</code></td></tr>\n    <tr><td>Auto</td><td align=\"right\">19</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/auto.m3u</code></td></tr>\n    <tr><td>Business</td><td align=\"right\">66</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/business.m3u</code></td></tr>\n    <tr><td>Classic</td><td align=\"right\">49</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/classic.m3u</code></td></tr>\n    <tr><td>Comedy</td><td align=\"right\">83</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/comedy.m3u</code></td></tr>\n    <tr><td>Cooking</td><td align=\"right\">29</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/cooking.m3u</code></td></tr>\n    <tr><td>Culture</td><td align=\"right\">155</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/culture.m3u</code></td></tr>\n    <tr><td>Documentary</td><td align=\"right\">113</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/documentary.m3u</code></td></tr>\n    <tr><td>Education</td><td align=\"right\">151</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/education.m3u</code></td></tr>\n    <tr><td>Entertainment</td><td align=\"right\">549</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/entertainment.m3u</code></td></tr>\n    <tr><td>Family</td><td align=\"right\">51</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/family.m3u</code></td></tr>\n    <tr><td>General</td><td align=\"right\">2209</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/general.m3u</code></td></tr>\n    <tr><td>Kids</td><td align=\"right\">246</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/kids.m3u</code></td></tr>\n    <tr><td>Legislative</td><td align=\"right\">187</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/legislative.m3u</code></td></tr>\n    <tr><td>Lifestyle</td><td align=\"right\">93</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/lifestyle.m3u</code></td></tr>\n    <tr><td>Movies</td><td align=\"right\">300</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/movies.m3u</code></td></tr>\n    <tr><td>Music</td><td align=\"right\">618</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/music.m3u</code></td></tr>\n    <tr><td>News</td><td align=\"right\">753</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/news.m3u</code></td></tr>\n    <tr><td>Outdoor</td><td align=\"right\">49</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/outdoor.m3u</code></td></tr>\n    <tr><td>Relax</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/relax.m3u</code></td></tr>\n    <tr><td>Religious</td><td align=\"right\">678</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/religious.m3u</code></td></tr>\n    <tr><td>Science</td><td align=\"right\">24</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/science.m3u</code></td></tr>\n    <tr><td>Series</td><td align=\"right\">258</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/series.m3u</code></td></tr>\n    <tr><td>Shop</td><td align=\"right\">88</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/shop.m3u</code></td></tr>\n    <tr><td>Sports</td><td align=\"right\">266</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/sports.m3u</code></td></tr>\n    <tr><td>Travel</td><td align=\"right\">43</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/travel.m3u</code></td></tr>\n    <tr><td>Weather</td><td align=\"right\">13</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/weather.m3u</code></td></tr>\n    <tr><td>XXX</td><td align=\"right\">0</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/xxx.m3u</code></td></tr>\n    <tr><td>Undefined</td><td align=\"right\">3842</td><td nowrap><code>https://iptv-org.github.io/iptv/categories/undefined.m3u</code></td></tr>\n  </tbody>\n</table>\n\n</details>\n\n### Grouped by language\n\nPlaylists in which channels are grouped by the language in which they are broadcast.\n\n<details>\n<summary>Expand</summary>\n<br>\n\n```\nhttps://iptv-org.github.io/iptv/index.language.m3u\n```\n\nSame thing, but split up into separate files:\n\n<!-- prettier-ignore -->\n<table>\n  <thead>\n    <tr><th align=\"left\">Language</th><th align=\"left\">Channels</th><th align=\"left\">Playlist</th></tr>\n  </thead>\n  <tbody>\n    <tr><td align=\"left\">Acoli</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ach.m3u</code></td></tr>\n    <tr><td align=\"left\">Adhola</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/adh.m3u</code></td></tr>\n    <tr><td align=\"left\">Afar</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/aar.m3u</code></td></tr>\n    <tr><td align=\"left\">Afghan Persian</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/prs.m3u</code></td></tr>\n    <tr><td align=\"left\">Afrikaans</td><td align=\"right\">3</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/afr.m3u</code></td></tr>\n    <tr><td align=\"left\">Albanian</td><td align=\"right\">66</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/sqi.m3u</code></td></tr>\n    <tr><td align=\"left\">Alur</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/alz.m3u</code></td></tr>\n    <tr><td align=\"left\">Amharic</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/amh.m3u</code></td></tr>\n    <tr><td align=\"left\">Arabic</td><td align=\"right\">396</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ara.m3u</code></td></tr>\n    <tr><td align=\"left\">Armenian</td><td align=\"right\">27</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/hye.m3u</code></td></tr>\n    <tr><td align=\"left\">Assamese</td><td align=\"right\">7</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/asm.m3u</code></td></tr>\n    <tr><td align=\"left\">Assyrian Neo-Aramaic</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/aii.m3u</code></td></tr>\n    <tr><td align=\"left\">Ayizo Gbe</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ayb.m3u</code></td></tr>\n    <tr><td align=\"left\">Aymara</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/aym.m3u</code></td></tr>\n    <tr><td align=\"left\">Azerbaijani</td><td align=\"right\">16</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/aze.m3u</code></td></tr>\n    <tr><td align=\"left\">Baatonum</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/bba.m3u</code></td></tr>\n    <tr><td align=\"left\">Bambara</td><td align=\"right\">5</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/bam.m3u</code></td></tr>\n    <tr><td align=\"left\">Bashkir</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/bak.m3u</code></td></tr>\n    <tr><td align=\"left\">Basque</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/eus.m3u</code></td></tr>\n    <tr><td align=\"left\">Belarusian</td><td align=\"right\">4</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/bel.m3u</code></td></tr>\n    <tr><td align=\"left\">Bengali</td><td align=\"right\">31</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ben.m3u</code></td></tr>\n    <tr><td align=\"left\">Bhojpuri</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/bho.m3u</code></td></tr>\n    <tr><td align=\"left\">Bosnian</td><td align=\"right\">13</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/bos.m3u</code></td></tr>\n    <tr><td align=\"left\">Bulgarian</td><td align=\"right\">29</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/bul.m3u</code></td></tr>\n    <tr><td align=\"left\">Burmese</td><td align=\"right\">10</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mya.m3u</code></td></tr>\n    <tr><td align=\"left\">Catalan</td><td align=\"right\">51</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/cat.m3u</code></td></tr>\n    <tr><td align=\"left\">Central Kurdish</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ckb.m3u</code></td></tr>\n    <tr><td align=\"left\">Chenoua</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/cnu.m3u</code></td></tr>\n    <tr><td align=\"left\">Chhattisgarhi</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/hne.m3u</code></td></tr>\n    <tr><td align=\"left\">Chiga</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/cgg.m3u</code></td></tr>\n    <tr><td align=\"left\">Chinese</td><td align=\"right\">219</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/zho.m3u</code></td></tr>\n    <tr><td align=\"left\">Croatian</td><td align=\"right\">22</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/hrv.m3u</code></td></tr>\n    <tr><td align=\"left\">Czech</td><td align=\"right\">40</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ces.m3u</code></td></tr>\n    <tr><td align=\"left\">Danish</td><td align=\"right\">19</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/dan.m3u</code></td></tr>\n    <tr><td align=\"left\">Dendi (Benin)</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ddn.m3u</code></td></tr>\n    <tr><td align=\"left\">Dhanwar (Nepal)</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/dhw.m3u</code></td></tr>\n    <tr><td align=\"left\">Dhivehi</td><td align=\"right\">10</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/div.m3u</code></td></tr>\n    <tr><td align=\"left\">Dholuo</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/luo.m3u</code></td></tr>\n    <tr><td align=\"left\">Dimili</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/zza.m3u</code></td></tr>\n    <tr><td align=\"left\">Dutch</td><td align=\"right\">183</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/nld.m3u</code></td></tr>\n    <tr><td align=\"left\">Egyptian Arabic</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/arz.m3u</code></td></tr>\n    <tr><td align=\"left\">English</td><td align=\"right\">2250</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/eng.m3u</code></td></tr>\n    <tr><td align=\"left\">Estonian</td><td align=\"right\">9</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/est.m3u</code></td></tr>\n    <tr><td align=\"left\">Ewe</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ewe.m3u</code></td></tr>\n    <tr><td align=\"left\">Faroese</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/fao.m3u</code></td></tr>\n    <tr><td align=\"left\">Fataleka</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/far.m3u</code></td></tr>\n    <tr><td align=\"left\">Filipino</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/fil.m3u</code></td></tr>\n    <tr><td align=\"left\">Finnish</td><td align=\"right\">29</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/fin.m3u</code></td></tr>\n    <tr><td align=\"left\">Fon</td><td align=\"right\">3</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/fon.m3u</code></td></tr>\n    <tr><td align=\"left\">French</td><td align=\"right\">411</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/fra.m3u</code></td></tr>\n    <tr><td align=\"left\">Fulah</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ful.m3u</code></td></tr>\n    <tr><td align=\"left\">Gaelic</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/gla.m3u</code></td></tr>\n    <tr><td align=\"left\">Galician</td><td align=\"right\">13</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/glg.m3u</code></td></tr>\n    <tr><td align=\"left\">Ganda</td><td align=\"right\">5</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/lug.m3u</code></td></tr>\n    <tr><td align=\"left\">Gen</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/gej.m3u</code></td></tr>\n    <tr><td align=\"left\">Georgian</td><td align=\"right\">13</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kat.m3u</code></td></tr>\n    <tr><td align=\"left\">German</td><td align=\"right\">311</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/deu.m3u</code></td></tr>\n    <tr><td align=\"left\">Gikuyu</td><td align=\"right\">3</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kik.m3u</code></td></tr>\n    <tr><td align=\"left\">Goan Konkani</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/gom.m3u</code></td></tr>\n    <tr><td align=\"left\">Greek</td><td align=\"right\">129</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ell.m3u</code></td></tr>\n    <tr><td align=\"left\">Guadeloupean Creole French</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/gcf.m3u</code></td></tr>\n    <tr><td align=\"left\">Gujarati</td><td align=\"right\">6</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/guj.m3u</code></td></tr>\n    <tr><td align=\"left\">Gun</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/guw.m3u</code></td></tr>\n    <tr><td align=\"left\">Haitian</td><td align=\"right\">5</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/hat.m3u</code></td></tr>\n    <tr><td align=\"left\">Hausa</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/hau.m3u</code></td></tr>\n    <tr><td align=\"left\">Hebrew</td><td align=\"right\">14</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/heb.m3u</code></td></tr>\n    <tr><td align=\"left\">Hindi</td><td align=\"right\">130</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/hin.m3u</code></td></tr>\n    <tr><td align=\"left\">Hmong</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/hmn.m3u</code></td></tr>\n    <tr><td align=\"left\">Hungarian</td><td align=\"right\">105</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/hun.m3u</code></td></tr>\n    <tr><td align=\"left\">Icelandic</td><td align=\"right\">5</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/isl.m3u</code></td></tr>\n    <tr><td align=\"left\">Indonesian</td><td align=\"right\">126</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ind.m3u</code></td></tr>\n    <tr><td align=\"left\">Inuktitut</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/iku.m3u</code></td></tr>\n    <tr><td align=\"left\">Iranian Persian</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/pes.m3u</code></td></tr>\n    <tr><td align=\"left\">Irish</td><td align=\"right\">7</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/gle.m3u</code></td></tr>\n    <tr><td align=\"left\">Isekiri</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/its.m3u</code></td></tr>\n    <tr><td align=\"left\">Italian</td><td align=\"right\">330</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ita.m3u</code></td></tr>\n    <tr><td align=\"left\">Japanese</td><td align=\"right\">16</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/jpn.m3u</code></td></tr>\n    <tr><td align=\"left\">Javanese</td><td align=\"right\">3</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/jav.m3u</code></td></tr>\n    <tr><td align=\"left\">Kabiyè</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kbp.m3u</code></td></tr>\n    <tr><td align=\"left\">Kabyle</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kab.m3u</code></td></tr>\n    <tr><td align=\"left\">Kannada</td><td align=\"right\">11</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kan.m3u</code></td></tr>\n    <tr><td align=\"left\">Kapampangan</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/pam.m3u</code></td></tr>\n    <tr><td align=\"left\">Kazakh</td><td align=\"right\">25</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kaz.m3u</code></td></tr>\n    <tr><td align=\"left\">Khmer</td><td align=\"right\">22</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/khm.m3u</code></td></tr>\n    <tr><td align=\"left\">Khorasani Turkish</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kmz.m3u</code></td></tr>\n    <tr><td align=\"left\">Kinyarwanda</td><td align=\"right\">7</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kin.m3u</code></td></tr>\n    <tr><td align=\"left\">Kirghiz</td><td align=\"right\">13</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kir.m3u</code></td></tr>\n    <tr><td align=\"left\">Kituba (Congo)</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mkw.m3u</code></td></tr>\n    <tr><td align=\"left\">Kongo</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kon.m3u</code></td></tr>\n    <tr><td align=\"left\">Konkani (macrolanguage)</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kok.m3u</code></td></tr>\n    <tr><td align=\"left\">Korean</td><td align=\"right\">95</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kor.m3u</code></td></tr>\n    <tr><td align=\"left\">Kumam</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kdi.m3u</code></td></tr>\n    <tr><td align=\"left\">Kurdish</td><td align=\"right\">35</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/kur.m3u</code></td></tr>\n    <tr><td align=\"left\">Lango (Uganda)</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/laj.m3u</code></td></tr>\n    <tr><td align=\"left\">Lao</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/lao.m3u</code></td></tr>\n    <tr><td align=\"left\">Latin</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/lat.m3u</code></td></tr>\n    <tr><td align=\"left\">Latvian</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/lav.m3u</code></td></tr>\n    <tr><td align=\"left\">Letzeburgesch</td><td align=\"right\">12</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ltz.m3u</code></td></tr>\n    <tr><td align=\"left\">Lingala</td><td align=\"right\">4</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/lin.m3u</code></td></tr>\n    <tr><td align=\"left\">Lithuanian</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/lit.m3u</code></td></tr>\n    <tr><td align=\"left\">Luba-Lulua</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/lua.m3u</code></td></tr>\n    <tr><td align=\"left\">Macedonian</td><td align=\"right\">34</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mkd.m3u</code></td></tr>\n    <tr><td align=\"left\">Malay</td><td align=\"right\">17</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/msa.m3u</code></td></tr>\n    <tr><td align=\"left\">Malayalam</td><td align=\"right\">66</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mal.m3u</code></td></tr>\n    <tr><td align=\"left\">Maltese</td><td align=\"right\">7</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mlt.m3u</code></td></tr>\n    <tr><td align=\"left\">Mandarin Chinese</td><td align=\"right\">3</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/cmn.m3u</code></td></tr>\n    <tr><td align=\"left\">Mandinka</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mnk.m3u</code></td></tr>\n    <tr><td align=\"left\">Maori</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mri.m3u</code></td></tr>\n    <tr><td align=\"left\">Marathi</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mar.m3u</code></td></tr>\n    <tr><td align=\"left\">Min Nan Chinese</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/nan.m3u</code></td></tr>\n    <tr><td align=\"left\">Mongolian</td><td align=\"right\">27</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mon.m3u</code></td></tr>\n    <tr><td align=\"left\">Montenegrin</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/cnr.m3u</code></td></tr>\n    <tr><td align=\"left\">Mycenaean Greek</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/gmy.m3u</code></td></tr>\n    <tr><td align=\"left\">Nepali</td><td align=\"right\">4</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/nep.m3u</code></td></tr>\n    <tr><td align=\"left\">Norwegian</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/nor.m3u</code></td></tr>\n    <tr><td align=\"left\">Norwegian Bokmål</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/nob.m3u</code></td></tr>\n    <tr><td align=\"left\">Nyankole</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/nyn.m3u</code></td></tr>\n    <tr><td align=\"left\">Nyoro</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/nyo.m3u</code></td></tr>\n    <tr><td align=\"left\">Oriya (macrolanguage)</td><td align=\"right\">7</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ori.m3u</code></td></tr>\n    <tr><td align=\"left\">Panjabi</td><td align=\"right\">26</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/pan.m3u</code></td></tr>\n    <tr><td align=\"left\">Papiamento</td><td align=\"right\">18</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/pap.m3u</code></td></tr>\n    <tr><td align=\"left\">Parsi-Dari</td><td align=\"right\">3</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/prd.m3u</code></td></tr>\n    <tr><td align=\"left\">Pashto</td><td align=\"right\">16</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/pus.m3u</code></td></tr>\n    <tr><td align=\"left\">Persian</td><td align=\"right\">158</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/fas.m3u</code></td></tr>\n    <tr><td align=\"left\">Polish</td><td align=\"right\">65</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/pol.m3u</code></td></tr>\n    <tr><td align=\"left\">Portuguese</td><td align=\"right\">245</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/por.m3u</code></td></tr>\n    <tr><td align=\"left\">Pulaar</td><td align=\"right\">3</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/fuc.m3u</code></td></tr>\n    <tr><td align=\"left\">Quechua</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/que.m3u</code></td></tr>\n    <tr><td align=\"left\">Romanian</td><td align=\"right\">129</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ron.m3u</code></td></tr>\n    <tr><td align=\"left\">Romany</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/rom.m3u</code></td></tr>\n    <tr><td align=\"left\">Russian</td><td align=\"right\">309</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/rus.m3u</code></td></tr>\n    <tr><td align=\"left\">Saint Lucian Creole French</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/acf.m3u</code></td></tr>\n    <tr><td align=\"left\">Samoan</td><td align=\"right\">2</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/smo.m3u</code></td></tr>\n    <tr><td align=\"left\">Santali</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/sat.m3u</code></td></tr>\n    <tr><td align=\"left\">Serbian</td><td align=\"right\">47</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/srp.m3u</code></td></tr>\n    <tr><td align=\"left\">Serbo-Croatian</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/hbs.m3u</code></td></tr>\n    <tr><td align=\"left\">Sinhala</td><td align=\"right\">6</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/sin.m3u</code></td></tr>\n    <tr><td align=\"left\">Slovak</td><td align=\"right\">49</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/slk.m3u</code></td></tr>\n    <tr><td align=\"left\">Slovenian</td><td align=\"right\">14</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/slv.m3u</code></td></tr>\n    <tr><td align=\"left\">Somali</td><td align=\"right\">12</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/som.m3u</code></td></tr>\n    <tr><td align=\"left\">South African Sign Language</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/sfs.m3u</code></td></tr>\n    <tr><td align=\"left\">South Ndebele</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/nbl.m3u</code></td></tr>\n    <tr><td align=\"left\">Spanish</td><td align=\"right\">1721</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/spa.m3u</code></td></tr>\n    <tr><td align=\"left\">Swahili</td><td align=\"right\">19</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/swa.m3u</code></td></tr>\n    <tr><td align=\"left\">Swati</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ssw.m3u</code></td></tr>\n    <tr><td align=\"left\">Swedish</td><td align=\"right\">14</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/swe.m3u</code></td></tr>\n    <tr><td align=\"left\">Syriac</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/syr.m3u</code></td></tr>\n    <tr><td align=\"left\">Tachawit</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/shy.m3u</code></td></tr>\n    <tr><td align=\"left\">Tagalog</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tgl.m3u</code></td></tr>\n    <tr><td align=\"left\">Tahitian</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tah.m3u</code></td></tr>\n    <tr><td align=\"left\">Tajik</td><td align=\"right\">15</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tgk.m3u</code></td></tr>\n    <tr><td align=\"left\">Tamashek</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tmh.m3u</code></td></tr>\n    <tr><td align=\"left\">Tamasheq</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/taq.m3u</code></td></tr>\n    <tr><td align=\"left\">Tamil</td><td align=\"right\">50</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tam.m3u</code></td></tr>\n    <tr><td align=\"left\">Tatar</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tat.m3u</code></td></tr>\n    <tr><td align=\"left\">Telugu</td><td align=\"right\">9</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tel.m3u</code></td></tr>\n    <tr><td align=\"left\">Thai</td><td align=\"right\">63</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tha.m3u</code></td></tr>\n    <tr><td align=\"left\">Tibetan</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/bod.m3u</code></td></tr>\n    <tr><td align=\"left\">Tooro</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ttj.m3u</code></td></tr>\n    <tr><td align=\"left\">Tsonga</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tso.m3u</code></td></tr>\n    <tr><td align=\"left\">Tumzabt</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/mzb.m3u</code></td></tr>\n    <tr><td align=\"left\">Turkish</td><td align=\"right\">265</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tur.m3u</code></td></tr>\n    <tr><td align=\"left\">Turkmen</td><td align=\"right\">8</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/tuk.m3u</code></td></tr>\n    <tr><td align=\"left\">Uighur</td><td align=\"right\">3</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/uig.m3u</code></td></tr>\n    <tr><td align=\"left\">Ukrainian</td><td align=\"right\">61</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ukr.m3u</code></td></tr>\n    <tr><td align=\"left\">Urdu</td><td align=\"right\">41</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/urd.m3u</code></td></tr>\n    <tr><td align=\"left\">Uzbek</td><td align=\"right\">24</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/uzb.m3u</code></td></tr>\n    <tr><td align=\"left\">Venda</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/ven.m3u</code></td></tr>\n    <tr><td align=\"left\">Vietnamese</td><td align=\"right\">69</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/vie.m3u</code></td></tr>\n    <tr><td align=\"left\">Welsh</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/cym.m3u</code></td></tr>\n    <tr><td align=\"left\">Western Frisian</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/fry.m3u</code></td></tr>\n    <tr><td align=\"left\">Wolof</td><td align=\"right\">10</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/wol.m3u</code></td></tr>\n    <tr><td align=\"left\">Xhosa</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/xho.m3u</code></td></tr>\n    <tr><td align=\"left\">Yakut</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/sah.m3u</code></td></tr>\n    <tr><td align=\"left\">Yoruba</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/yor.m3u</code></td></tr>\n    <tr><td align=\"left\">Yucatec Maya</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/yua.m3u</code></td></tr>\n    <tr><td align=\"left\">Yue Chinese</td><td align=\"right\">5</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/yue.m3u</code></td></tr>\n    <tr><td align=\"left\">Zarma</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/dje.m3u</code></td></tr>\n    <tr><td align=\"left\">Zulu</td><td align=\"right\">1</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/zul.m3u</code></td></tr>\n    <tr><td align=\"left\">Undefined</td><td align=\"right\">2348</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/languages/undefined.m3u</code></td></tr>\n  </tbody>\n</table>\n\n</details>\n\n### Grouped by country\n\nPlaylists in which channels are grouped by country for which they are broadcasted.\n\n<details>\n<summary>Expand</summary>\n<br>\n\n```\nhttps://iptv-org.github.io/iptv/index.country.m3u\n```\n\nSame thing, but split up into separate files:\n\n<!-- prettier-ignore -->\n<table>\n  <thead>\n    <tr><th align=\"left\">Country</th><th align=\"left\">Channels</th><th align=\"left\">Playlist</th></tr>\n  </thead>\n  <tbody>\n    <tr><td>🇦🇫 Afghanistan</td><td align=\"right\">38</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/af.m3u</code></td></tr>\n    <tr><td>🇦🇱 Albania</td><td align=\"right\">60</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/al.m3u</code></td></tr>\n    <tr><td>🇩🇿 Algeria</td><td align=\"right\">77</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/dz.m3u</code></td></tr>\n    <tr><td>🇦🇸 American Samoa</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/as.m3u</code></td></tr>\n    <tr><td>🇦🇩 Andorra</td><td align=\"right\">42</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ad.m3u</code></td></tr>\n    <tr><td>🇦🇴 Angola</td><td align=\"right\">30</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ao.m3u</code></td></tr>\n    <tr><td>🇦🇮 Anguilla</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ai.m3u</code></td></tr>\n    <tr><td>🇦🇬 Antigua and Barbuda</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ag.m3u</code></td></tr>\n    <tr><td>🇦🇷 Argentina</td><td align=\"right\">142</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ar.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Buenos Aires</td><td align=\"right\">11</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-b.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Catamarca</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-k.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chaco</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-h.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chubut</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-u.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ciudad Autonoma de Buenos Aires</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-c.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cordoba</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-x.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Corrientes</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-w.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Entre Rios</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-e.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Formosa</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-p.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jujuy</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-y.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La Pampa</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-l.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La Rioja</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-f.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Misiones</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-n.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Neuquen</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-q.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Salta</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-a.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;San Juan</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-j.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Santa Cruz</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-z.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Santa Fe</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-s.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Santiago del Estero</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-g.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tucuman</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ar-t.m3u</code></td></tr>\n    <tr><td>🇦🇲 Armenia</td><td align=\"right\">67</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/am.m3u</code></td></tr>\n    <tr><td>🇦🇼 Aruba</td><td align=\"right\">25</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/aw.m3u</code></td></tr>\n    <tr><td>🇦🇺 Australia</td><td align=\"right\">65</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/au.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;New South Wales</td><td align=\"right\">10</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/au-nsw.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queensland</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/au-qld.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;South Australia</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/au-sa.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tasmania</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/au-tas.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Victoria</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/au-vic.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Western Australia</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/au-wa.m3u</code></td></tr>\n    <tr><td>🇦🇹 Austria</td><td align=\"right\">89</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/at.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Karnten</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/at-2.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Steiermark</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/at-6.m3u</code></td></tr>\n    <tr><td>🇦🇿 Azerbaijan</td><td align=\"right\">58</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/az.m3u</code></td></tr>\n    <tr><td>🇧🇸 Bahamas</td><td align=\"right\">19</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bs.m3u</code></td></tr>\n    <tr><td>🇧🇭 Bahrain</td><td align=\"right\">50</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bh.m3u</code></td></tr>\n    <tr><td>🇧🇩 Bangladesh</td><td align=\"right\">25</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bd.m3u</code></td></tr>\n    <tr><td>🇧🇧 Barbados</td><td align=\"right\">17</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bb.m3u</code></td></tr>\n    <tr><td>🇧🇾 Belarus</td><td align=\"right\">58</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/by.m3u</code></td></tr>\n    <tr><td>🇧🇪 Belgium</td><td align=\"right\">78</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/be.m3u</code></td></tr>\n    <tr><td>🇧🇿 Belize</td><td align=\"right\">9</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bz.m3u</code></td></tr>\n    <tr><td>🇧🇯 Benin</td><td align=\"right\">34</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bj.m3u</code></td></tr>\n    <tr><td>🇧🇲 Bermuda</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bm.m3u</code></td></tr>\n    <tr><td>🇧🇹 Bhutan</td><td align=\"right\">11</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bt.m3u</code></td></tr>\n    <tr><td>🇧🇴 Bolivia</td><td align=\"right\">86</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bo.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cochabamba</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/bo-c.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Oruro</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/bo-o.m3u</code></td></tr>\n    <tr><td>🇧🇶 Bonaire</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bq.m3u</code></td></tr>\n    <tr><td>🇧🇦 Bosnia and Herzegovina</td><td align=\"right\">54</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ba.m3u</code></td></tr>\n    <tr><td>🇧🇼 Botswana</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bw.m3u</code></td></tr>\n    <tr><td>🇧🇻 Bouvet Island</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bv.m3u</code></td></tr>\n    <tr><td>🇧🇷 Brazil</td><td align=\"right\">213</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/br.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Alagoas</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-al.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Amazonas</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-am.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bahia</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-ba.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ceara</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-ce.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Distrito Federal</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-df.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Espirito Santo</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-es.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Goias</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-go.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Maranhao</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-ma.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mato Grosso</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-mt.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Minas Gerais</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-mg.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Para</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-pa.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Paraiba</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-pb.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Parana</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-pr.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pernambuco</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-pe.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rio de Janeiro</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-rj.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rio Grande do Norte</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-rn.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rio Grande do Sul</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-rs.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rondonia</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-ro.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Santa Catarina</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-sc.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sao Paulo</td><td align=\"right\">18</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/br-sp.m3u</code></td></tr>\n    <tr><td>🇻🇬 British Virgin Islands</td><td align=\"right\">18</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/vg.m3u</code></td></tr>\n    <tr><td>🇧🇳 Brunei</td><td align=\"right\">25</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bn.m3u</code></td></tr>\n    <tr><td>🇧🇬 Bulgaria</td><td align=\"right\">69</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bg.m3u</code></td></tr>\n    <tr><td>🇧🇫 Burkina Faso</td><td align=\"right\">28</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bf.m3u</code></td></tr>\n    <tr><td>🇧🇮 Burundi</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bi.m3u</code></td></tr>\n    <tr><td>🇰🇭 Cambodia</td><td align=\"right\">43</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/kh.m3u</code></td></tr>\n    <tr><td>🇨🇲 Cameroon</td><td align=\"right\">35</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cm.m3u</code></td></tr>\n    <tr><td>🇨🇦 Canada</td><td align=\"right\">183</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ca.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Alberta</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-ab.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;British Columbia</td><td align=\"right\">6</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-bc.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Manitoba</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-mb.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;New Brunswick</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-nb.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Newfoundland and Labrador</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-nl.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Northwest Territories</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-nt.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nova Scotia</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-ns.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nunavut</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-nu.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ontario</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-on.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prince Edward Island</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-pe.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Quebec</td><td align=\"right\">15</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-qc.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Saskatchewan</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ca-sk.m3u</code></td></tr>\n    <tr><td>🇨🇻 Cape Verde</td><td align=\"right\">31</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cv.m3u</code></td></tr>\n    <tr><td>🇰🇾 Cayman Islands</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ky.m3u</code></td></tr>\n    <tr><td>🇨🇫 Central African Republic</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cf.m3u</code></td></tr>\n    <tr><td>🇹🇩 Chad</td><td align=\"right\">27</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/td.m3u</code></td></tr>\n    <tr><td>🇨🇱 Chile</td><td align=\"right\">133</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cl.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La Araucania</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/cl-ar.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Libertador General Bernardo O'Higgins</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/cl-li.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nuble</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/cl-nb.m3u</code></td></tr>\n    <tr><td>🇨🇳 China</td><td align=\"right\">176</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cn.m3u</code></td></tr>\n    <tr><td>🇨🇴 Colombia</td><td align=\"right\">132</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/co.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Antioquia</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-ant.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Atlantico</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-atl.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bolivar</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-bol.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Caldas</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-cal.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cauca</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-cau.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Choco</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-cho.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Huila</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-hui.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Narino</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-nar.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Quindio</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-qui.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Risaralda</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-ris.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;San Andres, Providencia y Santa Catalina</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-sap.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Valle del Cauca</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/co-vac.m3u</code></td></tr>\n    <tr><td>🇰🇲 Comoros</td><td align=\"right\">66</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/km.m3u</code></td></tr>\n    <tr><td>🇨🇰 Cook Islands</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ck.m3u</code></td></tr>\n    <tr><td>🇨🇷 Costa Rica</td><td align=\"right\">137</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cr.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Puntarenas</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/cr-p.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;San Jose</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/cr-sj.m3u</code></td></tr>\n    <tr><td>🇭🇷 Croatia</td><td align=\"right\">69</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/hr.m3u</code></td></tr>\n    <tr><td>🇨🇺 Cuba</td><td align=\"right\">67</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cu.m3u</code></td></tr>\n    <tr><td>🇨🇼 Curacao</td><td align=\"right\">22</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cw.m3u</code></td></tr>\n    <tr><td>🇨🇾 Cyprus</td><td align=\"right\">80</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cy.m3u</code></td></tr>\n    <tr><td>🇨🇿 Czech Republic</td><td align=\"right\">82</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cz.m3u</code></td></tr>\n    <tr><td>🇨🇩 Democratic Republic of the Congo</td><td align=\"right\">48</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cd.m3u</code></td></tr>\n    <tr><td>🇩🇰 Denmark</td><td align=\"right\">65</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/dk.m3u</code></td></tr>\n    <tr><td>🇩🇯 Djibouti</td><td align=\"right\">69</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/dj.m3u</code></td></tr>\n    <tr><td>🇩🇲 Dominica</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/dm.m3u</code></td></tr>\n    <tr><td>🇩🇴 Dominican Republic</td><td align=\"right\">257</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/do.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Distrito Nacional (Santo Domingo)</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/do-01.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;El Seibo</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/do-08.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La Altagracia</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/do-11.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La Vega</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/do-13.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Monsenor Nouel</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/do-28.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Puerto Plata</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/do-18.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;San Juan</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/do-22.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Santiago</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/do-25.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Valverde</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/do-27.m3u</code></td></tr>\n    <tr><td>🇹🇱 East Timor</td><td align=\"right\">18</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tl.m3u</code></td></tr>\n    <tr><td>🇪🇨 Ecuador</td><td align=\"right\">137</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ec.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Azuay</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ec-a.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Loja</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ec-l.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Orellana</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ec-d.m3u</code></td></tr>\n    <tr><td>🇪🇬 Egypt</td><td align=\"right\">100</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/eg.m3u</code></td></tr>\n    <tr><td>🇸🇻 El Salvador</td><td align=\"right\">99</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sv.m3u</code></td></tr>\n    <tr><td>🇬🇶 Equatorial Guinea</td><td align=\"right\">28</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gq.m3u</code></td></tr>\n    <tr><td>🇪🇷 Eritrea</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/er.m3u</code></td></tr>\n    <tr><td>🇪🇪 Estonia</td><td align=\"right\">73</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ee.m3u</code></td></tr>\n    <tr><td>🇪🇹 Ethiopia</td><td align=\"right\">31</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/et.m3u</code></td></tr>\n    <tr><td>🇫🇰 Falkland Islands</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/fk.m3u</code></td></tr>\n    <tr><td>🇫🇴 Faroe Islands</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/fo.m3u</code></td></tr>\n    <tr><td>🇫🇯 Fiji</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/fj.m3u</code></td></tr>\n    <tr><td>🇫🇮 Finland</td><td align=\"right\">76</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/fi.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Keski-Suomi</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/fi-08.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pohjanmaa</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/fi-12.m3u</code></td></tr>\n    <tr><td>🇫🇷 France</td><td align=\"right\">219</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/fr.m3u</code></td></tr>\n    <tr><td>🇬🇫 French Guiana</td><td align=\"right\">19</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gf.m3u</code></td></tr>\n    <tr><td>🇵🇫 French Polynesia</td><td align=\"right\">9</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pf.m3u</code></td></tr>\n    <tr><td>🇹🇫 French Southern Territories</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tf.m3u</code></td></tr>\n    <tr><td>🇬🇦 Gabon</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ga.m3u</code></td></tr>\n    <tr><td>🇬🇲 Gambia</td><td align=\"right\">28</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gm.m3u</code></td></tr>\n    <tr><td>🇬🇪 Georgia</td><td align=\"right\">50</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ge.m3u</code></td></tr>\n    <tr><td>🇩🇪 Germany</td><td align=\"right\">305</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/de.m3u</code></td></tr>\n    <tr><td>🇬🇭 Ghana</td><td align=\"right\">48</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gh.m3u</code></td></tr>\n    <tr><td>🇬🇷 Greece</td><td align=\"right\">157</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gr.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Attiki</td><td align=\"right\">9</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gr-i.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dytiki Ellada</td><td align=\"right\">6</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gr-g.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dytiki Makedonia</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gr-c.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ionia Nisia</td><td align=\"right\">6</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gr-f.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ipeiros</td><td align=\"right\">6</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gr-d.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kentriki Makedonia</td><td align=\"right\">6</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gr-b.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Notio Aigaio</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gr-l.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sterea Ellada</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gr-h.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Thessalia</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gr-e.m3u</code></td></tr>\n    <tr><td>🇬🇱 Greenland</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gl.m3u</code></td></tr>\n    <tr><td>🇬🇩 Grenada</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gd.m3u</code></td></tr>\n    <tr><td>🇬🇵 Guadeloupe</td><td align=\"right\">31</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gp.m3u</code></td></tr>\n    <tr><td>🇬🇺 Guam</td><td align=\"right\">10</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gu.m3u</code></td></tr>\n    <tr><td>🇬🇹 Guatemala</td><td align=\"right\">147</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gt.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Escuintla</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gt-05.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Huehuetenango</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gt-13.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Izabal</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gt-18.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Quiche</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gt-14.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sacatepequez</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gt-03.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;San Marcos</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gt-12.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Santa Rosa</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gt-06.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Solola</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gt-07.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Totonicapan</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/gt-08.m3u</code></td></tr>\n    <tr><td>🇬🇬 Guernsey</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gg.m3u</code></td></tr>\n    <tr><td>🇬🇳 Guinea</td><td align=\"right\">31</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gn.m3u</code></td></tr>\n    <tr><td>🇬🇼 Guinea-Bissau</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gw.m3u</code></td></tr>\n    <tr><td>🇬🇾 Guyana</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gy.m3u</code></td></tr>\n    <tr><td>🇭🇹 Haiti</td><td align=\"right\">49</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ht.m3u</code></td></tr>\n    <tr><td>🇭🇳 Honduras</td><td align=\"right\">138</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/hn.m3u</code></td></tr>\n    <tr><td>🇭🇰 Hong Kong</td><td align=\"right\">12</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/hk.m3u</code></td></tr>\n    <tr><td>🇭🇺 Hungary</td><td align=\"right\">146</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/hu.m3u</code></td></tr>\n    <tr><td>🇮🇸 Iceland</td><td align=\"right\">41</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/is.m3u</code></td></tr>\n    <tr><td>🇮🇳 India</td><td align=\"right\">328</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/in.m3u</code></td></tr>\n    <tr><td>🇮🇩 Indonesia</td><td align=\"right\">150</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/id.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Aceh</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-ac.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bali</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-ba.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Banten</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-bt.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bengkulu</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-be.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gorontalo</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-go.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jakarta Raya</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-jk.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jambi</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-ja.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jawa Barat</td><td align=\"right\">10</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-jb.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jawa Tengah</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-jt.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jawa Timur</td><td align=\"right\">9</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-ji.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kalimantan Barat</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-kb.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kalimantan Selatan</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-ks.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kalimantan Tengah</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-kt.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kalimantan Timur</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-ki.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kepulauan Bangka Belitung</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-bb.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lampung</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-la.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Maluku</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-ml.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Maluku Utara</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-mu.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nusa Tenggara Barat</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-nb.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nusa Tenggara Timur</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-nt.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Papua</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-pp.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Riau</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-ri.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sulawesi Barat</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-sr.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sulawesi Selatan</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-sn.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sulawesi Tengah</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-st.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sulawesi Tenggara</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-sg.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sumatera Barat</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-sb.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sumatera Selatan</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-ss.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Yogyakarta</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/id-yo.m3u</code></td></tr>\n    <tr><td>🇮🇷 Iran</td><td align=\"right\">145</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ir.m3u</code></td></tr>\n    <tr><td>🇮🇶 Iraq</td><td align=\"right\">117</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/iq.m3u</code></td></tr>\n    <tr><td>🇮🇪 Ireland</td><td align=\"right\">63</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ie.m3u</code></td></tr>\n    <tr><td>🇮🇱 Israel</td><td align=\"right\">23</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/il.m3u</code></td></tr>\n    <tr><td>🇮🇹 Italy</td><td align=\"right\">373</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/it.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Trentino-Alto Adige</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/it-32.m3u</code></td></tr>\n    <tr><td>🇨🇮 Ivory Coast</td><td align=\"right\">47</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ci.m3u</code></td></tr>\n    <tr><td>🇯🇲 Jamaica</td><td align=\"right\">23</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/jm.m3u</code></td></tr>\n    <tr><td>🇯🇵 Japan</td><td align=\"right\">19</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/jp.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chiba</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/jp-12.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kanagawa</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/jp-14.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Saitama</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/jp-11.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tokyo</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/jp-13.m3u</code></td></tr>\n    <tr><td>🇯🇴 Jordan</td><td align=\"right\">72</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/jo.m3u</code></td></tr>\n    <tr><td>🇰🇿 Kazakhstan</td><td align=\"right\">75</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/kz.m3u</code></td></tr>\n    <tr><td>🇰🇪 Kenya</td><td align=\"right\">66</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ke.m3u</code></td></tr>\n    <tr><td>🇰🇮 Kiribati</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ki.m3u</code></td></tr>\n    <tr><td>🇽🇰 Kosovo</td><td align=\"right\">28</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/xk.m3u</code></td></tr>\n    <tr><td>🇰🇼 Kuwait</td><td align=\"right\">57</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/kw.m3u</code></td></tr>\n    <tr><td>🇰🇬 Kyrgyzstan</td><td align=\"right\">25</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/kg.m3u</code></td></tr>\n    <tr><td>🇱🇦 Laos</td><td align=\"right\">32</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/la.m3u</code></td></tr>\n    <tr><td>🇱🇻 Latvia</td><td align=\"right\">60</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/lv.m3u</code></td></tr>\n    <tr><td>🇱🇧 Lebanon</td><td align=\"right\">74</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/lb.m3u</code></td></tr>\n    <tr><td>🇱🇸 Lesotho</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ls.m3u</code></td></tr>\n    <tr><td>🇱🇷 Liberia</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/lr.m3u</code></td></tr>\n    <tr><td>🇱🇾 Libya</td><td align=\"right\">75</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ly.m3u</code></td></tr>\n    <tr><td>🇱🇮 Liechtenstein</td><td align=\"right\">37</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/li.m3u</code></td></tr>\n    <tr><td>🇱🇹 Lithuania</td><td align=\"right\">55</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/lt.m3u</code></td></tr>\n    <tr><td>🇱🇺 Luxembourg</td><td align=\"right\">64</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/lu.m3u</code></td></tr>\n    <tr><td>🇲🇴 Macao</td><td align=\"right\">7</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mo.m3u</code></td></tr>\n    <tr><td>🇲🇬 Madagascar</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mg.m3u</code></td></tr>\n    <tr><td>🇲🇼 Malawi</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mw.m3u</code></td></tr>\n    <tr><td>🇲🇾 Malaysia</td><td align=\"right\">41</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/my.m3u</code></td></tr>\n    <tr><td>🇲🇻 Maldives</td><td align=\"right\">21</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mv.m3u</code></td></tr>\n    <tr><td>🇲🇱 Mali</td><td align=\"right\">32</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ml.m3u</code></td></tr>\n    <tr><td>🇲🇹 Malta</td><td align=\"right\">52</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mt.m3u</code></td></tr>\n    <tr><td>🇲🇭 Marshall Islands</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mh.m3u</code></td></tr>\n    <tr><td>🇲🇶 Martinique</td><td align=\"right\">29</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mq.m3u</code></td></tr>\n    <tr><td>🇲🇷 Mauritania</td><td align=\"right\">67</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mr.m3u</code></td></tr>\n    <tr><td>🇲🇺 Mauritius</td><td align=\"right\">27</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mu.m3u</code></td></tr>\n    <tr><td>🇾🇹 Mayotte</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/yt.m3u</code></td></tr>\n    <tr><td>🇲🇽 Mexico</td><td align=\"right\">256</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mx.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chihuahua</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-chh.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Coahuila de Zaragoza</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-coa.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Durango</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-dur.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Morelos</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-mor.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nuevo Leon</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-nle.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Puebla</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-pue.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queretaro</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-que.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Quintana Roo</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-roo.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;San Luis Potosi</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-slp.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Yucatan</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/mx-yuc.m3u</code></td></tr>\n    <tr><td>🇫🇲 Micronesia</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/fm.m3u</code></td></tr>\n    <tr><td>🇲🇩 Moldova</td><td align=\"right\">74</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/md.m3u</code></td></tr>\n    <tr><td>🇲🇨 Monaco</td><td align=\"right\">41</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mc.m3u</code></td></tr>\n    <tr><td>🇲🇳 Mongolia</td><td align=\"right\">29</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mn.m3u</code></td></tr>\n    <tr><td>🇲🇪 Montenegro</td><td align=\"right\">44</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/me.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ulcinj</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/me-20.m3u</code></td></tr>\n    <tr><td>🇲🇸 Montserrat</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ms.m3u</code></td></tr>\n    <tr><td>🇲🇦 Morocco</td><td align=\"right\">78</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ma.m3u</code></td></tr>\n    <tr><td>🇲🇿 Mozambique</td><td align=\"right\">30</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mz.m3u</code></td></tr>\n    <tr><td>🇲🇲 Myanmar</td><td align=\"right\">30</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mm.m3u</code></td></tr>\n    <tr><td>🇳🇦 Namibia</td><td align=\"right\">27</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/na.m3u</code></td></tr>\n    <tr><td>🇳🇷 Nauru</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/nr.m3u</code></td></tr>\n    <tr><td>🇳🇵 Nepal</td><td align=\"right\">18</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/np.m3u</code></td></tr>\n    <tr><td>🇳🇱 Netherlands</td><td align=\"right\">200</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/nl.m3u</code></td></tr>\n    <tr><td>🇳🇨 New Caledonia</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/nc.m3u</code></td></tr>\n    <tr><td>🇳🇿 New Zealand</td><td align=\"right\">28</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/nz.m3u</code></td></tr>\n    <tr><td>🇳🇮 Nicaragua</td><td align=\"right\">81</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ni.m3u</code></td></tr>\n    <tr><td>🇳🇪 Niger</td><td align=\"right\">28</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ne.m3u</code></td></tr>\n    <tr><td>🇳🇬 Nigeria</td><td align=\"right\">86</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ng.m3u</code></td></tr>\n    <tr><td>🇳🇺 Niue</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/nu.m3u</code></td></tr>\n    <tr><td>🇳🇫 Norfolk Island</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/nf.m3u</code></td></tr>\n    <tr><td>🇰🇵 North Korea</td><td align=\"right\">10</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/kp.m3u</code></td></tr>\n    <tr><td>🇲🇰 North Macedonia</td><td align=\"right\">73</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mk.m3u</code></td></tr>\n    <tr><td>🇲🇵 Northern Mariana Islands</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mp.m3u</code></td></tr>\n    <tr><td>🇳🇴 Norway</td><td align=\"right\">47</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/no.m3u</code></td></tr>\n    <tr><td>🇴🇲 Oman</td><td align=\"right\">51</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/om.m3u</code></td></tr>\n    <tr><td>🇵🇰 Pakistan</td><td align=\"right\">45</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pk.m3u</code></td></tr>\n    <tr><td>🇵🇼 Palau</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pw.m3u</code></td></tr>\n    <tr><td>🇵🇸 Palestine</td><td align=\"right\">67</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ps.m3u</code></td></tr>\n    <tr><td>🇵🇦 Panama</td><td align=\"right\">83</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pa.m3u</code></td></tr>\n    <tr><td>🇵🇬 Papua New Guinea</td><td align=\"right\">9</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pg.m3u</code></td></tr>\n    <tr><td>🇵🇾 Paraguay</td><td align=\"right\">119</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/py.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Alto Parana</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/py-10.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Boqueron</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/py-19.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Caaguazu</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/py-5.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Central</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/py-11.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Itapua</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/py-7.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Presidente Hayes</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/py-15.m3u</code></td></tr>\n    <tr><td>🇵🇪 Peru</td><td align=\"right\">136</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pe.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Apurimac</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-apu.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Arequipa</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-are.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cusco</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-cus.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Junin</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-jun.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La Libertad</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-lal.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lima</td><td align=\"right\">6</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-lim.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Loreto</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-lor.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Moquegua</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-moq.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Puno</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-pun.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;San Martin</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/pe-sam.m3u</code></td></tr>\n    <tr><td>🇵🇭 Philippines</td><td align=\"right\">34</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ph.m3u</code></td></tr>\n    <tr><td>🇵🇳 Pitcairn Islands</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pn.m3u</code></td></tr>\n    <tr><td>🇵🇱 Poland</td><td align=\"right\">109</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pl.m3u</code></td></tr>\n    <tr><td>🇵🇹 Portugal</td><td align=\"right\">93</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pt.m3u</code></td></tr>\n    <tr><td>🇵🇷 Puerto Rico</td><td align=\"right\">99</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pr.m3u</code></td></tr>\n    <tr><td>🇶🇦 Qatar</td><td align=\"right\">58</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/qa.m3u</code></td></tr>\n    <tr><td>🇨🇬 Republic of the Congo</td><td align=\"right\">31</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/cg.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Brazzaville</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/cg-bzv.m3u</code></td></tr>\n    <tr><td>🇷🇪 Reunion</td><td align=\"right\">27</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/re.m3u</code></td></tr>\n    <tr><td>🇷🇴 Romania</td><td align=\"right\">152</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ro.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gorj</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ro-gj.m3u</code></td></tr>\n    <tr><td>🇷🇺 Russia</td><td align=\"right\">271</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ru.m3u</code></td></tr>\n    <tr><td>🇷🇼 Rwanda</td><td align=\"right\">34</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/rw.m3u</code></td></tr>\n    <tr><td>🇧🇱 Saint Barthélemy</td><td align=\"right\">25</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/bl.m3u</code></td></tr>\n    <tr><td>🇸🇭 Saint Helena</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sh.m3u</code></td></tr>\n    <tr><td>🇰🇳 Saint Kitts and Nevis</td><td align=\"right\">17</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/kn.m3u</code></td></tr>\n    <tr><td>🇱🇨 Saint Lucia</td><td align=\"right\">17</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/lc.m3u</code></td></tr>\n    <tr><td>🇲🇫 Saint Martin</td><td align=\"right\">25</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/mf.m3u</code></td></tr>\n    <tr><td>🇵🇲 Saint Pierre and Miquelon</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/pm.m3u</code></td></tr>\n    <tr><td>🇻🇨 Saint Vincent and the Grenadines</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/vc.m3u</code></td></tr>\n    <tr><td>🇼🇸 Samoa</td><td align=\"right\">10</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ws.m3u</code></td></tr>\n    <tr><td>🇸🇲 San Marino</td><td align=\"right\">38</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sm.m3u</code></td></tr>\n    <tr><td>🇸🇹 Sao Tome and Principe</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/st.m3u</code></td></tr>\n    <tr><td>🇸🇦 Saudi Arabia</td><td align=\"right\">94</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sa.m3u</code></td></tr>\n    <tr><td>🇸🇳 Senegal</td><td align=\"right\">52</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sn.m3u</code></td></tr>\n    <tr><td>🇷🇸 Serbia</td><td align=\"right\">79</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/rs.m3u</code></td></tr>\n    <tr><td>🇸🇨 Seychelles</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sc.m3u</code></td></tr>\n    <tr><td>🇸🇱 Sierra Leone</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sl.m3u</code></td></tr>\n    <tr><td>🇸🇬 Singapore</td><td align=\"right\">30</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sg.m3u</code></td></tr>\n    <tr><td>🇸🇽 Sint Maarten</td><td align=\"right\">24</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sx.m3u</code></td></tr>\n    <tr><td>🇸🇰 Slovakia</td><td align=\"right\">101</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sk.m3u</code></td></tr>\n    <tr><td>🇸🇮 Slovenia</td><td align=\"right\">64</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/si.m3u</code></td></tr>\n    <tr><td>🇸🇧 Solomon Islands</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sb.m3u</code></td></tr>\n    <tr><td>🇸🇴 Somalia</td><td align=\"right\">75</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/so.m3u</code></td></tr>\n    <tr><td>🇿🇦 South Africa</td><td align=\"right\">66</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/za.m3u</code></td></tr>\n    <tr><td>🇬🇸 South Georgia and the South Sandwich Islands</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/gs.m3u</code></td></tr>\n    <tr><td>🇰🇷 South Korea</td><td align=\"right\">98</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/kr.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Busan-gwangyeoksi</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-26.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chungcheongbuk-do</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-43.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Daegu-gwangyeoksi</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-27.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Daejeon-gwangyeoksi</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-30.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gangwon-do</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-42.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gyeonggi-do</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-41.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gyeongsangbuk-do</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-47.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gyeongsangnam-do</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-48.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jeju-teukbyeoljachido</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-49.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jeollabuk-do</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-45.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jeollanam-do</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-46.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Seoul-teukbyeolsi</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-11.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ulsan-gwangyeoksi</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/kr-31.m3u</code></td></tr>\n    <tr><td>🇸🇸 South Sudan</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ss.m3u</code></td></tr>\n    <tr><td>🇪🇸 Spain</td><td align=\"right\">380</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/es.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Andalucia</td><td align=\"right\">38</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-an.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Aragon</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-ar.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Asturias, Principado de</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-as.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Canarias</td><td align=\"right\">11</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-cn.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Castilla y Leon</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-cl.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Castilla-La Mancha</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-cm.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Catalunya</td><td align=\"right\">37</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-ct.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ceuta</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-ce.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Extremadura</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-ex.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Galicia</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-ga.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Illes Balears</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-ib.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La Rioja</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-ri.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Madrid, Comunidad de</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-md.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Murcia, Region de</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-mc.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Navarra, Comunidad Foral de</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-nc.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pais Vasco</td><td align=\"right\">6</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-pv.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Valenciana, Comunidad</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/es-vc.m3u</code></td></tr>\n    <tr><td>🇱🇰 Sri Lanka</td><td align=\"right\">20</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/lk.m3u</code></td></tr>\n    <tr><td>🇸🇩 Sudan</td><td align=\"right\">71</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sd.m3u</code></td></tr>\n    <tr><td>🇸🇷 Suriname</td><td align=\"right\">7</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sr.m3u</code></td></tr>\n    <tr><td>🇸🇿 Swaziland</td><td align=\"right\">27</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sz.m3u</code></td></tr>\n    <tr><td>🇸🇪 Sweden</td><td align=\"right\">64</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/se.m3u</code></td></tr>\n    <tr><td>🇨🇭 Switzerland</td><td align=\"right\">85</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ch.m3u</code></td></tr>\n    <tr><td>🇸🇾 Syria</td><td align=\"right\">50</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/sy.m3u</code></td></tr>\n    <tr><td>🇹🇼 Taiwan</td><td align=\"right\">35</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tw.m3u</code></td></tr>\n    <tr><td>🇹🇯 Tajikistan</td><td align=\"right\">32</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tj.m3u</code></td></tr>\n    <tr><td>🇹🇿 Tanzania</td><td align=\"right\">31</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tz.m3u</code></td></tr>\n    <tr><td>🇹🇭 Thailand</td><td align=\"right\">83</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/th.m3u</code></td></tr>\n    <tr><td>🇹🇬 Togo</td><td align=\"right\">38</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tg.m3u</code></td></tr>\n    <tr><td>🇹🇰 Tokelau</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tk.m3u</code></td></tr>\n    <tr><td>🇹🇴 Tonga</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/to.m3u</code></td></tr>\n    <tr><td>🇹🇹 Trinidad and Tobago</td><td align=\"right\">19</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tt.m3u</code></td></tr>\n    <tr><td>🇹🇳 Tunisia</td><td align=\"right\">71</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tn.m3u</code></td></tr>\n    <tr><td>🇹🇷 Turkey</td><td align=\"right\">292</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tr.m3u</code></td></tr>\n    <tr><td>🇹🇲 Turkmenistan</td><td align=\"right\">17</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tm.m3u</code></td></tr>\n    <tr><td>🇹🇨 Turks and Caicos Islands</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tc.m3u</code></td></tr>\n    <tr><td>🇹🇻 Tuvalu</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/tv.m3u</code></td></tr>\n    <tr><td>🇻🇮 U.S. Virgin Islands</td><td align=\"right\">16</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/vi.m3u</code></td></tr>\n    <tr><td>🇺🇬 Uganda</td><td align=\"right\">55</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ug.m3u</code></td></tr>\n    <tr><td>🇺🇦 Ukraine</td><td align=\"right\">102</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ua.m3u</code></td></tr>\n    <tr><td>🇦🇪 United Arab Emirates</td><td align=\"right\">92</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ae.m3u</code></td></tr>\n    <tr><td>🇬🇧 United Kingdom</td><td align=\"right\">226</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/uk.m3u</code></td></tr>\n    <tr><td>🇺🇸 United States</td><td align=\"right\">1474</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/us.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Alabama</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-al.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Alaska</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ak.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Arizona</td><td align=\"right\">12</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-az.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Arkansas</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ar.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;California</td><td align=\"right\">144</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ca.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Colorado</td><td align=\"right\">14</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-co.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Connecticut</td><td align=\"right\">20</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ct.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Delaware</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-de.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;District of Columbia</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-dc.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Florida</td><td align=\"right\">47</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-fl.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Georgia</td><td align=\"right\">11</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ga.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hawaii</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-hi.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Idaho</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-id.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Illinois</td><td align=\"right\">7</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-il.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Indiana</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-in.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Iowa</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ia.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kansas</td><td align=\"right\">14</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ks.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kentucky</td><td align=\"right\">7</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ky.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Louisiana</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-la.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Maine</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-me.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Maryland</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-md.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Massachusetts</td><td align=\"right\">11</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ma.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Michigan</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-mi.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Minnesota</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-mn.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mississippi</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ms.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Missouri</td><td align=\"right\">28</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-mo.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Montana</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-mt.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nebraska</td><td align=\"right\">6</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ne.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nevada</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-nv.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;New Hampshire</td><td align=\"right\">7</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-nh.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;New Jersey</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-nj.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;New Mexico</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-nm.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;New York</td><td align=\"right\">22</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ny.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;North Carolina</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-nc.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;North Dakota</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-nd.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ohio</td><td align=\"right\">7</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-oh.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Oklahoma</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ok.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Oregon</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-or.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pennsylvania</td><td align=\"right\">12</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-pa.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rhode Island</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ri.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;South Carolina</td><td align=\"right\">3</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-sc.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tennessee</td><td align=\"right\">5</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-tn.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Texas</td><td align=\"right\">21</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-tx.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Utah</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-ut.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Virginia</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-va.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Washington</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-wa.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Wisconsin</td><td align=\"right\">4</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/us-wi.m3u</code></td></tr>\n    <tr><td>🇺🇾 Uruguay</td><td align=\"right\">63</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/uy.m3u</code></td></tr>\n    <tr><td>🇺🇿 Uzbekistan</td><td align=\"right\">42</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/uz.m3u</code></td></tr>\n    <tr><td>🇻🇺 Vanuatu</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/vu.m3u</code></td></tr>\n    <tr><td>🇻🇦 Vatican City</td><td align=\"right\">37</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/va.m3u</code></td></tr>\n    <tr><td>🇻🇪 Venezuela</td><td align=\"right\">115</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ve.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Aragua</td><td align=\"right\">2</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ve-d.m3u</code></td></tr>\n    <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lara</td><td align=\"right\">1</td><td nowrap><code>https://iptv-org.github.io/iptv/subdivisions/ve-k.m3u</code></td></tr>\n    <tr><td>🇻🇳 Vietnam</td><td align=\"right\">83</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/vn.m3u</code></td></tr>\n    <tr><td>🇼🇫 Wallis and Futuna</td><td align=\"right\">8</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/wf.m3u</code></td></tr>\n    <tr><td>🇪🇭 Western Sahara</td><td align=\"right\">30</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/eh.m3u</code></td></tr>\n    <tr><td>🇾🇪 Yemen</td><td align=\"right\">50</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/ye.m3u</code></td></tr>\n    <tr><td>🇿🇲 Zambia</td><td align=\"right\">26</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/zm.m3u</code></td></tr>\n    <tr><td>🇿🇼 Zimbabwe</td><td align=\"right\">27</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/zw.m3u</code></td></tr>\n    <tr><td>Undefined</td><td align=\"right\">2348</td><td nowrap><code>https://iptv-org.github.io/iptv/countries/undefined.m3u</code></td></tr>\n  </tbody>\n</table>\n\n</details>\n\n### Grouped by region\n\nPlaylists in which channels are grouped by the region for which they are broadcasted.\n\n<details>\n<summary>Expand</summary>\n<br>\n\n```\nhttps://iptv-org.github.io/iptv/index.region.m3u\n```\n\nSame thing, but split up into separate files:\n\n<!-- prettier-ignore -->\n<table>\n  <thead>\n    <tr><th align=\"left\">Region</th><th align=\"left\">Channels</th><th align=\"left\">Playlist</th></tr>\n  </thead>\n  <tbody>\n    <tr><td align=\"left\">Africa</td><td align=\"right\">475</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/afr.m3u</code></td></tr>\n    <tr><td align=\"left\">Americas</td><td align=\"right\">3192</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/amer.m3u</code></td></tr>\n    <tr><td align=\"left\">Arab world</td><td align=\"right\">415</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/arab.m3u</code></td></tr>\n    <tr><td align=\"left\">Asia</td><td align=\"right\">2251</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/asia.m3u</code></td></tr>\n    <tr><td align=\"left\">Asia-Pacific</td><td align=\"right\">1163</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/apac.m3u</code></td></tr>\n    <tr><td align=\"left\">Association of Southeast Asian Nations</td><td align=\"right\">358</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/asean.m3u</code></td></tr>\n    <tr><td align=\"left\">Balkan</td><td align=\"right\">726</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/balkan.m3u</code></td></tr>\n    <tr><td align=\"left\">Benelux</td><td align=\"right\">245</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/benelux.m3u</code></td></tr>\n    <tr><td align=\"left\">Caribbean</td><td align=\"right\">368</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/carib.m3u</code></td></tr>\n    <tr><td align=\"left\">Central America</td><td align=\"right\">370</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/cenamer.m3u</code></td></tr>\n    <tr><td align=\"left\">Central and Eastern Europe</td><td align=\"right\">987</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/cee.m3u</code></td></tr>\n    <tr><td align=\"left\">Central Asia</td><td align=\"right\">128</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/cas.m3u</code></td></tr>\n    <tr><td align=\"left\">Commonwealth of Independent States</td><td align=\"right\">422</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/cis.m3u</code></td></tr>\n    <tr><td align=\"left\">Europe</td><td align=\"right\">3072</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/eur.m3u</code></td></tr>\n    <tr><td align=\"left\">Europe, the Middle East and Africa</td><td align=\"right\">3938</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/emea.m3u</code></td></tr>\n    <tr><td align=\"left\">European Union</td><td align=\"right\">2064</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/eu.m3u</code></td></tr>\n    <tr><td align=\"left\">Hispanic America</td><td align=\"right\">1312</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/hispam.m3u</code></td></tr>\n    <tr><td align=\"left\">Latin America</td><td align=\"right\">1539</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/latam.m3u</code></td></tr>\n    <tr><td align=\"left\">Latin America and the Caribbean</td><td align=\"right\">1580</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/lac.m3u</code></td></tr>\n    <tr><td align=\"left\">Maghreb</td><td align=\"right\">93</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/maghreb.m3u</code></td></tr>\n    <tr><td align=\"left\">Middle East</td><td align=\"right\">852</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/mideast.m3u</code></td></tr>\n    <tr><td align=\"left\">Middle East and North Africa</td><td align=\"right\">881</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/mena.m3u</code></td></tr>\n    <tr><td align=\"left\">Nordics</td><td align=\"right\">125</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/nord.m3u</code></td></tr>\n    <tr><td align=\"left\">North America</td><td align=\"right\">2484</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/noram.m3u</code></td></tr>\n    <tr><td align=\"left\">Northern America</td><td align=\"right\">1641</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/nam.m3u</code></td></tr>\n    <tr><td align=\"left\">Northern Europe</td><td align=\"right\">162</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/neur.m3u</code></td></tr>\n    <tr><td align=\"left\">Oceania</td><td align=\"right\">82</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/oce.m3u</code></td></tr>\n    <tr><td align=\"left\">South America</td><td align=\"right\">785</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/southam.m3u</code></td></tr>\n    <tr><td align=\"left\">South Asia</td><td align=\"right\">428</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/sas.m3u</code></td></tr>\n    <tr><td align=\"left\">Southeast Asia</td><td align=\"right\">358</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/sea.m3u</code></td></tr>\n    <tr><td align=\"left\">Southern Europe</td><td align=\"right\">1147</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/ser.m3u</code></td></tr>\n    <tr><td align=\"left\">Sub-Saharan Africa</td><td align=\"right\">419</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/ssa.m3u</code></td></tr>\n    <tr><td align=\"left\">West Africa</td><td align=\"right\">237</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/wafr.m3u</code></td></tr>\n    <tr><td align=\"left\">Western Europe</td><td align=\"right\">943</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/wer.m3u</code></td></tr>\n    <tr><td align=\"left\">Worldwide</td><td align=\"right\">59</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/int.m3u</code></td></tr>\n    <tr><td align=\"left\">Undefined</td><td align=\"right\">2348</td><td align=\"left\" nowrap><code>https://iptv-org.github.io/iptv/regions/undefined.m3u</code></td></tr>\n  </tbody>\n</table>\n\n</details>\n\n## EPG\n\n[Electronic Program Guide](https://en.wikipedia.org/wiki/Electronic_program_guide) for most of the channels can be downloaded using utilities published in the [iptv-org/epg](https://github.com/iptv-org/epg) repository.\n\n## Database\n\nAll channel data is taken from the [iptv-org/database](https://github.com/iptv-org/database) repository. If you find any errors please open a new [issue](https://github.com/iptv-org/database/issues) there.\n\n## API\n\nThe API documentation can be found in the [iptv-org/api](https://github.com/iptv-org/api) repository.\n\n## Resources\n\nLinks to other useful IPTV-related resources can be found in the [iptv-org/awesome-iptv](https://github.com/iptv-org/awesome-iptv) repository.\n\n## Discussions\n\nIf you need help finding a channel, have a question or idea, welcome to the [Discussions](https://github.com/orgs/iptv-org/discussions).\n\n## FAQ\n\nThe answers to the most popular questions can be found in the [FAQ.md](FAQ.md) file.\n\n## Contribution\n\nPlease make sure to read the [Contributing Guide](CONTRIBUTING.md) before sending an issue or making a pull request.\n\nAnd thank you to everyone who has already contributed!\n\n### Backers\n\n<a href=\"https://opencollective.com/iptv-org\"><img src=\"https://opencollective.com/iptv-org/backers.svg?width=890\" /></a>\n\n### Contributors\n\n<a href=\"https://github.com/iptv-org/iptv/graphs/contributors\"><img src=\"https://opencollective.com/iptv-org/contributors.svg?width=890\" /></a>\n\n## Legal\n\nNo video files are stored in this repository. The repository simply contains user-submitted links to publicly available video stream URLs, which to the best of our knowledge have been intentionally made publicly by the copyright holders. If any links in these playlists infringe on your rights as a copyright holder, they may be removed by sending a [pull request](https://github.com/iptv-org/iptv/pulls) or opening an [issue](https://github.com/iptv-org/iptv/issues/new?assignees=freearhey&labels=removal+request&template=--removal-request.yml&title=Remove%3A+). However, note that we have **no control** over the destination of the link, and just removing the link from the playlist will not remove its contents from the web. Note that linking does not directly infringe copyright because no copy is made on the site providing the link, and thus this is **not** a valid reason to send a DMCA notice to GitHub. To remove this content from the web, you should contact the web host that's actually hosting the content (**not** GitHub, nor the maintainers of this repository).\n\n## License\n\n[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](LICENSE)",
    "summary": "IPTV仓库收集了全球公开可用的IPTV电视频道链接，并提供多种分组方式的播放列表，包括按类别、语言、国家和地区分组。项目还关联了EPG（电子节目指南）、频道数据库和API等资源，方便用户通过支持直播的视频播放器直接观看，或开发者利用其数据和接口。",
    "keywords": [
      "IPTV",
      "电视频道",
      "播放列表",
      "直播",
      "开源项目",
      "M3U"
    ],
    "area": [
      "其他",
      "视频理解",
      "人工智能"
    ],
    "published_time": "2025-05-26T00:12:32Z",
    "download_time": "2024-05-18 10:00:00",
    "visual_resource": [
      "https://github.com/iptv-org/iptv/raw/master/.readme/preview.png"
    ],
    "extra_info": null
  },
  {
    "id": "developer-roadmap",
    "source": "GitHub",
    "url": "https://github.com/kamranahmedse/developer-roadmap",
    "title": "roadmap.sh",
    "content": "<p align=\"center\">\n  <a href=\"https://roadmap.sh/\"><img src=\"public/images/brand.png\" height=\"128\"></a>\n  <h2 align=\"center\"><a href=\"https://roadmap.sh\">roadmap.sh</a></h2>\n  <p align=\"center\">Community driven roadmaps, articles and resources for developers<p>\n  <p align=\"center\">\n    <a href=\"https://roadmap.sh/roadmaps\">\n    \t<img src=\"https://img.shields.io/badge/%E2%9C%A8-Roadmaps%20-0a0a0a.svg?style=flat&colorA=0a0a0a\" alt=\"roadmaps\" />\n    </a>\n    <a href=\"https://roadmap.sh/best-practices\">\n    \t<img src=\"https://img.shields.io/badge/%E2%9C%A8-Best%20Practices-0a0a0a.svg?style=flat&colorA=0a0a0a\" alt=\"best practices\" />\n    </a>\n    <a href=\"https://roadmap.sh/questions\">\n    \t<img src=\"https://img.shields.io/badge/%E2%9C%A8-Questions-0a0a0a.svg?style=flat&colorA=0a0a0a\" alt=\"videos\" />\n    </a>\n    <a href=\"https://www.youtube.com/channel/UCA0H2KIWgWTwpTFjSxp0now?sub_confirmation=1\">\n    \t<img src=\"https://img.shields.io/badge/%E2%9C%A8-YouTube%20Channel-0a0a0a.svg?style=flat&colorA=0a0a0a\" alt=\"roadmaps\" />\n    </a>\n  </p>\n</p>\n\n<br>\n\n![](https://i.imgur.com/waxVImv.png)\n\nRoadmaps are now interactive, you can click the nodes to read more about the topics.\n\n### [View all Roadmaps](https://roadmap.sh) &nbsp;&middot;&nbsp; [Best Practices](https://roadmap.sh/best-practices) &nbsp;&middot;&nbsp; [Questions](https://roadmap.sh/questions)\n\n![](https://i.imgur.com/waxVImv.png)\n\nHere is the list of available roadmaps with more being actively worked upon.\n\n> Have a look at the [get started](https://roadmap.sh/get-started) page that might help you pick up a path.\n\n- [Frontend Roadmap](https://roadmap.sh/frontend) / [Frontend Beginner Roadmap](https://roadmap.sh/frontend?r=frontend-beginner)\n- [Backend Roadmap](https://roadmap.sh/backend) / [Backend Beginner Roadmap](https://roadmap.sh/backend?r=backend-beginner)\n- [DevOps Roadmap](https://roadmap.sh/devops) / [DevOps Beginner Roadmap](https://roadmap.sh/devops?r=devops-beginner)\n- [Full Stack Roadmap](https://roadmap.sh/full-stack)\n- [Git and GitHub](https://roadmap.sh/git-github)\n- [API Design Roadmap](https://roadmap.sh/api-design)\n- [Computer Science Roadmap](https://roadmap.sh/computer-science)\n- [Data Structures and Algorithms Roadmap](https://roadmap.sh/datastructures-and-algorithms)\n- [AI and Data Scientist Roadmap](https://roadmap.sh/ai-data-scientist)\n- [AI Engineer Roadmap](https://roadmap.sh/ai-engineer)\n- [AWS Roadmap](https://roadmap.sh/aws)\n- [Cloudflare Roadmap](https://roadmap.sh/cloudflare)\n- [Linux Roadmap](https://roadmap.sh/linux)\n- [Terraform Roadmap](https://roadmap.sh/terraform)\n- [Data Analyst Roadmap](https://roadmap.sh/data-analyst)\n- [MLOps Roadmap](https://roadmap.sh/mlops)\n- [Product Manager Roadmap](https://roadmap.sh/product-manager)\n- [Engineering Manager Roadmap](https://roadmap.sh/engineering-manager)\n- [QA Roadmap](https://roadmap.sh/qa)\n- [Python Roadmap](https://roadmap.sh/python)\n- [Software Architect Roadmap](https://roadmap.sh/software-architect)\n- [Game Developer Roadmap](https://roadmap.sh/game-developer) / [Server Side Game Developer](https://roadmap.sh/server-side-game-developer)\n- [Software Design and Architecture Roadmap](https://roadmap.sh/software-design-architecture)\n- [JavaScript Roadmap](https://roadmap.sh/javascript)\n- [TypeScript Roadmap](https://roadmap.sh/typescript)\n- [C++ Roadmap](https://roadmap.sh/cpp)\n- [React Roadmap](https://roadmap.sh/react)\n- [React Native Roadmap](https://roadmap.sh/react-native)\n- [Vue Roadmap](https://roadmap.sh/vue)\n- [Angular Roadmap](https://roadmap.sh/angular)\n- [Node.js Roadmap](https://roadmap.sh/nodejs)\n- [PHP Roadmap](https://roadmap.sh/php)\n- [GraphQL Roadmap](https://roadmap.sh/graphql)\n- [Android Roadmap](https://roadmap.sh/android)\n- [iOS Roadmap](https://roadmap.sh/ios)\n- [Flutter Roadmap](https://roadmap.sh/flutter)\n- [Go Roadmap](https://roadmap.sh/golang)\n- [Rust Roadmap](https://roadmap.sh/rust)\n- [Java Roadmap](https://roadmap.sh/java)\n- [Spring Boot Roadmap](https://roadmap.sh/spring-boot)\n- [Design System Roadmap](https://roadmap.sh/design-system)\n- [PostgreSQL Roadmap](https://roadmap.sh/postgresql-dba)\n- [SQL Roadmap](https://roadmap.sh/sql)\n- [Redis Roadmap](https://roadmap.sh/redis)\n- [Blockchain Roadmap](https://roadmap.sh/blockchain)\n- [ASP.NET Core Roadmap](https://roadmap.sh/aspnet-core)\n- [System Design Roadmap](https://roadmap.sh/system-design)\n- [Kubernetes Roadmap](https://roadmap.sh/kubernetes)\n- [Cyber Security Roadmap](https://roadmap.sh/cyber-security)\n- [MongoDB Roadmap](https://roadmap.sh/mongodb)\n- [UX Design Roadmap](https://roadmap.sh/ux-design)\n- [Docker Roadmap](https://roadmap.sh/docker)\n- [Prompt Engineering Roadmap](https://roadmap.sh/prompt-engineering)\n- [Technical Writer Roadmap](https://roadmap.sh/technical-writer)\n- [DevRel Engineer Roadmap](https://roadmap.sh/devrel)\n- [AI Red Teaming Roadmap](https://roadmap.sh/ai-red-teaming)\n- [AI Agents Roadmap](https://roadmap.sh/ai-agents)\n\nThere are also interactive best practices:\n\n- [Backend Performance Best Practices](https://roadmap.sh/best-practices/backend-performance)\n- [Frontend Performance Best Practices](https://roadmap.sh/best-practices/frontend-performance)\n- [Code Review Best Practices](https://roadmap.sh/best-practices/code-review)\n- [API Security Best Practices](https://roadmap.sh/best-practices/api-security)\n- [AWS Best Practices](https://roadmap.sh/best-practices/aws)\n\n..and questions to help you test, rate and improve your knowledge\n\n- [JavaScript Questions](https://roadmap.sh/questions/javascript)\n- [Node.js Questions](https://roadmap.sh/questions/nodejs)\n- [React Questions](https://roadmap.sh/questions/react)\n- [Backend Questions](https://roadmap.sh/questions/backend)\n- [Frontend Questions](https://roadmap.sh/questions/frontend)\n\n![](https://i.imgur.com/waxVImv.png)\n\n## Share with the community\n\nPlease consider sharing a post about [roadmap.sh](https://roadmap.sh) and the value it provides. It really does help!\n\n[![GitHub Repo stars](https://img.shields.io/badge/share%20on-reddit-red?logo=reddit)](https://reddit.com/submit?url=https://roadmap.sh&title=Interactive%20roadmaps,%20guides%20and%20other%20educational%20content%20for%20Developers)\n[![GitHub Repo stars](https://img.shields.io/badge/share%20on-hacker%20news-orange?logo=ycombinator)](https://news.ycombinator.com/submitlink?u=https://roadmap.sh)\n[![GitHub Repo stars](https://img.shields.io/badge/share%20on-twitter-03A9F4?logo=twitter)](https://twitter.com/share?url=https://roadmap.sh&text=Interactive%20roadmaps,%20guides%20and%20other%20educational%20content%20for%20Developers)\n[![GitHub Repo stars](https://img.shields.io/badge/share%20on-facebook-1976D2?logo=facebook)](https://www.facebook.com/sharer/sharer.php?u=https://roadmap.sh)\n[![GitHub Repo stars](https://img.shields.io/badge/share%20on-linkedin-3949AB?logo=linkedin)](https://www.linkedin.com/shareArticle?url=https://roadmap.sh&title=Interactive%20roadmaps,%20guides%20and%20other%20educational%20content%20for%20Developers)\n\n## Development\n\nClone the repository, install the dependencies and start the application\n\n```bash\ngit clone git@github.com:kamranahmedse/developer-roadmap.git\ncd developer-roadmap\nnpm install\nnpm run dev\n```\n\nNote: use the `depth` parameter to reduce the clone size and speed up the clone.\n\n```sh\ngit clone --depth=1 https://github.com/kamranahmedse/developer-roadmap.git\n```\n\n## Contribution\n\n> Have a look at [contribution docs](./contributing.md) for how to update any of the roadmaps\n\n- Add content to roadmaps\n- Add new roadmaps\n- Suggest changes to existing roadmaps\n- Discuss ideas in issues\n- Spread the word\n\n## Thanks to all contributors ❤\n\n <a href = \"https://github.com/kamranahmedse/developer-roadmap/graphs/contributors\">\n   <img src = \"https://contrib.rocks/image?repo=kamranahmedse/developer-roadmap\"/>\n </a>\n\n## License\n\nHave a look at the [license file](./license) for details",
    "summary": "roadmap.sh项目提供社区驱动的开发者技术路线图、文章和学习资源。这些交互式路线图涵盖前端、后端、DevOps、人工智能、数据科学、编程语言、云技术、系统设计、网络安全等众多技术领域。项目还包含技术最佳实践和面试问题集，旨在帮助全球开发者系统规划学习路径、提升专业技能并测试知识掌握程度，是开发者成长的综合性学习平台。",
    "keywords": [
      "技术路线图",
      "开发者教育",
      "前端开发",
      "后端开发",
      "DevOps",
      "人工智能",
      "数据科学",
      "编程语言"
    ],
    "area": [
      "其他",
      "人工智能",
      "机器学习"
    ],
    "published_time": "2025-05-26T12:17:01Z",
    "download_time": "2024-07-26 10:00:00",
    "visual_resource": [
      "screenshot/github_developer-roadmap.png"
    ],
    "extra_info": null
  },
  {
    "id": "lobe-chat",
    "source": "GitHub",
    "url": "https://github.com/lobehub/lobe-chat",
    "title": "Lobe Chat",
    "content": "<div align=\"center\"><a name=\"readme-top\"></a>\n\n[![][image-banner]][vercel-link]\n\n# Lobe Chat\n\nAn open-source, modern-design ChatGPT/LLMs UI/Framework.<br/>\nSupports speech-synthesis, multi-modal, and extensible ([function call][docs-functionc-call]) plugin system.<br/>\nOne-click **FREE** deployment of your private OpenAI ChatGPT/Claude/Gemini/Groq/Ollama chat application.\n\n**English** · [简体中文](./README.zh-CN.md) · [Official Site][official-site] · [Changelog][changelog] · [Documents][docs] · [Blog][blog] · [Feedback][github-issues-link]\n\n<!-- SHIELD GROUP -->\n\n[![][github-release-shield]][github-release-link]\n[![][docker-release-shield]][docker-release-link]\n[![][vercel-shield]][vercel-link]\n[![][discord-shield]][discord-link]<br/>\n[![][codecov-shield]][codecov-link]\n[![][github-action-test-shield]][github-action-test-link]\n[![][github-action-release-shield]][github-action-release-link]\n[![][github-releasedate-shield]][github-releasedate-link]<br/>\n[![][github-contributors-shield]][github-contributors-link]\n[![][github-forks-shield]][github-forks-link]\n[![][github-stars-shield]][github-stars-link]\n[![][github-issues-shield]][github-issues-link]\n[![][github-license-shield]][github-license-link]<br>\n[![][sponsor-shield]][sponsor-link]\n\n**Share LobeChat Repository**\n\n[![][share-x-shield]][share-x-link]\n[![][share-telegram-shield]][share-telegram-link]\n[![][share-whatsapp-shield]][share-whatsapp-link]\n[![][share-reddit-shield]][share-reddit-link]\n[![][share-weibo-shield]][share-weibo-link]\n[![][share-mastodon-shield]][share-mastodon-link]\n[![][share-linkedin-shield]][share-linkedin-link]\n\n<sup>Pioneering the new age of thinking and creating. Built for you, the Super Individual.</sup>\n\n[![][github-trending-shield]][github-trending-url]\n\n![][image-overview]\n\n</div>\n\n<details>\n<summary><kbd>Table of contents</kbd></summary>\n\n#### TOC\n\n- [👋🏻 Getting Started & Join Our Community](#-getting-started--join-our-community)\n- [✨ Features](#-features)\n  - [`1` Chain of Thought](#1-chain-of-thought)\n  - [`2` Branching Conversations](#2-branching-conversations)\n  - [`3` Artifacts Support](#3-artifacts-support)\n  - [`4` File Upload /Knowledge Base](#4-file-upload-knowledge-base)\n  - [`5` Multi-Model Service Provider Support](#5-multi-model-service-provider-support)\n  - [`6` Local Large Language Model (LLM) Support](#6-local-large-language-model-llm-support)\n  - [`7` Model Visual Recognition](#7-model-visual-recognition)\n  - [`8` TTS & STT Voice Conversation](#8-tts--stt-voice-conversation)\n  - [`9` Text to Image Generation](#9-text-to-image-generation)\n  - [`10` Plugin System (Function Calling)](#10-plugin-system-function-calling)\n  - [`11` Agent Market (GPTs)](#11-agent-market-gpts)\n  - [`12` Support Local / Remote Database](#12-support-local--remote-database)\n  - [`13` Support Multi-User Management](#13-support-multi-user-management)\n  - [`14` Progressive Web App (PWA)](#14-progressive-web-app-pwa)\n  - [`15` Mobile Device Adaptation](#15-mobile-device-adaptation)\n  - [`16` Custom Themes](#16-custom-themes)\n  - [`*` What's more](#-whats-more)\n- [⚡️ Performance](#️-performance)\n- [🛳 Self Hosting](#-self-hosting)\n  - [`A` Deploying with Vercel, Zeabur , Sealos or Alibaba Cloud](#a-deploying-with-vercel-zeabur--sealos-or-alibaba-cloud)\n  - [`B` Deploying with Docker](#b-deploying-with-docker)\n  - [Environment Variable](#environment-variable)\n- [📦 Ecosystem](#-ecosystem)\n- [🧩 Plugins](#-plugins)\n- [⌨️ Local Development](#️-local-development)\n- [🤝 Contributing](#-contributing)\n- [❤️ Sponsor](#️-sponsor)\n- [🔗 More Products](#-more-products)\n\n####\n\n<br/>\n\n</details>\n\n## 👋🏻 Getting Started & Join Our Community\n\nWe are a group of e/acc design-engineers, hoping to provide modern design components and tools for AIGC.\nBy adopting the Bootstrapping approach, we aim to provide developers and users with a more open, transparent, and user-friendly product ecosystem.\n\nWhether for users or professional developers, LobeHub will be your AI Agent playground. Please be aware that LobeChat is currently under active development, and feedback is welcome for any [issues][issues-link] encountered.\n\n| [![][vercel-shield-badge]][vercel-link]   | No installation or registration necessary! Visit our website to experience it firsthand.                           |\n| :---------------------------------------- | :----------------------------------------------------------------------------------------------------------------- |\n| [![][discord-shield-badge]][discord-link] | Join our Discord community! This is where you can connect with developers and other enthusiastic users of LobeHub. |\n\n> \\[!IMPORTANT]\n>\n> **Star Us**, You will receive all release notifications from GitHub without any delay ~ ⭐️\n\n[![][image-star]][github-stars-link]\n\n<details>\n  <summary><kbd>Star History</kbd></summary>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=lobehub%2Flobe-chat&theme=dark&type=Date\">\n    <img width=\"100%\" src=\"https://api.star-history.com/svg?repos=lobehub%2Flobe-chat&type=Date\">\n  </picture>\n</details>\n\n## ✨ Features\n\n[![][image-feat-cot]][docs-feat-cot]\n\n### `1` [Chain of Thought][docs-feat-cot]\n\nExperience AI reasoning like never before. Watch as complex problems unfold step by step through our innovative Chain of Thought (CoT) visualization. This breakthrough feature provides unprecedented transparency into AI's decision-making process, allowing you to observe how conclusions are reached in real-time.\n\nBy breaking down complex reasoning into clear, logical steps, you can better understand and validate the AI's problem-solving approach. Whether you're debugging, learning, or simply curious about AI reasoning, CoT visualization transforms abstract thinking into an engaging, interactive experience.\n\n[![][back-to-top]](#readme-top)\n\n[![][image-feat-branch]][docs-feat-branch]\n\n### `2` [Branching Conversations][docs-feat-branch]\n\nIntroducing a more natural and flexible way to chat with AI. With Branch Conversations, your discussions can flow in multiple directions, just like human conversations do. Create new conversation branches from any message, giving you the freedom to explore different paths while preserving the original context.\n\nChoose between two powerful modes:\n\n- **Continuation Mode:** Seamlessly extend your current discussion while maintaining valuable context\n- **Standalone Mode:** Start fresh with a new topic based on any previous message\n\nThis groundbreaking feature transforms linear conversations into dynamic, tree-like structures, enabling deeper exploration of ideas and more productive interactions.\n\n[![][back-to-top]](#readme-top)\n\n[![][image-feat-artifacts]][docs-feat-artifacts]\n\n### `3` [Artifacts Support][docs-feat-artifacts]\n\nExperience the power of Claude Artifacts, now integrated into LobeChat. This revolutionary feature expands the boundaries of AI-human interaction, enabling real-time creation and visualization of diverse content formats.\n\nCreate and visualize with unprecedented flexibility:\n\n- Generate and display dynamic SVG graphics\n- Build and render interactive HTML pages in real-time\n- Produce professional documents in multiple formats\n\n[![][back-to-top]](#readme-top)\n\n[![][image-feat-knowledgebase]][docs-feat-knowledgebase]\n\n### `4` [File Upload /Knowledge Base][docs-feat-knowledgebase]\n\nLobeChat supports file upload and knowledge base functionality. You can upload various types of files including documents, images, audio, and video, as well as create knowledge bases, making it convenient for users to manage and search for files. Additionally, you can utilize files and knowledge base features during conversations, enabling a richer dialogue experience.\n\n<https://github.com/user-attachments/assets/faa8cf67-e743-4590-8bf6-ebf6ccc34175>\n\n> \\[!TIP]\n>\n> Learn more on [📘 LobeChat Knowledge Base Launch — From Now On, Every Step Counts](https://lobehub.com/blog/knowledge-base)\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-privoder]][docs-feat-provider]\n\n### `5` [Multi-Model Service Provider Support][docs-feat-provider]\n\nIn the continuous development of LobeChat, we deeply understand the importance of diversity in model service providers for meeting the needs of the community when providing AI conversation services. Therefore, we have expanded our support to multiple model service providers, rather than being limited to a single one, in order to offer users a more diverse and rich selection of conversations.\n\nIn this way, LobeChat can more flexibly adapt to the needs of different users, while also providing developers with a wider range of choices.\n\n#### Supported Model Service Providers\n\nWe have implemented support for the following model service providers:\n\n<!-- PROVIDER LIST -->\n\n- **[OpenAI](https://lobechat.com/discover/provider/openai)**: OpenAI is a global leader in artificial intelligence research, with models like the GPT series pushing the frontiers of natural language processing. OpenAI is committed to transforming multiple industries through innovative and efficient AI solutions. Their products demonstrate significant performance and cost-effectiveness, widely used in research, business, and innovative applications.\n- **[Ollama](https://lobechat.com/discover/provider/ollama)**: Ollama provides models that cover a wide range of fields, including code generation, mathematical operations, multilingual processing, and conversational interaction, catering to diverse enterprise-level and localized deployment needs.\n- **[Anthropic](https://lobechat.com/discover/provider/anthropic)**: Anthropic is a company focused on AI research and development, offering a range of advanced language models such as Claude 3.5 Sonnet, Claude 3 Sonnet, Claude 3 Opus, and Claude 3 Haiku. These models achieve an ideal balance between intelligence, speed, and cost, suitable for various applications from enterprise workloads to rapid-response scenarios. Claude 3.5 Sonnet, as their latest model, has excelled in multiple evaluations while maintaining a high cost-performance ratio.\n- **[Bedrock](https://lobechat.com/discover/provider/bedrock)**: Bedrock is a service provided by Amazon AWS, focusing on delivering advanced AI language and visual models for enterprises. Its model family includes Anthropic's Claude series, Meta's Llama 3.1 series, and more, offering a range of options from lightweight to high-performance, supporting tasks such as text generation, conversation, and image processing for businesses of varying scales and needs.\n- **[Google](https://lobechat.com/discover/provider/google)**: Google's Gemini series represents its most advanced, versatile AI models, developed by Google DeepMind, designed for multimodal capabilities, supporting seamless understanding and processing of text, code, images, audio, and video. Suitable for various environments from data centers to mobile devices, it significantly enhances the efficiency and applicability of AI models.\n- **[DeepSeek](https://lobechat.com/discover/provider/deepseek)**: DeepSeek is a company focused on AI technology research and application, with its latest model DeepSeek-V2.5 integrating general dialogue and code processing capabilities, achieving significant improvements in human preference alignment, writing tasks, and instruction following.\n- **[PPIO](https://lobechat.com/discover/provider/ppio)**: PPIO supports stable and cost-efficient open-source LLM APIs, such as DeepSeek, Llama, Qwen etc.\n- **[HuggingFace](https://lobechat.com/discover/provider/huggingface)**: The HuggingFace Inference API provides a fast and free way for you to explore thousands of models for various tasks. Whether you are prototyping for a new application or experimenting with the capabilities of machine learning, this API gives you instant access to high-performance models across multiple domains.\n- **[OpenRouter](https://lobechat.com/discover/provider/openrouter)**: OpenRouter is a service platform providing access to various cutting-edge large model interfaces, supporting OpenAI, Anthropic, LLaMA, and more, suitable for diverse development and application needs. Users can flexibly choose the optimal model and pricing based on their requirements, enhancing the AI experience.\n- **[Cloudflare Workers AI](https://lobechat.com/discover/provider/cloudflare)**: Run serverless GPU-powered machine learning models on Cloudflare's global network.\n\n<details><summary><kbd>See more providers (+31)</kbd></summary>\n\n- **[GitHub](https://lobechat.com/discover/provider/github)**: With GitHub Models, developers can become AI engineers and leverage the industry's leading AI models.\n- **[Novita](https://lobechat.com/discover/provider/novita)**: Novita AI is a platform providing a variety of large language models and AI image generation API services, flexible, reliable, and cost-effective. It supports the latest open-source models like Llama3 and Mistral, offering a comprehensive, user-friendly, and auto-scaling API solution for generative AI application development, suitable for the rapid growth of AI startups.\n- **[PPIO](https://lobechat.com/discover/provider/ppio)**: PPIO supports stable and cost-efficient open-source LLM APIs, such as DeepSeek, Llama, Qwen etc.\n- **[Together AI](https://lobechat.com/discover/provider/togetherai)**: Together AI is dedicated to achieving leading performance through innovative AI models, offering extensive customization capabilities, including rapid scaling support and intuitive deployment processes to meet various enterprise needs.\n- **[Fireworks AI](https://lobechat.com/discover/provider/fireworksai)**: Fireworks AI is a leading provider of advanced language model services, focusing on functional calling and multimodal processing. Its latest model, Firefunction V2, is based on Llama-3, optimized for function calling, conversation, and instruction following. The visual language model FireLLaVA-13B supports mixed input of images and text. Other notable models include the Llama series and Mixtral series, providing efficient multilingual instruction following and generation support.\n- **[Groq](https://lobechat.com/discover/provider/groq)**: Groq's LPU inference engine has excelled in the latest independent large language model (LLM) benchmarks, redefining the standards for AI solutions with its remarkable speed and efficiency. Groq represents instant inference speed, demonstrating strong performance in cloud-based deployments.\n- **[Perplexity](https://lobechat.com/discover/provider/perplexity)**: Perplexity is a leading provider of conversational generation models, offering various advanced Llama 3.1 models that support both online and offline applications, particularly suited for complex natural language processing tasks.\n- **[Mistral](https://lobechat.com/discover/provider/mistral)**: Mistral provides advanced general, specialized, and research models widely used in complex reasoning, multilingual tasks, and code generation. Through functional calling interfaces, users can integrate custom functionalities for specific applications.\n- **[Ai21Labs](https://lobechat.com/discover/provider/ai21)**: AI21 Labs builds foundational models and AI systems for enterprises, accelerating the application of generative AI in production.\n- **[Upstage](https://lobechat.com/discover/provider/upstage)**: Upstage focuses on developing AI models for various business needs, including Solar LLM and document AI, aiming to achieve artificial general intelligence (AGI) for work. It allows for the creation of simple conversational agents through Chat API and supports functional calling, translation, embedding, and domain-specific applications.\n- **[xAI](https://lobechat.com/discover/provider/xai)**: xAI is a company dedicated to building artificial intelligence to accelerate human scientific discovery. Our mission is to advance our collective understanding of the universe.\n- **[Qwen](https://lobechat.com/discover/provider/qwen)**: Tongyi Qianwen is a large-scale language model independently developed by Alibaba Cloud, featuring strong natural language understanding and generation capabilities. It can answer various questions, create written content, express opinions, and write code, playing a role in multiple fields.\n- **[Wenxin](https://lobechat.com/discover/provider/wenxin)**: An enterprise-level one-stop platform for large model and AI-native application development and services, providing the most comprehensive and user-friendly toolchain for the entire process of generative artificial intelligence model development and application development.\n- **[Hunyuan](https://lobechat.com/discover/provider/hunyuan)**: A large language model developed by Tencent, equipped with powerful Chinese creative capabilities, logical reasoning abilities in complex contexts, and reliable task execution skills.\n- **[ZhiPu](https://lobechat.com/discover/provider/zhipu)**: Zhipu AI offers an open platform for multimodal and language models, supporting a wide range of AI application scenarios, including text processing, image understanding, and programming assistance.\n- **[SiliconCloud](https://lobechat.com/discover/provider/siliconcloud)**: SiliconFlow is dedicated to accelerating AGI for the benefit of humanity, enhancing large-scale AI efficiency through an easy-to-use and cost-effective GenAI stack.\n- **[01.AI](https://lobechat.com/discover/provider/zeroone)**: 01.AI focuses on AI 2.0 era technologies, vigorously promoting the innovation and application of 'human + artificial intelligence', using powerful models and advanced AI technologies to enhance human productivity and achieve technological empowerment.\n- **[Spark](https://lobechat.com/discover/provider/spark)**: iFlytek's Spark model provides powerful AI capabilities across multiple domains and languages, utilizing advanced natural language processing technology to build innovative applications suitable for smart hardware, smart healthcare, smart finance, and other vertical scenarios.\n- **[SenseNova](https://lobechat.com/discover/provider/sensenova)**: SenseNova, backed by SenseTime's robust infrastructure, offers efficient and user-friendly full-stack large model services.\n- **[Stepfun](https://lobechat.com/discover/provider/stepfun)**: StepFun's large model possesses industry-leading multimodal and complex reasoning capabilities, supporting ultra-long text understanding and powerful autonomous scheduling search engine functions.\n- **[Moonshot](https://lobechat.com/discover/provider/moonshot)**: Moonshot is an open-source platform launched by Beijing Dark Side Technology Co., Ltd., providing various natural language processing models with a wide range of applications, including but not limited to content creation, academic research, intelligent recommendations, and medical diagnosis, supporting long text processing and complex generation tasks.\n- **[Baichuan](https://lobechat.com/discover/provider/baichuan)**: Baichuan Intelligence is a company focused on the research and development of large AI models, with its models excelling in domestic knowledge encyclopedias, long text processing, and generative creation tasks in Chinese, surpassing mainstream foreign models. Baichuan Intelligence also possesses industry-leading multimodal capabilities, performing excellently in multiple authoritative evaluations. Its models include Baichuan 4, Baichuan 3 Turbo, and Baichuan 3 Turbo 128k, each optimized for different application scenarios, providing cost-effective solutions.\n- **[Minimax](https://lobechat.com/discover/provider/minimax)**: MiniMax is a general artificial intelligence technology company established in 2021, dedicated to co-creating intelligence with users. MiniMax has independently developed general large models of different modalities, including trillion-parameter MoE text models, voice models, and image models, and has launched applications such as Conch AI.\n- **[InternLM](https://lobechat.com/discover/provider/internlm)**: An open-source organization dedicated to the research and development of large model toolchains. It provides an efficient and user-friendly open-source platform for all AI developers, making cutting-edge large models and algorithm technologies easily accessible.\n- **[Higress](https://lobechat.com/discover/provider/higress)**: Higress is a cloud-native API gateway that was developed internally at Alibaba to address the issues of Tengine reload affecting long-lived connections and the insufficient load balancing capabilities for gRPC/Dubbo.\n- **[Gitee AI](https://lobechat.com/discover/provider/giteeai)**: Gitee AI's Serverless API provides AI developers with an out of the box large model inference API service.\n- **[Taichu](https://lobechat.com/discover/provider/taichu)**: The Institute of Automation, Chinese Academy of Sciences, and Wuhan Artificial Intelligence Research Institute have launched a new generation of multimodal large models, supporting comprehensive question-answering tasks such as multi-turn Q&A, text creation, image generation, 3D understanding, and signal analysis, with stronger cognitive, understanding, and creative abilities, providing a new interactive experience.\n- **[360 AI](https://lobechat.com/discover/provider/ai360)**: 360 AI is an AI model and service platform launched by 360 Company, offering various advanced natural language processing models, including 360GPT2 Pro, 360GPT Pro, 360GPT Turbo, and 360GPT Turbo Responsibility 8K. These models combine large-scale parameters and multimodal capabilities, widely applied in text generation, semantic understanding, dialogue systems, and code generation. With flexible pricing strategies, 360 AI meets diverse user needs, supports developer integration, and promotes the innovation and development of intelligent applications.\n- **[Search1API](https://lobechat.com/discover/provider/search1api)**: Search1API provides access to the DeepSeek series of models that can connect to the internet as needed, including standard and fast versions, supporting a variety of model sizes.\n- **[InfiniAI](https://lobechat.com/discover/provider/infiniai)**: Provides high-performance, easy-to-use, and secure large model services for application developers, covering the entire process from large model development to service deployment.\n- **[Qiniu](https://lobechat.com/discover/provider/qiniu)**: Qiniu, as a long-established cloud service provider, delivers cost-effective and reliable AI inference services for both real-time and batch processing, with a simple and user-friendly experience.\n\n</details>\n\n> 📊 Total providers: [<kbd>**41**</kbd>](https://lobechat.com/discover/providers)\n\n <!-- PROVIDER LIST -->\n\nAt the same time, we are also planning to support more model service providers. If you would like LobeChat to support your favorite service provider, feel free to join our [💬 community discussion](https://github.com/lobehub/lobe-chat/discussions/1284).\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-local]][docs-feat-local]\n\n### `6` [Local Large Language Model (LLM) Support][docs-feat-local]\n\nTo meet the specific needs of users, LobeChat also supports the use of local models based on [Ollama](https://ollama.ai), allowing users to flexibly use their own or third-party models.\n\n> \\[!TIP]\n>\n> Learn more about [📘 Using Ollama in LobeChat][docs-usage-ollama] by checking it out.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-vision]][docs-feat-vision]\n\n### `7` [Model Visual Recognition][docs-feat-vision]\n\nLobeChat now supports OpenAI's latest [`gpt-4-vision`](https://platform.openai.com/docs/guides/vision) model with visual recognition capabilities,\na multimodal intelligence that can perceive visuals. Users can easily upload or drag and drop images into the dialogue box,\nand the agent will be able to recognize the content of the images and engage in intelligent conversation based on this,\ncreating smarter and more diversified chat scenarios.\n\nThis feature opens up new interactive methods, allowing communication to transcend text and include a wealth of visual elements.\nWhether it's sharing images in daily use or interpreting images within specific industries, the agent provides an outstanding conversational experience.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-tts]][docs-feat-tts]\n\n### `8` [TTS & STT Voice Conversation][docs-feat-tts]\n\nLobeChat supports Text-to-Speech (TTS) and Speech-to-Text (STT) technologies, enabling our application to convert text messages into clear voice outputs,\nallowing users to interact with our conversational agent as if they were talking to a real person. Users can choose from a variety of voices to pair with the agent.\n\nMoreover, TTS offers an excellent solution for those who prefer auditory learning or desire to receive information while busy.\nIn LobeChat, we have meticulously selected a range of high-quality voice options (OpenAI Audio, Microsoft Edge Speech) to meet the needs of users from different regions and cultural backgrounds.\nUsers can choose the voice that suits their personal preferences or specific scenarios, resulting in a personalized communication experience.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-t2i]][docs-feat-t2i]\n\n### `9` [Text to Image Generation][docs-feat-t2i]\n\nWith support for the latest text-to-image generation technology, LobeChat now allows users to invoke image creation tools directly within conversations with the agent. By leveraging the capabilities of AI tools such as [`DALL-E 3`](https://openai.com/dall-e-3), [`MidJourney`](https://www.midjourney.com/), and [`Pollinations`](https://pollinations.ai/), the agents are now equipped to transform your ideas into images.\n\nThis enables a more private and immersive creative process, allowing for the seamless integration of visual storytelling into your personal dialogue with the agent.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-plugin]][docs-feat-plugin]\n\n### `10` [Plugin System (Function Calling)][docs-feat-plugin]\n\nThe plugin ecosystem of LobeChat is an important extension of its core functionality, greatly enhancing the practicality and flexibility of the LobeChat assistant.\n\n<video controls src=\"https://github.com/lobehub/lobe-chat/assets/28616219/f29475a3-f346-4196-a435-41a6373ab9e2\" muted=\"false\"></video>\n\nBy utilizing plugins, LobeChat assistants can obtain and process real-time information, such as searching for web information and providing users with instant and relevant news.\n\nIn addition, these plugins are not limited to news aggregation, but can also extend to other practical functions, such as quickly searching documents, generating images, obtaining data from various platforms like Bilibili, Steam, and interacting with various third-party services.\n\n> \\[!TIP]\n>\n> Learn more about [📘 Plugin Usage][docs-usage-plugin] by checking it out.\n\n<!-- PLUGIN LIST -->\n\n| Recent Submits                                                                                                                            | Description                                                                                                                |\n| ----------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |\n| [Web](https://lobechat.com/discover/plugin/web)<br/><sup>By **Proghit** on **2025-01-24**</sup>                                           | Smart web search that reads and analyzes pages to deliver comprehensive answers from Google results.<br/>`web` `search`    |\n| [Bing_websearch](https://lobechat.com/discover/plugin/Bingsearch-identifier)<br/><sup>By **FineHow** on **2024-12-22**</sup>              | Search for information from the internet base BingApi<br/>`bingsearch`                                                     |\n| [Google CSE](https://lobechat.com/discover/plugin/google-cse)<br/><sup>By **vsnthdev** on **2024-12-02**</sup>                            | Searches Google through their official CSE API.<br/>`web` `search`                                                         |\n| [Tongyi wanxiang Image Generator](https://lobechat.com/discover/plugin/alps-tongyi-image)<br/><sup>By **YoungTx** on **2024-08-09**</sup> | This plugin uses Alibaba's Tongyi Wanxiang model to generate images based on text prompts.<br/>`image` `tongyi` `wanxiang` |\n\n> 📊 Total plugins: [<kbd>**42**</kbd>](https://lobechat.com/discover/plugins)\n\n <!-- PLUGIN LIST -->\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-agent]][docs-feat-agent]\n\n### `11` [Agent Market (GPTs)][docs-feat-agent]\n\nIn LobeChat Agent Marketplace, creators can discover a vibrant and innovative community that brings together a multitude of well-designed agents,\nwhich not only play an important role in work scenarios but also offer great convenience in learning processes.\nOur marketplace is not just a showcase platform but also a collaborative space. Here, everyone can contribute their wisdom and share the agents they have developed.\n\n> \\[!TIP]\n>\n> By [🤖/🏪 Submit Agents][submit-agents-link], you can easily submit your agent creations to our platform.\n> Importantly, LobeChat has established a sophisticated automated internationalization (i18n) workflow,\n> capable of seamlessly translating your agent into multiple language versions.\n> This means that no matter what language your users speak, they can experience your agent without barriers.\n\n> \\[!IMPORTANT]\n>\n> We welcome all users to join this growing ecosystem and participate in the iteration and optimization of agents.\n> Together, we can create more interesting, practical, and innovative agents, further enriching the diversity and practicality of the agent offerings.\n\n<!-- AGENT LIST -->\n\n| Recent Submits                                                                                                                                                                                        | Description                                                                                                                                                      |\n| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [学术论文综述专家](https://lobechat.com/discover/assistant/academic-paper-overview)<br/><sup>By **[arvinxx](https://github.com/arvinxx)** on **2025-03-11**</sup>                                     | 擅长高质量文献检索与分析的学术研究助手<br/>`学术研究` `文献检索` `数据分析` `信息提取` `咨询`                                                                    |\n| [Cron Expression Assistant](https://lobechat.com/discover/assistant/crontab-generate)<br/><sup>By **[edgesider](https://github.com/edgesider)** on **2025-02-17**</sup>                               | Crontab Expression Generator<br/>`crontab` `time-expression` `trigger-time` `generator` `technical-assistance`                                                   |\n| [Xiao Zhi French Translation Assistant](https://lobechat.com/discover/assistant/xiao-zhi-french-translation-asst-v-1)<br/><sup>By **[WeR-Best](https://github.com/WeR-Best)** on **2025-02-10**</sup> | A friendly, professional, and empathetic AI assistant for French translation<br/>`ai-assistant` `french-translation` `cross-cultural-communication` `creativity` |\n| [Investment Assistant](https://lobechat.com/discover/assistant/graham-investmentassi)<br/><sup>By **[farsightlin](https://github.com/farsightlin)** on **2025-02-06**</sup>                           | Helps users calculate the data needed for valuation<br/>`investment` `valuation` `financial-analysis` `calculator`                                               |\n\n> 📊 Total agents: [<kbd>**488**</kbd> ](https://lobechat.com/discover/assistants)\n\n <!-- AGENT LIST -->\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-database]][docs-feat-database]\n\n### `12` [Support Local / Remote Database][docs-feat-database]\n\nLobeChat supports the use of both server-side and local databases. Depending on your needs, you can choose the appropriate deployment solution:\n\n- **Local database**: suitable for users who want more control over their data and privacy protection. LobeChat uses CRDT (Conflict-Free Replicated Data Type) technology to achieve multi-device synchronization. This is an experimental feature aimed at providing a seamless data synchronization experience.\n- **Server-side database**: suitable for users who want a more convenient user experience. LobeChat supports PostgreSQL as a server-side database. For detailed documentation on how to configure the server-side database, please visit [Configure Server-side Database](https://lobehub.com/docs/self-hosting/advanced/server-database).\n\nRegardless of which database you choose, LobeChat can provide you with an excellent user experience.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-auth]][docs-feat-auth]\n\n### `13` [Support Multi-User Management][docs-feat-auth]\n\nLobeChat supports multi-user management and provides two main user authentication and management solutions to meet different needs:\n\n- **next-auth**: LobeChat integrates `next-auth`, a flexible and powerful identity verification library that supports multiple authentication methods, including OAuth, email login, credential login, etc. With `next-auth`, you can easily implement user registration, login, session management, social login, and other functions to ensure the security and privacy of user data.\n\n- [**Clerk**](https://go.clerk.com/exgqLG0): For users who need more advanced user management features, LobeChat also supports `Clerk`, a modern user management platform. `Clerk` provides richer functions, such as multi-factor authentication (MFA), user profile management, login activity monitoring, etc. With `Clerk`, you can get higher security and flexibility, and easily cope with complex user management needs.\n\nRegardless of which user management solution you choose, LobeChat can provide you with an excellent user experience and powerful functional support.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-pwa]][docs-feat-pwa]\n\n### `14` [Progressive Web App (PWA)][docs-feat-pwa]\n\nWe deeply understand the importance of providing a seamless experience for users in today's multi-device environment.\nTherefore, we have adopted Progressive Web Application ([PWA](https://support.google.com/chrome/answer/9658361)) technology,\na modern web technology that elevates web applications to an experience close to that of native apps.\n\nThrough PWA, LobeChat can offer a highly optimized user experience on both desktop and mobile devices while maintaining its lightweight and high-performance characteristics.\nVisually and in terms of feel, we have also meticulously designed the interface to ensure it is indistinguishable from native apps,\nproviding smooth animations, responsive layouts, and adapting to different device screen resolutions.\n\n> \\[!NOTE]\n>\n> If you are unfamiliar with the installation process of PWA, you can add LobeChat as your desktop application (also applicable to mobile devices) by following these steps:\n>\n> - Launch the Chrome or Edge browser on your computer.\n> - Visit the LobeChat webpage.\n> - In the upper right corner of the address bar, click on the <kbd>Install</kbd> icon.\n> - Follow the instructions on the screen to complete the PWA Installation.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-mobile]][docs-feat-mobile]\n\n### `15` [Mobile Device Adaptation][docs-feat-mobile]\n\nWe have carried out a series of optimization designs for mobile devices to enhance the user's mobile experience. Currently, we are iterating on the mobile user experience to achieve smoother and more intuitive interactions. If you have any suggestions or ideas, we welcome you to provide feedback through GitHub Issues or Pull Requests.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-theme]][docs-feat-theme]\n\n### `16` [Custom Themes][docs-feat-theme]\n\nAs a design-engineering-oriented application, LobeChat places great emphasis on users' personalized experiences,\nhence introducing flexible and diverse theme modes, including a light mode for daytime and a dark mode for nighttime.\nBeyond switching theme modes, a range of color customization options allow users to adjust the application's theme colors according to their preferences.\nWhether it's a desire for a sober dark blue, a lively peach pink, or a professional gray-white, users can find their style of color choices in LobeChat.\n\n> \\[!TIP]\n>\n> The default configuration can intelligently recognize the user's system color mode and automatically switch themes to ensure a consistent visual experience with the operating system.\n> For users who like to manually control details, LobeChat also offers intuitive setting options and a choice between chat bubble mode and document mode for conversation scenarios.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n### `*` What's more\n\nBeside these features, LobeChat also have much better basic technique underground:\n\n- [x] 💨 **Quick Deployment**: Using the Vercel platform or docker image, you can deploy with just one click and complete the process within 1 minute without any complex configuration.\n- [x] 🌐 **Custom Domain**: If users have their own domain, they can bind it to the platform for quick access to the dialogue agent from anywhere.\n- [x] 🔒 **Privacy Protection**: All data is stored locally in the user's browser, ensuring user privacy.\n- [x] 💎 **Exquisite UI Design**: With a carefully designed interface, it offers an elegant appearance and smooth interaction. It supports light and dark themes and is mobile-friendly. PWA support provides a more native-like experience.\n- [x] 🗣️ **Smooth Conversation Experience**: Fluid responses ensure a smooth conversation experience. It fully supports Markdown rendering, including code highlighting, LaTex formulas, Mermaid flowcharts, and more.\n\n> ✨ more features will be added when LobeChat evolve.\n\n---\n\n> \\[!NOTE]\n>\n> You can find our upcoming [Roadmap][github-project-link] plans in the Projects section.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## ⚡️ Performance\n\n> \\[!NOTE]\n>\n> The complete list of reports can be found in the [📘 Lighthouse Reports][docs-lighthouse]\n\n|                   Desktop                   |                   Mobile                   |\n| :-----------------------------------------: | :----------------------------------------: |\n|              ![][chat-desktop]              |              ![][chat-mobile]              |\n| [📑 Lighthouse Report][chat-desktop-report] | [📑 Lighthouse Report][chat-mobile-report] |\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## 🛳 Self Hosting\n\nLobeChat provides Self-Hosted Version with Vercel, Alibaba Cloud, and [Docker Image][docker-release-link]. This allows you to deploy your own chatbot within a few minutes without any prior knowledge.\n\n> \\[!TIP]\n>\n> Learn more about [📘 Build your own LobeChat][docs-self-hosting] by checking it out.\n\n### `A` Deploying with Vercel, Zeabur , Sealos or Alibaba Cloud\n\n\"If you want to deploy this service yourself on Vercel, Zeabur or Alibaba Cloud, you can follow these steps:\n\n- Prepare your [OpenAI API Key](https://platform.openai.com/account/api-keys).\n- Click the button below to start deployment: Log in directly with your GitHub account, and remember to fill in the `OPENAI_API_KEY`(required) and `ACCESS_CODE` (recommended) on the environment variable section.\n- After deployment, you can start using it.\n- Bind a custom domain (optional): The DNS of the domain assigned by Vercel is polluted in some areas; binding a custom domain can connect directly.\n\n<div align=\"center\">\n\n|           Deploy with Vercel            |                     Deploy with Zeabur                      |                     Deploy with Sealos                      |                       Deploy with RepoCloud                       |                         Deploy with Alibaba Cloud                         |\n| :-------------------------------------: | :---------------------------------------------------------: | :---------------------------------------------------------: | :---------------------------------------------------------------: | :-----------------------------------------------------------------------: |\n| [![][deploy-button-image]][deploy-link] | [![][deploy-on-zeabur-button-image]][deploy-on-zeabur-link] | [![][deploy-on-sealos-button-image]][deploy-on-sealos-link] | [![][deploy-on-repocloud-button-image]][deploy-on-repocloud-link] | [![][deploy-on-alibaba-cloud-button-image]][deploy-on-alibaba-cloud-link] |\n\n</div>\n\n#### After Fork\n\nAfter fork, only retain the upstream sync action and disable other actions in your repository on GitHub.\n\n#### Keep Updated\n\nIf you have deployed your own project following the one-click deployment steps in the README, you might encounter constant prompts indicating \"updates available.\" This is because Vercel defaults to creating a new project instead of forking this one, resulting in an inability to detect updates accurately.\n\n> \\[!TIP]\n>\n> We suggest you redeploy using the following steps, [📘 Auto Sync With Latest][docs-upstream-sync]\n\n<br/>\n\n### `B` Deploying with Docker\n\n[![][docker-release-shield]][docker-release-link]\n[![][docker-size-shield]][docker-size-link]\n[![][docker-pulls-shield]][docker-pulls-link]\n\nWe provide a Docker image for deploying the LobeChat service on your own private device. Use the following command to start the LobeChat service:\n\n1. create a folder to for storage files\n\n```fish\n$ mkdir lobe-chat-db && cd lobe-chat-db\n```\n\n2. init the LobeChat infrastructure\n\n```fish\nbash <(curl -fsSL https://lobe.li/setup.sh)\n```\n\n3. Start the LobeChat service\n\n```fish\ndocker compose up -d\n```\n\n> \\[!NOTE]\n>\n> For detailed instructions on deploying with Docker, please refer to the [📘 Docker Deployment Guide][docs-docker]\n\n<br/>\n\n### Environment Variable\n\nThis project provides some additional configuration items set with environment variables:\n\n| Environment Variable | Required | Description                                                                                                                                                               | Example                                                                                                              |\n| -------------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |\n| `OPENAI_API_KEY`     | Yes      | This is the API key you apply on the OpenAI account page                                                                                                                  | `sk-xxxxxx...xxxxxx`                                                                                                 |\n| `OPENAI_PROXY_URL`   | No       | If you manually configure the OpenAI interface proxy, you can use this configuration item to override the default OpenAI API request base URL                             | `https://api.chatanywhere.cn` or `https://aihubmix.com/v1` <br/>The default value is<br/>`https://api.openai.com/v1` |\n| `ACCESS_CODE`        | No       | Add a password to access this service; you can set a long password to avoid leaking. If this value contains a comma, it is a password array.                              | `awCTe)re_r74` or `rtrt_ewee3@09!` or `code1,code2,code3`                                                            |\n| `OPENAI_MODEL_LIST`  | No       | Used to control the model list. Use `+` to add a model, `-` to hide a model, and `model_name=display_name` to customize the display name of a model, separated by commas. | `qwen-7b-chat,+glm-6b,-gpt-3.5-turbo`                                                                                |\n\n> \\[!NOTE]\n>\n> The complete list of environment variables can be found in the [📘 Environment Variables][docs-env-var]\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## 📦 Ecosystem\n\n| NPM                               | Repository                              | Description                                                                                           | Version                                   |\n| --------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------------------------------- | ----------------------------------------- |\n| [@lobehub/ui][lobe-ui-link]       | [lobehub/lobe-ui][lobe-ui-github]       | Open-source UI component library dedicated to building AIGC web applications.                         | [![][lobe-ui-shield]][lobe-ui-link]       |\n| [@lobehub/icons][lobe-icons-link] | [lobehub/lobe-icons][lobe-icons-github] | Popular AI / LLM Model Brand SVG Logo and Icon Collection.                                            | [![][lobe-icons-shield]][lobe-icons-link] |\n| [@lobehub/tts][lobe-tts-link]     | [lobehub/lobe-tts][lobe-tts-github]     | High-quality & reliable TTS/STT React Hooks library                                                   | [![][lobe-tts-shield]][lobe-tts-link]     |\n| [@lobehub/lint][lobe-lint-link]   | [lobehub/lobe-lint][lobe-lint-github]   | Configurations for ESlint, Stylelint, Commitlint, Prettier, Remark, and Semantic Release for LobeHub. | [![][lobe-lint-shield]][lobe-lint-link]   |\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## 🧩 Plugins\n\nPlugins provide a means to extend the [Function Calling][docs-functionc-call] capabilities of LobeChat. They can be used to introduce new function calls and even new ways to render message results. If you are interested in plugin development, please refer to our [📘 Plugin Development Guide][docs-plugin-dev] in the Wiki.\n\n- [lobe-chat-plugins][lobe-chat-plugins]: This is the plugin index for LobeChat. It accesses index.json from this repository to display a list of available plugins for LobeChat to the user.\n- [chat-plugin-template][chat-plugin-template]: This is the plugin template for LobeChat plugin development.\n- [@lobehub/chat-plugin-sdk][chat-plugin-sdk]: The LobeChat Plugin SDK assists you in creating exceptional chat plugins for Lobe Chat.\n- [@lobehub/chat-plugins-gateway][chat-plugins-gateway]: The LobeChat Plugins Gateway is a backend service that provides a gateway for LobeChat plugins. We deploy this service using Vercel. The primary API POST /api/v1/runner is deployed as an Edge Function.\n\n> \\[!NOTE]\n>\n> The plugin system is currently undergoing major development. You can learn more in the following issues:\n>\n> - [x] [**Plugin Phase 1**](https://github.com/lobehub/lobe-chat/issues/73): Implement separation of the plugin from the main body, split the plugin into an independent repository for maintenance, and realize dynamic loading of the plugin.\n> - [x] [**Plugin Phase 2**](https://github.com/lobehub/lobe-chat/issues/97): The security and stability of the plugin's use, more accurately presenting abnormal states, the maintainability of the plugin architecture, and developer-friendly.\n> - [x] [**Plugin Phase 3**](https://github.com/lobehub/lobe-chat/issues/149): Higher-level and more comprehensive customization capabilities, support for plugin authentication, and examples.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## ⌨️ Local Development\n\nYou can use GitHub Codespaces for online development:\n\n[![][codespaces-shield]][codespaces-link]\n\nOr clone it for local development:\n\n```fish\n$ git clone https://github.com/lobehub/lobe-chat.git\n$ cd lobe-chat\n$ pnpm install\n$ pnpm dev\n```\n\nIf you would like to learn more details, please feel free to look at our [📘 Development Guide][docs-dev-guide].\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## 🤝 Contributing\n\nContributions of all types are more than welcome; if you are interested in contributing code, feel free to check out our GitHub [Issues][github-issues-link] and [Projects][github-project-link] to get stuck in to show us what you're made of.\n\n> \\[!TIP]\n>\n> We are creating a technology-driven forum, fostering knowledge interaction and the exchange of ideas that may culminate in mutual inspiration and collaborative innovation.\n>\n> Help us make LobeChat better. Welcome to provide product design feedback, user experience discussions directly to us.\n>\n> **Principal Maintainers:** [@arvinxx](https://github.com/arvinxx) [@canisminor1990](https://github.com/canisminor1990)\n\n[![][pr-welcome-shield]][pr-welcome-link]\n[![][submit-agents-shield]][submit-agents-link]\n[![][submit-plugin-shield]][submit-plugin-link]\n\n<a href=\"https://github.com/lobehub/lobe-chat/graphs/contributors\" target=\"_blank\">\n  <table>\n    <tr>\n      <th colspan=\"2\">\n        <br><img src=\"https://contrib.rocks/image?repo=lobehub/lobe-chat\"><br><br>\n      </th>\n    </tr>\n    <tr>\n      <td>\n        <picture>\n          <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?activity=active&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=2x3&color_scheme=dark\">\n          <img src=\"https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?activity=active&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=2x3&color_scheme=light\">\n        </picture>\n      </td>\n      <td rowspan=\"2\">\n        <picture>\n          <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://next.ossinsight.io/widgets/official/compose-org-participants-growth/thumbnail.png?activity=active&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=4x7&color_scheme=dark\">\n          <img src=\"https://next.ossinsight.io/widgets/official/compose-org-participants-growth/thumbnail.png?activity=active&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=4x7&color_scheme=light\">\n        </picture>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        <picture>\n          <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?activity=new&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=2x3&color_scheme=dark\">\n          <img src=\"https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?activity=new&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=2x3&color_scheme=light\">\n        </picture>\n      </td>\n    </tr>\n  </table>\n</a>\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## ❤️ Sponsor\n\nEvery bit counts and your one-time donation sparkles in our galaxy of support! You're a shooting star, making a swift and bright impact on our journey. Thank you for believing in us – your generosity guides us toward our mission, one brilliant flash at a time.\n\n<a href=\"https://opencollective.com/lobehub\" target=\"_blank\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/lobehub/.github/blob/main/static/sponsor-dark.png?raw=true\">\n    <img  src=\"https://github.com/lobehub/.github/blob/main/static/sponsor-light.png?raw=true\">\n  </picture>\n</a>\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## 🔗 More Products\n\n- **[🅰️ Lobe SD Theme][lobe-theme]:** Modern theme for Stable Diffusion WebUI, exquisite interface design, highly customizable UI, and efficiency-boosting features.\n- **[⛵️ Lobe Midjourney WebUI][lobe-midjourney-webui]:** WebUI for Midjourney, leverages AI to quickly generate a wide array of rich and diverse images from text prompts, sparking creativity and enhancing conversations.\n- **[🌏 Lobe i18n][lobe-i18n] :** Lobe i18n is an automation tool for the i18n (internationalization) translation process, powered by ChatGPT. It supports features such as automatic splitting of large files, incremental updates, and customization options for the OpenAI model, API proxy, and temperature.\n- **[💌 Lobe Commit][lobe-commit]:** Lobe Commit is a CLI tool that leverages Langchain/ChatGPT to generate Gitmoji-based commit messages.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n---\n\n<details><summary><h4>📝 License</h4></summary>\n\n[![][fossa-license-shield]][fossa-license-link]\n\n</details>\n\nCopyright © 2025 [LobeHub][profile-link]. <br />\nThis project is [Apache 2.0](./LICENSE) licensed.\n\n<!-- LINK GROUP -->\n\n[back-to-top]: https://img.shields.io/badge/-BACK_TO_TOP-151515?style=flat-square\n[blog]: https://lobehub.com/blog\n[changelog]: https://lobehub.com/changelog\n[chat-desktop]: https://raw.githubusercontent.com/lobehub/lobe-chat/lighthouse/lighthouse/chat/desktop/pagespeed.svg\n[chat-desktop-report]: https://lobehub.github.io/lobe-chat/lighthouse/chat/desktop/chat_preview_lobehub_com_chat.html\n[chat-mobile]: https://raw.githubusercontent.com/lobehub/lobe-chat/lighthouse/lighthouse/chat/mobile/pagespeed.svg\n[chat-mobile-report]: https://lobehub.github.io/lobe-chat/lighthouse/chat/mobile/chat_preview_lobehub_com_chat.html\n[chat-plugin-sdk]: https://github.com/lobehub/chat-plugin-sdk\n[chat-plugin-template]: https://github.com/lobehub/chat-plugin-template\n[chat-plugins-gateway]: https://github.com/lobehub/chat-plugins-gateway\n[codecov-link]: https://codecov.io/gh/lobehub/lobe-chat\n[codecov-shield]: https://img.shields.io/codecov/c/github/lobehub/lobe-chat?labelColor=black&style=flat-square&logo=codecov&logoColor=white\n[codespaces-link]: https://codespaces.new/lobehub/lobe-chat\n[codespaces-shield]: https://github.com/codespaces/badge.svg\n[deploy-button-image]: https://vercel.com/button\n[deploy-link]: https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat&env=OPENAI_API_KEY,ACCESS_CODE&envDescription=Find%20your%20OpenAI%20API%20Key%20by%20click%20the%20right%20Learn%20More%20button.%20%7C%20Access%20Code%20can%20protect%20your%20website&envLink=https%3A%2F%2Fplatform.openai.com%2Faccount%2Fapi-keys&project-name=lobe-chat&repository-name=lobe-chat\n[deploy-on-alibaba-cloud-button-image]: https://service-info-public.oss-cn-hangzhou.aliyuncs.com/computenest-en.svg\n[deploy-on-alibaba-cloud-link]: https://computenest.console.aliyun.com/service/instance/create/default?type=user&ServiceName=LobeChat%E7%A4%BE%E5%8C%BA%E7%89%88\n[deploy-on-repocloud-button-image]: https://d16t0pc4846x52.cloudfront.net/deploylobe.svg\n[deploy-on-repocloud-link]: https://repocloud.io/details/?app_id=248\n[deploy-on-sealos-button-image]: https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg\n[deploy-on-sealos-link]: https://template.usw.sealos.io/deploy?templateName=lobe-chat-db\n[deploy-on-zeabur-button-image]: https://zeabur.com/button.svg\n[deploy-on-zeabur-link]: https://zeabur.com/templates/VZGGTI\n[discord-link]: https://discord.gg/AYFPHvv2jT\n[discord-shield]: https://img.shields.io/discord/1127171173982154893?color=5865F2&label=discord&labelColor=black&logo=discord&logoColor=white&style=flat-square\n[discord-shield-badge]: https://img.shields.io/discord/1127171173982154893?color=5865F2&label=discord&labelColor=black&logo=discord&logoColor=white&style=for-the-badge\n[docker-pulls-link]: https://hub.docker.com/r/lobehub/lobe-chat-database\n[docker-pulls-shield]: https://img.shields.io/docker/pulls/lobehub/lobe-chat-database?color=45cc11&labelColor=black&style=flat-square&sort=semver\n[docker-release-link]: https://hub.docker.com/r/lobehub/lobe-chat-database\n[docker-release-shield]: https://img.shields.io/docker/v/lobehub/lobe-chat-database?color=369eff&label=docker&labelColor=black&logo=docker&logoColor=white&style=flat-square&sort=semver\n[docker-size-link]: https://hub.docker.com/r/lobehub/lobe-chat-database\n[docker-size-shield]: https://img.shields.io/docker/image-size/lobehub/lobe-chat-database?color=369eff&labelColor=black&style=flat-square&sort=semver\n[docs]: https://lobehub.com/docs/usage/start\n[docs-dev-guide]: https://github.com/lobehub/lobe-chat/wiki/index\n[docs-docker]: https://lobehub.com/docs/self-hosting/server-database/docker-compose\n[docs-env-var]: https://lobehub.com/docs/self-hosting/environment-variables\n[docs-feat-agent]: https://lobehub.com/docs/usage/features/agent-market\n[docs-feat-artifacts]: https://lobehub.com/docs/usage/features/artifacts\n[docs-feat-auth]: https://lobehub.com/docs/usage/features/auth\n[docs-feat-branch]: https://lobehub.com/docs/usage/features/branching-conversations\n[docs-feat-cot]: https://lobehub.com/docs/usage/features/cot\n[docs-feat-database]: https://lobehub.com/docs/usage/features/database\n[docs-feat-knowledgebase]: https://lobehub.com/blog/knowledge-base\n[docs-feat-local]: https://lobehub.com/docs/usage/features/local-llm\n[docs-feat-mobile]: https://lobehub.com/docs/usage/features/mobile\n[docs-feat-plugin]: https://lobehub.com/docs/usage/features/plugin-system\n[docs-feat-provider]: https://lobehub.com/docs/usage/features/multi-ai-providers\n[docs-feat-pwa]: https://lobehub.com/docs/usage/features/pwa\n[docs-feat-t2i]: https://lobehub.com/docs/usage/features/text-to-image\n[docs-feat-theme]: https://lobehub.com/docs/usage/features/theme\n[docs-feat-tts]: https://lobehub.com/docs/usage/features/tts\n[docs-feat-vision]: https://lobehub.com/docs/usage/features/vision\n[docs-functionc-call]: https://lobehub.com/blog/openai-function-call\n[docs-lighthouse]: https://github.com/lobehub/lobe-chat/wiki/Lighthouse\n[docs-plugin-dev]: https://lobehub.com/docs/usage/plugins/development\n[docs-self-hosting]: https://lobehub.com/docs/self-hosting/start\n[docs-upstream-sync]: https://lobehub.com/docs/self-hosting/advanced/upstream-sync\n[docs-usage-ollama]: https://lobehub.com/docs/usage/providers/ollama\n[docs-usage-plugin]: https://lobehub.com/docs/usage/plugins/basic\n[fossa-license-link]: https://app.fossa.com/projects/git%2Bgithub.com%2Flobehub%2Flobe-chat\n[fossa-license-shield]: https://app.fossa.com/api/projects/git%2Bgithub.com%2Flobehub%2Flobe-chat.svg?type=large\n[github-action-release-link]: https://github.com/actions/workflows/lobehub/lobe-chat/release.yml\n[github-action-release-shield]: https://img.shields.io/github/actions/workflow/status/lobehub/lobe-chat/release.yml?label=release&labelColor=black&logo=githubactions&logoColor=white&style=flat-square\n[github-action-test-link]: https://github.com/actions/workflows/lobehub/lobe-chat/test.yml\n[github-action-test-shield]: https://img.shields.io/github/actions/workflow/status/lobehub/lobe-chat/test.yml?label=test&labelColor=black&logo=githubactions&logoColor=white&style=flat-square\n[github-contributors-link]: https://github.com/lobehub/lobe-chat/graphs/contributors\n[github-contributors-shield]: https://img.shields.io/github/contributors/lobehub/lobe-chat?color=c4f042&labelColor=black&style=flat-square\n[github-forks-link]: https://github.com/lobehub/lobe-chat/network/members\n[github-forks-shield]: https://img.shields.io/github/forks/lobehub/lobe-chat?color=8ae8ff&labelColor=black&style=flat-square\n[github-issues-link]: https://github.com/lobehub/lobe-chat/issues\n[github-issues-shield]: https://img.shields.io/github/issues/lobehub/lobe-chat?color=ff80eb&labelColor=black&style=flat-square\n[github-license-link]: https://github.com/lobehub/lobe-chat/blob/main/LICENSE\n[github-license-shield]: https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&style=flat-square\n[github-project-link]: https://github.com/lobehub/lobe-chat/projects\n[github-release-link]: https://github.com/lobehub/lobe-chat/releases\n[github-release-shield]: https://img.shields.io/github/v/release/lobehub/lobe-chat?color=369eff&labelColor=black&logo=github&style=flat-square\n[github-releasedate-link]: https://github.com/lobehub/lobe-chat/releases\n[github-releasedate-shield]: https://img.shields.io/github/release-date/lobehub/lobe-chat?labelColor=black&style=flat-square\n[github-stars-link]: https://github.com/lobehub/lobe-chat/network/stargazers\n[github-stars-shield]: https://img.shields.io/github/stars/lobehub/lobe-chat?color=ffcb47&labelColor=black&style=flat-square\n[github-trending-shield]: https://trendshift.io/api/badge/repositories/2256\n[github-trending-url]: https://trendshift.io/repositories/2256\n[image-banner]: https://github.com/user-attachments/assets/6f293c7f-47b4-47eb-9202-fe68a942d35b\n[image-feat-agent]: https://github.com/user-attachments/assets/b3ab6e35-4fbc-468d-af10-e3e0c687350f\n[image-feat-artifacts]: https://github.com/user-attachments/assets/7f95fad6-b210-4e6e-84a0-7f39e96f3a00\n[image-feat-auth]: https://github.com/user-attachments/assets/80bb232e-19d1-4f97-98d6-e291f3585e6d\n[image-feat-branch]: https://github.com/user-attachments/assets/92f72082-02bd-4835-9c54-b089aad7fd41\n[image-feat-cot]: https://github.com/user-attachments/assets/f74f1139-d115-4e9c-8c43-040a53797a5e\n[image-feat-database]: https://github.com/user-attachments/assets/f1697c8b-d1fb-4dac-ba05-153c6295d91d\n[image-feat-knowledgebase]: https://github.com/user-attachments/assets/7da7a3b2-92fd-4630-9f4e-8560c74955ae\n[image-feat-local]: https://github.com/user-attachments/assets/1239da50-d832-4632-a7ef-bd754c0f3850\n[image-feat-mobile]: https://github.com/user-attachments/assets/32cf43c4-96bd-4a4c-bfb6-59acde6fe380\n[image-feat-plugin]: https://github.com/user-attachments/assets/66a891ac-01b6-4e3f-b978-2eb07b489b1b\n[image-feat-privoder]: https://github.com/user-attachments/assets/e553e407-42de-4919-977d-7dbfcf44a821\n[image-feat-pwa]: https://github.com/user-attachments/assets/9647f70f-b71b-43b6-9564-7cdd12d1c24d\n[image-feat-t2i]: https://github.com/user-attachments/assets/708274a7-2458-494b-a6ec-b73dfa1fa7c2\n[image-feat-theme]: https://github.com/user-attachments/assets/b47c39f1-806f-492b-8fcb-b0fa973937c1\n[image-feat-tts]: https://github.com/user-attachments/assets/50189597-2cc3-4002-b4c8-756a52ad5c0a\n[image-feat-vision]: https://github.com/user-attachments/assets/18574a1f-46c2-4cbc-af2c-35a86e128a07\n[image-overview]: https://github.com/user-attachments/assets/dbfaa84a-2c82-4dd9-815c-5be616f264a4\n[image-star]: https://github.com/user-attachments/assets/c3b482e7-cef5-4e94-bef9-226900ecfaab\n[issues-link]: https://img.shields.io/github/issues/lobehub/lobe-chat.svg?style=flat\n[lobe-chat-plugins]: https://github.com/lobehub/lobe-chat-plugins\n[lobe-commit]: https://github.com/lobehub/lobe-commit/tree/master/packages/lobe-commit\n[lobe-i18n]: https://github.com/lobehub/lobe-commit/tree/master/packages/lobe-i18n\n[lobe-icons-github]: https://github.com/lobehub/lobe-icons\n[lobe-icons-link]: https://www.npmjs.com/package/@lobehub/icons\n[lobe-icons-shield]: https://img.shields.io/npm/v/@lobehub/icons?color=369eff&labelColor=black&logo=npm&logoColor=white&style=flat-square\n[lobe-lint-github]: https://github.com/lobehub/lobe-lint\n[lobe-lint-link]: https://www.npmjs.com/package/@lobehub/lint\n[lobe-lint-shield]: https://img.shields.io/npm/v/@lobehub/lint?color=369eff&labelColor=black&logo=npm&logoColor=white&style=flat-square\n[lobe-midjourney-webui]: https://github.com/lobehub/lobe-midjourney-webui\n[lobe-theme]: https://github.com/lobehub/sd-webui-lobe-theme\n[lobe-tts-github]: https://github.com/lobehub/lobe-tts\n[lobe-tts-link]: https://www.npmjs.com/package/@lobehub/tts\n[lobe-tts-shield]: https://img.shields.io/npm/v/@lobehub/tts?color=369eff&labelColor=black&logo=npm&logoColor=white&style=flat-square\n[lobe-ui-github]: https://github.com/lobehub/lobe-ui\n[lobe-ui-link]: https://www.npmjs.com/package/@lobehub/ui\n[lobe-ui-shield]: https://img.shields.io/npm/v/@lobehub/ui?color=369eff&labelColor=black&logo=npm&logoColor=white&style=flat-square\n[official-site]: https://lobehub.com\n[pr-welcome-link]: https://github.com/lobehub/lobe-chat/pulls\n[pr-welcome-shield]: https://img.shields.io/badge/🤯_pr_welcome-%E2%86%92-ffcb47?labelColor=black&style=for-the-badge\n[profile-link]: https://github.com/lobehub\n[share-linkedin-link]: https://linkedin.com/feed\n[share-linkedin-shield]: https://img.shields.io/badge/-share%20on%20linkedin-black?labelColor=black&logo=linkedin&logoColor=white&style=flat-square\n[share-mastodon-link]: https://mastodon.social/share?text=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source,%20extensible%20%28Function%20Calling%29,%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20https://github.com/lobehub/lobe-chat%20#chatbot%20#chatGPT%20#openAI\n[share-mastodon-shield]: https://img.shields.io/badge/-share%20on%20mastodon-black?labelColor=black&logo=mastodon&logoColor=white&style=flat-square\n[share-reddit-link]: https://www.reddit.com/submit?title=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20%23chatbot%20%23chatGPT%20%23openAI&url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat\n[share-reddit-shield]: https://img.shields.io/badge/-share%20on%20reddit-black?labelColor=black&logo=reddit&logoColor=white&style=flat-square\n[share-telegram-link]: https://t.me/share/url\"?text=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20%23chatbot%20%23chatGPT%20%23openAI&url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat\n[share-telegram-shield]: https://img.shields.io/badge/-share%20on%20telegram-black?labelColor=black&logo=telegram&logoColor=white&style=flat-square\n[share-weibo-link]: http://service.weibo.com/share/share.php?sharesource=weibo&title=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20%23chatbot%20%23chatGPT%20%23openAI&url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat\n[share-weibo-shield]: https://img.shields.io/badge/-share%20on%20weibo-black?labelColor=black&logo=sinaweibo&logoColor=white&style=flat-square\n[share-whatsapp-link]: https://api.whatsapp.com/send?text=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat%20%23chatbot%20%23chatGPT%20%23openAI\n[share-whatsapp-shield]: https://img.shields.io/badge/-share%20on%20whatsapp-black?labelColor=black&logo=whatsapp&logoColor=white&style=flat-square\n[share-x-link]: https://x.com/intent/tweet?hashtags=chatbot%2CchatGPT%2CopenAI&text=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.&url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat\n[share-x-shield]: https://img.shields.io/badge/-share%20on%20x-black?labelColor=black&logo=x&logoColor=white&style=flat-square\n[sponsor-link]: https://opencollective.com/lobehub 'Become ❤️ LobeHub Sponsor'\n[sponsor-shield]: https://img.shields.io/badge/-Sponsor%20LobeHub-f04f88?logo=opencollective&logoColor=white&style=flat-square\n[submit-agents-link]: https://github.com/lobehub/lobe-chat-agents\n[submit-agents-shield]: https://img.shields.io/badge/🤖/🏪_submit_agent-%E2%86%92-c4f042?labelColor=black&style=for-the-badge\n[submit-plugin-link]: https://github.com/lobehub/lobe-chat-plugins\n[submit-plugin-shield]: https://img.shields.io/badge/🧩/🏪_submit_plugin-%E2%86%92-95f3d9?labelColor=black&style=for-the-badge\n[vercel-link]: https://chat-preview.lobehub.com\n[vercel-shield]: https://img.shields.io/badge/vercel-online-55b467?labelColor=black&logo=vercel&style=flat-square\n[vercel-shield-badge]: https://img.shields.io/badge/TRY%20LOBECHAT-ONLINE-55b467?labelColor=black&logo=vercel&style=for-the-badge\n",
    "summary": "Lobe Chat是一个开源、现代设计的ChatGPT/LLMs UI框架，支持语音合成、多模态和可扩展的插件系统（函数调用）。它提供一键免费部署私有AI聊天应用的能力，支持OpenAI、Claude、Gemini、Groq、Ollama等多种模型服务商及本地模型。核心功能包括思维链可视化、分支对话、文件上传/知识库、模型视觉识别、TTS/STT语音对话、文生图、插件市场和智能体市场。项目注重性能、隐私保护（本地存储）、精美UI设计（PWA、移动适配、主题定制）和便捷的自我托管部署方式（Vercel, Docker）。",
    "keywords": [
      "开源",
      "大模型",
      "UI框架",
      "智能体",
      "插件系统",
      "多模态",
      "自我托管",
      "自然语言处理"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-05-26T16:10:15+00:00",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "screenshot/github_lobe-chat.png"
    ],
    "extra_info": null
  },
  {
    "id": "Ghost-Downloader-3",
    "source": "GitHub",
    "url": "https://github.com/XiaoYouChR/Ghost-Downloader-3",
    "title": "AI-powered next-generation cross-platform multithreaded downloader",
    "content": "<h4 align=\"right\">\n  <a href=\"README_zh.md\">简体中文</a> | English\n</h4>\n \n> [!IMPORTANT]\n> Due to the developer's preparation for the college entrance exam (Gaokao), project updates are temporarily suspended 😭 Join QQ group [`531928387`](https://qm.qq.com/q/PlUBdzqZCm) for latest updates\n\n> [!NOTE]\n> The project is still in its early stages, and there is still a lot of shortcomings.\n\n> [!TIP]\n> If you want to use Ghost-Downloader-3 on Windows 7, please download the version `v3.5.8-Portable`.\n\n<!-- PROJECT LOGO -->\n<div align=\"center\">\n\n![Banner](resources/banner.webp)\n\n<a href=\"https://trendshift.io/repositories/13847\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/13847\" alt=\"XiaoYouChR%2FGhost-Downloader-3 | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n<h3>\n    AI-powered next-generation cross-platform multithreaded downloader\n</h3>\n\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n[![Release][release-shield]][release-url]\n[![Downloads][downloads-shield]][release-url]\n\n<h4>\n  <a href=\"https://github.com/XiaoYouChR/Ghost-Downloader-3/issues/new?template=bug_report.yml\">Report Bug</a>\n·    \n  <a href=\"https://github.com/XiaoYouChR/Ghost-Downloader-3/issues/new?template=feature_request.yml\">Request Feature</a>\n</h4>\n\n</div>\n\n<!-- ABOUT THE PROJECT -->\n## About The Project\n\n* A downloader developed out of personal interest, and my first Python project 😣\n* Originally intended to help a Bilibili Uploader with resource integration 😵‍💫\n* Features include IDM-like intelligent chunking without file merging, and AI-powered smart boost 🚀\n* Thanks to Python's🐍 accessibility, the project will support plugins🧩 in the future to maximize Python's🐍 advantages\n\n|    Platform    | Required Version |  Architectures   | Compatible |\n|:--------------:|:----------------:|:----------------:|:----------:|\n|  🐧 **Linux**  |  `glibc 2.35+`   | `x86_64`/`arm64` |     ✅      |\n| 🪟 **Windows** |     `7 SP1+`     | `x86_64`/`arm64` |     ✅      |\n|  🍎 **macOS**  |     `11.0+`      | `x86_64`/`arm64` |     ✅      |\n\n> [!TIP]\n> **Arch Linux AUR support**: Community-maintained packages `ghost-downloader-bin` and `ghost-downloader-git` are now available (Maintainer: [@zxp19821005](https://github.com/zxp19821005))\n\n<!-- ROADMAP -->\n## Roadmap\n\n- ✅ Global settings\n- ✅ More detailed download information\n- ✅ Scheduled tasks\n- ✅ Browser extension optimization\n- ✅ Global speed limit\n- ✅ Memory optimization\n  - ✅ Upgrade Qt version\n  - ✅ Implement HttpClient reuse\n  - ✅ Replace some multithreading with coroutines\n- ❌ MVC → MVVM upgrade and a new architecture based on events (In progress...see branch: feature/Plugins)\n- ❌ Enhanced task editing (powerful features like binding multiple Clients to one task)\n- ❌ Magnet/BT download (Considering libtorrent implementation)\n- ❌ Powerful plugin system\n- ❌ Powerful browser extension features\n\nVisit [Open issues](https://github.com/XiaoYouChR/Ghost-Downloader-3/issues) to see all requested features (and known issues).\n\n<!-- SPONSOR -->\n## Sponsor\n\n| [![SignPath](https://signpath.org/assets/favicon-50x50.png)](https://signpath.org/) | Free code signing on Windows provided by [SignPath.io](https://signpath.io), certficate by [SignPath Foundation](https://signpath.org) |\n|-------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|\n\n<!-- CONTRIBUTING -->\n## Contributing\n\nContributions make the open source community an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.\n\nIf you have a suggestion, fork the repo and create a pull request. You can also simply open an issue with the \"Enhancement\" tag. Don't forget to give the project a star⭐! Thanks again!\n\n1. Fork the Project\n2. Create your Feature Branch (git checkout -b feature/AmazingFeature)\n3. Commit your Changes (git commit -m 'Add some AmazingFeature')\n4. Push to the Branch (git push origin feature/AmazingFeature)\n5. Open a Pull Request\n\nThanks to all contributors who have participated in this project!\n\n[![Contributors](http://contrib.nn.ci/api?repo=XiaoYouChR/Ghost-Downloader-3)](https://github.com/XiaoYouChR/Ghost-Downloader-3/graphs/contributors)\n\n<!-- SCREEN SHOTS -->\n## Screenshots\n\n[![Demo Screenshot][product-screenshot]](https://space.bilibili.com/437313511)\n\n<!-- LICENSE -->\n## License\n\nDistributed under the GPL v3.0 License. See `LICENSE` for more information.\n\nCopyright © 2025 XiaoYouChR.\n\n<!-- CONTACT -->\n## Contact\n\n* [E-mail](mailto:XiaoYouChR@qq.com) - XiaoYouChR@qq.com\n* [QQ Group](https://qm.qq.com/q/PlUBdzqZCm) - 531928387\n\n<!-- ACKNOWLEDGMENTS -->\n## References\n\n* [PyQt-Fluent-Widgets](https://github.com/zhiyiYo/PyQt-Fluent-Widgets) Powerful, extensible and beautiful Fluent Design widgets\n* [Httpx](https://github.com/projectdiscovery/httpx) A fast and multi-purpose HTTP toolkit\n* [Aiofiles](https://github.com/Tinche/aiofiles) File support for asyncio\n* [Loguru](https://github.com/Delgan/loguru) A library which aims to bring enjoyable logging in Python\n* [Nuitka](https://github.com/Nuitka/Nuitka) The Python compiler\n* [PySide6](https://github.com/PySide/pyside-setup) The official Python module\n* [Darkdetect](https://github.com/albertosottile/darkdetect) Allow to detect if the user is using Dark Mode on\n* [pyqt5-concurrent](https://github.com/AresConnor/pyqt5-concurrent) A QThreadPool based task concurrency library\n\n## Acknowledgments\n\n* [@zhiyiYo](https://github.com/zhiyiYo/) Provided great help for this project!\n* [@一只透明人-](https://space.bilibili.com/554365148/) Tested almost every version since Ghost-Downloader-1！\n* [@Sky·SuGar](https://github.com/SuGar0218/) Created the project banner！\n\n<picture>\n  <source\n    media=\"(prefers-color-scheme: dark)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=XiaoYouChR/Ghost-Downloader-3&type=Date&theme=dark\n    \"\n  />\n  <source\n    media=\"(prefers-color-scheme: light)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=XiaoYouChR/Ghost-Downloader-3&type=Date&theme=dark\n    \"\n  />\n  <img\n    alt=\"Star History Chart\"\n    src=\"https://api.star-history.com/svg?repos=XiaoYouChR/Ghost-Downloader-3&type=Date&theme=dark\"\n  />\n</picture>\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n[forks-shield]: https://img.shields.io/github/forks/XiaoYouChR/Ghost-Downloader-3.svg?style=for-the-badge\n[forks-url]: https://github.com/XiaoYouChR/Ghost-Downloader-3/network/members\n[stars-shield]: https://img.shields.io/github/stars/XiaoYouChR/Ghost-Downloader-3.svg?style=for-the-badge\n[stars-url]: https://github.com/XiaoYouChR/Ghost-Downloader-3/stargazers\n[issues-shield]: https://img.shields.io/github/issues/XiaoYouChR/Ghost-Downloader-3.svg?style=for-the-badge\n[issues-url]: https://github.com/XiaoYouChR/Ghost-Downloader-3/issues\n[product-screenshot]: resources/screenshot.png\n[release-shield]: https://img.shields.io/github/v/release/XiaoYouChR/Ghost-Downloader-3?style=for-the-badge\n[release-url]: https://github.com/XiaoYouChR/Ghost-Downloader-3/releases/latest\n[downloads-shield]: https://img.shields.io/github/downloads/XiaoYouChR/Ghost-Downloader-3/total?style=for-the-badge",
    "summary": "Ghost-Downloader-3是一款基于Python开发的下一代跨平台多线程下载器。项目处于早期阶段，核心功能包括类似IDM的智能分块下载（无需合并文件）和AI智能加速。该下载器支持Windows、Linux和macOS三大主流操作系统，并计划在未来引入强大的插件系统。项目路线图涵盖了架构升级、增强任务编辑、支持磁力/BT下载等。",
    "keywords": [
      "下载器",
      "多线程",
      "跨平台",
      "Python",
      "人工智能",
      "插件",
      "下载管理"
    ],
    "area": [
      "人工智能",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-25T12:14:47Z",
    "download_time": "2024-05-25 12:15:00",
    "visual_resource": [
      "https://github.com/XiaoYouChR/Ghost-Downloader-3/raw/main/resources/banner.webp",
      "https://github.com/XiaoYouChR/Ghost-Downloader-3/raw/main/resources/screenshot.png"
    ],
    "extra_info": null
  },
  {
    "id": "mindsdb",
    "source": "GitHub",
    "url": "https://github.com/mindsdb/mindsdb",
    "title": "MindsDB",
    "content": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n\t<a href=\"https://pypi.org/project/MindsDB/\" target=\"_blank\"><img src=\"https://badge.fury.io/py/MindsDB.svg\" alt=\"MindsDB Release\"></a>\n\t<a href=\"https://www.python.org/downloads/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg\" alt=\"Python supported\"></a>\n\t<a href=\"https://ossrank.com/p/630\"><img src=\"https://shields.io/endpoint?url=https://ossrank.com/shield/630\"></a>\n\t<img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dm/Mindsdb\">\n\t<a href=\"https://hub.docker.com/u/mindsdb\" target=\"_blank\"><img src=\"https://img.shields.io/docker/pulls/mindsdb/mindsdb\" alt=\"Docker pulls\"></a>\n\n  <br />\n  <br />\n\n  <a href=\"https://github.com/mindsdb/mindsdb\">\n    <img src=\"/docs/assets/mindsdb_logo.png\" alt=\"MindsDB\" width=\"300\">\n  </a>\n\n  <p align=\"center\">\n    <br />\n    <a href=\"https://www.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\">Website</a>\n    ·\n    <a href=\"https://docs.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\">Docs</a>\n    ·\n    <a href=\"https://mdb.ai/register\">Demo</a>\n    ·\n    <a href=\"https://mindsdb.com/joincommunity?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\">Community Slack</a>\n  </p>\n</div>\n\n----------------------------------------\n\n\nMindsDB enables humans, AI, agents, and applications to get highly accurate answers across sprawled and large scale data sources.\n\n<img width=\"1454\" alt=\"image\" src=\"https://github.com/user-attachments/assets/87930824-2624-4c71-ac08-475e12e5475f\" />\n\n[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated data—spanning databases, data warehouses, and SaaS applications.\n\n## Minds [Demo](https://mdb.ai/register)\nPlay with [Minds demo](https://mdb.ai/register), and see the power of MindsDB at answering questions from structured to unstructured data, whether it's scattered across SaaS applications, databases, or... hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered.\n \n## Install MindsDB Server \n\nMindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.\n\n  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.\n  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.\n  * [Using PyPI](https://docs.mindsdb.com/contribute/install). This option enables you to contribute to MindsDB.\n\n----------------------------------------\n\n# Core Philosophy: Connect, Unify, Respond\n\nMindsDB's architecture is built around three fundamental capabilities:\n\n## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data\n\nYou can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.\n\n## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data\n\nOnce connected, these data sources can be queried using a full SQL dialect, as if they were all part of a single database. MindsDB’s federated query engine translates your SQL queries and executes them on the appropriate connected data sources.\n\nWhen working with many data sources, it’s important to prepare and unify your data before generating responses from it. MindsDB SQL offers virtual tables (views, knowledge bases, ml-models) to allow working with heterogeneous data as if it were unified in a single organized system.\n\n* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) – Simplify data access by creating unified views across different sources (no-ETL).\n* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) – Index and organize unstructured data for efficient retrieval.\n* [**ML MODELS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/model) – Apply AI/ML transformations to gain insights from your data.\n\nUnification of data can be automated using JOBs\n\n* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) – Schedule synchronization and transformation tasks for real-time processing.\n\n\n## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data\n\nChat with Your Data\n\n* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) – Configure built-in agents specialized in answering questions over your connected and unified data.\n* [**MCP**](https://docs.mindsdb.com/mcp/overview) – Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.\n\n----------------------------------------\n\n## 🤝 Contribute\n\nInterested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\nYou can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\nWe welcome suggestions! Feel free to open new issues with your ideas, and we’ll guide you.\n\nThis project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.\n\nAlso, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\n## 🤍 Support\n\nIf you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).\n\nHere’s how you can get community support:\n\n* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).\n* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.\n\nFor commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\n## 💚 Current Contributors\n\n<a href=\"https://github.com/mindsdb/mindsdb/graphs/contributors\">\n  <img src=\"https://contributors-img.web.app/image?repo=mindsdb/mindsdb\" />\n</a>\n\nGenerated with [contributors-img](https://contributors-img.web.app).\n\n## 🔔 Subscribe for Updates\n\nJoin our [Slack community](https://mindsdb.com/joincommunity)",
    "summary": "MindsDB是一个开源的AI数据库层，旨在帮助用户、AI、智能体和应用从分散的大规模数据源（包括数据库、数据仓库和SaaS应用）中获取准确答案。其核心理念是连接、统一和响应数据。MindsDB支持连接数百种数据源，通过SQL方言统一查询，并提供虚拟表（视图、知识库、ML模型、任务）进行数据准备和转换。用户可以通过内置智能体或MCP协议与数据进行交互。它支持Docker和PyPI部署，可广泛应用于构建基于现有数据基础设施的AI应用。",
    "keywords": [
      "AI数据库",
      "智能体",
      "数据集成",
      "联邦查询",
      "SQL接口",
      "机器学习模型",
      "知识库",
      "开源"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "智能体"
    ],
    "published_time": "2025-05-26T14:00:36+00:00",
    "download_time": "2024-05-27 10:00:00",
    "visual_resource": [
      "screenshot/github_mindsdb.png"
    ],
    "extra_info": null
  },
  {
    "id": "computer-science",
    "source": "GitHub",
    "url": "https://github.com/ossu/computer-science",
    "title": "Open Source Society University",
    "content": "<div align=\"center\" style=\"text-align: center\">\n<img src=\"images/ossu-logo.webp\" alt=\"Open Source Society logo\"/>\n<h3>Open Source Society University</h3>\n<p>\n  Path to a free self-taught education in Computer Science!\n</p>\n<p>\n  <a href=\"https://github.com/sindresorhus/awesome\">\n    <img alt=\"Awesome\" src=\"https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg\"\n  ></a>\n  <a href=\"https://github.com/ossu/computer-science\">\n    <img alt=\"Open Source Society University - Computer Science\" src=\"https://img.shields.io/badge/OSSU-computer--science-blue.svg\"\n  ></a>\n</p>\n</div>\n\n# Contents\n\n- [Summary](#summary)\n- [Community](#community)\n- [Curriculum](#curriculum)\n- [Code of conduct](#code-of-conduct)\n- [Team](#team)\n\n# Summary\n\nThe OSSU curriculum is a **complete education in computer science** using online materials.\nIt's not merely for career training or professional development.\nIt's for those who want a proper, *well-rounded* grounding in concepts fundamental to all computing disciplines,\nand for those who have the discipline, will, and (most importantly!) good habits to obtain this education largely on their own,\nbut with support from a worldwide community of fellow learners.\n\nIt is designed according to the degree requirements of undergraduate computer science majors, minus general education (non-CS) requirements,\nas it is assumed most of the people following this curriculum are already educated outside the field of CS.\nThe courses themselves are among the very best in the world, often coming from Harvard, Princeton, MIT, etc.,\nbut specifically chosen to meet the following criteria.\n\n**Courses must**:\n- Be open for enrollment\n- Run regularly (ideally in self-paced format, otherwise running multiple times per year)\n- Be of generally high quality in teaching materials and pedagogical principles\n- Match the curricular standards of the [CS 2013](CURRICULAR_GUIDELINES.md): Curriculum Guidelines for Undergraduate Degree Programs in Computer Science\n\nWhen no course meets the above criteria, the coursework is supplemented with a book.\nWhen there are courses or books that don't fit into the curriculum but are otherwise of high quality,\nthey belong in [extras/courses](extras/courses.md) or [extras/readings](extras/readings.md).\n\n**Organization**. The curriculum is designed as follows:\n- *Intro CS*: for students to try out CS and see if it's right for them\n- *Core CS*: corresponds roughly to the first three years of a computer science curriculum, taking classes that all majors would be required to take\n- *Advanced CS*: corresponds roughly to the final year of a computer science curriculum, taking electives according to the student's interests\n- *Final Project*: a project for students to validate, consolidate, and display their knowledge, to be evaluated by their peers worldwide\n\n**Duration**. It is possible to finish within about 2 years if you plan carefully and devote roughly 20 hours/week to your studies. Learners can use [this spreadsheet](https://docs.google.com/spreadsheets/u/3/d/1Std_G_5dnajzm289vlsthIJPFnuxN5yOYNDOoiz9Juc/copy) to estimate their end date. Make a copy and input your start date and expected hours per week in the `Timeline` sheet. As you work through courses you can enter your actual course completion dates in the `Curriculum Data` sheet and get updated completion estimates.\n  \n> **Warning:** While the spreadsheet is a useful tool to estimate the time you need to complete this curriculum, it may not always be up-to-date with the curriculum. Use the [OSSU CS website](https://cs.ossu.dev) or [the repo](https://github.com/ossu/computer-science) to see what courses to do.\n\n**Cost**. All or nearly all course material is available for free. However, some courses may charge money for assignments/tests/projects to be graded.\nNote that both [Coursera](https://www.coursera.support/s/article/209819033-Apply-for-Financial-Aid-or-a-Scholarship?language=en_US) and [edX](https://courses.edx.org/financial-assistance/) offer financial aid.\n\nDecide how much or how little to spend based on your own time and budget;\njust remember that you can't purchase success!\n\n**Process**. Students can work through the curriculum alone or in groups, in order or out of order.\n- We recommend doing all courses in Core CS, only skipping a course when you are certain that you've already learned the material previously.\n- For simplicity, we recommend working through courses (especially Core CS) in order from top to bottom. Some students choose to study multiple courses at a time in order to vary the material they are working on in a day/week. A popular option is to take the math courses in parallel with the introductory courses. Course prerequisites are listed to help you determine if you are prepared for a given course.\n- Courses in Advanced CS are electives. Choose one subject (e.g. Advanced programming) you want to become an expert in and take all the courses under that heading. You can also create your own custom subject; the Discord community may provide feedback on your planned subject.\n\n**Content policy**. If you plan on showing off some of your coursework publicly, you must share only files that you are allowed to.\n*Respect the code of conduct* that you signed in the beginning of each course!\n\n**[How to contribute](CONTRIBUTING.md)**\n\n**[Getting help](HELP.md)** (Details about our FAQ and chatroom)\n\n# Community\n\n- We have a Discord server! [![Discord](https://img.shields.io/discord/744385009028431943.svg?label=&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2)](https://discord.gg/wuytwK5s9h) This should be your first stop to talk with other OSSU students. Why don't you introduce yourself right now? [Join the OSSU Discord](https://discord.gg/wuytwK5s9h)\n- You can also interact through GitHub issues. If there is a problem with a course, or a change needs to be made to the curriculum, this is the place to start the conversation. Read more [here](CONTRIBUTING.md).\n- Add **Open Source Society University** to your [Linkedin](https://www.linkedin.com/school/11272443/) profile!\n\n> **Warning:** There are a few third-party/deprecated/outdated material that you might find when searching for OSSU. We recommend you to ignore them, and only use the [OSSU CS website](https://cs.ossu.dev) or [OSSU CS Github Repo](https://github.com/ossu/computer-science). Some known outdated materials are:\n>  - An unmaintained and deprecated firebase app. Read more in the [FAQ](./FAQ.md#why-is-the-firebase-ossu-app-different-or-broken).\n>  - An unmaintained and deprecated trello board\n>  - Third-party notion templates\n\n# Curriculum\n\n- [Prerequisites](#prerequisites)\n- [Intro CS](#intro-cs)\n- [Core CS](#core-cs)\n  - [Core programming](#core-programming)\n  - [Core math](#core-math)\n  - [CS Tools](#cs-tools)\n  - [Core systems](#core-systems)\n  - [Core theory](#core-theory)\n  - [Core security](#core-security)\n  - [Core applications](#core-applications)\n  - [Core ethics](#core-ethics)\n- [Advanced CS](#advanced-cs)\n  - [Advanced programming](#advanced-programming)\n  - [Advanced systems](#advanced-systems)\n  - [Advanced theory](#advanced-theory)\n  - [Advanced information security](#advanced-information-security)\n  - [Advanced math](#advanced-math)\n- [Final project](#final-project)\n\n---\n\n## Prerequisites\n\n- [Core CS](#core-cs) assumes the student has already taken [high school math](https://ossu.dev/precollege-math), including algebra, geometry, and pre-calculus.\n- [Advanced CS](#advanced-cs) assumes the student has already taken the entirety of Core CS\nand is knowledgeable enough now to decide which electives to take.\n- Note that [Advanced systems](#advanced-systems) assumes the student has taken a basic physics course (e.g. AP Physics in high school).\n\n## Intro CS\n\nThis course will introduce you to the world of computer science and programming. This course gives you a flavor of the material to come. If you finish the course wanting more, Computer Science is likely for you!\n\n**Topics covered**:\n`computation`\n`imperative programming`\n`basic data structures and algorithms`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Introduction to Computer Science and Programming using Python](coursepages/intro-cs/README.md) | 14 weeks | 6-10 hours/week | [high school algebra](https://ossu.dev/precollege-math) | [chat](https://discord.gg/jvchSm9)\n\n## Core CS\n\nAll coursework under Core CS is **required**, unless otherwise indicated.\n\n### Core programming\n**Topics covered**:\n`functional programming`\n`design for testing`\n`program requirements`\n`common design patterns`\n`unit testing`\n`object-oriented design`\n`static typing`\n`dynamic typing`\n`ML-family languages (via Standard ML)`\n`Lisp-family languages (via Racket)`\n`Ruby`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Systematic Program Design](coursepages/spd/README.md) | 13 weeks | 8-10 hours/week | none | chat: [part 1](https://discord.gg/RfqAmGJ) / [part 2](https://discord.gg/kczJzpm)\n[Class-based Program Design](https://course.ccs.neu.edu/cs2510sp22/index.html) | 13 weeks | 5-10 hours/week | Systematic Program Design, High School Math | [chat](https://discord.com/channels/744385009028431943/891411727294562314)\n[Programming Languages, Part A](https://www.coursera.org/learn/programming-languages) | 5 weeks | 4-8 hours/week | Systematic Program Design ([Hear instructor](https://www.coursera.org/lecture/programming-languages/recommended-background-k1yuh)) | [chat](https://discord.gg/8BkJtXN)\n[Programming Languages, Part B](https://www.coursera.org/learn/programming-languages-part-b) | 3 weeks | 4-8 hours/week | Programming Languages, Part A | [chat](https://discord.gg/EeA7VR9)\n[Programming Languages, Part C](https://www.coursera.org/learn/programming-languages-part-c) | 3 weeks | 4-8 hours/week | Programming Languages, Part B | [chat](https://discord.gg/8EZUVbA)\n[Object-Oriented Design](https://course.ccs.neu.edu/cs3500f19/) | 13 weeks | 5-10 hours/week | Class Based Program Design | [chat](https://discord.com/channels/744385009028431943/891412022120579103)\n[Software Architecture](https://www.coursera.org/learn/software-architecture) | 4 weeks | 2-5 hours/week | Object Oriented Design | [chat](https://discord.com/channels/744385009028431943/891412169638432788)\n\n### Core math\nDiscrete math (Math for CS) is a prerequisite and closely related to the study of algorithms and data structures. Calculus both prepares students for discrete math and helps students develop mathematical maturity.\n\n**Topics covered**:\n`discrete mathematics`\n`mathematical proofs`\n`basic statistics`\n`O-notation`\n`discrete probability`\n`and more`\n\nCourses | Duration | Effort | Notes | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--: | :--:\n[Calculus 1A: Differentiation](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+18.01.1x+2T2019/about) ([alternative](https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010/index.htm)) | 13 weeks | 6-10 hours/week | The alternate covers this and the following 2 courses | [high school math](https://ossu.dev/precollege-math) | [chat](https://discord.gg/mPCt45F)\n[Calculus 1B: Integration](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+18.01.2x+3T2019/about) | 13 weeks | 5-10 hours/week | - | Calculus 1A | [chat](https://discord.gg/sddAsZg)\n[Calculus 1C: Coordinate Systems & Infinite Series](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+18.01.3x+1T2020/about) | 6 weeks | 5-10 hours/week | - | Calculus 1B | [chat](https://discord.gg/FNEcNNq)\n[Mathematics for Computer Science](https://openlearninglibrary.mit.edu/courses/course-v1:OCW+6.042J+2T2019/about) ([alternative](https://ocw.mit.edu/courses/6-042j-mathematics-for-computer-science-fall-2010/)) | 13 weeks | 5 hours/week | [2015/2019 solutions](https://github.com/spamegg1/Math-for-CS-solutions) [2010 solutions](https://github.com/frevib/mit-cs-math-6042-fall-2010-problems) [2005 solutions](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2005/assignments/). | Calculus 1C | [chat](https://discord.gg/EuTzNbF)\n\n\n### CS Tools\nUnderstanding theory is important, but you will also be expected to create programs. There are a number of tools that are widely used to make that process easier. Learn them now to ease your future work writing programs.\n\n**Topics covered**:\n`terminals and shell scripting`\n`vim`\n`command line environments`\n`version control`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[The Missing Semester of Your CS Education](https://missing.csail.mit.edu/) | 2 weeks | 12 hours/week | - | [chat](https://discord.gg/5FvKycS)\n\n### Core systems\n\n**Topics covered**:\n`procedural programming`\n`manual memory management`\n`boolean algebra`\n`gate logic`\n`memory`\n`computer architecture`\n`assembly`\n`machine language`\n`virtual machines`\n`high-level languages`\n`compilers`\n`operating systems`\n`network protocols`\n`and more`\n\nCourses | Duration | Effort | Additional Text / Assignments| Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--: | :--:\n[Build a Modern Computer from First Principles: From Nand to Tetris](https://www.coursera.org/learn/build-a-computer) ([alternative](https://www.nand2tetris.org/)) | 6 weeks | 7-13 hours/week | - | C-like programming language | [chat](https://discord.gg/vxB2DRV)\n[Build a Modern Computer from First Principles: Nand to Tetris Part II ](https://www.coursera.org/learn/nand2tetris2) | 6 weeks | 12-18 hours/week | - | one of [these programming languages](https://user-images.githubusercontent.com/2046800/35426340-f6ce6358-026a-11e8-8bbb-4e95ac36b1d7.png), From Nand to Tetris Part I | [chat](https://discord.gg/AsUXcPu)\n[Operating Systems: Three Easy Pieces](coursepages/ostep/README.md) | 10-12 weeks | 6-10 hours/week | - | Nand to Tetris Part II | [chat](https://discord.gg/wZNgpep)\n[Computer Networking: a Top-Down Approach](http://gaia.cs.umass.edu/kurose_ross/online_lectures.htm)| 8 weeks | 4–12 hours/week | [Wireshark Labs](http://gaia.cs.umass.edu/kurose_ross/wireshark.php) | algebra, probability, basic CS | [chat](https://discord.gg/MJ9YXyV)\n\n### Core theory\n\n**Topics covered**:\n`divide and conquer`\n`sorting and searching`\n`randomized algorithms`\n`graph search`\n`shortest paths`\n`data structures`\n`greedy algorithms`\n`minimum spanning trees`\n`dynamic programming`\n`NP-completeness`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Divide and Conquer, Sorting and Searching, and Randomized Algorithms](https://www.coursera.org/learn/algorithms-divide-conquer) | 4 weeks | 4-8 hours/week | any programming language, Mathematics for Computer Science | [chat](https://discord.gg/mKRS7tY)\n[Graph Search, Shortest Paths, and Data Structures](https://www.coursera.org/learn/algorithms-graphs-data-structures) | 4 weeks | 4-8 hours/week | Divide and Conquer, Sorting and Searching, and Randomized Algorithms | [chat](https://discord.gg/Qstqe4t)\n[Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming](https://www.coursera.org/learn/algorithms-greedy) | 4 weeks | 4-8 hours/week | Graph Search, Shortest Paths, and Data Structures | [chat](https://discord.gg/dWVvjuz)\n[Shortest Paths Revisited, NP-Complete Problems and What To Do About Them](https://www.coursera.org/learn/algorithms-npcomplete) | 4 weeks | 4-8 hours/week | Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming | [chat](https://discord.gg/dYuY78u)\n\n### Core security\n**Topics covered**\n`Confidentiality, Integrity, Availability`\n`Secure Design`\n`Defensive Programming`\n`Threats and Attacks`\n`Network Security`\n`Cryptography`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Cybersecurity Fundamentals](https://www.edx.org/course/cybersecurity-fundamentals) | 8 weeks | 10-12 hours/week | - | [chat](https://discord.gg/XdY3AwTFK4)\n[Principles of Secure Coding](https://www.coursera.org/learn/secure-coding-principles)| 4 weeks | 4 hours/week | - | [chat](https://discord.gg/5gMdeSK)\n[Identifying Security Vulnerabilities](https://www.coursera.org/learn/identifying-security-vulnerabilities) | 4 weeks | 4 hours/week | - | [chat](https://discord.gg/V78MjUS)\n\nChoose **one** of the following:\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Identifying Security Vulnerabilities in C/C++Programming](https://www.coursera.org/learn/identifying-security-vulnerabilities-c-programming) | 4 weeks | 5 hours/week | - | [chat](https://discord.gg/Vbxce7A)\n[Exploiting and Securing Vulnerabilities in Java Applications](https://www.coursera.org/learn/exploiting-securing-vulnerabilities-java-applications) | 4 weeks | 5 hours/week | - | [chat](https://discord.gg/QxC22rR)\n\n### Core applications\n\n**Topics covered**:\n`Agile methodology`\n`REST`\n`software specifications`\n`refactoring`\n`relational databases`\n`transaction processing`\n`data modeling`\n`neural networks`\n`supervised learning`\n`unsupervised learning`\n`OpenGL`\n`ray tracing`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Databases: Modeling and Theory](https://www.edx.org/course/modeling-and-theory)| 2 weeks | 10 hours/week | core programming | [chat](https://discord.gg/pMFqNf4)\n[Databases: Relational Databases and SQL](https://www.edx.org/course/databases-5-sql)| 2 weeks | 10 hours/week | core programming | [chat](https://discord.gg/P8SPPyF)\n[Databases: Semistructured Data](https://www.edx.org/course/semistructured-data)| 2 weeks | 10 hours/week | core programming | [chat](https://discord.gg/duCJ3GN)\n[Machine Learning](https://www.coursera.org/specializations/machine-learning-introduction)| 11 weeks | 9 hours/week | Basic coding | [chat](https://discord.gg/NcXHDjy)\n[Computer Graphics](https://www.edx.org/course/computer-graphics-2) ([alternative](https://cseweb.ucsd.edu/~viscomp/classes/cse167/wi22/schedule.html))| 6 weeks | 12 hours/week | C++ or Java, [Basic Linear Algebra](https://ossu.dev/precollege-math/coursepages/precalculus) | [chat](https://discord.gg/68WqMNV)\n[Software Engineering: Introduction](https://www.edx.org/learn/software-engineering/university-of-british-columbia-software-engineering-introduction) ([alternative](https://github.com/ubccpsc/310/blob/main/resources/README.md)) | 6 weeks | 8-10 hours/week | Core Programming, and a [sizable project](FAQ.md#why-require-experience-with-a-sizable-project-before-the-Software-Engineering-courses) | [chat](https://discord.gg/5Qtcwtz)\n\n### Core ethics\n\n**Topics covered**:\n`Social Context`\n`Analytical Tools`\n`Professional Ethics`\n`Intellectual Property`\n`Privacy and Civil Liberties`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Ethics, Technology and Engineering](https://www.coursera.org/learn/ethics-technology-engineering)| 9 weeks | 2 hours/week | none | [chat](https://discord.gg/6ttjPmzZbe)\n[Introduction to  Intellectual Property](https://www.coursera.org/learn/introduction-intellectual-property)| 4 weeks | 2 hours/week | none | [chat](https://discord.gg/YbuERswpAK)\n[Data Privacy Fundamentals](https://www.coursera.org/learn/northeastern-data-privacy)| 3 weeks | 3 hours/week | none | [chat](https://discord.gg/64J34ajNBd)\n\n## Advanced CS\n\nAfter completing **every required course** in Core CS, students should choose a subset of courses from Advanced CS based on interest.\nNot every course from a subcategory needs to be taken.\nBut students should take *every* course that is relevant to the field they intend to go into.\n\n### Advanced programming\n\n**Topics covered**:\n`debugging theory and practice`\n`goal-oriented programming`\n`parallel computing`\n`object-oriented analysis and design`\n`UML`\n`large-scale software architecture and design`\n`and more`\n\nCourses | Duration | Effort | Prerequisites\n:-- | :--: | :--: | :--:\n[Parallel Programming](https://www.coursera.org/learn/scala-parallel-programming)| 4 weeks | 6-8 hours/week | Scala programming\n[Compilers](https://www.edx.org/course/compilers) | 9 weeks | 6-8 hours/week | none\n[Introduction to Haskell](https://www.seas.upenn.edu/~cis194/fall16/)| 14 weeks | - | -\n[Learn Prolog Now!](https://www.let.rug.nl/bos/lpn//lpnpage.php?pageid=online) ([alternative](https://github.com/ossu/computer-science/files/6085884/lpn.pdf))*| 12 weeks | - | -\n[Software Debugging](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkxK63TiT88oEe-AIBhr96A)| 8 weeks | 6 hours/week | Python, object-oriented programming\n[Software Testing](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkWVHeC_8aSIbSxE_NXI76g) | 4 weeks | 6 hours/week | Python, programming experience\n\n(*) book by Blackburn, Bos, Striegnitz (compiled from [source](https://github.com/LearnPrologNow/lpn), redistributed under [CC license](https://creativecommons.org/licenses/by-sa/4.0/))\n\n### Advanced systems\n\n**Topics covered**:\n`digital signaling`\n`combinational logic`\n`CMOS technologies`\n`sequential logic`\n`finite state machines`\n`processor instruction sets`\n`caches`\n`pipelining`\n`virtualization`\n`parallel processing`\n`virtual memory`\n`synchronization primitives`\n`system call interface`\n`and more`\n\nCourses | Duration | Effort | Prerequisites | Notes\n:-- | :--: | :--: | :--: | :--:\n[Computation Structures 1: Digital Circuits](https://learning.edx.org/course/course-v1:MITx+6.004.1x_3+3T2016) [alternative 1](https://ocw.mit.edu/courses/6-004-computation-structures-spring-2017/) [alternative 2](https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/) | 10 weeks | 6 hours/week | [Nand2Tetris II](https://www.coursera.org/learn/nand2tetris2) | Alternate links contain all 3 courses.\n[Computation Structures 2: Computer Architecture](https://learning.edx.org/course/course-v1:MITx+6.004.2x+3T2015) | 10 weeks | 6 hours/week | Computation Structures 1 | - \n[Computation Structures 3: Computer Organization](https://learning.edx.org/course/course-v1:MITx+6.004.3x_2+1T2017) | 10 weeks | 6 hours/week | Computation Structures 2 | -\n\n### Advanced theory\n\n**Topics covered**:\n`formal languages`\n`Turing machines`\n`computability`\n`event-driven concurrency`\n`automata`\n`distributed shared memory`\n`consensus algorithms`\n`state machine replication`\n`computational geometry theory`\n`propositional logic`\n`relational logic`\n`Herbrand logic`\n`game trees`\n`and more`\n\nCourses | Duration | Effort | Prerequisites\n:-- | :--: | :--: | :--:\n[Theory of Computation](https://ocw.mit.edu/courses/18-404j-theory-of-computation-fall-2020/) ([alternative](https://www.youtube.com/playlist?list=PLEE7DF8F5E0203A56)) | 13 weeks | 10 hours/week | [Mathematics for Computer Science](https://openlearninglibrary.mit.edu/courses/course-v1:OCW+6.042J+2T2019/about), logic, algorithms\n[Computational Geometry](https://www.edx.org/course/computational-geometry) | 16 weeks | 8 hours/week | algorithms, C++\n[Game Theory](https://www.coursera.org/learn/game-theory-1) | 8 weeks | 3 hours/week | mathematical thinking, probability, calculus\n\n### Advanced Information Security\n\nCourses | Duration | Effort | Prerequisites\n:-- | :--: | :--: | :--:\n[Web Security Fundamentals](https://www.edx.org/course/web-security-fundamentals) | 5 weeks | 4-6 hours/week | understanding basic web technologies\n[Security Governance & Compliance](https://www.coursera.org/learn/security-governance-compliance) | 3 weeks | 3 hours/week | -\n[Digital Forensics Concepts](https://www.coursera.org/learn/digital-forensics-concepts) | 3 weeks | 2-3 hours/week | Core Security\n[Secure Software Development: Requirements, Design, and Reuse](https://www.edx.org/course/secure-software-development-requirements-design-and-reuse) | 7 weeks | 1-2 hours/week | Core Programming and Core Security\n[Secure Software Development: Implementation](https://www.edx.org/course/secure-software-development-implementation) | 7 weeks | 1-2 hours/week | Secure Software Development: Requirements, Design, and Reuse\n[Secure Software Development: Verification and More Specialized Topics](https://www.edx.org/course/secure-software-development-verification-and-more-specialized-topics) | 7 weeks | 1-2 hours/week | Secure Software Development: Implementation\n\n### Advanced math\n\nCourses | Duration | Effort | Prerequisites | Discussion\n:-- | :--: | :--: | :--: | :--:\n[Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) | - | - | [high school math](https://ossu.dev/precollege-math) | [chat](https://discord.gg/m6wHbP6)\n[Linear Algebra](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/) | 14 weeks | 12 hours/week | corequisite: Essence of Linear Algebra | [chat](https://discord.gg/k7nSWJH)\n[Introduction to Numerical Methods](https://ocw.mit.edu/courses/mathematics/18-335j-introduction-to-numerical-methods-spring-2019/index.htm)| 14 weeks | 12 hours/week | [Linear Algebra](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/) | [chat](https://discord.gg/FNEcNNq)\n[Introduction to Formal Logic](https://forallx.openlogicproject.org/) | 10 weeks | 4-8 hours/week | [Set Theory](https://www.youtube.com/playlist?list=PL5KkMZvBpo5AH_5GpxMiryJT6Dkj32H6N) | [chat](https://discord.gg/MbM2Gg5)\n[Probability](https://projects.iq.harvard.edu/stat110/home) | 15 weeks | 5-10 hours/week | [Differentiation and Integration](https://www.edx.org/course/calculus-1b-integration) | [chat](https://discord.gg/UVjs9BU)\n\n## Final project\n\nPart of learning is doing.\nThe assignments and exams for each course are to prepare you to use your knowledge to solve real-world problems.\n\nAfter you've completed Core CS and the parts of Advanced CS relevant to you,\nyou should identify a problem that you can solve using the knowledge you've acquired.\nYou can create something entirely new, or you can improve some tool/program that you use and wish were better.\n\nStudents who would like more guidance in creating a project may choose to use a series of project oriented courses.\nHere is a sample of options\n(many more are available, at this point you should be capable of identifying a series that is interesting and relevant to you):\n\nCourses | Duration | Effort | Prerequisites\n:-- | :--: | :--: | :--:\n[Fullstack Open](https://fullstackopen.com/en/) | 12 weeks | 15 hours/week | programming\n[Modern Robotics (Specialization)](https://www.coursera.org/specializations/modernrobotics) | 26 weeks | 2-5 hours/week | freshman-level physics, linear algebra, calculus, [linear ordinary differential equations](https://www.khanacademy.org/math/differential-equations)\n[Data Mining (Specialization)](https://www.coursera.org/specializations/data-mining) | 30 weeks | 2-5 hours/week | machine learning\n[Big Data (Specialization)](https://www.coursera.org/specializations/big-data) | 30 weeks | 3-5 hours/week | none\n[Internet of Things (Specialization)](https://www.coursera.org/specializations/internet-of-things) | 30 weeks | 1-5 hours/week | strong programming\n[Cloud Computing (Specialization)](https://www.coursera.org/specializations/cloud-computing) | 30 weeks | 2-6 hours/week | C++ programming\n[Data Science (Specialization)](https://www.coursera.org/specializations/jhu-data-science) | 43 weeks | 1-6 hours/week | none\n[Functional Programming in Scala (Specialization)](https://www.coursera.org/specializations/scala) | 29 weeks | 4-5 hours/week | One year programming experience\n[Game Design and Development with Unity 2020 (Specialization)](https://www.coursera.org/specializations/game-design-and-development) | 6 months | 5 hours/week | programming, interactive design\n\n## Congratulations\n\nAfter completing the requirements of the curriculum above,\nyou will have completed the equivalent of a full bachelor's degree in Computer Science.\nCongratulations!\n\nWhat is next for you? The possibilities are boundless and overlapping:\n\n- Look for a job as a developer!\n- Check out the [readings](extras/readings.md) for classic books you can read that will sharpen your skills and expand your knowledge.\n- Join a local developer meetup (e.g. via [meetup.com](https://www.meetup.com/)).\n- Pay attention to emerging technologies in the world of software development:\n  + Explore the **actor model** through [Elixir](https://elixir-lang.org/), a new functional programming language for the web based on the battle-tested Erlang Virtual Machine!\n  + Explore **borrowing and lifetimes** through [Rust](https://www.rust-lang.org/), a systems language which achieves memory- and thread-safety without a garbage collector!\n  + Explore **dependent type systems** through [Idris](https://www.idris-lang.org/), a new Haskell-inspired language with unprecedented support for type-driven development.\n\n![keep learning](images/keep-learning.webp)\n\n# Code of conduct\n[OSSU's code of conduct](https://github.com/ossu/code-of-conduct).\n\n## How to show your progress\n\n[Fork](https://www.freecodecamp.org/news/how-to-fork-a-github-repository/) the [GitHub repo](https://github.com/ossu/computer-science) into your own GitHub account and put ✅ next to the stuff you've completed as you complete it. This can serve as your [kanban board](https://en.wikipedia.org/wiki/Kanban_board) and will be faster to implement than any other solution (giving you time to spend on the courses).\n\n# Team\n\n* **[Eric Douglas](https://github.com/ericdouglas)**: founder of OSSU\n* **[Josh Hanson](https://github.com/joshmhanson)**: lead technical maintainer\n* **[Waciuma Wanjohi](https://github.com/waciumawanjohi)**: lead academic maintainer\n* **[Contributors](https://github.com/ossu/computer-science/graphs/contributors)**",
    "summary": "Open Source Society University (OSSU) 提供一个完整的计算机科学免费自学教育路径，其课程体系参照本科学位要求设计，涵盖计算机科学的核心基础概念。课程精选自世界顶尖大学的在线资源，结构化地组织了从入门到核心再到高级的编程、数学、系统、理论、安全、应用等主题。该项目适合有自律性、希望系统性掌握计算机科学基础知识的个人，并提供全球学习者社区支持。预计投入每周20小时，约可在两年内完成全部核心及部分高级课程。",
    "keywords": [
      "计算机科学",
      "在线教育",
      "自学课程",
      "编程",
      "算法",
      "数据结构",
      "操作系统",
      "软件工程"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-21T12:07:52Z",
    "download_time": "2024-05-21 10:00:00",
    "visual_resource": [
      "screenshot/github_computer-science.png"
    ],
    "extra_info": null
  },
  {
    "id": "hyperswitch",
    "source": "GitHub",
    "url": "https://github.com/juspay/hyperswitch",
    "title": "Open-Source Payments Orchestration",
    "content": "<p align=\"center\">\n  <img src=\"./docs/imgs/hyperswitch-logo-dark.svg#gh-dark-mode-only\" alt=\"Hyperswitch-Logo\" width=\"40%\" />\n  <img src=\"./docs/imgs/hyperswitch-logo-light.svg#gh-light-mode-only\" alt=\"Hyperswitch-Logo\" width=\"40%\" />\n</p>\n\n<h1 align=\"center\">Open-Source Payments Orchestration</h1>\n\n<div align=\"center\" >\nSingle API to access the payments ecosystem and its features\n</div>\n\n<p align=\"center\">\n  <a href=\"https://github.com/juspay/hyperswitch/actions?query=workflow%3ACI+branch%3Amain\">\n    <img src=\"https://github.com/juspay/hyperswitch/workflows/CI-push/badge.svg\" />\n  </a>\n  <a href=\"https://github.com/juspay/hyperswitch/blob/main/LICENSE\">\n    <img src=\"https://img.shields.io/github/license/juspay/hyperswitch\" />\n  </a>\n  <a href=\"https://github.com/juspay/hyperswitch/blob/main/LICENSE\">\n    <img src=\"https://img.shields.io/badge/Made_in-Rust-orange\" />\n  </a>\n  <!-- Uncomment when we reach >50% coverage -->\n  <!-- <a href=\"https://codecov.io/github/juspay/hyperswitch\" >\n    <img src=\"https://codecov.io/github/juspay/hyperswitch/graph/badge.svg\"/>\n  </a> -->\n</p>\n<p align=\"center\">\n  <a href=\"https://www.linkedin.com/company/hyperswitch/\">\n    <img src=\"https://img.shields.io/badge/follow-hyperswitch-blue?logo=linkedin&labelColor=grey\"/>\n  </a>\n  <a href=\"https://x.com/hyperswitchio\">\n    <img src=\"https://img.shields.io/badge/follow-%40hyperswitchio-white?logo=x&labelColor=grey\"/>\n  </a>\n  <a href=\"https://join.slack.com/t/hyperswitch-io/shared_invite/zt-2jqxmpsbm-WXUENx022HjNEy~Ark7Orw\">\n    <img src=\"https://img.shields.io/badge/chat-on_slack-blue?logo=slack&labelColor=grey&color=%233f0e40\"/>\n  </a>\n  <a href=\"https://deepwiki.com/juspay/hyperswitch\">\n    <img src=\"https://deepwiki.com/badge.svg\" alt=\"Ask DeepWiki\">\n  </a>\n</p>\n\n<hr>\n\n## Table of Contents\n\n1. [Introduction](#introduction)\n2. [Try Hyperswitch](#try-hyperswitch)\n3. [Architectural Overview](#architectural-overview)\n4. [Community & Contributions](#community-and-contributions)\n5. [Feature requests & Bugs](#feature-requests)  \n6. [Our Vision](#our-vision)  \n7. [Versioning](#versioning)  \n8. [Copyright and License](#copyright-and-license)\n\n<a href=\"#introduction\">\n  <h2 id=\"introduction\">Introduction</h2>\n</a>\nJuspay, founded in 2012, is a global leader in payment orchestration and checkout solutions, trusted by 400+ leading enterprises and brands worldwide. Hyperswitch is Juspay's new generation of composable, commercial open-source payments platform for merchant and brands. It is an enterprise-grade, transparent and modular payments platform designed to provide digital businesses access to the best payments infrastructure.\n\nHere are the key components of Hyperswitch that deliver the whole solution:\n\n* [Hyperswitch Backend](https://github.com/juspay/hyperswitch): Hyperswitch backend enables seamless payment processing with comprehensive support for various payment flows - authorization, authentication, void and capture workflows along with robust management of post-payment processes like refunds and chargeback handling. Additionally, Hyperswitch supports non-payment use cases by enabling connections with external FRM or authentication providers as part of the payment flow. The backend optimizes payment routing with customizable workflows, including success rate-based routing, rule-based routing, volume distribution, fallback handling, and intelligent retry mechanisms for failed payments based on specific error codes.\n\n* [SDK (Frontend)](https://github.com/juspay/hyperswitch-web): The SDK, available for web, [Android, and iOS](https://github.com/juspay/hyperswitch-client-core), unifies the payment experience across various methods such as cards, wallets, BNPL, bank transfers, and more, while supporting the diverse payment flows of underlying PSPs. When paired with the locker, it surfaces the user's saved payment methods.    \n\n* [Control Center](https://github.com/juspay/hyperswitch-control-center): The Control Center enables users to manage the entire payments stack without any coding. It allows the creation of workflows for routing, payment retries, and defining conditions to invoke 3DS, fraud risk management (FRM), and surcharge modules. The Control Center provides access to transaction, refund, and chargeback operations across all integrated PSPs, transaction-level logs for initial debugging, and detailed analytics and insights into payment performance.\n\nRead more at [Hyperswitch docs](https://docs.hyperswitch.io/).\n\n<a href=\"#try-hyperswitch\">\n  <h2 id=\"try-hyperswitch\">Try Hyperswitch</h2>\n</a>\n\n### 1. Local Setup\n\n#### One-Click Setup (Recommended)\n\nYou can run Hyperswitch on your system with a single command using our one-click setup script:\n\n```shell\ngit clone --depth 1 --branch latest https://github.com/juspay/hyperswitch\ncd hyperswitch\nscripts/setup.sh\n```\n\nThe above script will:\n- Check for prerequisites (Docker Compose/Podman)\n- Set up necessary configurations\n- Let you select a deployment profile:\n  - **Standard**: Recommended - App server + Control Center + Web SDK.\n  - **Full**: Standard + Monitoring + Scheduler.\n  - **Standalone App Server**: Core services only (Hyperswitch server, PostgreSQL, Redis)\n- Start the selected services\n- Check service health\n- Provide access information\n\nThe next step is to [configure a connector][configure-a-connector] with the Hyperswitch Control Center and [try a payment][try-a-payment].\n\nCheck out the [local setup guide][local-setup-guide] for more details on setting up the entire stack or component wise.\n\n\n### 2. Deployment on cloud\n\nThe fastest and easiest way to try Hyperswitch on AWS is via our CDK scripts\n\n1. Click on the following button for a quick standalone deployment on AWS, suitable for prototyping.\n   No code or setup is required in your system and the deployment is covered within the AWS free-tier setup.\n\n   <a href=\"https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=HyperswitchBootstarp&templateURL=https://hyperswitch-synth.s3.eu-central-1.amazonaws.com/hs-starter-config.yaml\"><img src=\"https://github.com/juspay/hyperswitch/blob/main/docs/imgs/aws_button.png?raw=true\" height=\"35\"></a>\n\n2. Sign-in to your AWS console.\n\n3. Follow the instructions provided on the console to successfully deploy Hyperswitch. This takes 30-45mins and gives the following output \n\n| Service| Host|\n|----------------------------------------------|----------------------------------------------|\n| App server running on                        | `http://hyperswitch-<host-id.region>.elb.amazonaws.com` |\n| HyperloaderJS Hosted at                      | `http://<cloudfront.host-id>/0.103.1/v0/HyperLoader.js` |\n| Control center server running on             | `http://hyperswitch-control-center-<host-id.region>.elb.amazonaws.com`, Login with Email: `test@gmail.com` |\n| Hyperswitch Demo Store running on            | `http://hyperswitch-sdk-demo-<host-id.region>.elb.amazonaws.com` |\n| Logs server running on                       | `http://hyperswitch-logs-<host-id.region>.elb.amazonaws.com`, Login with username: `admin`, password: `admin` |\n\nWe support deployment on GCP and Azure via Helm charts which takes 30-45mins. You can read more at [Hyperswitch docs](https://docs.hyperswitch.io/hyperswitch-open-source/deploy-on-kubernetes-using-helm). \n\n### 3. Hosted Sandbox\n\nYou can experience the product by signing up for our [hosted sandbox](https://app.hyperswitch.io/). The signup process accepts any email ID and provides access to the entire Control Center. You can set up connectors, define workflows for routing and retries, and even try payments from the dashboard.\n\n<a href=\"#architectural-overview\">\n  <h2 id=\"architectural-overview\">Architectural Overview</h2>\n</a>\n<img src=\"./docs/imgs/features.png\" />\n<img src=\"./docs/imgs/non-functional-features.png\" />\n\n<img src=\"./docs/imgs/hyperswitch-architecture-v1.png\" />\n\n[docs-link-for-enterprise]: https://docs.hyperswitch.io/hyperswitch-cloud/quickstart\n[docs-link-for-developers]: https://docs.hyperswitch.io/hyperswitch-open-source/overview\n[contributing-guidelines]: docs/CONTRIBUTING.md\n[dashboard-link]: https://app.hyperswitch.io/\n[website-link]: https://hyperswitch.io/\n[learning-resources]: https://docs.hyperswitch.io/learn-more/payment-flows\n[local-setup-guide]: /docs/try_local_system.md\n[docker-compose-scheduler-monitoring]: /docs/try_local_system.md#running-additional-services\n[configure-a-connector]: https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/using-hyperswitch-control-center#add-a-payment-processor\n[try-a-payment]: https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/test-a-payment\n\n<a href=\"community-and-contributions\">\n  <h2 id=\"community-and-contributions\">Community & Contributions</h2>\n</a>\n\nIf you have any questions, feel free to drop them in our [Slack community](https://join.slack.com/t/hyperswitch-io/shared_invite/zt-2jqxmpsbm-WXUENx022HjNEy~Ark7Orw).\n\nWe welcome contributors from around the world to help build Hyperswitch. To get started, please read our [contribution guidelines](contributing-guidelines).\n\n<a href=\"feature-requests\">\n  <h2 id=\"feature-requests\">Feature requests & Bugs</h2>\n</a>\n\nFor new product features, enhancements, roadmap discussions, or to share queries and ideas, visit our [GitHub Discussions](https://github.com/juspay/hyperswitch/discussions)\n\nFor reporting a bug, please read the issue guidelines and search for [existing and closed issues]. If your problem or idea is not addressed yet, please [open a new issue].\n\n[existing and closed issues]: https://github.com/juspay/hyperswitch/issues\n[open a new issue]: https://github.com/juspay/hyperswitch/issues/new/choose\n\n<a href=\"our-vision\">\n  <h2 id=\"our-vision\">Our Vision</h2>\n</a>\n\n> Linux for Payments\n\nPayments are evolving rapidly worldwide, with hundreds of processors, fraud detection systems, authentication modules, and new payment methods and flows emerging. Businesses building or managing their own payment stacks often face similar challenges, struggle with comparable issues, and find it hard to innovate at the desired pace.\n\nHyperswitch serves as a well-architected designed reference platform, built on best-in-class design principles, empowering businesses to own and customize their payment stack. It provides a reusable core payments stack that can be tailored to specific requirements while relying on the Hyperswitch team for enhancements, support, and continuous innovation.\n\n### Our Values\n\n1. Embrace Payments Diversity: It will drive innovation in the ecosystem in\n   multiple ways.\n2. Make it Open Source: Increases trust; Improves the quality and reusability of\n   software.\n3. Be community driven: It enables participatory design and development.\n4. Build it like Systems Software: This sets a high bar for Reliability,\n   Security and Performance SLAs.\n5. Maximise Value Creation: For developers, customers & partners.\n\nThis project is being created and maintained by [Juspay](https://juspay.io)\n\n<a href=\"#versioning\">\n  <h2 id=\"versioning\">Versioning</h2>\n</a>\n\nCheck the [CHANGELOG.md](./CHANGELOG.md) file for details.\n\n<a href=\"#copyright-and-license\">\n  <h2 id=\"copyright-and-license\">Copyright and License</h2>\n</a>\n\nThis product is licensed under the [Apache 2.0 License](LICENSE).\n\n\n<a href=\"team-behind-hyperswitch\">\n  <h2 id=\"team-behind-hyperswitch\">Team behind Hyperswitch</h2>\n</a>\n\nThe core team of 150+ engineers building Hyperswitch. Keep up the great work! 🥂\n\n<a href=\"https://github.com/juspay/hyperswitch/graphs/contributors\">\n  <img src=\"https://contributors-img.web.app/image?repo=juspay/hyperswitch\" alt=\"Contributors\"/>\n</a>\n",
    "summary": "Hyperswitch是由Juspay推出的开源支付编排平台，旨在通过单一API简化对全球支付生态系统的访问。该平台采用Rust语言构建，提供企业级、透明且模块化的支付解决方案，核心组件包括处理支付流程的后端、统一支付体验的多平台SDK以及无需编码即可管理支付栈的控制中心。它支持智能支付路由、风险管理和多种支付方式，赋能数字企业构建和定制高效的支付基础设施。",
    "keywords": [
      "支付编排",
      "开源",
      "Rust",
      "支付处理",
      "支付路由",
      "SDK",
      "控制中心",
      "支付基础设施"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-26T07:22:17Z",
    "download_time": "2024-05-18 10:00:00",
    "visual_resource": [
      "https://github.com/juspay/hyperswitch/raw/main/docs/imgs/features.png",
      "https://github.com/juspay/hyperswitch/raw/main/docs/imgs/non-functional-features.png",
      "https://github.com/juspay/hyperswitch/raw/main/docs/imgs/hyperswitch-architecture-v1.png"
    ],
    "extra_info": null
  },
  {
    "id": "coreutils",
    "source": "GitHub",
    "url": "https://github.com/uutils/coreutils",
    "title": "uutils coreutils",
    "content": "<!-- markdownlint-disable MD033 MD041 MD002 -->\n<!-- markdownlint-disable commands-show-output no-duplicate-heading -->\n<!-- spell-checker:ignore markdownlint ; (options) DESTDIR UTILNAME manpages reimplementation oranda -->\n<div class=\"oranda-hide\">\n<div align=\"center\">\n\n![uutils logo](docs/src/logo.svg)\n\n# uutils coreutils\n\n[![Crates.io](https://img.shields.io/crates/v/coreutils.svg)](https://crates.io/crates/coreutils)\n[![Discord](https://img.shields.io/badge/discord-join-7289DA.svg?logo=discord&longCache=true&style=flat)](https://discord.gg/wQVJbvJ)\n[![License](http://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/uutils/coreutils/blob/main/LICENSE)\n[![dependency status](https://deps.rs/repo/github/uutils/coreutils/status.svg)](https://deps.rs/repo/github/uutils/coreutils)\n\n[![CodeCov](https://codecov.io/gh/uutils/coreutils/branch/master/graph/badge.svg)](https://codecov.io/gh/uutils/coreutils)\n![MSRV](https://img.shields.io/badge/MSRV-1.85.0-brightgreen)\n\n</div>\n\n---\n\n</div>\n\nuutils coreutils is a cross-platform reimplementation of the GNU coreutils in\n[Rust](http://www.rust-lang.org). While all programs have been implemented, some\noptions might be missing or different behavior might be experienced.\n\n<div class=\"oranda-hide\">\n\nTo install it:\n\n```shell\ncargo install coreutils\n~/.cargo/bin/coreutils\n```\n\n</div>\n\n<!-- markdownlint-disable-next-line MD026 -->\n\n## Goals\n\nuutils aims to be a drop-in replacement for the GNU utils. Differences with GNU\nare treated as bugs.\n\nuutils aims to work on as many platforms as possible, to be able to use the same\nutils on Linux, macOS, Windows and other platforms. This ensures, for example,\nthat scripts can be easily transferred between platforms.\n\n<div class=\"oranda-hide\">\n\n## Documentation\nuutils has both user and developer documentation available:\n\n- [User Manual](https://uutils.github.io/coreutils/docs/)\n- [Developer Documentation](https://docs.rs/crate/coreutils/)\n\nBoth can also be generated locally, the instructions for that can be found in\nthe [coreutils docs](https://github.com/uutils/uutils.github.io) repository.\n\n\n<!-- ANCHOR: build (this mark is needed for mdbook) -->\n\n## Requirements\n\n- Rust (`cargo`, `rustc`)\n- GNU Make (optional)\n\n### Rust Version\n\nuutils follows Rust's release channels and is tested against stable, beta and\nnightly. The current Minimum Supported Rust Version (MSRV) is `1.85.0`.\n\n## Building\n\nThere are currently two methods to build the uutils binaries: either Cargo or\nGNU Make.\n\n> Building the full package, including all documentation, requires both Cargo\n> and GNU Make on a Unix platform.\n\nFor either method, we first need to fetch the repository:\n\n```shell\ngit clone https://github.com/uutils/coreutils\ncd coreutils\n```\n\n### Cargo\n\nBuilding uutils using Cargo is easy because the process is the same as for every\nother Rust program:\n\n```shell\ncargo build --release\n```\n\nThis command builds the most portable common core set of uutils into a multicall\n(BusyBox-type) binary, named 'coreutils', on most Rust-supported platforms.\n\nAdditional platform-specific uutils are often available. Building these expanded\nsets of uutils for a platform (on that platform) is as simple as specifying it\nas a feature:\n\n```shell\ncargo build --release --features macos\n# or ...\ncargo build --release --features windows\n# or ...\ncargo build --release --features unix\n```\n\nIf you don't want to build every utility available on your platform into the\nfinal binary, you can also specify which ones you want to build manually. For\nexample:\n\n```shell\ncargo build --features \"base32 cat echo rm\" --no-default-features\n```\n\nIf you don't want to build the multicall binary and would prefer to build the\nutilities as individual binaries, that is also possible. Each utility is\ncontained in its own package within the main repository, named \"uu_UTILNAME\". To\nbuild individual utilities, use cargo to build just the specific packages (using\nthe `--package` [aka `-p`] option). For example:\n\n```shell\ncargo build -p uu_base32 -p uu_cat -p uu_echo -p uu_rm\n```\n\n### GNU Make\n\nBuilding using `make` is a simple process as well.\n\nTo simply build all available utilities:\n\n```shell\nmake\n```\n\nIn release mode:\n\n```shell\nmake PROFILE=release\n```\n\nTo build all but a few of the available utilities:\n\n```shell\nmake SKIP_UTILS='UTILITY_1 UTILITY_2'\n```\n\nTo build only a few of the available utilities:\n\n```shell\nmake UTILS='UTILITY_1 UTILITY_2'\n```\n\n## Installation\n\n### Install with Cargo\n\nLikewise, installing can simply be done using:\n\n```shell\ncargo install --path . --locked\n```\n\nThis command will install uutils into Cargo's _bin_ folder (_e.g._\n`$HOME/.cargo/bin`).\n\nThis does not install files necessary for shell completion or manpages. For\nmanpages or shell completion to work, use `GNU Make` or see\n`Manually install shell completions`/`Manually install manpages`.\n\n### Install with GNU Make\n\nTo install all available utilities:\n\n```shell\nmake install\n```\n\nTo install using `sudo` switch `-E` must be used:\n\n```shell\nsudo -E make install\n```\n\nTo install all but a few of the available utilities:\n\n```shell\nmake SKIP_UTILS='UTILITY_1 UTILITY_2' install\n```\n\nTo install only a few of the available utilities:\n\n```shell\nmake UTILS='UTILITY_1 UTILITY_2' install\n```\n\nTo install every program with a prefix (e.g. uu-echo uu-cat):\n\n```shell\nmake PROG_PREFIX=PREFIX_GOES_HERE install\n```\n\nTo install the multicall binary:\n\n```shell\nmake MULTICALL=y install\n```\n\nSet install parent directory (default value is /usr/local):\n\n```shell\n# DESTDIR is also supported\nmake PREFIX=/my/path install\n```\n\nInstalling with `make` installs shell completions for all installed utilities\nfor `bash`, `fish` and `zsh`. Completions for `elvish` and `powershell` can also\nbe generated; See `Manually install shell completions`.\n\nTo skip installation of completions and manpages:\n\n```shell\nmake COMPLETIONS=n MANPAGES=n install\n```\n\n### Manually install shell completions\n\nThe `coreutils` binary can generate completions for the `bash`, `elvish`,\n`fish`, `powershell` and `zsh` shells. It prints the result to stdout.\n\nThe syntax is:\n\n```shell\ncargo run completion <utility> <shell>\n```\n\nSo, to install completions for `ls` on `bash` to\n`/usr/local/share/bash-completion/completions/ls`, run:\n\n```shell\ncargo run completion ls bash > /usr/local/share/bash-completion/completions/ls\n```\n\n### Manually install manpages\n\nTo generate manpages, the syntax is:\n\n```bash\ncargo run manpage <utility>\n```\n\nSo, to install the manpage for `ls` to `/usr/local/share/man/man1/ls.1` run:\n\n```bash\ncargo run manpage ls > /usr/local/share/man/man1/ls.1\n```\n\n## Un-installation\n\nUn-installation differs depending on how you have installed uutils. If you used\nCargo to install, use Cargo to uninstall. If you used GNU Make to install, use\nMake to uninstall.\n\n### Uninstall with Cargo\n\nTo uninstall uutils:\n\n```shell\ncargo uninstall coreutils\n```\n\n### Uninstall with GNU Make\n\nTo uninstall all utilities:\n\n```shell\nmake uninstall\n```\n\nTo uninstall every program with a set prefix:\n\n```shell\nmake PROG_PREFIX=PREFIX_GOES_HERE uninstall\n```\n\nTo uninstall the multicall binary:\n\n```shell\nmake MULTICALL=y uninstall\n```\n\nTo uninstall from a custom parent directory:\n\n```shell\n# DESTDIR is also supported\nmake PREFIX=/my/path uninstall\n```\n\n<!-- ANCHOR_END: build (this mark is needed for mdbook) -->\n\n## GNU test suite compatibility\n\nBelow is the evolution of how many GNU tests uutils passes. A more detailed\nbreakdown of the GNU test results of the main branch can be found\n[in the user manual](https://uutils.github.io/coreutils/docs/test_coverage.html).\n\nSee <https://github.com/orgs/uutils/projects/1> for the main meta bugs\n(many are missing).\n\n![Evolution over time](https://github.com/uutils/coreutils-tracking/blob/main/gnu-results.svg?raw=true)\n\n</div> <!-- close oranda-hide div -->\n\n## Contributing\n\nTo contribute to uutils, please see [CONTRIBUTING](CONTRIBUTING.md).\n\n## License\n\nuutils is licensed under the MIT License - see the `LICENSE` file for details\n\nGNU Coreutils is licensed under the GPL 3.0 or later.\n",
    "summary": "uutils coreutils项目是使用Rust语言对GNU coreutils进行跨平台重新实现。其目标是成为GNU工具集的直接替代品，并确保在Linux、macOS、Windows等多种操作系统上提供一致的行为，从而提高脚本的可移植性。项目支持通过Cargo或GNU Make进行构建和安装，可生成多功能二进制文件或独立的工具程序。uutils致力于实现与GNU工具集的行为一致性，并持续提升兼容性。",
    "keywords": [
      "Rust",
      "coreutils",
      "命令行工具",
      "跨平台",
      "GNU工具集",
      "系统工具"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-26T15:34:47Z",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://github.com/uutils/coreutils/raw/main/docs/src/logo.svg",
      "https://github.com/uutils/coreutils-tracking/blob/main/gnu-results.svg?raw=true"
    ],
    "extra_info": null
  },
  {
    "id": "nvim-treesitter",
    "source": "GitHub",
    "url": "https://github.com/nvim-treesitter/nvim-treesitter",
    "title": "nvim-treesitter",
    "content": "<div align=\"center\">\n  <h1>nvim-treesitter</h1>\n  <p>\n    <a href=\"https://matrix.to/#/#nvim-treesitter:matrix.org\">\n      <img alt=\"Matrix Chat\" src=\"https://img.shields.io/matrix/nvim-treesitter:matrix.org\" />\n    </a>\n    <a href=\"https://github.com/nvim-treesitter/nvim-treesitter/actions?query=workflow%3A%22Linting+and+style+checking%22+branch%3Amaster\">\n      <img alt=\"Linting and Style\" src=\"https://github.com/nvim-treesitter/nvim-treesitter/workflows/Linting%20and%20style%20checking/badge.svg\" />\n    </a>\n    <a href=\"https://github.com/nvim-treesitter/nvim-treesitter/actions?query=workflow%3A%22Check+loading+of+syntax+files%22+branch%3Amaster\">\n      <img alt=\"Syntax files\" src=\"https://github.com/nvim-treesitter/nvim-treesitter/workflows/Check%20loading%20of%20syntax%20files/badge.svg\" />\n    </a>\n  </p>\n</div>\n\n<div align=\"center\">\n  <p>\n    <img src=\"assets/logo.png\" align=\"center\" alt=\"Logo\" />\n  </p>\n  <p>\n    <a href=\"https://github.com/tree-sitter/tree-sitter\">Treesitter</a>\n    configurations and abstraction layer for\n    <a href=\"https://github.com/neovim/neovim/\">Neovim</a>.\n  </p>\n  <p>\n    <i>\n      Logo by <a href=\"https://github.com/steelsojka\">@steelsojka</a>\n    </i>\n  </p>\n</div>\n\n>[!CAUTION]\n> The `master` branch is frozen and provided for backward compatibility only. All future updates happen on the [`main` branch](https://github.com/nvim-treesitter/nvim-treesitter/blob/main/README.md), which will become the default branch in the future.\n\nThe goal of `nvim-treesitter` is both to provide a simple and easy way to use the interface for [tree-sitter](https://github.com/tree-sitter/tree-sitter) in Neovim and to provide some basic functionality such as highlighting based on it:\n\n![example-cpp](https://user-images.githubusercontent.com/2361214/202753610-e923bf4e-e88f-494b-bb1e-d22a7688446f.png)\n\nTraditional highlighting (left) vs Treesitter-based highlighting (right).\nMore examples can be found in [our gallery](https://github.com/nvim-treesitter/nvim-treesitter/wiki/Gallery).\n\n**Warning: Treesitter and nvim-treesitter highlighting are an experimental feature of Neovim.\nPlease consider the experience with this plug-in as experimental until Tree-Sitter support in Neovim is stable!\nWe recommend using the nightly builds of Neovim if possible.\nYou can find the current roadmap [here](https://github.com/nvim-treesitter/nvim-treesitter/issues/4767).\nThe roadmap and all features of this plugin are open to change, and any suggestion will be highly appreciated!**\n\nNvim-treesitter is based on three interlocking features: [**language parsers**](#language-parsers), [**queries**](#adding-queries), and [**modules**](#available-modules), where _modules_ provide features – e.g., highlighting – based on _queries_ for syntax objects extracted from a given buffer by _language parsers_.\nUsers will generally only need to interact with parsers and modules as explained in the next section.\nFor more detailed information on setting these up, see [\"Advanced setup\"](#advanced-setup).\n\n---\n\n### Table of contents\n\n- [Quickstart](#quickstart)\n- [Supported languages](#supported-languages)\n- [Available modules](#available-modules)\n- [Advanced setup](#advanced-setup)\n- [Extra features](#extra-features)\n- [Troubleshooting](#troubleshooting)\n\n---\n\n# Quickstart\n\n## Requirements\n\n- **Neovim 0.10** or later (supported up to Neovim 0.12);\n- `tar` and `curl` in your path (or alternatively `git`);\n- a C compiler in your path and libstdc++ installed ([Windows users please read this!](https://github.com/nvim-treesitter/nvim-treesitter/wiki/Windows-support)).\n\n## Installation\n\nYou can install `nvim-treesitter` with your favorite package manager (or using the native `package` feature of vim, see `:h packages`).\n\n**NOTE: This plugin is only guaranteed to work with specific versions of language parsers** (as specified in the `lockfile.json`). **When upgrading the plugin, you must make sure that all installed parsers are updated to the latest version** via `:TSUpdate`.\nIt is strongly recommended to automate this; e.g., if you are using `lazy.nvim`, put this in your `init.lua` file:\n\n```lua\nrequire(\"lazy\").setup({\n  {\"nvim-treesitter/nvim-treesitter\", branch = 'master', lazy = false, build = \":TSUpdate\"}\n})\n```\n\n>[!CAUTION]\n> * This plugin does not support lazy-loading.\n> * Make sure to specify the `master` branch, as the default branch will switch to `main` in the future.\n\nFor other plugin managers such as `packer.nvim`, see this [Installation page from the wiki](https://github.com/nvim-treesitter/nvim-treesitter/wiki/Installation) (Note that this page is community maintained).\n\n## Language parsers\n\nTreesitter uses a different _parser_ for every language, which needs to be generated via `tree-sitter-cli` from a `grammar.js` file, then compiled to a `.so` library that needs to be placed in neovim's `runtimepath` (typically under `parser/{language}.so`).\nTo simplify this, `nvim-treesitter` provides commands to automate this process.\nIf the language is already [supported by `nvim-treesitter`](#supported-languages), you can install it with\n\n```vim\n:TSInstall <language_to_install>\n```\n\nThis command supports tab expansion.\nYou can also get a list of all available languages and their installation status with `:TSInstallInfo`.\nParsers not on this list can be added manually by following the steps described under [\"Adding parsers\"](#adding-parsers) below.\n\nTo make sure a parser is at the latest compatible version (as specified in `nvim-treesitter`'s `lockfile.json`), use `:TSUpdate {language}`. To update all parsers unconditionally, use `:TSUpdate all` or just `:TSUpdate`.\n\n## Modules\n\nEach module provides a distinct tree-sitter-based feature such as [highlighting](#highlight), [indentation](#indentation), or [folding](#folding); see [`:h nvim-treesitter-modules`](doc/nvim-treesitter.txt) or [\"Available modules\"](#available-modules) below for a list of modules and their options.\n\nFollowing examples assume that you are configuring neovim with lua. If you are using vimscript, see `:h lua-heredoc`.\nAll modules are disabled by default and need to be activated explicitly in your `init.lua`, e.g., via\n\n```lua\nrequire'nvim-treesitter.configs'.setup {\n  -- A list of parser names, or \"all\" (the listed parsers MUST always be installed)\n  ensure_installed = { \"c\", \"lua\", \"vim\", \"vimdoc\", \"query\", \"markdown\", \"markdown_inline\" },\n\n  -- Install parsers synchronously (only applied to `ensure_installed`)\n  sync_install = false,\n\n  -- Automatically install missing parsers when entering buffer\n  -- Recommendation: set to false if you don't have `tree-sitter` CLI installed locally\n  auto_install = true,\n\n  -- List of parsers to ignore installing (or \"all\")\n  ignore_install = { \"javascript\" },\n\n  ---- If you need to change the installation directory of the parsers (see -> Advanced Setup)\n  -- parser_install_dir = \"/some/path/to/store/parsers\", -- Remember to run vim.opt.runtimepath:append(\"/some/path/to/store/parsers\")!\n\n  highlight = {\n    enable = true,\n\n    -- NOTE: these are the names of the parsers and not the filetype. (for example if you want to\n    -- disable highlighting for the `tex` filetype, you need to include `latex` in this list as this is\n    -- the name of the parser)\n    -- list of language that will be disabled\n    disable = { \"c\", \"rust\" },\n    -- Or use a function for more flexibility, e.g. to disable slow treesitter highlight for large files\n    disable = function(lang, buf)\n        local max_filesize = 100 * 1024 -- 100 KB\n        local ok, stats = pcall(vim.loop.fs_stat, vim.api.nvim_buf_get_name(buf))\n        if ok and stats and stats.size > max_filesize then\n            return true\n        end\n    end,\n\n    -- Setting this to true will run `:h syntax` and tree-sitter at the same time.\n    -- Set this to `true` if you depend on 'syntax' being enabled (like for indentation).\n    -- Using this option may slow down your editor, and you may see some duplicate highlights.\n    -- Instead of true it can also be a list of languages\n    additional_vim_regex_highlighting = false,\n  },\n}\n```\n\nEach module can also be enabled or disabled interactively through the following commands:\n\n```vim\n:TSBufEnable {module} \" enable module on current buffer\n:TSBufDisable {module} \" disable module on current buffer\n:TSEnable {module} [{ft}] \" enable module on every buffer. If filetype is specified, enable only for this filetype.\n:TSDisable {module} [{ft}] \" disable module on every buffer. If filetype is specified, disable only for this filetype.\n:TSModuleInfo [{module}] \" list information about modules state for each filetype\n```\n\nCheck [`:h nvim-treesitter-commands`](doc/nvim-treesitter.txt) for a list of all available commands.\nIt may be necessary to reload the buffer (e.g., via `:e`) after enabling a module interactively.\n\n# Supported languages\n\nFor `nvim-treesitter` to support a specific feature for a specific language requires both a parser for that language and an appropriate language-specific query file for that feature.\n\nThe following is a list of languages for which a parser can be installed through `:TSInstall`; a checked box means that `nvim-treesitter` also contains queries at least for the `highlight` module.\n\nExperimental parsers are parsers that have a maintainer but are not stable enough for\ndaily use yet.\n\nWe are looking for maintainers to add more parsers and to write query files for their languages. Check our [tracking issue](https://github.com/nvim-treesitter/nvim-treesitter/issues/2282) for open language requests.\n\n<!--This section of the README is automatically updated by a CI job-->\n<!--parserinfo-->\n- [x] [ada](https://github.com/briot/tree-sitter-ada) (maintained by @briot)\n- [x] [agda](https://github.com/tree-sitter/tree-sitter-agda) (maintained by @Decodetalkers)\n- [x] [angular](https://github.com/dlvandenberg/tree-sitter-angular) (experimental, maintained by @dlvandenberg)\n- [x] [apex](https://github.com/aheber/tree-sitter-sfapex) (maintained by @aheber, @xixiaofinland)\n- [x] [arduino](https://github.com/ObserverOfTime/tree-sitter-arduino) (maintained by @ObserverOfTime)\n- [x] [asm](https://github.com/RubixDev/tree-sitter-asm) (maintained by @RubixDev)\n- [x] [astro](https://github.com/virchau13/tree-sitter-astro) (maintained by @virchau13)\n- [x] [authzed](https://github.com/mleonidas/tree-sitter-authzed) (maintained by @mattpolzin)\n- [ ] [awk](https://github.com/Beaglefoot/tree-sitter-awk)\n- [x] [bash](https://github.com/tree-sitter/tree-sitter-bash) (maintained by @TravonteD)\n- [x] [bass](https://github.com/vito/tree-sitter-bass) (maintained by @amaanq)\n- [x] [beancount](https://github.com/polarmutex/tree-sitter-beancount) (maintained by @polarmutex)\n- [x] [bibtex](https://github.com/latex-lsp/tree-sitter-bibtex) (maintained by @theHamsta, @clason)\n- [x] [bicep](https://github.com/amaanq/tree-sitter-bicep) (maintained by @amaanq)\n- [x] [bitbake](https://github.com/amaanq/tree-sitter-bitbake) (maintained by @amaanq)\n- [x] [blade](https://github.com/EmranMR/tree-sitter-blade) (maintained by @calebdw)\n- [x] [blueprint](https://gitlab.com/gabmus/tree-sitter-blueprint.git) (experimental, maintained by @gabmus)\n- [x] [bp](https://github.com/ambroisie/tree-sitter-bp) (maintained by @ambroisie)\n- [x] [brightscript](https://github.com/ajdelcimmuto/tree-sitter-brightscript) (maintained by @ajdelcimmuto)\n- [x] [c](https://github.com/tree-sitter/tree-sitter-c) (maintained by @amaanq)\n- [x] [c_sharp](https://github.com/tree-sitter/tree-sitter-c-sharp) (maintained by @amaanq)\n- [x] [caddy](https://github.com/opa-oz/tree-sitter-caddy) (maintained by @opa-oz)\n- [x] [cairo](https://github.com/amaanq/tree-sitter-cairo) (maintained by @amaanq)\n- [x] [capnp](https://github.com/amaanq/tree-sitter-capnp) (maintained by @amaanq)\n- [x] [chatito](https://github.com/ObserverOfTime/tree-sitter-chatito) (maintained by @ObserverOfTime)\n- [x] [circom](https://github.com/Decurity/tree-sitter-circom) (maintained by @alexandr-martirosyan)\n- [x] [clojure](https://github.com/sogaiu/tree-sitter-clojure) (maintained by @NoahTheDuke)\n- [x] [cmake](https://github.com/uyha/tree-sitter-cmake) (maintained by @uyha)\n- [x] [comment](https://github.com/stsewd/tree-sitter-comment) (maintained by @stsewd)\n- [x] [commonlisp](https://github.com/theHamsta/tree-sitter-commonlisp) (maintained by @theHamsta)\n- [x] [cooklang](https://github.com/addcninblue/tree-sitter-cooklang) (maintained by @addcninblue)\n- [x] [corn](https://github.com/jakestanger/tree-sitter-corn) (maintained by @jakestanger)\n- [x] [cpon](https://github.com/amaanq/tree-sitter-cpon) (maintained by @amaanq)\n- [x] [cpp](https://github.com/tree-sitter/tree-sitter-cpp) (maintained by @theHamsta)\n- [x] [css](https://github.com/tree-sitter/tree-sitter-css) (maintained by @TravonteD)\n- [x] [csv](https://github.com/amaanq/tree-sitter-csv) (maintained by @amaanq)\n- [x] [cuda](https://github.com/theHamsta/tree-sitter-cuda) (maintained by @theHamsta)\n- [x] [cue](https://github.com/eonpatapon/tree-sitter-cue) (maintained by @amaanq)\n- [x] [cylc](https://github.com/elliotfontaine/tree-sitter-cylc) (maintained by @elliotfontaine)\n- [x] [d](https://github.com/gdamore/tree-sitter-d) (maintained by @amaanq)\n- [x] [dart](https://github.com/UserNobody14/tree-sitter-dart) (maintained by @akinsho)\n- [x] [desktop](https://github.com/ValdezFOmar/tree-sitter-desktop) (maintained by @ValdezFOmar)\n- [x] [devicetree](https://github.com/joelspadin/tree-sitter-devicetree) (maintained by @jedrzejboczar)\n- [x] [dhall](https://github.com/jbellerb/tree-sitter-dhall) (maintained by @amaanq)\n- [x] [diff](https://github.com/the-mikedavis/tree-sitter-diff) (maintained by @gbprod)\n- [x] [disassembly](https://github.com/ColinKennedy/tree-sitter-disassembly) (maintained by @ColinKennedy)\n- [x] [djot](https://github.com/treeman/tree-sitter-djot) (maintained by @NoahTheDuke)\n- [x] [dockerfile](https://github.com/camdencheek/tree-sitter-dockerfile) (maintained by @camdencheek)\n- [x] [dot](https://github.com/rydesun/tree-sitter-dot) (maintained by @rydesun)\n- [x] [doxygen](https://github.com/amaanq/tree-sitter-doxygen) (maintained by @amaanq)\n- [x] [dtd](https://github.com/tree-sitter-grammars/tree-sitter-xml) (maintained by @ObserverOfTime)\n- [x] [earthfile](https://github.com/glehmann/tree-sitter-earthfile) (maintained by @glehmann)\n- [x] [ebnf](https://github.com/RubixDev/ebnf) (experimental, maintained by @RubixDev)\n- [x] [editorconfig](https://github.com/ValdezFOmar/tree-sitter-editorconfig) (maintained by @ValdezFOmar)\n- [x] [eds](https://github.com/uyha/tree-sitter-eds) (maintained by @uyha)\n- [x] [eex](https://github.com/connorlay/tree-sitter-eex) (maintained by @connorlay)\n- [x] [elixir](https://github.com/elixir-lang/tree-sitter-elixir) (maintained by @connorlay)\n- [x] [elm](https://github.com/elm-tooling/tree-sitter-elm) (maintained by @zweimach)\n- [x] [elsa](https://github.com/glapa-grossklag/tree-sitter-elsa) (maintained by @glapa-grossklag, @amaanq)\n- [x] [elvish](https://github.com/elves/tree-sitter-elvish) (maintained by @elves)\n- [ ] [embedded_template](https://github.com/tree-sitter/tree-sitter-embedded-template)\n- [x] [enforce](https://github.com/simonvic/tree-sitter-enforce) (maintained by @simonvic)\n- [x] [erlang](https://github.com/WhatsApp/tree-sitter-erlang) (maintained by @filmor)\n- [x] [facility](https://github.com/FacilityApi/tree-sitter-facility) (maintained by @bryankenote)\n- [x] [faust](https://github.com/khiner/tree-sitter-faust) (maintained by @khiner)\n- [x] [fennel](https://github.com/alexmozaidze/tree-sitter-fennel) (maintained by @alexmozaidze)\n- [x] [fidl](https://github.com/google/tree-sitter-fidl) (maintained by @chaopeng)\n- [x] [firrtl](https://github.com/amaanq/tree-sitter-firrtl) (maintained by @amaanq)\n- [x] [fish](https://github.com/ram02z/tree-sitter-fish) (maintained by @ram02z)\n- [x] [foam](https://github.com/FoamScience/tree-sitter-foam) (experimental, maintained by @FoamScience)\n- [x] [forth](https://github.com/AlexanderBrevig/tree-sitter-forth) (maintained by @amaanq)\n- [x] [fortran](https://github.com/stadelmanma/tree-sitter-fortran) (maintained by @amaanq)\n- [x] [fsh](https://github.com/mgramigna/tree-sitter-fsh) (maintained by @mgramigna)\n- [x] [fsharp](https://github.com/ionide/tree-sitter-fsharp) (maintained by @nsidorenco)\n- [x] [func](https://github.com/amaanq/tree-sitter-func) (maintained by @amaanq)\n- [x] [fusion](https://gitlab.com/jirgn/tree-sitter-fusion.git) (maintained by @jirgn)\n- [x] [GAP system](https://github.com/gap-system/tree-sitter-gap) (maintained by @reiniscirpons)\n- [x] [GAP system test files](https://github.com/gap-system/tree-sitter-gaptst) (maintained by @reiniscirpons)\n- [x] [Godot (gdscript)](https://github.com/PrestonKnopp/tree-sitter-gdscript) (maintained by @PrestonKnopp)\n- [x] [gdshader](https://github.com/GodOfAvacyn/tree-sitter-gdshader) (maintained by @godofavacyn)\n- [x] [git_config](https://github.com/the-mikedavis/tree-sitter-git-config) (maintained by @amaanq)\n- [x] [git_rebase](https://github.com/the-mikedavis/tree-sitter-git-rebase) (maintained by @gbprod)\n- [x] [gitattributes](https://github.com/ObserverOfTime/tree-sitter-gitattributes) (maintained by @ObserverOfTime)\n- [x] [gitcommit](https://github.com/gbprod/tree-sitter-gitcommit) (maintained by @gbprod)\n- [x] [gitignore](https://github.com/shunsambongi/tree-sitter-gitignore) (maintained by @theHamsta)\n- [x] [gleam](https://github.com/gleam-lang/tree-sitter-gleam) (maintained by @amaanq)\n- [x] [Glimmer and Ember](https://github.com/ember-tooling/tree-sitter-glimmer) (maintained by @NullVoxPopuli)\n- [x] [glimmer_javascript](https://github.com/NullVoxPopuli/tree-sitter-glimmer-javascript) (maintained by @NullVoxPopuli)\n- [x] [glimmer_typescript](https://github.com/NullVoxPopuli/tree-sitter-glimmer-typescript) (maintained by @NullVoxPopuli)\n- [x] [glsl](https://github.com/theHamsta/tree-sitter-glsl) (maintained by @theHamsta)\n- [x] [GN (Generate Ninja)](https://github.com/amaanq/tree-sitter-gn) (maintained by @amaanq)\n- [x] [gnuplot](https://github.com/dpezto/tree-sitter-gnuplot) (maintained by @dpezto)\n- [x] [go](https://github.com/tree-sitter/tree-sitter-go) (maintained by @theHamsta, @WinWisely268)\n- [x] [goctl](https://github.com/chaozwn/tree-sitter-goctl) (maintained by @chaozwn)\n- [x] [Godot Resources (gdresource)](https://github.com/PrestonKnopp/tree-sitter-godot-resource) (maintained by @pierpo)\n- [x] [gomod](https://github.com/camdencheek/tree-sitter-go-mod) (maintained by @camdencheek)\n- [x] [gosum](https://github.com/amaanq/tree-sitter-go-sum) (maintained by @amaanq)\n- [x] [gotmpl](https://github.com/ngalaiko/tree-sitter-go-template) (maintained by @qvalentin)\n- [x] [gowork](https://github.com/omertuc/tree-sitter-go-work) (maintained by @omertuc)\n- [x] [gpg](https://github.com/ObserverOfTime/tree-sitter-gpg-config) (maintained by @ObserverOfTime)\n- [x] [graphql](https://github.com/bkegley/tree-sitter-graphql) (maintained by @bkegley)\n- [x] [gren](https://github.com/MaeBrooks/tree-sitter-gren) (maintained by @MaeBrooks)\n- [x] [groovy](https://github.com/murtaza64/tree-sitter-groovy) (maintained by @murtaza64)\n- [x] [gstlaunch](https://github.com/theHamsta/tree-sitter-gstlaunch) (maintained by @theHamsta)\n- [ ] [hack](https://github.com/slackhq/tree-sitter-hack)\n- [x] [hare](https://github.com/amaanq/tree-sitter-hare) (maintained by @amaanq)\n- [x] [haskell](https://github.com/tree-sitter/tree-sitter-haskell) (maintained by @mrcjkb)\n- [x] [haskell_persistent](https://github.com/MercuryTechnologies/tree-sitter-haskell-persistent) (maintained by @lykahb)\n- [x] [hcl](https://github.com/MichaHoffmann/tree-sitter-hcl) (maintained by @MichaHoffmann)\n- [x] [heex](https://github.com/connorlay/tree-sitter-heex) (maintained by @connorlay)\n- [x] [helm](https://github.com/ngalaiko/tree-sitter-go-template) (maintained by @qvalentin)\n- [x] [hjson](https://github.com/winston0410/tree-sitter-hjson) (maintained by @winston0410)\n- [x] [hlsl](https://github.com/theHamsta/tree-sitter-hlsl) (maintained by @theHamsta)\n- [x] [hlsplaylist](https://github.com/Freed-Wu/tree-sitter-hlsplaylist) (maintained by @Freed-Wu)\n- [x] [hocon](https://github.com/antosha417/tree-sitter-hocon) (maintained by @antosha417)\n- [x] [hoon](https://github.com/urbit-pilled/tree-sitter-hoon) (experimental, maintained by @urbit-pilled)\n- [x] [html](https://github.com/tree-sitter/tree-sitter-html) (maintained by @TravonteD)\n- [x] [htmldjango](https://github.com/interdependence/tree-sitter-htmldjango) (experimental, maintained by @ObserverOfTime)\n- [x] [http](https://github.com/rest-nvim/tree-sitter-http) (maintained by @amaanq, @NTBBloodbath)\n- [x] [hurl](https://github.com/pfeiferj/tree-sitter-hurl) (maintained by @pfeiferj)\n- [x] [hyprlang](https://github.com/luckasRanarison/tree-sitter-hyprlang) (maintained by @luckasRanarison)\n- [x] [idl](https://github.com/cathaysia/tree-sitter-idl) (maintained by @cathaysia)\n- [x] [idris](https://github.com/kayhide/tree-sitter-idris) (maintained by @srghma)\n- [x] [ini](https://github.com/justinmk/tree-sitter-ini) (experimental, maintained by @theHamsta)\n- [x] [inko](https://github.com/inko-lang/tree-sitter-inko) (maintained by @yorickpeterse)\n- [x] [ipkg](https://github.com/srghma/tree-sitter-ipkg) (maintained by @srghma)\n- [x] [ispc](https://github.com/fab4100/tree-sitter-ispc) (maintained by @fab4100)\n- [x] [janet_simple](https://github.com/sogaiu/tree-sitter-janet-simple) (maintained by @sogaiu)\n- [x] [java](https://github.com/tree-sitter/tree-sitter-java) (maintained by @p00f)\n- [x] [javadoc](https://github.com/rmuir/tree-sitter-javadoc) (maintained by @rmuir)\n- [x] [javascript](https://github.com/tree-sitter/tree-sitter-javascript) (maintained by @steelsojka)\n- [x] [jinja](https://github.com/cathaysia/tree-sitter-jinja) (maintained by @cathaysia)\n- [x] [jinja_inline](https://github.com/cathaysia/tree-sitter-jinja) (maintained by @cathaysia)\n- [x] [jq](https://github.com/flurie/tree-sitter-jq) (maintained by @ObserverOfTime)\n- [x] [jsdoc](https://github.com/tree-sitter/tree-sitter-jsdoc) (maintained by @steelsojka)\n- [x] [json](https://github.com/tree-sitter/tree-sitter-json) (maintained by @steelsojka)\n- [x] [json5](https://github.com/Joakker/tree-sitter-json5) (maintained by @Joakker)\n- [x] [JSON with comments](https://gitlab.com/WhyNotHugo/tree-sitter-jsonc.git) (maintained by @WhyNotHugo)\n- [x] [jsonnet](https://github.com/sourcegraph/tree-sitter-jsonnet) (maintained by @nawordar)\n- [x] [julia](https://github.com/tree-sitter/tree-sitter-julia) (maintained by @fredrikekre)\n- [x] [just](https://github.com/IndianBoy42/tree-sitter-just) (maintained by @Hubro)\n- [x] [kcl](https://github.com/kcl-lang/tree-sitter-kcl) (maintained by @bertbaron)\n- [x] [kconfig](https://github.com/amaanq/tree-sitter-kconfig) (maintained by @amaanq)\n- [x] [kdl](https://github.com/amaanq/tree-sitter-kdl) (maintained by @amaanq)\n- [x] [kotlin](https://github.com/fwcd/tree-sitter-kotlin) (maintained by @SalBakraa)\n- [x] [koto](https://github.com/koto-lang/tree-sitter-koto) (maintained by @irh)\n- [x] [kusto](https://github.com/Willem-J-an/tree-sitter-kusto) (maintained by @Willem-J-an)\n- [x] [lalrpop](https://github.com/traxys/tree-sitter-lalrpop) (maintained by @traxys)\n- [x] [latex](https://github.com/latex-lsp/tree-sitter-latex) (maintained by @theHamsta, @clason)\n- [x] [ledger](https://github.com/cbarrete/tree-sitter-ledger) (maintained by @cbarrete)\n- [x] [leo](https://github.com/r001/tree-sitter-leo) (maintained by @r001)\n- [x] [linkerscript](https://github.com/amaanq/tree-sitter-linkerscript) (maintained by @amaanq)\n- [x] [liquid](https://github.com/hankthetank27/tree-sitter-liquid) (maintained by @hankthetank27)\n- [x] [liquidsoap](https://github.com/savonet/tree-sitter-liquidsoap) (maintained by @toots)\n- [x] [llvm](https://github.com/benwilliamgraham/tree-sitter-llvm) (maintained by @benwilliamgraham)\n- [x] [lua](https://github.com/MunifTanjim/tree-sitter-lua) (maintained by @muniftanjim)\n- [x] [luadoc](https://github.com/amaanq/tree-sitter-luadoc) (maintained by @amaanq)\n- [x] [lua patterns](https://github.com/amaanq/tree-sitter-luap) (maintained by @amaanq)\n- [x] [luau](https://github.com/amaanq/tree-sitter-luau) (maintained by @amaanq)\n- [x] [m68k](https://github.com/grahambates/tree-sitter-m68k) (maintained by @grahambates)\n- [x] [make](https://github.com/alemuller/tree-sitter-make) (maintained by @lewis6991)\n- [x] [markdown (basic highlighting)](https://github.com/MDeiml/tree-sitter-markdown) (experimental, maintained by @MDeiml)\n- [x] [markdown_inline (needed for full highlighting)](https://github.com/MDeiml/tree-sitter-markdown) (experimental, maintained by @MDeiml)\n- [x] [matlab](https://github.com/acristoffers/tree-sitter-matlab) (maintained by @acristoffers)\n- [x] [menhir](https://github.com/Kerl13/tree-sitter-menhir) (maintained by @Kerl13)\n- [ ] [mermaid](https://github.com/monaqa/tree-sitter-mermaid) (experimental)\n- [x] [meson](https://github.com/Decodetalkers/tree-sitter-meson) (maintained by @Decodetalkers)\n- [x] [mlir](https://github.com/artagnon/tree-sitter-mlir) (experimental, maintained by @artagnon)\n- [x] [muttrc](https://github.com/neomutt/tree-sitter-muttrc) (maintained by @Freed-Wu)\n- [x] [nasm](https://github.com/naclsn/tree-sitter-nasm) (maintained by @ObserverOfTime)\n- [x] [nginx](https://github.com/opa-oz/tree-sitter-nginx) (maintained by @opa-oz)\n- [ ] [nickel](https://github.com/nickel-lang/tree-sitter-nickel)\n- [x] [nim](https://github.com/alaviss/tree-sitter-nim) (maintained by @aMOPel)\n- [x] [nim_format_string](https://github.com/aMOPel/tree-sitter-nim-format-string) (maintained by @aMOPel)\n- [x] [ninja](https://github.com/alemuller/tree-sitter-ninja) (maintained by @alemuller)\n- [x] [nix](https://github.com/cstrahan/tree-sitter-nix) (maintained by @leo60228)\n- [x] [norg](https://github.com/nvim-neorg/tree-sitter-norg) (maintained by @JoeyGrajciar, @vhyrro)\n- [x] [nqc](https://github.com/amaanq/tree-sitter-nqc) (maintained by @amaanq)\n- [x] [nu](https://github.com/nushell/tree-sitter-nu) (maintained by @abhisheksingh0x558)\n- [x] [objc](https://github.com/amaanq/tree-sitter-objc) (maintained by @amaanq)\n- [x] [objdump](https://github.com/ColinKennedy/tree-sitter-objdump) (maintained by @ColinKennedy)\n- [x] [ocaml](https://github.com/tree-sitter/tree-sitter-ocaml) (maintained by @undu)\n- [x] [ocaml_interface](https://github.com/tree-sitter/tree-sitter-ocaml) (maintained by @undu)\n- [x] [ocamllex](https://github.com/atom-ocaml/tree-sitter-ocamllex) (maintained by @undu)\n- [x] [odin](https://github.com/amaanq/tree-sitter-odin) (maintained by @amaanq)\n- [x] [pascal](https://github.com/Isopod/tree-sitter-pascal) (maintained by @Isopod)\n- [x] [passwd](https://github.com/ath3/tree-sitter-passwd) (maintained by @amaanq)\n- [x] [pem](https://github.com/ObserverOfTime/tree-sitter-pem) (maintained by @ObserverOfTime)\n- [x] [perl](https://github.com/tree-sitter-perl/tree-sitter-perl) (maintained by @RabbiVeesh, @LeoNerd)\n- [x] [php](https://github.com/tree-sitter/tree-sitter-php) (maintained by @tk-shirasaka, @calebdw)\n- [x] [php_only](https://github.com/tree-sitter/tree-sitter-php) (maintained by @tk-shirasaka, @calebdw)\n- [x] [phpdoc](https://github.com/claytonrcarter/tree-sitter-phpdoc) (experimental, maintained by @mikehaertl)\n- [x] [pioasm](https://github.com/leo60228/tree-sitter-pioasm) (maintained by @leo60228)\n- [x] [po](https://github.com/erasin/tree-sitter-po) (maintained by @amaanq)\n- [x] [pod](https://github.com/tree-sitter-perl/tree-sitter-pod) (maintained by @RabbiVeesh, @LeoNerd)\n- [x] [Path of Exile item filter](https://github.com/ObserverOfTime/tree-sitter-poe-filter) (experimental, maintained by @ObserverOfTime)\n- [x] [pony](https://github.com/amaanq/tree-sitter-pony) (maintained by @amaanq, @mfelsche)\n- [x] [powershell](https://github.com/airbus-cert/tree-sitter-powershell) (maintained by @L2jLiga)\n- [x] [printf](https://github.com/ObserverOfTime/tree-sitter-printf) (maintained by @ObserverOfTime)\n- [x] [prisma](https://github.com/victorhqc/tree-sitter-prisma) (maintained by @elianiva)\n- [x] [problog](https://github.com/foxyseta/tree-sitter-prolog) (maintained by @foxyseta)\n- [x] [prolog](https://github.com/foxyseta/tree-sitter-prolog) (maintained by @foxyseta)\n- [x] [promql](https://github.com/MichaHoffmann/tree-sitter-promql) (maintained by @MichaHoffmann)\n- [x] [properties](https://github.com/tree-sitter-grammars/tree-sitter-properties) (maintained by @ObserverOfTime)\n- [x] [proto](https://github.com/treywood/tree-sitter-proto) (maintained by @treywood)\n- [x] [prql](https://github.com/PRQL/tree-sitter-prql) (maintained by @matthias-Q)\n- [x] [psv](https://github.com/amaanq/tree-sitter-csv) (maintained by @amaanq)\n- [x] [pug](https://github.com/zealot128/tree-sitter-pug) (experimental, maintained by @zealot128)\n- [x] [puppet](https://github.com/amaanq/tree-sitter-puppet) (maintained by @amaanq)\n- [x] [purescript](https://github.com/postsolar/tree-sitter-purescript) (maintained by @postsolar)\n- [x] [PyPA manifest](https://github.com/ObserverOfTime/tree-sitter-pymanifest) (maintained by @ObserverOfTime)\n- [x] [python](https://github.com/tree-sitter/tree-sitter-python) (maintained by @stsewd, @theHamsta)\n- [x] [ql](https://github.com/tree-sitter/tree-sitter-ql) (maintained by @pwntester)\n- [x] [qmldir](https://github.com/Decodetalkers/tree-sitter-qmldir) (maintained by @amaanq)\n- [x] [qmljs](https://github.com/yuja/tree-sitter-qmljs) (maintained by @Decodetalkers)\n- [x] [Tree-Sitter query language](https://github.com/nvim-treesitter/tree-sitter-query) (maintained by @steelsojka)\n- [x] [r](https://github.com/r-lib/tree-sitter-r) (maintained by @ribru17)\n- [ ] [racket](https://github.com/6cdh/tree-sitter-racket)\n- [x] [ralph](https://github.com/alephium/tree-sitter-ralph) (maintained by @tdroxler)\n- [x] [rasi](https://github.com/Fymyte/tree-sitter-rasi) (maintained by @Fymyte)\n- [x] [razor](https://github.com/tris203/tree-sitter-razor) (maintained by @tris203)\n- [x] [rbs](https://github.com/joker1007/tree-sitter-rbs) (maintained by @joker1007)\n- [x] [re2c](https://github.com/amaanq/tree-sitter-re2c) (maintained by @amaanq)\n- [x] [readline](https://github.com/ribru17/tree-sitter-readline) (maintained by @ribru17)\n- [x] [regex](https://github.com/tree-sitter/tree-sitter-regex) (maintained by @theHamsta)\n- [x] [rego](https://github.com/FallenAngel97/tree-sitter-rego) (maintained by @FallenAngel97)\n- [x] [pip requirements](https://github.com/ObserverOfTime/tree-sitter-requirements) (maintained by @ObserverOfTime)\n- [x] [rescript](https://github.com/rescript-lang/tree-sitter-rescript) (maintained by @ribru17)\n- [x] [rnoweb](https://github.com/bamonroe/tree-sitter-rnoweb) (maintained by @bamonroe)\n- [x] [robot](https://github.com/Hubro/tree-sitter-robot) (maintained by @Hubro)\n- [x] [robots](https://github.com/opa-oz/tree-sitter-robots-txt) (maintained by @opa-oz)\n- [x] [roc](https://github.com/faldor20/tree-sitter-roc) (maintained by @nat-418)\n- [x] [ron](https://github.com/amaanq/tree-sitter-ron) (maintained by @amaanq)\n- [x] [rst](https://github.com/stsewd/tree-sitter-rst) (maintained by @stsewd)\n- [x] [ruby](https://github.com/tree-sitter/tree-sitter-ruby) (maintained by @TravonteD)\n- [x] [runescript](https://github.com/2004Scape/tree-sitter-runescript) (maintained by @2004Scape)\n- [x] [rust](https://github.com/tree-sitter/tree-sitter-rust) (maintained by @amaanq)\n- [x] [scala](https://github.com/tree-sitter/tree-sitter-scala) (maintained by @stevanmilic)\n- [x] [scfg](https://github.com/rockorager/tree-sitter-scfg) (maintained by @WhyNotHugo)\n- [ ] [scheme](https://github.com/6cdh/tree-sitter-scheme)\n- [x] [scss](https://github.com/serenadeai/tree-sitter-scss) (maintained by @elianiva)\n- [x] [sflog](https://github.com/aheber/tree-sitter-sfapex) (maintained by @aheber, @xixiaofinland)\n- [x] [slang](https://github.com/theHamsta/tree-sitter-slang) (experimental, maintained by @theHamsta)\n- [x] [slim](https://github.com/theoo/tree-sitter-slim) (maintained by @theoo)\n- [x] [slint](https://github.com/slint-ui/tree-sitter-slint) (maintained by @hunger)\n- [x] [smali](https://github.com/tree-sitter-grammars/tree-sitter-smali) (maintained by @amaanq)\n- [x] [smithy](https://github.com/indoorvivants/tree-sitter-smithy) (maintained by @amaanq, @keynmol)\n- [ ] [snakemake](https://github.com/osthomas/tree-sitter-snakemake) (experimental)\n- [x] [solidity](https://github.com/JoranHonig/tree-sitter-solidity) (maintained by @amaanq)\n- [x] [soql](https://github.com/aheber/tree-sitter-sfapex) (maintained by @aheber, @xixiaofinland)\n- [x] [sosl](https://github.com/aheber/tree-sitter-sfapex) (maintained by @aheber, @xixiaofinland)\n- [x] [sourcepawn](https://github.com/nilshelmig/tree-sitter-sourcepawn) (maintained by @Sarrus1)\n- [x] [sparql](https://github.com/GordianDziwis/tree-sitter-sparql) (maintained by @GordianDziwis)\n- [x] [sql](https://github.com/derekstride/tree-sitter-sql) (maintained by @derekstride)\n- [x] [squirrel](https://github.com/amaanq/tree-sitter-squirrel) (maintained by @amaanq)\n- [x] [ssh_config](https://github.com/ObserverOfTime/tree-sitter-ssh-config) (maintained by @ObserverOfTime)\n- [x] [starlark](https://github.com/amaanq/tree-sitter-starlark) (maintained by @amaanq)\n- [x] [strace](https://github.com/sigmaSd/tree-sitter-strace) (maintained by @amaanq)\n- [x] [styled](https://github.com/mskelton/tree-sitter-styled) (maintained by @mskelton)\n- [x] [supercollider](https://github.com/madskjeldgaard/tree-sitter-supercollider) (maintained by @madskjeldgaard)\n- [x] [superhtml](https://github.com/kristoff-it/superhtml) (maintained by @rockorager)\n- [x] [surface](https://github.com/connorlay/tree-sitter-surface) (maintained by @connorlay)\n- [x] [svelte](https://github.com/tree-sitter-grammars/tree-sitter-svelte) (maintained by @amaanq)\n- [x] [sway](https://github.com/FuelLabs/tree-sitter-sway.git) (maintained by @ribru17)\n- [x] [swift](https://github.com/alex-pinkus/tree-sitter-swift) (maintained by @alex-pinkus)\n- [x] [sxhkdrc](https://github.com/RaafatTurki/tree-sitter-sxhkdrc) (maintained by @RaafatTurki)\n- [x] [systemtap](https://github.com/ok-ryoko/tree-sitter-systemtap) (maintained by @ok-ryoko)\n- [x] [t32](https://gitlab.com/xasc/tree-sitter-t32.git) (maintained by @xasc)\n- [x] [tablegen](https://github.com/amaanq/tree-sitter-tablegen) (maintained by @amaanq)\n- [x] [tact](https://github.com/tact-lang/tree-sitter-tact) (maintained by @novusnota)\n- [x] [tcl](https://github.com/tree-sitter-grammars/tree-sitter-tcl) (maintained by @lewis6991)\n- [x] [teal](https://github.com/euclidianAce/tree-sitter-teal) (maintained by @euclidianAce)\n- [x] [templ](https://github.com/vrischmann/tree-sitter-templ) (maintained by @vrischmann)\n- [x] [tera](https://github.com/uncenter/tree-sitter-tera) (maintained by @uncenter)\n- [x] [terraform](https://github.com/MichaHoffmann/tree-sitter-hcl) (maintained by @MichaHoffmann)\n- [x] [textproto](https://github.com/PorterAtGoogle/tree-sitter-textproto) (maintained by @Porter)\n- [x] [thrift](https://github.com/duskmoon314/tree-sitter-thrift) (maintained by @amaanq, @duskmoon314)\n- [x] [tiger](https://github.com/ambroisie/tree-sitter-tiger) (maintained by @ambroisie)\n- [x] [tlaplus](https://github.com/tlaplus-community/tree-sitter-tlaplus) (maintained by @ahelwer, @susliko)\n- [x] [tmux](https://github.com/Freed-Wu/tree-sitter-tmux) (maintained by @Freed-Wu)\n- [x] [todotxt](https://github.com/arnarg/tree-sitter-todotxt) (experimental, maintained by @arnarg)\n- [x] [toml](https://github.com/tree-sitter-grammars/tree-sitter-toml) (maintained by @tk-shirasaka)\n- [x] [tsv](https://github.com/amaanq/tree-sitter-csv) (maintained by @amaanq)\n- [x] [tsx](https://github.com/tree-sitter/tree-sitter-typescript) (maintained by @steelsojka)\n- [x] [turtle](https://github.com/GordianDziwis/tree-sitter-turtle) (maintained by @GordianDziwis)\n- [x] [twig](https://github.com/gbprod/tree-sitter-twig) (maintained by @gbprod)\n- [x] [typescript](https://github.com/tree-sitter/tree-sitter-typescript) (maintained by @steelsojka)\n- [x] [typespec](https://github.com/happenslol/tree-sitter-typespec) (maintained by @happenslol)\n- [x] [typoscript](https://github.com/Teddytrombone/tree-sitter-typoscript) (maintained by @Teddytrombone)\n- [x] [typst](https://github.com/uben0/tree-sitter-typst) (maintained by @uben0, @RaafatTurki)\n- [x] [udev](https://github.com/ObserverOfTime/tree-sitter-udev) (maintained by @ObserverOfTime)\n- [x] [ungrammar](https://github.com/Philipp-M/tree-sitter-ungrammar) (maintained by @Philipp-M, @amaanq)\n- [x] [unison](https://github.com/kylegoetz/tree-sitter-unison) (maintained by @tapegram)\n- [x] [usd](https://github.com/ColinKennedy/tree-sitter-usd) (maintained by @ColinKennedy)\n- [x] [uxn tal](https://github.com/amaanq/tree-sitter-uxntal) (maintained by @amaanq)\n- [x] [v](https://github.com/vlang/v-analyzer) (maintained by @kkharji, @amaanq)\n- [x] [vala](https://github.com/vala-lang/tree-sitter-vala) (maintained by @Prince781)\n- [x] [vento](https://github.com/ventojs/tree-sitter-vento) (maintained by @wrapperup, @oscarotero)\n- [x] [verilog](https://github.com/gmlarumbe/tree-sitter-systemverilog) (maintained by @zhangwwpeng)\n- [x] [vhdl](https://github.com/jpt13653903/tree-sitter-vhdl) (maintained by @jpt13653903)\n- [x] [vhs](https://github.com/charmbracelet/tree-sitter-vhs) (maintained by @caarlos0)\n- [x] [vim](https://github.com/neovim/tree-sitter-vim) (maintained by @clason)\n- [x] [vimdoc](https://github.com/neovim/tree-sitter-vimdoc) (maintained by @clason)\n- [x] [vrl](https://github.com/belltoy/tree-sitter-vrl) (maintained by @belltoy)\n- [x] [vue](https://github.com/tree-sitter-grammars/tree-sitter-vue) (maintained by @WhyNotHugo, @lucario387)\n- [x] [wgsl](https://github.com/szebniok/tree-sitter-wgsl) (maintained by @szebniok)\n- [x] [wgsl_bevy](https://github.com/theHamsta/tree-sitter-wgsl-bevy) (maintained by @theHamsta)\n- [x] [wing](https://github.com/winglang/tree-sitter-wing) (maintained by @gshpychka, @MarkMcCulloh)\n- [x] [wit](https://github.com/liamwh/tree-sitter-wit) (maintained by @liamwh)\n- [x] [xcompose](https://github.com/ObserverOfTime/tree-sitter-xcompose) (maintained by @ObserverOfTime)\n- [x] [xml](https://github.com/tree-sitter-grammars/tree-sitter-xml) (maintained by @ObserverOfTime)\n- [x] [xresources](https://github.com/ValdezFOmar/tree-sitter-xresources) (maintained by @ValdezFOmar)\n- [x] [yaml](https://github.com/tree-sitter-grammars/tree-sitter-yaml) (maintained by @amaanq)\n- [x] [yang](https://github.com/Hubro/tree-sitter-yang) (maintained by @Hubro)\n- [x] [yuck](https://github.com/Philipp-M/tree-sitter-yuck) (maintained by @Philipp-M, @amaanq)\n- [x] [zathurarc](https://github.com/Freed-Wu/tree-sitter-zathurarc) (maintained by @Freed-Wu)\n- [x] [zig](https://github.com/tree-sitter-grammars/tree-sitter-zig) (maintained by @amaanq)\n- [x] [ziggy](https://github.com/kristoff-it/ziggy) (maintained by @rockorager)\n- [x] [ziggy_schema](https://github.com/kristoff-it/ziggy) (maintained by @rockorager)\n<!--parserinfo-->\n\nFor related information on the supported languages, including related plugins, see [this wiki page](https://github.com/nvim-treesitter/nvim-treesitter/wiki/Supported-Languages-Information).\n\n# Available modules\n\nModules provide the top-level features of `nvim-treesitter`.\nThe following is a list of modules included in `nvim-treesitter` and their configuration via `init.lua` (where multiple modules can be combined in a single call to `setup`).\nNote that not all modules work for all languages (depending on the queries available for them).\nAdditional modules can be provided as [external plugins](https://github.com/nvim-treesitter/nvim-treesitter/wiki/Extra-modules-and-plugins).\n\n#### Highlight\n\nConsistent syntax highlighting.\n\n```lua\nrequire'nvim-treesitter.configs'.setup {\n  highlight = {\n    enable = true,\n    -- Setting this to true will run `:h syntax` and tree-sitter at the same time.\n    -- Set this to `true` if you depend on 'syntax' being enabled (like for indentation).\n    -- Using this option may slow down your editor, and you may see some duplicate highlights.\n    -- Instead of true it can also be a list of languages\n    additional_vim_regex_highlighting = false,\n  },\n}\n```\n\nTo customize the syntax highlighting of a capture, simply define or link a highlight group of the same name:\n\n```lua\n-- Highlight the @foo.bar capture group with the \"Identifier\" highlight group\nvim.api.nvim_set_hl(0, \"@foo.bar\", { link = \"Identifier\" })\n```\n\nFor a language-specific highlight, append the name of the language:\n\n```lua\n-- Highlight @foo.bar as \"Identifier\" only in Lua files\nvim.api.nvim_set_hl(0, \"@foo.bar.lua\", { link = \"Identifier\" })\n```\n\nSee `:h treesitter-highlight-groups` for details.\n\n#### Incremental selection\n\nIncremental selection based on the named nodes from the grammar.\n\n```lua\nrequire'nvim-treesitter.configs'.setup {\n  incremental_selection = {\n    enable = true,\n    keymaps = {\n      init_selection = \"gnn\", -- set to `false` to disable one of the mappings\n      node_incremental = \"grn\",\n      scope_incremental = \"grc\",\n      node_decremental = \"grm\",\n    },\n  },\n}\n```\n\n#### Indentation\n\nIndentation based on treesitter for the `=` operator.\n**NOTE: This is an experimental feature**.\n\n```lua\nrequire'nvim-treesitter.configs'.setup {\n  indent = {\n    enable = true\n  }\n}\n```\n\n#### Folding\n\nTree-sitter based folding (implemented in Neovim itself, see `:h vim.treesitter.foldexpr()`). To enable it for the current window, set\n\n```lua\nvim.wo.foldmethod = 'expr'\nvim.wo.foldexpr = 'v:lua.vim.treesitter.foldexpr()'\n```\n\nThis will respect your `foldminlines` and `foldnestmax` settings.\n\n# Advanced setup\n\n## Changing the parser install directory\n\nIf you want to install the parsers to a custom directory you can specify this\ndirectory with `parser_install_dir` option in that is passed to `setup`.\n`nvim-treesitter` will then install the parser files into this directory.\n\nThis directory must be writeable and must be explicitly prepended to the\n`runtimepath`. For example:\n\n```lua\n  -- It MUST be at the beginning of runtimepath. Otherwise the parsers from Neovim itself\n  -- is loaded that may not be compatible with the queries from the 'nvim-treesitter' plugin.\n  vim.opt.runtimepath:prepend(\"/some/path/to/store/parsers\")\n\n  require'nvim-treesitter.configs'.setup {\n    parser_install_dir = \"/some/path/to/store/parsers\",\n\n    ...\n\n  }\n```\n\nIf this option is not included in the setup options, or is explicitly set to\n`nil` then the default install directories will be used. If this value is set\nthe default directories will be ignored.\n\nBear in mind that any parser installed into a parser folder on the runtime path\nwill still be considered installed. (For example if\n\"~/.local/share/nvim/site/parser/c.so\" exists then the \"c\" parser will be\nconsidered installed, even though it is not in `parser_install_dir`)\n\nThe default paths are:\n\n1. first the package folder. Where `nvim-treesitter` is installed.\n2. second the site directory. This is the \"site\" subdirectory of `stdpath(\"data\")`.\n\n## Adding parsers\n\nIf you have a parser that is not on the list of supported languages (either as a repository on Github or in a local directory), you can add it manually for use by `nvim-treesitter` as follows:\n\n1. Clone the repository or [create a new project](https://tree-sitter.github.io/tree-sitter/creating-parsers#project-setup) in, say, `~/projects/tree-sitter-zimbu`. Make sure that the `tree-sitter-cli` executable is installed and in your path; see <https://tree-sitter.github.io/tree-sitter/creating-parsers#installation> for installation instructions.\n2. Run `tree-sitter generate` in this directory (followed by `tree-sitter test` for good measure).\n3. Add the following snippet to your `init.lua`:\n\n```lua\nlocal parser_config = require \"nvim-treesitter.parsers\".get_parser_configs()\nparser_config.zimbu = {\n  install_info = {\n    url = \"~/projects/tree-sitter-zimbu\", -- local path or git repo\n    files = {\"src/parser.c\"}, -- note that some parsers also require src/scanner.c or src/scanner.cc\n    -- optional entries:\n    branch = \"main\", -- default branch in case of git repo if different from master\n    generate_requires_npm = false, -- if stand-alone parser without npm dependencies\n    requires_generate_from_grammar = false, -- if folder contains pre-generated src/parser.c\n  },\n  filetype = \"zu\", -- if filetype does not match the parser name\n}\n```\n\nIf you wish to set a specific parser for a filetype, you should use `vim.treesitter.language.register()`:\n\n```lua\nvim.treesitter.language.register('python', 'someft')  -- the someft filetype will use the python parser and queries.\n```\n\nNote this requires Nvim v0.9.\n\n4. Start `nvim` and `:TSInstall zimbu`.\n\nYou can also skip step 2 and use `:TSInstallFromGrammar zimbu` to install directly from a `grammar.js` in the top-level directory specified by `url`.\nOnce the parser is installed, you can update it (from the latest revision of the `main` branch if `url` is a Github repository) with `:TSUpdate zimbu`.\n\nNote that neither `:TSInstall` nor `:TSInstallFromGrammar` copy query files from the grammar repository.\nIf you want your installed grammar to be useful, you must manually [add query files](#adding-queries) to your local nvim-treesitter installation.\nNote also that module functionality is only triggered if your language's filetype is correctly identified.\nIf Neovim does not detect your language's filetype by default, you can use [Neovim's `vim.filetype.add()`](<https://neovim.io/doc/user/lua.html#vim.filetype.add()>) to add a custom detection rule.\n\nIf you use a git repository for your parser and want to use a specific version, you can set the `revision` key\nin the `install_info` table for you parser config.\n\n## Adding queries\n\nQueries are what `nvim-treesitter` uses to extract information from the syntax tree;\nthey are located in the `queries/{language}/*` runtime directories (see `:h rtp`),\nlike the `queries` folder of this plugin, e.g. `queries/{language}/{locals,highlights,textobjects}.scm`.\nOther modules may require additional queries such as `folding.scm`. You can find a\nlist of all supported capture names in [CONTRIBUTING.md](https://github.com/nvim-treesitter/nvim-treesitter/blob/master/CONTRIBUTING.md#parser-configurations).\n\nThe first query file on `runtimepath` will be used (see `:h treesitter-query`).\nIf you want to make a query on the user config extend other queries instead of\nreplacing them, see `:h treesitter-query-modeline-extends`.\n\nIf you want to completely override a query, you can use `:h vim.treesitter.query.set()`.\nFor example, to override the `injections` queries from `c` with your own:\n\n```lua\nvim.treesitter.query.set(\"c\", \"injections\", \"(comment) @comment\")\n```\n\nNote: when using `query.set()`, all queries in the runtime directories will be ignored.\n\n## Adding modules\n\nIf you wish you write your own module, you need to support\n\n- tree-sitter language detection support;\n- attaching and detaching to buffers;\n- all nvim-treesitter commands.\n\nAt the top level, you can use the `define_modules` function to define one or more modules or module groups:\n\n```lua\nrequire'nvim-treesitter'.define_modules {\n  my_cool_plugin = {\n    attach = function(bufnr, lang)\n      -- Do cool stuff here\n    end,\n    detach = function(bufnr)\n      -- Undo cool stuff here\n    end,\n    is_supported = function(lang)\n      -- Check if the language is supported\n    end\n  }\n}\n```\n\nwith the following properties:\n\n- `module_path` specifies a require path (string) that exports a module with an `attach` and `detach` function. This is not required if the functions are on this definition.\n- `enable` determines if the module is enabled by default. This is usually overridden by the user.\n- `disable` takes a list of languages that this module is disabled for. This is usually overridden by the user.\n- `is_supported` takes a function that takes a language and determines if this module supports that language.\n- `attach` takes a function that attaches to a buffer. This is required if `module_path` is not provided.\n- `detach` takes a function that detaches from a buffer. This is required if `module_path` is not provided.\n\n# Extra features\n\n### Statusline indicator\n\n```vim\necho nvim_treesitter#statusline(90)  \" 90 can be any length\nmodule->expression_statement->call->identifier\n```\n\n### Utilities\n\nYou can get some utility functions with\n\n```lua\nlocal ts_utils = require 'nvim-treesitter.ts_utils'\n```\n\nCheck [`:h nvim-treesitter-utils`](doc/nvim-treesitter.txt) for more information.\n\n# Troubleshooting\n\nBefore doing anything, make sure you have the latest version of this plugin and run `:checkhealth nvim-treesitter`.\nIt can also help to update the parsers via `:TSUpdate`.\n\n#### Feature `X` does not work for `{language}`...\n\nFirst, check the `health#nvim_treesitter#check` and the `health#treesitter#check` sections of `:checkhealth` for any warning.\nIf there is one, it's highly likely that this is the cause of the problem.\n\nNext check the `## Parser/Features` subsection of the `health#nvim_treesitter#check` section of `:checkhealth` to ensure the desired module is enabled for your language.\nIf not, you might be missing query files; see [Adding queries](#adding-queries).\n\nFinally, ensure Neovim is correctly identifying your language's filetype using the `:echo &filetype` command while one of your language's files is open in Neovim.\nIf not, add a short Vimscript file to nvim-treesitter's `ftdetect` runtime directory following [Neovim's documentation](https://neovim.io/doc/user/filetype.html#new-filetype) on filetype detection.\nYou can also quickly & temporarily set the filetype for a single buffer with the `:set filetype=langname` command to test whether it fixes the problem.\n\nIf everything is okay, then it might be an actual error.\nIn that case, feel free to [open an issue here](https://github.com/nvim-treesitter/nvim-treesitter/issues/new/choose).\n\n#### I get `module 'vim.treesitter.query' not found`\n\nMake sure you have the latest version of Neovim.\n\n#### I get `Error detected while processing .../plugin/nvim-treesitter.vim` every time I open Neovim\n\nThis is probably due to a change in a parser's grammar or its queries.\nTry updating the parser that you suspect has changed (`:TSUpdate {language}`) or all of them (`:TSUpdate`).\nIf the error persists after updating all parsers,\nplease [open an issue](https://github.com/nvim-treesitter/nvim-treesitter/issues/new/choose).\n\n#### I get `query error: invalid node type at position`\n\nThis could be due a query file outside this plugin using outdated nodes,\nor due to an outdated parser.\n\n- Make sure you have the parsers up to date with `:TSUpdate`\n- Make sure you don't have more than one `parser` runtime directory.\n  You can execute this command `:echo nvim_get_runtime_file('parser', v:true)` to find all runtime directories.\n  If you get more than one path, remove the ones that are outside this plugin (`nvim-treesitter` directory),\n  so the correct version of the parser is used.\n\n#### I experience weird highlighting issues similar to [#78](https://github.com/nvim-treesitter/nvim-treesitter/issues/78)\n\nThis is a well known issue, which arises when the tree and the buffer have gotten out of sync.\nAs this is an upstream issue, we don't have any definite fix.\nTo get around this, you can force reparsing the buffer with\n\n```vim\n:write | edit | TSBufEnable highlight\n```\n\nThis will save, restore and enable highlighting for the current buffer.\n\n#### I experience bugs when using `nvim-treesitter`'s `foldexpr` similar to [#194](https://github.com/nvim-treesitter/nvim-treesitter/issues/194)\n\nThis might happen, and is known to happen, with `vim-clap`.\nTo avoid these kind of errors, please use `setlocal` instead of `set` for the respective filetypes.\n\n#### I run into errors like `module 'nvim-treesitter.configs' not found` at startup\n\nThis is because of `rtp` management in `nvim`, adding `packadd\nnvim-treesitter` should fix the issue.\n\n#### I want to use Git instead of curl for downloading the parsers\n\nIn your Lua config:\n\n```lua\nrequire(\"nvim-treesitter.install\").prefer_git = true\n```\n\n#### I want to use a HTTP proxy for downloading the parsers\n\nYou can either configure curl to use additional CLI arguments in your Lua config:\n\n```lua\nrequire(\"nvim-treesitter.install\").command_extra_args = {\n    curl = { \"--proxy\", \"<proxy url>\" },\n}\n```\n\nor you can configure git via `.gitconfig` and use git instead of curl\n\n```lua\nrequire(\"nvim-treesitter.install\").prefer_git = true\n```\n\n#### I want to use a mirror instead of \"https://github.com/\"\n\nIn your Lua config:\n\n```lua\nfor _, config in pairs(require(\"nvim-treesitter.parsers\").get_parser_configs()) do\n  config.install_info.url = config.install_info.url:gsub(\"https://github.com/\", \"something else\")\nend\n\nrequire'nvim-treesitter.configs'.setup {\n    --\n    --\n}\n```\n\n#### Using an existing parser for another filetype\n\nFor example, to use the `bash` tree-sitter to highlight file with\n`filetype=apkbuild`, use:\n\n```lua\nvim.treesitter.language.register(\"bash\", \"apkbuild\")\n```\n\nThe `bash` tree-sitter must be installed following the usual procedure [as\ndescribed above](#language-parsers).\n",
    "summary": "nvim-treesitter是Neovim的插件，为Tree-sitter语法解析器提供了接口和抽象层。它基于语言解析器、查询和模块，提供语法高亮、增量选择、缩进和代码折叠等核心功能。该插件简化了在Neovim中使用Tree-sitter的过程，支持多种语言，显著提升了代码编辑的体验和效率。用户可以轻松安装和管理各种语言的解析器及相关模块。",
    "keywords": [
      "Neovim",
      "Tree-sitter",
      "语法高亮",
      "代码折叠",
      "缩进",
      "增量选择",
      "语法解析",
      "插件"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-24T13:53:33+00:00",
    "download_time": "2024-05-24 14:00:00",
    "visual_resource": [
      "https://user-images.githubusercontent.com/2361214/202753610-e923bf4e-e88f-494b-bb1e-d22a7688446f.png"
    ],
    "extra_info": null
  },
  {
    "id": "langflow",
    "source": "GitHub",
    "url": "https://github.com/langflow-ai/langflow",
    "title": "Langflow",
    "content": "<!-- markdownlint-disable MD030 -->\n\n![Langflow logo](./docs/static/img/langflow-logo-color-black-solid.svg)\n\n\n[![Release Notes](https://img.shields.io/github/release/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/releases)\n[![PyPI - License](https://img.shields.io/badge/license-MIT-orange)](https://opensource.org/licenses/MIT)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/langflow?style=flat-square)](https://pypistats.org/packages/langflow)\n[![GitHub star chart](https://img.shields.io/github/stars/langflow-ai/langflow?style=flat-square)](https://star-history.com/#langflow-ai/langflow)\n[![Open Issues](https://img.shields.io/github/issues-raw/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/issues)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langflow-ai.svg?style=social&label=Follow%20%40Langflow)](https://twitter.com/langflow_ai)\n[![YouTube Channel](https://img.shields.io/youtube/channel/subscribers/UCn2bInQrjdDYKEEmbpwblLQ?label=Subscribe)](https://www.youtube.com/@Langflow)\n[![Discord Server](https://img.shields.io/discord/1116803230643527710?logo=discord&style=social&label=Join)](https://discord.gg/EqksyE2EX9)\n\n\n[Langflow](https://langflow.org) is a powerful tool for building and deploying AI-powered agents and workflows. It provides developers with both a visual authoring experience and a built-in API server that turns every agent into an API endpoint that can be integrated into applications built on any framework or stack. Langflow comes with batteries included and supports all major LLMs, vector databases and a growing library of AI tools.\n\n## ✨ Highlight features\n\n1. **Visual Builder** to get started quickly and iterate. \n1. **Access to Code** so developers can tweak any component using Python.\n1. **Playground** to immediately test and iterate on their flows with step-by-step control.\n1. **Multi-agent** orchestration and conversation management and retrieval.\n1. **Deploy as an API** or export as JSON for Python apps.\n1. **Observability** with LangSmith, LangFuse and other integrations.\n1. **Enterprise-ready** security and scalability.\n\n## ⚡️ Quickstart\n\nLangflow works with Python 3.10 to 3.13.\n\nInstall with uv **(recommended)** \n\n```shell\nuv pip install langflow\n```\n\nInstall with pip\n\n```shell\npip install langflow\n```\n\n## 📦 Deployment\n\n### Self-managed\n\nLangflow is completely open source and you can deploy it to all major deployment clouds. Follow this [guide](https://docs.langflow.org/deployment-docker) to learn how to use Docker to deploy Langflow.\n\n### Fully-managed by DataStax\n\nDataStax Langflow is a full-managed environment with zero setup. Developers can [sign up for a free account](https://astra.datastax.com/signup?type=langflow) to get started.\n\n## ⭐ Stay up-to-date\n\nStar Langflow on GitHub to be instantly notified of new releases.\n\n![Star Langflow](https://github.com/user-attachments/assets/03168b17-a11d-4b2a-b0f7-c1cce69e5a2c)\n\n## 👋 Contribute\n\nWe welcome contributions from developers of all levels. If you'd like to contribute, please check our [contributing guidelines](./CONTRIBUTING.md) and help make Langflow more accessible.\n\n---\n\n[![Star History Chart](https://api.star-history.com/svg?repos=langflow-ai/langflow&type=Timeline)](https://star-history.com/#langflow-ai/langflow&Date)\n\n## ❤️ Contributors\n\n[![langflow contributors](https://contrib.rocks/image?repo=langflow-ai/langflow)](https://github.com/langflow-ai/langflow/graphs/contributors)\n",
    "summary": "Langflow是一个强大的工具，专注于构建和部署AI驱动的智能体及工作流。它为开发者提供了直观的可视化构建界面和内置API服务器，能将智能体转化为可集成到任何应用中的API端点。Langflow支持主流的大语言模型、向量数据库及丰富的AI工具库，具备代码访问、Playground测试、多智能体编排、API部署和可观测性等核心功能，支持自部署和全托管。",
    "keywords": [
      "AI智能体",
      "工作流",
      "可视化编程",
      "API服务",
      "大语言模型",
      "向量数据库",
      "多智能体",
      "可观测性"
    ],
    "area": [
      "人工智能",
      "大模型",
      "智能体"
    ],
    "published_time": "2025-05-23T22:39:56+00:00",
    "download_time": "2024-05-24 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/langflow-ai/langflow/main/docs/static/img/langflow-logo-color-black-solid.svg"
    ],
    "extra_info": null
  },
  {
    "id": "LLaMA-Factory",
    "source": "GitHub",
    "url": "https://github.com/hiyouga/LLaMA-Factory",
    "title": "LLaMA Factory",
    "content": "![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)\n[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-476-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/LLaMA-Factory/pulls)\n\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)\n[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)\n\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)\n[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A7%97-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![SageMaker](https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue)](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/)\n\n### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.\n\n<div align=\"center\" markdown=\"1\">\n\n### Supporters ❤️\n\n<a href=\"https://warp.dev/llama-factory\">\n    <img alt=\"Warp sponsorship\" width=\"400\" src=\"https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae\">\n</a>\n\n#### [Warp, the agentic terminal for developers](https://warp.dev/llama-factory)\n\n[Available for MacOS, Linux, & Windows](https://warp.dev/llama-factory)\n\n----\n\n### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n\n![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)\n\n</div>\n\n👋 Join our [WeChat](assets/wechat.jpg) or [NPU user group](assets/wechat_npu.jpg).\n\\[ English | [中文](README_zh.md) ]\n\n**Fine-tuning a large language model can be easy as...**\n\nhttps://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e\n\nChoose your path:\n\n- **Documentation**: https://llamafactory.readthedocs.io/en/latest/\n- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n- **Local machine**: Please refer to [usage](#getting-started)\n- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory\n\n> [!NOTE]\n> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.\n\n## Table of Contents\n\n- [Features](#features)\n- [Blogs](#blogs)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Data Preparation](#data-preparation)\n  - [Quickstart](#quickstart)\n  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n  - [Build Docker](#build-docker)\n  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)\n  - [Download from ModelScope Hub](#download-from-modelscope-hub)\n  - [Download from Modelers Hub](#download-from-modelers-hub)\n  - [Use W&B Logger](#use-wb-logger)\n  - [Use SwanLab Logger](#use-swanlab-logger)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n## Features\n\n- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.\n- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.\n- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.\n- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.\n- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.\n- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.\n- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.\n- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).\n\n### Day-N Support for Fine-Tuning Cutting-Edge Models\n\n| Support Date | Model Name                                                   |\n| ------------ | ------------------------------------------------------------ |\n| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / InternLM 3 / MiniCPM-o-2.6    |\n| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4       |\n\n## Blogs\n\n- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)\n- [Easy Dataset × LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)\n- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)\n\n<details><summary>All Blogs</summary>\n\n- [A One-Stop Code-Free Model Fine-Tuning & Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)\n- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)\n- [LLaMA Factory: Fine-tuning the LLaMA3 Model for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)\n\n</details>\n\n## Changelog\n\n[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.\n\n[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)'s PR.\n\n[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.\n\n[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.\n\n[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.\n\n<details><summary>Full Changelog</summary>\n\n[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.\n\n[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.\n\n[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.\n\n[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.\n\n[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.\n\n[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.\n\n[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.\n\n[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.\n\n[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)'s PR.\n\n[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)'s PR.\n\n[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.\n\n[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.\n\n[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1-SFT](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.\n\n[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.\n\n[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.\n\n[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)'s PR.\n\n[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.\n\n[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)'s PR.\n\n[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)'s PR.\n\n[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.\n\n[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.\n\n[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.\n\n[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.\n\n[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.\n\n[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.\n\n[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI's implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).\n\n[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.\n\n[24/03/21] Our paper \"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)\" is available at arXiv!\n\n[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.\n\n[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.\n\n[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.\n\n[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.\n\n[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.\n\n[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.\n\n[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.\n\n[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.\n\n[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).\n\n[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.\n\n[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.\n\n[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.\n\n[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.\n\n[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.\n\n[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.\n\n[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.\n\n[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.\n\n[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.\n\n[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** ⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.\n\n[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI's](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.\n\n[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.\n\n</details>\n\n> [!TIP]\n> If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.\n\n## Supported Models\n\n| Model                                                             | Model size                       | Template            |\n| ----------------------------------------------------------------- | -------------------------------- | ------------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2           |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                   |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3            |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere              |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek            |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/671B                        | deepseek3           |\n| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)       | 1.5B/7B/8B/14B/32B/70B/671B      | deepseekr1          |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon              |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma               |\n| [Gemma 3](https://huggingface.co/google)                          | 1B/4B/12B/27B                    | gemma3/gemma (1B)   |\n| [GLM-4/GLM-4-0414/GLM-Z1](https://huggingface.co/THUDM)           | 9B/32B                           | glm4/glmz1          |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                   |\n| [Granite 3.0-3.3](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3            |\n| [Hunyuan](https://huggingface.co/tencent/)                        | 7B                               | hunyuan             |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index               |\n| [InternLM 2-3](https://huggingface.co/internlm)                   | 7B/8B/20B                        | intern2             |\n| [InternVL 2.5-3](https://huggingface.co/OpenGVLab)                | 1B/2B/8B/14B/38B/78B             | intern_vl           |\n| [Kimi-VL](https://huggingface.co/moonshotai)                      | 16B                              | kimi_vl             |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                   |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2              |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3              |\n| [Llama 4](https://huggingface.co/meta-llama)                      | 109B/402B                        | llama4              |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama              |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava               |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next          |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video    |\n| [MiMo](https://huggingface.co/XiaomiMiMo)                         | 7B                               | mimo                |\n| [MiniCPM](https://huggingface.co/openbmb)                         | 1B/2B/4B                         | cpm/cpm3            |\n| [MiniCPM-o-2.6/MiniCPM-V-2.6](https://huggingface.co/openbmb)     | 8B                               | minicpm_o/minicpm_v |\n| [Ministral/Mistral-Nemo](https://huggingface.co/mistralai)        | 8B/12B                           | ministral           |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral             |\n| [Mistral Small](https://huggingface.co/mistralai)                 | 24B                              | mistral_small       |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                   |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma           |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                   |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi                 |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small           |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4                |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral             |\n| [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen                |\n| [Qwen3 (MoE)](https://huggingface.co/Qwen)                        | 0.6B/1.7B/4B/8B/14B/32B/235B     | qwen3               |\n| [Qwen2-Audio](https://huggingface.co/Qwen)                        | 7B                               | qwen2_audio         |\n| [Qwen2.5-Omni](https://huggingface.co/Qwen)                       | 3B/7B                            | qwen2_omni          |\n| [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)            | 2B/3B/7B/32B/72B                 | qwen2_vl            |\n| [Seed Coder](https://huggingface.co/ByteDance-Seed)               | 8B                               | seed_coder          |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1          |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                   |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2           |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse              |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi                  |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl               |\n| [Yuan 2](https://huggingface.co/IEIT-Yuan)                         | 2B/51B/102B                      | yuan                |\n\n> [!NOTE]\n> For the \"base\" models, the `template` argument can be chosen from `default`, `alpaca`, `vicuna` etc. But make sure to use the **corresponding template** for the \"instruct/chat\" models.\n>\n> Remember to use the **SAME** template in training and inference.\n>\n> *: You should install the `transformers` from main branch and use `DISABLE_VERSION_CHECK=1` to skip version check.\n>\n> **: You need to install a specific version of `transformers` to use the corresponding model.\n\nPlease refer to [constants.py](src/llamafactory/extras/constants.py) for a full list of models we supported.\n\nYou also can add a custom chat template to [template.py](src/llamafactory/data/template.py).\n\n## Supported Training Approaches\n\n| Approach               |     Full-tuning    |    Freeze-tuning   |       LoRA         |       QLoRA        |\n| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ |\n| Pre-Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Supervised Fine-Tuning | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Reward Modeling        | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO Training          | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO Training         | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> The implementation details of PPO can be found in [this blog](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html).\n\n## Provided Datasets\n\n<details><summary>Pre-training datasets</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>Supervised fine-tuning datasets</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [Open-Thoughts (en)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n- [Open-R1-Math (en)](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)\n- [Chinese-DeepSeek-R1-Distill (zh)](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>Preference datasets</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [COIG-P (en&zh)](https://huggingface.co/datasets/m-a-p/COIG-P)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [RLAIF-V (en)](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n## Requirement\n\n| Mandatory    | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| python       | 3.9     | 3.10      |\n| torch        | 2.0.0   | 2.6.0     |\n| torchvision  | 0.15.0  | 0.21.0    |\n| transformers | 4.45.0  | 4.50.0    |\n| datasets     | 2.16.0  | 3.2.0     |\n| accelerate   | 0.34.0  | 1.2.1     |\n| peft         | 0.14.0  | 0.15.1    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| Optional     | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.16.4    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.8.2     |\n| flash-attn   | 2.5.6   | 2.7.2     |\n\n### Hardware Requirement\n\n\\* *estimated*\n\n| Method                          | Bits |   7B  |  14B  |  30B  |   70B  |   `x`B  |\n| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n| Full (`bf16` or `fp16`)         |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n| Full (`pure_bf16`)              |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |\n| Freeze/LoRA/GaLore/APOLLO/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |\n| QLoRA                           |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |\n| QLoRA                           |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |\n| QLoRA                           |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |\n\n## Getting Started\n\n### Installation\n\n> [!IMPORTANT]\n> Installation is mandatory.\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation\n```\n\nExtra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, modelscope, openmind, swanlab, quality\n\n> [!TIP]\n> Use `pip install -e . --no-deps --no-build-isolation` to resolve package conflicts.\n\n<details><summary>Setting up a virtual environment with <b>uv</b></summary>\n\nCreate an isolated Python environment with [uv](https://github.com/astral-sh/uv):\n\n```bash\nuv sync --extra torch --extra metrics --prerelease=allow\n```\n\nRun LLaMA-Factory in the isolated environment:\n\n```bash\nuv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n```\n\n</details>\n\n<details><summary>For Windows users</summary>\n\n#### Install PyTorch\n\nYou need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the [official website](https://pytorch.org/get-started/locally/) and the following command to install PyTorch with CUDA support:\n\n```bash\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\nIf you see `True` then you have successfully installed PyTorch with CUDA support.\n\nTry `dataloader_num_workers: 0` if you encounter `Can't pickle local object` error.\n\n#### Install BitsAndBytes\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.2, please select the appropriate [release version](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels) based on your CUDA version.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\n#### Install Flash Attention-2\n\nTo enable FlashAttention-2 on the Windows platform, please use the script from [flash-attention-windows-wheel](https://huggingface.co/lldacing/flash-attention-windows-wheel) to compile and install it by yourself.\n\n</details>\n\n<details><summary>For Ascend NPU users</summary>\n\nTo install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: `pip install -e \".[torch-npu,metrics]\"`. Additionally, you need to install the **[Ascend CANN Toolkit and Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**. Please follow the [installation tutorial](https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html) or use the following commands:\n\n```bash\n# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| Requirement  | Minimum | Recommend      |\n| ------------ | ------- | -------------- |\n| CANN         | 8.0.RC1 | 8.0.0.alpha002 |\n| torch        | 2.1.0   | 2.4.0          |\n| torch-npu    | 2.1.0   | 2.4.0.post2    |\n| deepspeed    | 0.13.2  | 0.13.2         |\n| vllm-ascend  | -       | 0.7.3          |\n\nRemember to use `ASCEND_RT_VISIBLE_DEVICES` instead of `CUDA_VISIBLE_DEVICES` to specify the device to use.\n\nIf you cannot infer model on NPU devices, try setting `do_sample: false` in the configurations.\n\nDownload the pre-built Docker images: [32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n#### Install BitsAndBytes\n\nTo use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:\n\n1. Manually compile bitsandbytes: Refer to [the installation documentation](https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&platform=Ascend+NPU) for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.\n\n```bash\n# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .\n```\n\n2. Install transformers from the main branch.\n\n```bash\ngit clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install .\n```\n\n3. Set `double_quantization: false` in the configuration. You can refer to the [example](examples/train_qlora/llama3_lora_sft_bnb_npu.yaml).\n\n</details>\n\n### Data Preparation\n\nPlease refer to [data/README.md](data/README.md) for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.\n\n> [!NOTE]\n> Please update `data/dataset_info.json` to use your custom dataset.\n\nYou can also use **[Easy Dataset](https://github.com/ConardLi/easy-dataset)** or **[GraphGen](https://github.com/open-sciencelab/GraphGen)** to create synthetic data for fine-tuning.\n\n### Quickstart\n\nUse the following 3 commands to run LoRA **fine-tuning**, **inference** and **merging** of the Llama3-8B-Instruct model, respectively.\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\nSee [examples/README.md](examples/README.md) for advanced usage (including distributed training).\n\n> [!TIP]\n> Use `llamafactory-cli help` to show help information.\n>\n> Read [FAQs](https://github.com/hiyouga/LLaMA-Factory/issues/4614) first if you encounter any problems.\n\n### Fine-Tuning with LLaMA Board GUI (powered by [Gradio](https://github.com/gradio-app/gradio))\n\n```bash\nllamafactory-cli webui\n```\n\n### Build Docker\n\nFor CUDA users:\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>Build without Docker Compose</summary>\n\nFor CUDA users:\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg INSTALL_BNB=false \\\n    --build-arg INSTALL_VLLM=false \\\n    --build-arg INSTALL_DEEPSPEED=false \\\n    --build-arg INSTALL_FLASHATTN=false \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    -t llamafactory:latest .\n\ndocker run -dit --gpus=all \\\n    -v ./hf_cache:/root/.cache/huggingface \\\n    -v ./ms_cache:/root/.cache/modelscope \\\n    -v ./om_cache:/root/.cache/openmind \\\n    -v ./data:/app/data \\\n    -v ./output:/app/output \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --shm-size 16G \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\n# Choose docker image upon your environment\ndocker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg INSTALL_DEEPSPEED=false \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    -t llamafactory:latest .\n\n# Change `device` upon your resources\ndocker run -dit \\\n    -v ./hf_cache:/root/.cache/huggingface \\\n    -v ./ms_cache:/root/.cache/modelscope \\\n    -v ./om_cache:/root/.cache/openmind \\\n    -v ./data:/app/data \\\n    -v ./output:/app/output \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --shm-size 16G \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg INSTALL_BNB=false \\\n    --build-arg INSTALL_VLLM=false \\\n    --build-arg INSTALL_DEEPSPEED=false \\\n    --build-arg INSTALL_FLASHATTN=false \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    -t llamafactory:latest .\n\ndocker run -dit \\\n    -v ./hf_cache:/root/.cache/huggingface \\\n    -v ./ms_cache:/root/.cache/modelscope \\\n    -v ./om_cache:/root/.cache/openmind \\\n    -v ./data:/app/data \\\n    -v ./output:/app/output \\\n    -v ./saves:/app/saves \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --shm-size 16G \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>Details about volume</summary>\n\n- `hf_cache`: Utilize Hugging Face cache on the host machine. Reassignable if a cache already exists in a different directory.\n- `ms_cache`: Similar to Hugging Face cache but for ModelScope users.\n- `om_cache`: Similar to Hugging Face cache but for Modelers users.\n- `data`: Place datasets on this dir of the host machine so that they can be selected on LLaMA Board GUI.\n- `output`: Set export dir to this location so that the merged result can be accessed directly on the host machine.\n\n</details>\n\n### Deploy with OpenAI-style API and vLLM\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true\n```\n\n> [!TIP]\n> Visit [this page](https://platform.openai.com/docs/api-reference/chat/create) for API document.\n>\n> Examples: [Image understanding](scripts/api_example/test_image.py) | [Function calling](scripts/api_example/test_toolcall.py)\n\n### Download from ModelScope Hub\n\nIf you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the ModelScope Hub as the `model_name_or_path`. You can find a full list of model IDs at [ModelScope Hub](https://modelscope.cn/models), e.g., `LLM-Research/Meta-Llama-3-8B-Instruct`.\n\n### Download from Modelers Hub\n\nYou can also use Modelers Hub to download models and datasets.\n\n```bash\nexport USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the Modelers Hub as the `model_name_or_path`. You can find a full list of model IDs at [Modelers Hub](https://modelers.cn/models), e.g., `TeleAI/TeleChat-7B-pt`.\n\n### Use W&B Logger\n\nTo use [Weights & Biases](https://wandb.ai) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nreport_to: wandb\nrun_name: test_run # optional\n```\n\nSet `WANDB_API_KEY` to [your key](https://wandb.ai/authorize) when launching training tasks to log in with your W&B account.\n\n### Use SwanLab Logger\n\nTo use [SwanLab](https://github.com/SwanHubX/SwanLab) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # optional\n```\n\nWhen launching training tasks, you can log in to SwanLab in three ways:\n\n1. Add `swanlab_api_key=<your_api_key>` to the yaml file, and set it to your [API key](https://swanlab.cn/settings).\n2. Set the environment variable `SWANLAB_API_KEY` to your [API key](https://swanlab.cn/settings).\n3. Use the `swanlab login` command to complete the login.\n\n## Projects using LLaMA Factory\n\nIf you have a project that should be incorporated, please contact via email or create a pull request.\n\n<details><summary>Click to show</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. \"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**: A large language model specialized in generate metadata for stable diffusion. [[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**: A document-level relation extraction system based on large language models.\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**: A modified library that supports long sequence SFT & DPO using ring attention.\n1. **[Sky-T1](https://novasky-ai.github.io/posts/sky-t1/)**: An o1-like model fine-tuned by NovaSky AI with very small cost.\n1. **[WeClone](https://github.com/xming521/WeClone)**: One-stop solution for creating your digital avatar from chat logs.\n\n</details>\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\nPlease follow the model licenses to use the corresponding model weights: [Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [Llama 4](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## Citation\n\nIf this work is helpful, please kindly cite as:\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [PEFT](https://github.com/huggingface/peft), [TRL](https://github.com/huggingface/trl), [QLoRA](https://github.com/artidoro/qlora) and [FastChat](https://github.com/lm-sys/FastChat). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)",
    "summary": "LLaMA Factory是一个统一且高效的大语言模型微调框架，旨在简化超过100种大型语言模型的微调过程。该框架支持多种微调方法，包括SFT、DPO、KTO、ORPO等，并集成了全参数微调、Freeze、LoRA及2-8位QLoRA等资源优化技术。它提供了零代码的命令行界面和Web UI（LLaMA Board），支持多模态任务，并整合了FlashAttention-2、vLLM等多种加速技术，显著提升训练和推理效率。LLaMA Factory已被Amazon、NVIDIA等知名机构采用，广泛应用于模型定制和部署场景。",
    "keywords": [
      "大语言模型",
      "微调",
      "LoRA",
      "QLoRA",
      "SFT",
      "DPO",
      "多模态",
      "推理加速"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "深度学习"
    ],
    "published_time": "2025-05-26T15:19:58Z",
    "download_time": "2024-07-26 08:00:00",
    "visual_resource": [
      "screenshot/github_LLaMA-Factory.png"
    ],
    "extra_info": null
  },
  {
    "id": "RD-Agent",
    "source": "GitHub",
    "url": "https://github.com/microsoft/RD-Agent",
    "title": "The Best Machine Learning Engineering Agent!",
    "content": "<h4 align=\"center\">\n  <img src=\"docs/_static/logo.png\" alt=\"RA-Agent logo\" style=\"width:70%; \">\n  \n  <a href=\"https://rdagent.azurewebsites.net\" target=\"_blank\">🖥️ Live Demo</a> |\n  <a href=\"https://rdagent.azurewebsites.net/factor_loop\" target=\"_blank\">🎥 Demo Video</a> <a href=\"https://www.youtube.com/watch?v=JJ4JYO3HscM&list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR\" target=\"_blank\">▶️YouTube</a>   |\n  <a href=\"https://rdagent.readthedocs.io/en/latest/index.html\" target=\"_blank\">📖 Documentation</a> |\n  <a href=\"https://aka.ms/RD-Agent-Tech-Report\" target=\"_blank\">📄 Tech Report</a> |\n  <a href=\"#-paperwork-list\"> 📃 Papers </a>\n</h4>\n\n\n[![CI](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml)\n[![CodeQL](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql)\n[![Dependabot Updates](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates)\n[![Lint PR Title](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml)\n[![Release.yml](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml)\n[![Platform](https://img.shields.io/badge/platform-Linux-blue)](https://pypi.org/project/rdagent/#files)\n[![PyPI](https://img.shields.io/pypi/v/rdagent)](https://pypi.org/project/rdagent/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/rdagent)](https://pypi.org/project/rdagent/)\n[![Release](https://img.shields.io/github/v/release/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/releases)\n[![GitHub](https://img.shields.io/github/license/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/blob/main/LICENSE)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)\n[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)\n[![Documentation Status](https://readthedocs.org/projects/rdagent/badge/?version=latest)](https://rdagent.readthedocs.io/en/latest/?badge=latest)\n[![Readthedocs Preview](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml) <!-- this badge is too long, please place it in the last one to make it pretty --> \n[![arXiv](https://img.shields.io/badge/arXiv-2505.14738-00ff00.svg)](https://arxiv.org/abs/2505.14738)\n\n\n\n# 🏆 The Best Machine Learning Engineering Agent!\n\n[MLE-bench](https://github.com/openai/mle-bench) is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems' capabilities in real-world ML engineering scenarios.\n\nR&D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:\n\n| Agent | Low == Lite (%) | Medium (%) | High (%) | All (%) |\n|---------|--------|-----------|---------|----------|\n| R&D-Agent o1-preview | 48.18 ± 2.49 | 8.95 ± 2.36 | 18.67 ± 2.98 | 22.4 ± 1.1 |\n| R&D-Agent o3(R)+GPT-4.1(D) | 51.52 ± 6.21 | 7.89 ± 3.33 | 16.67 ± 3.65 | 22.45 ± 2.45 |\n| AIDE o1-preview | 34.3 ± 2.4 | 8.8 ± 1.1 | 10.0 ± 1.9 | 16.9 ± 1.1 |\n\n**Notes:**\n- **O3(R)+GPT-4.1(D)**: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).\n- **AIDE o1-preview**: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.\n- Average and standard deviation results for R&D-Agent o1-preview is based on a independent of 5 seeds and for R&D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.\n- According to MLE-Bench, the 75 competitions are categorized into three levels of complexity: **Low==Lite** if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models; **Medium** if it takes between 2 and 10 hours; and **High** if it takes more than 10 hours.\n\nYou can inspect the detailed runs of the above results online.\n- [R&D-Agent o1-preview detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O1-preview)\n- [R&D-Agent o3(R)+GPT-4.1(D) detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41)\n\nFor running R&D-Agent on MLE-bench, refer to **[MLE-bench Guide: Running ML Engineering via MLE-bench](https://rdagent.readthedocs.io/en/latest/scens/data_science.html)**\n\n\n# 📰 News\n| 🗞️ News        | 📝 Description                 |\n| --            | ------      |\n| [Technical Report Release](#overall-technical-report) | Overall framework description and results on MLE-bench |\n| [R&D-Agent-Quant Release](#deep-application-in-diverse-scenarios) | Apply R&D-Agent to quant trading |\n| MLE-Bench Results Released | R&D-Agent currently leads as the [top-performing machine learning engineering agent](#-the-best-machine-learning-engineering-agent) on MLE-bench |\n| Support LiteLLM Backend | We now fully support **[LiteLLM](https://github.com/BerriAI/litellm)** as a backend for integration with multiple LLM providers. |\n| More General Data Science Agent | 🚀Coming soon! |\n| Kaggle Scenario release | We release **[Kaggle Agent](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html)**, try the new features!                  |\n| Official WeChat group release  | We created a WeChat group, welcome to join! (🗪[QR Code](https://github.com/microsoft/RD-Agent/issues/880)) |\n| Official Discord release  | We launch our first chatting channel in Discord (🗪[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)) |\n| First release | **R&D-Agent** is released on GitHub |\n\n\n\n# Data Science Agent Preview\nCheck out our demo video showcasing the current progress of our Data Science Agent under development:\n\nhttps://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305\n\n# 🌟 Introduction\n<div align=\"center\">\n      <img src=\"docs/_static/scen.png\" alt=\"Our focused scenario\" style=\"width:80%; \">\n</div>\n\nR&D-Agent aims to automate the most critical and valuable aspects of the industrial R&D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. \nMethodologically, we have identified a framework with two key components: 'R' for proposing new ideas and 'D' for implementing them.\nWe believe that the automatic evolution of R&D will lead to solutions of significant industrial value.\n\n\n<!-- Tag Cloud -->\nR&D is a very general scenario. The advent of R&D-Agent can be your\n- 💰 **Automatic Quant Factory** ([🎥Demo Video](https://rdagent.azurewebsites.net/factor_loop)|[▶️YouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&t=6s))\n- 🤖 **Data Mining Agent:** Iteratively proposing data & models ([🎥Demo Video 1](https://rdagent.azurewebsites.net/model_loop)|[▶️YouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&t=104s)) ([🎥Demo Video 2](https://rdagent.azurewebsites.net/dmm)|[▶️YouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4))  and implementing them by gaining knowledge from data.\n- 🦾 **Research Copilot:** Auto read research papers ([🎥Demo Video](https://rdagent.azurewebsites.net/report_model)|[▶️YouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o)) / financial reports ([🎥Demo Video](https://rdagent.azurewebsites.net/report_factor)|[▶️YouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)) and implement model structures or building datasets.\n- 🤖 **Kaggle Agent:** Auto Model Tuning and Feature Engineering([🎥Demo Video Coming Soon...]()) and implementing them to achieve more in competitions.\n- ...\n\nYou can click the links above to view the demo. We're continuously adding more methods and scenarios to the project to enhance your R&D processes and boost productivity. \n\nAdditionally, you can take a closer look at the examples in our **[🖥️ Live Demo](https://rdagent.azurewebsites.net/)**.\n\n<div align=\"center\">\n    <a href=\"https://rdagent.azurewebsites.net/\" target=\"_blank\">\n        <img src=\"docs/_static/demo.png\" alt=\"Watch the demo\" width=\"80%\">\n    </a>\n</div>\n\n\n# ⚡ Quick start\n\nYou can try above demos by running the following command:\n\n### 🐳 Docker installation.\nUsers must ensure Docker is installed before attempting most scenarios. Please refer to the [official 🐳Docker page](https://docs.docker.com/engine/install/) for installation instructions.\nEnsure the current user can run Docker commands **without using sudo**. You can verify this by executing `docker run hello-world`.\n\n### 🐍 Create a Conda Environment\n- Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):\n  ```sh\n  conda create -n rdagent python=3.10\n  ```\n- Activate the environment:\n  ```sh\n  conda activate rdagent\n  ```\n\n### 🛠️ Install the R&D-Agent\n- You can directly install the R&D-Agent package from PyPI:\n  ```sh\n  pip install rdagent\n  ```\n\n### 💊 Health check\n- rdagent provides a health check that currently checks two things.\n  - whether the docker installation was successful.\n  - whether the default port used by the [rdagent ui](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) is occupied.\n  ```sh\n  rdagent health_check\n  ```\n\n\n### ⚙️ Configuration\n- The demos requires following ability:\n  - ChatCompletion\n  - json_mode\n  - embedding query\n\n- For example: If you are using the `OpenAI API`, you have to configure your GPT model in the `.env` file like this.\n  ```bash\n  cat << EOF  > .env\n  OPENAI_API_KEY=<replace_with_your_openai_api_key>\n  # EMBEDDING_MODEL=text-embedding-3-small\n  CHAT_MODEL=gpt-4-turbo\n  EOF\n  ```\n- However, not every API services support these features by default. For example: `AZURE OpenAI`, you have to configure your GPT model in the `.env` file like this.\n  ```bash\n  cat << EOF  > .env\n  USE_AZURE=True\n  EMBEDDING_OPENAI_API_KEY=<replace_with_your_azure_openai_api_key>\n  EMBEDDING_AZURE_API_BASE=<replace_with_your_azure_endpoint>\n  EMBEDDING_AZURE_API_VERSION=<replace_with_the_version_of_your_azure_openai_api>\n  EMBEDDING_MODEL=text-embedding-3-small\n  CHAT_OPENAI_API_KEY=<replace_with_your_azure_openai_api_key>\n  CHAT_AZURE_API_BASE=<replace_with_your_azure_endpoint>\n  CHAT_AZURE_API_VERSION=<replace_with_the_version_of_your_azure_openai_api>\n  CHAT_MODEL=<replace_it_with_the_name_of_your_azure_chat_model>\n  EOF\n  ```\n\n- We now support LiteLLM as a backend for integration with multiple LLM providers. If you use LiteLLM Backend to use models, you can configure as follows:\n  ```bash\n  cat << EOF  > .env\n  BACKEND=rdagent.oai.backend.LiteLLMAPIBackend\n  # It can be modified to any model supported by LiteLLM.\n  CHAT_MODEL=gpt-4o\n  EMBEDDING_MODEL=text-embedding-3-small\n  # The backend api_key fully follow the convention of litellm.\n  OPENAI_API_KEY=<replace_with_your_openai_api_key>\n  ```\n  \n- For more configuration information, please refer to the [documentation](https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html).\n\n### 🚀 Run the Application\n\nThe **[🖥️ Live Demo](https://rdagent.azurewebsites.net/)** is implemented by the following commands(each item represents one demo, you can select the one you prefer):\n\n- Run the **Automated Quantitative Trading & Iterative Factors Evolution**:  [Qlib](http://github.com/microsoft/qlib) self-loop factor proposal and implementation application\n  ```sh\n  rdagent fin_factor\n  ```\n\n- Run the **Automated Quantitative Trading & Iterative Model Evolution**: [Qlib](http://github.com/microsoft/qlib) self-loop model proposal and implementation application\n  ```sh\n  rdagent fin_model\n  ```\n\n- Run the **Automated Medical Prediction Model Evolution**: Medical self-loop model proposal and implementation application\n  >(1) Apply for an account at [PhysioNet](https://physionet.org/). <br /> (2) Request access to FIDDLE preprocessed data: [FIDDLE Dataset](https://physionet.org/content/mimic-eicu-fiddle-feature/1.0.0/). <br />\n  (3) Place your username and password in `.env`.\n  ```bash\n  cat << EOF  >> .env\n  DM_USERNAME=<your_username>\n  DM_PASSWORD=<your_password>\n  EOF\n  ```\n  ```sh\n  rdagent med_model\n  ```\n\n- Run the **Automated Quantitative Trading & Factors Extraction from Financial Reports**:  Run the [Qlib](http://github.com/microsoft/qlib) factor extraction and implementation application based on financial reports\n  ```sh\n  # 1. Generally, you can run this scenario using the following command:\n  rdagent fin_factor_report --report_folder=<Your financial reports folder path>\n\n  # 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:\n  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip\n  unzip all_reports.zip -d git_ignore_folder/reports\n  rdagent fin_factor_report --report_folder=git_ignore_folder/reports\n  ```\n\n- Run the **Automated Model Research & Development Copilot**: model extraction and implementation application\n  ```sh\n  # 1. Generally, you can run your own papers/reports with the following command:\n  rdagent general_model <Your paper URL>\n\n  # 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:\n  rdagent general_model  \"https://arxiv.org/pdf/2210.09789\"\n  ```\n\n- Run the **Automated Kaggle Model Tuning & Feature Engineering**:  self-loop model proposal and feature engineering implementation application <br />\n  > Using **sf-crime** *(San Francisco Crime Classification)* as an example. <br />\n  > 1. Register and login on the [Kaggle](https://www.kaggle.com/) website. <br />\n  > 2. Configuring the Kaggle API. <br />\n  > (1) Click on the avatar (usually in the top right corner of the page) -> `Settings` -> `Create New Token`, A file called `kaggle.json` will be downloaded. <br />\n  > (2) Move `kaggle.json` to `~/.config/kaggle/` <br />\n  > (3) Modify the permissions of the kaggle.json file. Reference command: `chmod 600 ~/.config/kaggle/kaggle.json` <br />\n  > 3. Join the competition: Click `Join the competition` -> `I Understand and Accept` at the bottom of the [competition details page](https://www.kaggle.com/competitions/sf-crime/data).\n  ```bash\n  # Generally, you can run the Kaggle competition program with the following command:\n  rdagent kaggle --competition <your competition name>\n\n  # Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:\n  \n  # 1. Install chromedriver.\n\n  # 2. Add the competition description file path to the `.env` file.\n  mkdir -p ./git_ignore_folder/kaggle_data\n  dotenv set KG_LOCAL_DATA_PATH \"$(pwd)/git_ignore_folder/kaggle_data\"\n\n  # 3. run the application\n  rdagent kaggle --competition sf-crime\n  ```\n  > **Description of the above example:** <br />\n  > - Kaggle competition data is roughly divided into three sections: competition description file (json file) and complete dataset for the competition and simplified dataset for the competition. <br />\n  > - The Kaggle competition data will be downloaded automatically, the download process depends on `chromedriver`, installation instructions can be found in the [documentation](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#example-guide). <br />\n\n### 🖥️ Monitor the Application Results\n- You can run the following command for our demo program to see the run logs.\n\n  ```sh\n  rdagent ui --port 19899 --log_dir <your log folder like \"log/\">\n  ```\n\n  **Note:** Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.\n\n  You can check if a port is occupied by running the following command.\n\n  ```sh\n  rdagent health_check\n  ```\n\n# 🏭 Scenarios\n\nWe have applied R&D-Agent to multiple valuable data-driven industrial scenarios.\n\n\n## 🎯 Goal: Agent for Data-driven R&D\n\nIn this project, we are aiming to build an Agent to automate Data-Driven R\\&D that can\n+ 📄 Read real-world material (reports, papers, etc.) and **extract** key formulas, descriptions of interested **features** and **models**, which are the key components of data-driven R&D .\n+ 🛠️ **Implement** the extracted formulas (e.g., features, factors, and models) in runnable codes.\n   + Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.\n+ 💡 Propose **new ideas** based on current knowledge and observations.\n\n<!-- ![Data-Centric R&D Overview](docs/_static/overview.png) -->\n\n## 📈 Scenarios/Demos\n\nIn the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: 🦾Copilot and 🤖Agent. \n- The 🦾Copilot follows human instructions to automate repetitive tasks. \n- The 🤖Agent, being more autonomous, actively proposes ideas for better results in the future.\n\nThe supported scenarios are listed below:\n\n| Scenario/Target | Model Implementation                   | Data Building                                                                      |\n| --              | --                                     | --                                                                                 |\n| **💹 Finance**      | 🤖 [Iteratively Proposing Ideas & Evolving](https://rdagent.azurewebsites.net/model_loop)[▶️YouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&t=104s) |  🤖 [Iteratively Proposing Ideas & Evolving](https://rdagent.azurewebsites.net/factor_loop) [▶️YouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&t=6s) <br/>   🦾 [Auto reports reading & implementation](https://rdagent.azurewebsites.net/report_factor)[▶️YouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)  |\n| **🩺 Medical**      | 🤖 [Iteratively Proposing Ideas & Evolving](https://rdagent.azurewebsites.net/dmm)[▶️YouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4) | -                                                                                  |\n| **🏭 General**      | 🦾 [Auto paper reading & implementation](https://rdagent.azurewebsites.net/report_model)[▶️YouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o) <br/> 🤖 Auto Kaggle Model Tuning   | 🤖Auto Kaggle feature Engineering |\n\n- **[RoadMap](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#roadmap)**: Currently, we are working hard to add new features to the Kaggle scenario.\n\nDifferent scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.\n\nHere is a gallery of [successful explorations](https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip) (5 traces showed in **[🖥️ Live Demo](https://rdagent.azurewebsites.net/)**). You can download and view the execution trace using [this command](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) from the documentation.\n\nPlease refer to **[📖readthedocs_scen](https://rdagent.readthedocs.io/en/latest/scens/catalog.html)** for more details of the scenarios.\n\n# ⚙️ Framework\n\n<div align=\"center\">\n    <img src=\"docs/_static/Framework-RDAgent.png\" alt=\"Framework-RDAgent\" width=\"85%\">\n</div>\n\n\nAutomating the R&D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.\n\nThe research questions within this framework can be divided into three main categories:\n| Research Area | Paper/Work List |\n|--------------------|-----------------|\n| **Benchmark the R&D abilities** | [Benchmark](#benchmark) |\n| **Idea proposal:** Explore new ideas or refine existing ones | [Research](#research) |\n| **Ability to realize ideas:** Implement and execute ideas | [Development](#development) |\n\nWe believe that the key to delivering high-quality solutions lies in the ability to evolve R&D capabilities. Agents should learn like human experts, continuously improving their R&D skills.\n\nMore documents can be found in the **[📖 readthedocs](https://rdagent.readthedocs.io/)**.\n\n# 📃 Paper/Work list\n\n## Overall Technical Report\n- [R&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution](https://arxiv.org/abs/2505.14738)\n```BibTeX\n@misc{yang2024rdagent,\n    title={R\\&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution},\n    author={Xu Yang and Xiao Yang and Shikai Fang and Bowen Xian and Yuante Li and Jian Wang and Minrui Xu and Haoran Pan and Xinpeng Hong and Weiqing Liu and Yelong Shen and Weizhu Chen and Jiang Bian},\n    year={2025},\n    eprint={2505.14738},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI},\n    url={https://arxiv.org/abs/2505.14738}\n}\n```\n![image](https://github.com/user-attachments/assets/28b0488d-a546-4fef-8dc5-563ed64a9b4d)\n\n## 📊 Benchmark\n- [Towards Data-Centric Automatic R&D](https://arxiv.org/abs/2404.11276)\n```BibTeX\n@misc{chen2024datacentric,\n    title={Towards Data-Centric Automatic R\\&D},\n    author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},\n    year={2024},\n    eprint={2404.11276},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/494f55d3-de9e-4e73-ba3d-a787e8f9e841)\n\n## 🔍 Research\n\nIn a data mining expert's daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.\n\nBased on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.\n\nFor more detail, please refer to our **[🖥️ Live Demo page](https://rdagent.azurewebsites.net)**.\n\n## 🛠️ Development\n\n- [Collaborative Evolving Strategy for Automatic Data-Centric Development](https://arxiv.org/abs/2407.18690)\n```BibTeX\n@misc{yang2024collaborative,\n    title={Collaborative Evolving Strategy for Automatic Data-Centric Development},\n    author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},\n    year={2024},\n    eprint={2407.18690},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/75d9769b-0edd-4caf-9d45-57d1e577054b)\n\n## Deep Application in Diverse Scenarios\n\n- [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n\n# 🤝 Contributing\n\nWe welcome contributions and suggestions to improve R&D-Agent. Please refer to the [Contributing Guide](CONTRIBUTING.md) for more details on how to contribute.\n\nBefore submitting a pull request, ensure that your code passes the automatic CI checks.\n\n## 📝 Guidelines\nThis project welcomes contributions and suggestions.\nContributing to this project is straightforward and rewarding. Whether it's solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve R&D-Agent.\n\nTo get started, you can explore the issues list, or search for `TODO:` comments in the codebase by running the command `grep -r \"TODO:\"`.\n\n<img src=\"https://img.shields.io/github/contributors-anon/microsoft/RD-Agent\"/>\n\n<a href=\"https://github.com/microsoft/RD-Agent/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/RD-Agent&max=100&columns=15\" />\n</a>\n\nBefore we released R&D-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.\n\n# ⚖️ Legal disclaimer\n<p style=\"line-height: 1; font-style: italic;\">The RD-agent is provided “as is”, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.</p>",
    "summary": "R&D-Agent是由微软开发的一款机器学习工程智能体，旨在自动化数据驱动的研发流程，特别聚焦于模型和数据的开发与迭代。其核心框架包含“研究”（提出新想法）和“开发”（实现想法）两大组件，通过与真实世界验证反馈相结合，实现能力的持续进化。该智能体在MLE-bench基准测试中位居前列，并已成功应用于量化交易、数据挖掘、研究辅助（如阅读论文/报告）和Kaggle竞赛等多个实际场景，支持集成多种大型语言模型后端，并提供可视化监控界面。",
    "keywords": [
      "智能体",
      "机器学习工程",
      "数据驱动研发",
      "大语言模型",
      "自动化研究",
      "自动化开发",
      "量化交易",
      "数据科学"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "智能体"
    ],
    "published_time": "2025-05-26T09:43:43+00:00",
    "download_time": "2024-07-29 10:00:00",
    "visual_resource": [
      "https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/scen.png",
      "https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/Framework-RDAgent.png"
    ],
    "extra_info": null
  },
  {
    "id": "f1-dash",
    "source": "GitHub",
    "url": "https://github.com/slowlydev/f1-dash",
    "title": "Real-time Formula 1 telemetry and timing",
    "content": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"./dash/public/tag-logo.png\" width=\"200\">\n    <img alt=\"f1-dash\" src=\"./dash/public/tag-logo.png\" width=\"200\">\n  </picture>\n</p>\n\n<h1 align=\"center\">Real-time Formula 1 telemetry and timing</h1>\n\n## f1-dash\n\nA real-time F1 dashboard that shows the leader board, tires, gaps, laps, mini sectors and much more\n\n## contributing\n\nI really appreciate it if you want to contribute to this project. I can recommend the GitHub issues marked as \"Good First Issue\" to get started. Also please read [`CONTRIBUTING.md`](CONTRIBUTING.md) to learn how to contribute and setup f1-dash on your local machine for development.\n\n## supporting\n\nIf you want to support me and make me focus more on this project, support me here by [buying me a coffee](https://www.buymeacoffee.com/slowlydev).\n\n## notice\n\nThis project/website is unofficial and is not associated in any way with the Formula 1 companies. F1, FORMULA ONE, FORMULA 1, FIA FORMULA ONE WORLD CHAMPIONSHIP, GRAND PRIX and related marks are trade marks of Formula One Licensing B.V",
    "summary": "f1-dash是一个实时一级方程式赛车（F1）遥测和计时仪表盘项目。该仪表盘能够实时显示比赛的排行榜、轮胎状态、车距、圈速、迷你扇区时间等关键数据，为用户提供全面的实时F1赛事信息。该项目旨在通过可视化界面呈现复杂的F1遥测数据，方便用户追踪比赛进展。请注意，这是一个非官方项目，与Formula 1公司无关联。",
    "keywords": [
      "一级方程式",
      "F1",
      "遥测",
      "计时",
      "仪表盘",
      "实时数据"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-22 16:00:39+00:00",
    "download_time": "2024-05-23 10:00:00",
    "visual_resource": [
      "https://github.com/slowlydev/f1-dash/raw/main/dash/public/tag-logo.png"
    ],
    "extra_info": null
  },
  {
    "id": "pathway",
    "source": "GitHub",
    "url": "https://github.com/pathwaycom/pathway",
    "title": "Pathway Live Data Framework",
    "content": "<div align=\"center\">\n  <a href=\"https://pathway.com/\">\n    <img src=\"https://pathway.com/logo-light.svg\"/>\n  </a>\n  <br /><br />\n  <a href=\"https://trendshift.io/repositories/10388\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/10388\" alt=\"pathwaycom%2Fpathway | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n  <br /><br />\n</div>\n<p align=\"center\">\n        <a href=\"https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml\">\n        <img src=\"https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg\" alt=\"ubuntu\"/>\n        <br>\n        <a href=\"https://github.com/pathwaycom/pathway/actions/workflows/release.yml\">\n        <img src=\"https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg\" alt=\"Last release\"/></a>\n        <a href=\"https://badge.fury.io/py/pathway\"><img src=\"https://badge.fury.io/py/pathway.svg\" alt=\"PyPI version\" height=\"18\"></a>\n        <a href=\"https://badge.fury.io/py/pathway\"><img src=\"https://static.pepy.tech/badge/pathway\" alt=\"PyPI downloads\" height=\"18\"></a>\n        <a href=\"https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt\">\n        <img src=\"https://img.shields.io/badge/license-BSL-green\" alt=\"License: BSL\"/></a>\n      <br>\n        <a href=\"https://discord.gg/pathway\">\n        <img src=\"https://img.shields.io/discord/1042405378304004156?logo=discord\"\n            alt=\"chat on Discord\"></a>\n        <a href=\"https://twitter.com/intent/follow?screen_name=pathway_com\">\n        <img src=\"https://img.shields.io/twitter/follow/pathwaycom\"\n            alt=\"follow on Twitter\"></a>\n        <a href=\"https://linkedin.com/company/pathway\">\n        <img src=\"https://img.shields.io/badge/pathway-0077B5?style=social&logo=linkedin\" alt=\"follow on LinkedIn\"></a>\n      <a href=\"https://github.com/dylanhogg/awesome-python/blob/main/README.md\">\n      <img src=\"https://awesome.re/badge.svg\" alt=\"Awesome Python\"></a>\n      <a href=\"https://gurubase.io/g/pathway\">\n      <img src=\"https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF\" alt=\"Pathway Guru\"></a>\n    <br>\n    <a href=\"#getting-started\">Getting Started</a> |\n    <a href=\"#deployment\">Deployment</a> |\n    <a href=\"#resources\">Documentation and Support</a> |\n    <a href=\"https://pathway.com/blog/\">Blog</a> |\n    <a href=\"#license\">License</a>\n\n  \n</p>\n\n# Pathway<a id=\"pathway\"> Live Data Framework</a>\n\n[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.\n\nPathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.\nPathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.\nThe same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.\n\nPathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.\nYour Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.\nAll the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.\n\nYou can install Pathway with pip:\n```\npip install -U pathway\n```\n\nFor any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).\n\n## Use-cases and templates\n\nReady to see what Pathway can do?\n\n[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!\n\nAvailable in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!\n\n### Event processing and real-time analytics pipelines\nWith its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:\n\n- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)\n- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)\n- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka/)\n- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)\n\n\n\n### AI Pipelines\n\nPathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).\n\nDon't hesitate to try one of our runnable examples featuring LLM tooling.\nYou can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).\n\n  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured/)\n  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)\n  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)\n  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)\n\n## Features\n\n- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.\n- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.\n- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!\n- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \"at least once\" consistency while the enterprise version provides the \"exactly once\" consistency.\n- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.\n- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.\n\n\n## Getting started<a id=\"getting-started\"></a>\n\n### Installation<a id=\"installation\"></a>\n\nPathway requires Python 3.10 or above.\n\nYou can install the current release of Pathway using `pip`:\n\n```\n$ pip install -U pathway\n```\n\n⚠️ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.\n\n\n### Example: computing the sum of positive values in real time.<a id=\"example\"></a>\n\n```python\nimport pathway as pw\n\n# Define the schema of your data (Optional)\nclass InputSchema(pw.Schema):\n  value: int\n\n# Connect to your data using connectors\ninput_table = pw.io.csv.read(\n  \"./input/\",\n  schema=InputSchema\n)\n\n#Define your operations on the data\nfiltered_table = input_table.filter(input_table.value>=0)\nresult_table = filtered_table.reduce(\n  sum_value = pw.reducers.sum(filtered_table.value)\n)\n\n# Load your results to external systems\npw.io.jsonlines.write(result_table, \"output.jsonl\")\n\n# Run the computation\npw.run()\n```\n\nRun Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).\n\nYou can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).\n\n\n## Deployment<a id=\"deployment\"></a>\n\n### Locally<a id=\"running-pathway-locally\"></a>\n\nTo use Pathway, you only need to import it:\n\n```python\nimport pathway as pw\n```\n\nNow, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:\n\n```python\npw.run()\n```\n\nYou can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.\nPathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. \n\n<img src=\"https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png\" width=\"1326\" alt=\"Pathway dashboard\"/>\n\nAlternatively, you can use the pathway'ish version:\n\n```\n$ pathway spawn python main.py\n```\n\nPathway natively supports multithreading.\nTo launch your application with 3 threads, you can do as follows:\n```\n$ pathway spawn --threads 3 python main.py\n```\n\nTo jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).\n\n\n### Docker<a id=\"docker\"></a>\n\nYou can easily run Pathway using docker.\n\n#### Pathway image\n\nYou can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:\n\n```dockerfile\nFROM pathwaycom/pathway:latest\n\nWORKDIR /app\n\nCOPY requirements.txt ./\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [ \"python\", \"./your-script.py\" ]\n```\n\nYou can then build and run the Docker image:\n\n```console\ndocker build -t my-pathway-app .\ndocker run -it --rm --name my-pathway-app my-pathway-app\n```\n\n#### Run a single Python script\n\nWhen dealing with single-file projects, creating a full-fledged `Dockerfile`\nmight seem unnecessary. In such scenarios, you can execute a\nPython script directly using the Pathway Docker image. For example:\n\n```console\ndocker run -it --rm --name my-pathway-app -v \"$PWD\":/app pathwaycom/pathway:latest python my-pathway-app.py\n```\n\n#### Python docker image\n\nYou can also use a standard Python image and install Pathway using pip with a Dockerfile:\n\n```dockerfile\nFROM --platform=linux/x86_64 python:3.10\n\nRUN pip install -U pathway\nCOPY ./pathway-script.py pathway-script.py\n\nCMD [\"python\", \"-u\", \"pathway-script.py\"]\n```\n\n### Kubernetes and cloud<a id=\"k8s\"></a>\n\nDocker containers are ideally suited for deployment on the cloud with Kubernetes.\nIf you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.\nPathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.\nIt scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.\n\nYou can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).\n\nIf you are interested, don't hesitate to [contact us](mailto:contact@pathway.com) to learn more.\n\n## Performance<a id=\"performance\"></a>\n\nPathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).\n\nIf you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).\n\n<img src=\"https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png\" width=\"1326\" alt=\"WordCount Graph\"/>\n\n## Documentation and Support<a id=\"resources\"></a>\n\nThe entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).\n\nIf you have any question, don't hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).\n\n## License<a id=\"license\"></a>\n\nPathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.\n\n\n## Contribution guidelines<a id=\"contribution-guidelines\"></a>\n\nIf you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. \n\nFor all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don't hesitate to engage with Pathway's [Discord community](https://discord.gg/pathway).\n",
    "summary": "Pathway是一个基于Python的ETL框架，专为流处理、实时分析、LLM管道和RAG应用设计。其核心是高性能的Rust引擎，支持增量计算、多线程和分布式处理，能够有效处理批处理和流式数据。Pathway提供易用的Python API，兼容现有Python库，内置丰富的连接器和LLM工具，支持有状态转换、持久化和数据一致性。该框架易于部署，支持Docker和Kubernetes，性能优于传统流处理技术。",
    "keywords": [
      "流处理",
      "实时分析",
      "ETL",
      "LLM",
      "RAG",
      "增量计算",
      "分布式计算",
      "数据一致性"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "大模型"
    ],
    "published_time": "2025-05-26T05:01:22Z",
    "download_time": "2024-05-27 10:00:00",
    "visual_resource": [
      "https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png",
      "https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png"
    ],
    "extra_info": null
  },
  {
    "id": "wg-easy",
    "source": "GitHub",
    "url": "https://github.com/wg-easy/wg-easy",
    "title": "WireGuard Easy",
    "content": "# WireGuard Easy\n\n[![Build & Publish latest Image](https://github.com/wg-easy/wg-easy/actions/workflows/deploy.yml/badge.svg?branch=production)](https://github.com/wg-easy/wg-easy/actions/workflows/deploy.yml)\n[![Lint](https://github.com/wg-easy/wg-easy/actions/workflows/lint.yml/badge.svg?branch=master)](https://github.com/wg-easy/wg-easy/actions/workflows/lint.yml)\n[![GitHub Stars](https://img.shields.io/github/stars/wg-easy/wg-easy)](https://github.com/wg-easy/wg-easy/stargazers)\n[![License](https://img.shields.io/github/license/wg-easy/wg-easy)](LICENSE)\n[![GitHub Release](https://img.shields.io/github/v/release/wg-easy/wg-easy)](https://github.com/wg-easy/wg-easy/releases/latest)\n[![Image Pulls](https://img.shields.io/badge/image_pulls-12M+-blue)](https://github.com/wg-easy/wg-easy/pkgs/container/wg-easy)\n\n<!-- TODO: remove after release -->\n\n> [!WARNING]\n> You are viewing the README of the pre-release of v15.\n> If you want to setup wg-easy right now. Read the README in the production branch here: [README](https://github.com/wg-easy/wg-easy/tree/production) or here for the last nightly: [README](https://github.com/wg-easy/wg-easy/tree/c6dce0f6fb2e28e7e40ddac1498bd67e9bb17cba)\n\nYou have found the easiest way to install & manage WireGuard on any Linux host!\n\n<!-- TOOD: update screenshot -->\n\n<p align=\"center\">\n  <img src=\"./assets/screenshot.png\" width=\"802\" />\n</p>\n\n## Features\n\n- All-in-one: WireGuard + Web UI.\n- Easy installation, simple to use.\n- List, create, edit, delete, enable & disable clients.\n- Show a client's QR code.\n- Download a client's configuration file.\n- Statistics for which clients are connected.\n- Tx/Rx charts for each connected client.\n- Gravatar support.\n- Automatic Light / Dark Mode\n- Multilanguage Support\n- One Time Links\n- Client Expiration\n- Prometheus metrics support\n- IPv6 support\n- CIDR support\n- 2FA support\n\n> [!NOTE]\n> To better manage documentation for this project, it has its own site here: [https://wg-easy.github.io/wg-easy/latest](https://wg-easy.github.io/wg-easy/latest)\n\n<!-- TODO: remove after release -->\n\n> [!WARNING]\n> As the Docs are still in Pre-release, you can access them here [https://wg-easy.github.io/wg-easy/Pre-release](https://wg-easy.github.io/wg-easy/Pre-release)\n\n- [Getting Started](https://wg-easy.github.io/wg-easy/latest/getting-started/)\n- [Basic Installation](https://wg-easy.github.io/wg-easy/latest/examples/tutorials/basic-installation/)\n- [Caddy](https://wg-easy.github.io/wg-easy/latest/examples/tutorials/caddy/)\n- [Traefik](https://wg-easy.github.io/wg-easy/latest/examples/tutorials/traefik/)\n- [Podman](https://wg-easy.github.io/wg-easy/latest/examples/tutorials/podman-nft/)\n- [AdGuard Home](https://wg-easy.github.io/wg-easy/latest/examples/tutorials/adguard/)\n\n> [!NOTE]\n> If you want to migrate from the old version to the new version, you can find the migration guide here: [Migration Guide](https://wg-easy.github.io/wg-easy/latest/advanced/migrate/)\n\n## Installation\n\nThis is a quick start guide to get you up and running with WireGuard Easy.\n\nFor a more detailed installation guide, please refer to the [Getting Started](https://wg-easy.github.io/wg-easy/latest/getting-started/) page.\n\n### 1. Install Docker\n\nIf you haven't installed Docker yet, install it by running as root:\n\n```shell\ncurl -sSL https://get.docker.com | sh\nexit\n```\n\nAnd log in again.\n\n### 2. Run WireGuard Easy\n\nThe easiest way to run WireGuard Easy is with Docker Compose.\n\nJust download [`docker-compose.yml`](docker-compose.yml) and execute `sudo docker compose up -d`.\n\nNow setup a reverse proxy to be able to access the Web UI securely from the internet.\n\nIf you want to access the Web UI over HTTP, change the env var `INSECURE` to `true`. This is not recommended. Only use this for testing\n\n## Donate\n\nAre you enjoying this project? Consider donating.\n\nFounder: [Buy Emile a beer!](https://github.com/sponsors/WeeJeWel) 🍻\n\nMaintainer: [Buy kaaax0815 a coffee!](https://github.com/sponsors/kaaax0815) ☕\n\n## Development\n\n### Prerequisites\n\n- Docker\n- Node LTS & corepack enabled\n- Visual Studio Code\n\n### Dev Server\n\nThis starts the development server with docker\n\n```shell\npnpm dev\n```\n\n### Update Auto Imports\n\nIf you add something that should be auto-importable and VSCode complains, run:\n\n```shell\ncd src\npnpm install\ncd ..\n```\n\n### Test Cli\n\nThis starts the cli with docker\n\n```shell\npnpm cli:dev\n```\n\n## License\n\nThis project is licensed under the AGPL-3.0-only License - see the [LICENSE](LICENSE) file for details\n\nThis project is not affiliated, associated, authorized, endorsed by, or in any way officially connected with Jason A. Donenfeld, ZX2C4 or Edge Security\n\n\"WireGuard\" and the \"WireGuard\" logo are registered trademarks of Jason A. Donenfeld\n",
    "summary": "WireGuard Easy项目提供了一种在Linux主机上安装和管理WireGuard VPN的最简便方式。该项目将WireGuard服务与Web用户界面集成，用户可以通过直观的界面轻松创建、管理和监控VPN客户端。其核心功能包括客户端列表、增删改查、配置二维码生成、统计信息展示等。项目支持通过Docker或Docker Compose快速部署，极大地简化了WireGuard的配置和日常维护工作，是个人或小型团队快速搭建VPN的理想选择。",
    "keywords": [
      "WireGuard",
      "VPN",
      "Web界面",
      "Docker",
      "Linux",
      "网络管理",
      "安全",
      "客户端管理"
    ],
    "area": [
      "其他",
      "其他",
      "其他"
    ],
    "published_time": "2025-05-16T07:11:50Z",
    "download_time": "2024-07-28 10:00:00",
    "visual_resource": [
      "https://github.com/wg-easy/wg-easy/raw/production/assets/screenshot.png"
    ],
    "extra_info": null
  },
  {
    "id": "2505.18125",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18125",
    "title": "TabSTAR：一种具有语义目标感知表示的基础表格模型",
    "summary": "尽管深度学习在许多领域取得了显着成功，但在表格学习任务上历来表现不佳，这些任务仍由梯度提升决策树（GBDTS）主导。然而，最近的进展正在为表格基础模型铺平道路，这些模型可以利用现实世界知识并在不同数据集上泛化，尤其是在数据包含自由文本时。尽管已探索将语言模型能力纳入表格任务，但大多数现有方法使用静态、目标无关的文本表示，限制了其有效性。我们引入了 TabSTAR：一种具有语义目标感知表示的基础表格模型。TabSTAR 旨在实现对带有文本特征的表格数据的迁移学习，其架构不含数据集特定参数。它解冻了一个预训练的文本编码器，并接收目标 token 作为输入，这些 token 为模型提供了学习任务特定嵌入所需的上下文。TabSTAR 在带有文本特征的分类任务的已知基准测试中，对中型和大型数据集均实现了最先进的性能，并且其预训练阶段在数据集数量方面表现出缩放规律，为进一步提高性能提供了途径。",
    "keywords": [
      "TabSTAR",
      "表格学习",
      "基础模型",
      "文本特征",
      "目标感知表示"
    ],
    "area": [
      "机器学习",
      "自然语言处理",
      "大模型"
    ],
    "content": "尽管深度学习在许多领域取得了显着成功，但在表格学习任务上历来表现不佳，这些任务仍由梯度提升决策树（GBDTS）主导。然而，最近的进展正在为表格基础模型铺平道路，这些模型可以利用现实世界知识并在不同数据集上泛化，尤其是在数据包含自由文本时。尽管已探索将语言模型能力纳入表格任务，但大多数现有方法使用静态、目标无关的文本表示，限制了其有效性。我们引入了 TabSTAR：一种具有语义目标感知表示的基础表格模型。TabSTAR 旨在实现对带有文本特征的表格数据的迁移学习，其架构不含数据集特定参数。它解冻了一个预训练的文本编码器，并接收目标 token 作为输入，这些 token 为模型提供了学习任务特定嵌入所需的上下文。TabSTAR 在带有文本特征的分类任务的已知基准测试中，对中型和大型数据集均实现了最先进的性能，并且其预训练阶段在数据集数量方面表现出缩放规律，为进一步提高性能提供了途径。",
    "published_time": "2025-05-23T17:34:28.000Z",
    "download_time": "2025-05-26 03:50:57",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18125.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18125",
      "arxiv_url": "https://arxiv.org/abs/2505.18125"
    }
  },
  {
    "id": "2505.17667",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17667",
    "title": "QwenLong-L1：迈向基于强化学习的长上下文大型推理模型",
    "summary": "近期的大型推理模型 (LRMs) 通过强化学习 (RL) 展示了强大的推理能力。这些改进主要体现在短上下文推理任务中。相比之下，如何通过强化学习将大型推理模型有效扩展到处理和推理长上下文输入，仍然是一个关键的未解决挑战。为了弥合这一差距，我们首先形式化了长上下文推理强化学习范式，并指出了训练效率低下和优化过程不稳定等关键挑战。为解决这些问题，我们提出了 QwenLong-L1 框架，该框架通过渐进式上下文缩放将短上下文大型推理模型适应长上下文场景。具体而言，我们采用热启动的指令微调 (SFT) 阶段建立鲁棒的初始策略，继之以课程指导的分阶段强化学习技术稳定策略演进，并辅以难度感知的追溯采样策略激励策略探索。在七个长上下文文档问答基准上的实验表明，QwenLong-L1-32B 优于 OpenAI-o3-mini 和 Qwen3-235B-A22B 等旗舰级大型推理模型，性能与 Claude-3.7-Sonnet-Thinking 持平，证明了其在最先进大型推理模型中的领先性能。这项工作推动了能够对信息密集型环境进行鲁棒推理的实用长上下文大型推理模型的发展。",
    "keywords": [
      "长上下文",
      "大型推理模型",
      "强化学习",
      "渐进式上下文缩放",
      "文档问答"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "content": "近期的大型推理模型 (LRMs) 通过强化学习 (RL) 展示了强大的推理能力。这些改进主要体现在短上下文推理任务中。相比之下，如何通过强化学习将大型推理模型有效扩展到处理和推理长上下文输入，仍然是一个关键的未解决挑战。为了弥合这一差距，我们首先形式化了长上下文推理强化学习范式，并指出了训练效率低下和优化过程不稳定等关键挑战。为解决这些问题，我们提出了 QwenLong-L1 框架，该框架通过渐进式上下文缩放将短上下文大型推理模型适应长上下文场景。具体而言，我们采用热启动的指令微调 (SFT) 阶段建立鲁棒的初始策略，继之以课程指导的分阶段强化学习技术稳定策略演进，并辅以难度感知的追溯采样策略激励策略探索。在七个长上下文文档问答基准上的实验表明，QwenLong-L1-32B 优于 OpenAI-o3-mini 和 Qwen3-235B-A22B 等旗舰级大型推理模型，性能与 Claude-3.7-Sonnet-Thinking 持平，证明了其在最先进大型推理模型中的领先性能。这项工作推动了能够对信息密集型环境进行鲁棒推理的实用长上下文大型推理模型的发展。",
    "published_time": "2025-05-23T09:31:55.000Z",
    "download_time": "2025-05-26 03:51:09",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17667.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17667",
      "arxiv_url": "https://arxiv.org/abs/2505.17667"
    }
  },
  {
    "id": "2505.18129",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18129",
    "title": "一个强化学习模型通览全局：视觉三位一体统一强化学习",
    "summary": "强化学习（RL）显著提升了视觉-语言模型（VLMs）的推理能力。然而，RL在推理任务之外的应用仍未得到充分探索，尤其是在目标检测和视觉定位（grounding）等感知密集型任务方面。我们提出了 V-Triune，一个视觉三位一体统一强化学习系统，它使 VLMs 能够在单一训练流水线中联合学习视觉推理和感知任务。V-Triune 包含三个互补的组件：样本级数据格式化（用于统一多样的任务输入）、验证器级奖励计算（通过专门的验证器提供定制奖励）和源级度量监控（用于在数据源层面诊断问题）。我们进一步引入了一种新颖的动态 IoU 奖励，它为 V-Triune 处理的感知任务提供自适应、渐进且明确的反馈。我们的方法在现成的 RL 训练框架中实现，使用了开源的 7B 和 32B 主干模型。生成的新模型，名为 Orsta（一个强化学习模型通览全局），在推理和感知任务上均表现出持续改进。这种广泛的能力得益于其在多样化数据集上的训练，该数据集围绕四种代表性的视觉推理任务（数学、谜题、图表和科学）和四种视觉感知任务（视觉定位、目标检测、计数和 OCR）构建。随后，Orsta 在 MEGA-Bench Core 上取得了显著进步，其不同的 7B 和 32B 模型变体性能提升范围从 +2.1 到惊人的 +14.1，性能优势扩展到了广泛的下游任务。这些结果突显了我们为 VLMs 提出的统一 RL 方法的有效性和可扩展性。V-Triune 系统及 Orsta 模型已公开发布于 https://github.com/MiniMax-AI。",
    "keywords": [
      "视觉-语言模型",
      "强化学习",
      "统一学习",
      "视觉推理",
      "视觉感知"
    ],
    "area": [
      "多模态",
      "机器学习",
      "大模型"
    ],
    "content": "强化学习（RL）显著提升了视觉-语言模型（VLMs）的推理能力。然而，RL在推理任务之外的应用仍未得到充分探索，尤其是在目标检测和视觉定位（grounding）等感知密集型任务方面。我们提出了 V-Triune，一个视觉三位一体统一强化学习系统，它使 VLMs 能够在单一训练流水线中联合学习视觉推理和感知任务。V-Triune 包含三个互补的组件：样本级数据格式化（用于统一多样的任务输入）、验证器级奖励计算（通过专门的验证器提供定制奖励）和源级度量监控（用于在数据源层面诊断问题）。我们进一步引入了一种新颖的动态 IoU 奖励，它为 V-Triune 处理的感知任务提供自适应、渐进且明确的反馈。我们的方法在现成的 RL 训练框架中实现，使用了开源的 7B 和 32B 主干模型。生成的新模型，名为 Orsta（一个强化学习模型通览全局），在推理和感知任务上均表现出持续改进。这种广泛的能力得益于其在多样化数据集上的训练，该数据集围绕四种代表性的视觉推理任务（数学、谜题、图表和科学）和四种视觉感知任务（视觉定位、目标检测、计数和 OCR）构建。随后，Orsta 在 MEGA-Bench Core 上取得了显著进步，其不同的 7B 和 32B 模型变体性能提升范围从 +2.1 到惊人的 +14.1，性能优势扩展到了广泛的下游任务。这些结果突显了我们为 VLMs 提出的统一 RL 方法的有效性和可扩展性。V-Triune 系统及 Orsta 模型已公开发布于 https://github.com/MiniMax-AI。",
    "published_time": "2025-05-23T17:41:14.000Z",
    "download_time": "2025-05-26 03:51:29",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18129.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18129",
      "arxiv_url": "https://arxiv.org/abs/2505.18129"
    }
  },
  {
    "id": "2505.17612",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17612",
    "title": "将LLM智能体蒸馏到具备检索与代码工具的小型模型中",
    "summary": "大语言模型 (LLMs) 擅长处理复杂的推理任务，但计算成本高昂，限制了其实际部署。为解决这一问题，近期工作致力于利用教师LLM的思维链 (CoT) 轨迹，将推理能力蒸馏到小型语言模型 (sLMs) 中。然而，在需要不常见的事实知识或精确计算的场景中，这种方法面临挑战，sLMs由于能力有限常常出现幻觉。在这项工作中，我们提出智能体蒸馏 (Agent Distillation)，这是一个不仅将推理能力，还将基于LLM的智能体的完整任务解决行为转移到具备检索和代码工具的sLMs的框架。我们沿两个互补方向改进智能体蒸馏：(1) 我们引入一种名为“首思前缀” (first-thought prefix) 的提示方法，以提高教师生成的轨迹质量；(2) 我们提出一种自洽的动作生成方法，以提高小型智能体的测试时鲁棒性。我们在涵盖事实和数学领域的八个推理任务上评估了我们的方法，包括域内和域外的泛化能力。我们的结果表明，参数量小至0.5B、1.5B、3B的sLMs可以达到与使用CoT蒸馏微调的下一层级更大的1.5B、3B、7B模型相当的性能，证明了智能体蒸馏在构建实用的、使用工具的小型智能体方面的潜力。我们的代码可在 https://github.com/Nardien/agent-distillation 获取。",
    "keywords": [
      "大语言模型",
      "智能体蒸馏",
      "小型模型",
      "检索与代码工具",
      "推理任务"
    ],
    "area": [
      "大模型",
      "智能体",
      "自然语言处理"
    ],
    "content": "大语言模型 (LLMs) 擅长处理复杂的推理任务，但计算成本高昂，限制了其实际部署。为解决这一问题，近期工作致力于利用教师LLM的思维链 (CoT) 轨迹，将推理能力蒸馏到小型语言模型 (sLMs) 中。然而，在需要不常见的事实知识或精确计算的场景中，这种方法面临挑战，sLMs由于能力有限常常出现幻觉。在这项工作中，我们提出智能体蒸馏 (Agent Distillation)，这是一个不仅将推理能力，还将基于LLM的智能体的完整任务解决行为转移到具备检索和代码工具的sLMs的框架。我们沿两个互补方向改进智能体蒸馏：(1) 我们引入一种名为“首思前缀” (first-thought prefix) 的提示方法，以提高教师生成的轨迹质量；(2) 我们提出一种自洽的动作生成方法，以提高小型智能体的测试时鲁棒性。我们在涵盖事实和数学领域的八个推理任务上评估了我们的方法，包括域内和域外的泛化能力。我们的结果表明，参数量小至0.5B、1.5B、3B的sLMs可以达到与使用CoT蒸馏微调的下一层级更大的1.5B、3B、7B模型相当的性能，证明了智能体蒸馏在构建实用的、使用工具的小型智能体方面的潜力。我们的代码可在 https://github.com/Nardien/agent-distillation 获取。",
    "published_time": "2025-05-23T08:20:15.000Z",
    "download_time": "2025-05-26 03:51:45",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17612.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17612",
      "arxiv_url": "https://arxiv.org/abs/2505.17612"
    }
  },
  {
    "id": "2505.15929",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.15929",
    "title": "PhyX：您的模型是否具备物理推理的“才智”？",
    "summary": "现有基准未能捕捉到智能的一个关键方面：物理推理能力，这是一种整合领域知识、符号推理和现实世界约束理解的综合能力。为了弥补这一空白，我们引入了 PhyX：首个旨在评估模型在视觉场景中进行物理推理能力的大规模基准。PhyX 包含 3000 个精心策划的多模态问题，涵盖 6 种推理类型、25 个子领域和 6 个核心物理领域：热力学、电磁学、力学、现代物理、光学以及波&声学。在我们的全面评估中，即使是最先进的模型在物理推理方面也表现不佳。GPT-4o、Claude3.7-Sonnet 和 GPT-o4-mini 的准确率分别仅为 32.5%、42.2% 和 45.8%，与人类专家相比，性能差距超过 29%。我们的分析揭示了当前模型的关键局限性：过度依赖记忆性的学科知识、过度依赖数学公式，以及仅进行表面层的视觉模式匹配而非真正的物理理解。我们通过细粒度统计、详细案例分析和多种评估范式提供了深入分析，以全面考察物理推理能力。为了确保可重复性，我们基于 VLMEvalKit 等广泛使用的工具包实现了兼容的评估协议，支持一键评估。",
    "keywords": [
      "物理推理",
      "多模态",
      "基准",
      "大模型",
      "评估"
    ],
    "area": [
      "人工智能",
      "多模态",
      "大模型"
    ],
    "content": "现有基准未能捕捉到智能的一个关键方面：物理推理能力，这是一种整合领域知识、符号推理和现实世界约束理解的综合能力。为了弥补这一空白，我们引入了 PhyX：首个旨在评估模型在视觉场景中进行物理推理能力的大规模基准。PhyX 包含 3000 个精心策划的多模态问题，涵盖 6 种推理类型、25 个子领域和 6 个核心物理领域：热力学、电磁学、力学、现代物理、光学以及波&声学。在我们的全面评估中，即使是最先进的模型在物理推理方面也表现不佳。GPT-4o、Claude3.7-Sonnet 和 GPT-o4-mini 的准确率分别仅为 32.5%、42.2% 和 45.8%，与人类专家相比，性能差距超过 29%。我们的分析揭示了当前模型的关键局限性：过度依赖记忆性的学科知识、过度依赖数学公式，以及仅进行表面层的视觉模式匹配而非真正的物理理解。我们通过细粒度统计、详细案例分析和多种评估范式提供了深入分析，以全面考察物理推理能力。为了确保可重复性，我们基于 VLMEvalKit 等广泛使用的工具包实现了兼容的评估协议，支持一键评估。",
    "published_time": "2025-05-21T18:33:50.000Z",
    "download_time": "2025-05-26 03:52:00",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15929.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.15929",
      "arxiv_url": "https://arxiv.org/abs/2505.15929"
    }
  },
  {
    "id": "2505.18092",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.18092",
    "title": "QwenLong-CPRS：迈向无限长上下文LLM的动态上下文优化方法",
    "summary": "本技术报告介绍了QwenLong-CPRS，一个专为显式长上下文优化设计的上下文压缩框架，旨在解决预填充（prefill）阶段高昂的计算开销以及大型语言模型（LLMs）在处理长序列时出现的“迷失中间”（lost in the middle）性能下降问题。QwenLong-CPRS 通过一种新颖的动态上下文优化机制实现，它支持在自然语言指令引导下的多粒度上下文压缩，从而同时提升效率和性能。\nQwenLong-CPRS 从Qwen架构系列演进而来，引入了四项关键创新：(1) 自然语言引导的动态优化，(2) 用于增强边界感知的双向推理层，(3) 带有语言建模头的Token批评机制，以及(4) 窗口并行推理。\n在五个基准测试（涵盖4K至2M词的上下文）上的全面评估证明了QwenLong-CPRS三重有效性：(1) 在准确性和效率方面持续优于RAG和稀疏注意力等其他上下文管理方法。(2) 它能够与所有旗舰级LLMs（包括GPT-4o、Gemini2.0-pro、Claude3.7-sonnet、DeepSeek-v3和Qwen2.5-max）进行架构无关的集成，在实现21.59倍上下文压缩的同时，平均性能提升了19.15个百分点；(3) 与Qwen2.5-32B-Instruct一同部署时，QwenLong-CPRS在Ruler-128K和InfiniteBench上分别超越了主流专有LLMs 4.85和10.88个百分点，建立了新的最先进（SOTA）性能。",
    "keywords": [
      "长上下文LLM",
      "上下文压缩",
      "动态优化",
      "QwenLong-CPRS",
      "Lost in the Middle"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "深度学习"
    ],
    "content": "本技术报告介绍了QwenLong-CPRS，一个专为显式长上下文优化设计的上下文压缩框架，旨在解决预填充（prefill）阶段高昂的计算开销以及大型语言模型（LLMs）在处理长序列时出现的“迷失中间”（lost in the middle）性能下降问题。QwenLong-CPRS 通过一种新颖的动态上下文优化机制实现，它支持在自然语言指令引导下的多粒度上下文压缩，从而同时提升效率和性能。\nQwenLong-CPRS 从Qwen架构系列演进而来，引入了四项关键创新：(1) 自然语言引导的动态优化，(2) 用于增强边界感知的双向推理层，(3) 带有语言建模头的Token批评机制，以及(4) 窗口并行推理。\n在五个基准测试（涵盖4K至2M词的上下文）上的全面评估证明了QwenLong-CPRS三重有效性：(1) 在准确性和效率方面持续优于RAG和稀疏注意力等其他上下文管理方法。(2) 它能够与所有旗舰级LLMs（包括GPT-4o、Gemini2.0-pro、Claude3.7-sonnet、DeepSeek-v3和Qwen2.5-max）进行架构无关的集成，在实现21.59倍上下文压缩的同时，平均性能提升了19.15个百分点；(3) 与Qwen2.5-32B-Instruct一同部署时，QwenLong-CPRS在Ruler-128K和InfiniteBench上分别超越了主流专有LLMs 4.85和10.88个百分点，建立了新的最先进（SOTA）性能。",
    "published_time": "2025-05-23T16:47:00.000Z",
    "download_time": "2025-05-26 03:52:15",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18092.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.18092",
      "arxiv_url": "https://arxiv.org/abs/2505.18092"
    }
  },
  {
    "id": "2505.17225",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17225",
    "title": "推理模型为何僵化：诊断推理模型中的指令覆盖现象",
    "summary": "大型语言模型在处理长而复杂的推理任务中展现出了卓越的能力。然而，它们常常表现出对现有推理模式的过度依赖，我们称之为“推理僵化”现象。尽管用户给出了明确指令，这些模型仍经常忽略清晰指定的条件，转而采用习惯性的推理路径，导致得到错误结论。这种行为带来了重大挑战，尤其是在数学和逻辑谜题等领域，其中精确遵守指定约束至关重要。为了系统地研究推理僵化这一在先前工作中很大程度上未被探索的行为，我们引入了一个由专家精心策划的诊断数据集。我们的数据集包括对现有数学基准（即AIME和MATH500）进行特殊修改的版本，以及经过精心重新设计、要求偏离现有熟悉推理策略的知名谜题。利用该数据集，我们识别出了当模型转而采用根深蒂固的推理方式时出现的常见“污染”模式。具体而言，我们将这种污染分为三种不同的模式：（i）解读超载（Interpretation Overload），（ii）输入不信任（Input Distrust），以及（iii）部分指令关注（Partial Instruction Attention），每种模式都会导致模型忽视或扭曲所提供的指令。我们公开发布我们的诊断数据集，以促进未来关于缓解语言模型中推理僵化的研究。",
    "keywords": [
      "Reasoning rigidity",
      "Instruction Overriding",
      "Diagnostic Set",
      "Large Language Models",
      "Contamination Patterns"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "人工智能"
    ],
    "content": "大型语言模型在处理长而复杂的推理任务中展现出了卓越的能力。然而，它们常常表现出对现有推理模式的过度依赖，我们称之为“推理僵化”现象。尽管用户给出了明确指令，这些模型仍经常忽略清晰指定的条件，转而采用习惯性的推理路径，导致得到错误结论。这种行为带来了重大挑战，尤其是在数学和逻辑谜题等领域，其中精确遵守指定约束至关重要。为了系统地研究推理僵化这一在先前工作中很大程度上未被探索的行为，我们引入了一个由专家精心策划的诊断数据集。我们的数据集包括对现有数学基准（即AIME和MATH500）进行特殊修改的版本，以及经过精心重新设计、要求偏离现有熟悉推理策略的知名谜题。利用该数据集，我们识别出了当模型转而采用根深蒂固的推理方式时出现的常见“污染”模式。具体而言，我们将这种污染分为三种不同的模式：（i）解读超载（Interpretation Overload），（ii）输入不信任（Input Distrust），以及（iii）部分指令关注（Partial Instruction Attention），每种模式都会导致模型忽视或扭曲所提供的指令。我们公开发布我们的诊断数据集，以促进未来关于缓解语言模型中推理僵化的研究。",
    "published_time": "2025-05-22T19:00:01.000Z",
    "download_time": "2025-05-26 03:52:33",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17225.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17225",
      "arxiv_url": "https://arxiv.org/abs/2505.17225"
    }
  },
  {
    "id": "2505.14669",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.14669",
    "title": "Quartet：原生 FP4 训练对于大型语言模型可以是最优的",
    "summary": "大型语言模型（LLMs）的快速发展伴随着计算需求的空前增长，最先进模型的训练成本每隔几个月就会翻倍。直接以低精度算术训练模型提供了一种解决方案，能够提高计算吞吐量和能源效率。具体而言，NVIDIA 最近推出的 Blackwell 架构支持极低精度运算，特别是 FP4 变体，有望带来显著的效率提升。然而，当前用于 FP4 精度训练 LLMs 的算法面临显著的精度下降问题，并且常常依赖于混合精度回退。在本文中，我们系统地研究了硬件支持的 FP4 训练，并引入了 Quartet，这是一种新的方法，能够实现准确的端到端 FP4 训练，其中所有主要计算（例如线性层中的计算）都在低精度下执行。通过在 Llama 类型模型上进行大量评估，我们揭示了一种新的低精度缩放定律，该定律量化了不同位宽下的性能权衡，并使我们能够识别出一种在精度与计算量方面“接近最优”的低精度训练技术，即 Quartet。我们使用为 NVIDIA Blackwell GPU 定制优化的 CUDA 内核实现了 Quartet，并表明它可以达到 FP4 精度的最先进水平，成功训练了亿级规模的模型。我们的方法表明，完全基于 FP4 的训练是标准精度和 FP8 训练的一个有竞争力的替代方案。我们的代码可在 https://github.com/IST-DASLab/Quartet 获取。",
    "keywords": [
      "大模型",
      "FP4",
      "低精度训练",
      "Quartet",
      "Blackwell"
    ],
    "area": [
      "大模型",
      "深度学习",
      "自然语言处理"
    ],
    "content": "大型语言模型（LLMs）的快速发展伴随着计算需求的空前增长，最先进模型的训练成本每隔几个月就会翻倍。直接以低精度算术训练模型提供了一种解决方案，能够提高计算吞吐量和能源效率。具体而言，NVIDIA 最近推出的 Blackwell 架构支持极低精度运算，特别是 FP4 变体，有望带来显著的效率提升。然而，当前用于 FP4 精度训练 LLMs 的算法面临显著的精度下降问题，并且常常依赖于混合精度回退。在本文中，我们系统地研究了硬件支持的 FP4 训练，并引入了 Quartet，这是一种新的方法，能够实现准确的端到端 FP4 训练，其中所有主要计算（例如线性层中的计算）都在低精度下执行。通过在 Llama 类型模型上进行大量评估，我们揭示了一种新的低精度缩放定律，该定律量化了不同位宽下的性能权衡，并使我们能够识别出一种在精度与计算量方面“接近最优”的低精度训练技术，即 Quartet。我们使用为 NVIDIA Blackwell GPU 定制优化的 CUDA 内核实现了 Quartet，并表明它可以达到 FP4 精度的最先进水平，成功训练了亿级规模的模型。我们的方法表明，完全基于 FP4 的训练是标准精度和 FP8 训练的一个有竞争力的替代方案。我们的代码可在 https://github.com/IST-DASLab/Quartet 获取。",
    "published_time": "2025-05-20T17:55:50.000Z",
    "download_time": "2025-05-26 03:52:50",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14669.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.14669",
      "arxiv_url": "https://arxiv.org/abs/2505.14669"
    }
  },
  {
    "id": "2505.17941",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17941",
    "title": "VeriThinker：通过学习验证提升推理模型的效率",
    "summary": "大型推理模型（LRM）擅长使用思维链（CoT）推理解决复杂任务。然而，它们过度思考的倾向会导致不必要的冗长推理链，从而显著增加推理成本。为了缓解这个问题，我们引入了 VeriThinker，一种新颖的 CoT 压缩方法。与使用合成的简洁 CoT 数据直接在原始推理任务上微调 LRM 的传统方法不同，我们创新性地仅通过辅助验证任务来微调模型。通过训练 LRM 准确验证 CoT 解决方案的正确性，LRM 本身对后续自我反思步骤的必要性变得更具辨别力，从而有效抑制过度思考。大量实验验证了 VeriThinker 在大幅缩短推理链长度的同时，保持甚至略微提高了准确性。将其应用于 DeepSeek-R1-Distill-Qwen-7B 模型时，我们的方法在 MATH500 数据集上将推理 token 数从 3790 减少到 2125，同时准确率提高了 0.8%（从 94.0% 到 94.8%）；在 AIME25 数据集上，token 数从 14321 减少到 10287，准确率提高了 2.1%（从 38.7% 到 40.8%）。此外，我们的实验表明 VeriThinker 还可以零样本泛化到推测性推理。代码可在 https://github.com/czg1225/VeriThinker 获取。",
    "keywords": [
      "大模型",
      "思维链（CoT）",
      "推理效率",
      "验证学习",
      "CoT 压缩"
    ],
    "area": [
      "大模型",
      "生成式AI",
      "深度学习"
    ],
    "content": "大型推理模型（LRM）擅长使用思维链（CoT）推理解决复杂任务。然而，它们过度思考的倾向会导致不必要的冗长推理链，从而显著增加推理成本。为了缓解这个问题，我们引入了 VeriThinker，一种新颖的 CoT 压缩方法。与使用合成的简洁 CoT 数据直接在原始推理任务上微调 LRM 的传统方法不同，我们创新性地仅通过辅助验证任务来微调模型。通过训练 LRM 准确验证 CoT 解决方案的正确性，LRM 本身对后续自我反思步骤的必要性变得更具辨别力，从而有效抑制过度思考。大量实验验证了 VeriThinker 在大幅缩短推理链长度的同时，保持甚至略微提高了准确性。将其应用于 DeepSeek-R1-Distill-Qwen-7B 模型时，我们的方法在 MATH500 数据集上将推理 token 数从 3790 减少到 2125，同时准确率提高了 0.8%（从 94.0% 到 94.8%）；在 AIME25 数据集上，token 数从 14321 减少到 10287，准确率提高了 2.1%（从 38.7% 到 40.8%）。此外，我们的实验表明 VeriThinker 还可以零样本泛化到推测性推理。代码可在 https://github.com/czg1225/VeriThinker 获取。",
    "published_time": "2025-05-23T14:17:56.000Z",
    "download_time": "2025-05-26 03:53:08",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17941.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17941",
      "arxiv_url": "https://arxiv.org/abs/2505.17941"
    }
  },
  {
    "id": "2505.17561",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17561",
    "title": "模型已经知道最佳噪声：基于注意力机制的视频扩散模型贝叶斯主动噪声选择",
    "summary": "初始噪声的选择显著影响视频扩散模型的生成质量和与提示词的对齐程度，同一提示词使用不同的噪声种子可能导致截然不同的生成结果。尽管最近的方法依赖于外部设计的先验知识，如频率滤波器或帧间平滑，但它们往往忽略了指示哪些噪声种子本质上更优越的模型内部信号。为了解决这个问题，我们提出了 ANSE（用于生成的活性噪声选择），这是一个模型感知的框架，通过量化基于注意力机制的不确定性来选择高质量的噪声种子。其核心是 BANSA（通过注意力机制的贝叶斯活性噪声选择），这是一个采集函数，通过测量多个随机注意力采样之间的熵分歧来估计模型的置信度和一致性。为了高效的推理时间部署，我们引入了 BANSA 的伯努利掩码近似，该近似允许仅使用一步扩散和一个注意力层子集来进行得分估计。在 CogVideoX-2B 和 5B 上的实验表明，ANSE 提高了视频质量和时间连贯性，而推理时间仅分别增加了 8% 和 13%，为视频扩散中的噪声选择提供了一种原理性且可泛化的方法。请参阅我们的项目页面：https://anse-project.github.io/anse-project/",
    "keywords": [
      "视频扩散模型",
      "噪声选择",
      "注意力机制",
      "贝叶斯",
      "主动选择"
    ],
    "area": [
      "生成式AI",
      "深度学习",
      "计算机视觉"
    ],
    "content": "初始噪声的选择显著影响视频扩散模型的生成质量和与提示词的对齐程度，同一提示词使用不同的噪声种子可能导致截然不同的生成结果。尽管最近的方法依赖于外部设计的先验知识，如频率滤波器或帧间平滑，但它们往往忽略了指示哪些噪声种子本质上更优越的模型内部信号。为了解决这个问题，我们提出了 ANSE（用于生成的活性噪声选择），这是一个模型感知的框架，通过量化基于注意力机制的不确定性来选择高质量的噪声种子。其核心是 BANSA（通过注意力机制的贝叶斯活性噪声选择），这是一个采集函数，通过测量多个随机注意力采样之间的熵分歧来估计模型的置信度和一致性。为了高效的推理时间部署，我们引入了 BANSA 的伯努利掩码近似，该近似允许仅使用一步扩散和一个注意力层子集来进行得分估计。在 CogVideoX-2B 和 5B 上的实验表明，ANSE 提高了视频质量和时间连贯性，而推理时间仅分别增加了 8% 和 13%，为视频扩散中的噪声选择提供了一种原理性且可泛化的方法。请参阅我们的项目页面：https://anse-project.github.io/anse-project/",
    "published_time": "2025-05-23T07:09:10.000Z",
    "download_time": "2025-05-26 03:53:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17561.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17561",
      "arxiv_url": "https://arxiv.org/abs/2505.17561"
    }
  },
  {
    "id": "2505.17873",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17873",
    "title": "MOOSE-Chem3：面向模拟实验反馈的实验引导式假设排序",
    "summary": "假设排序是自动化科学发现的关键组成部分，尤其是在湿实验室实验成本高昂且吞吐量有限的自然科学领域。现有方法侧重于预实验排序，仅依赖于大型语言模型的内部推理，而未整合实验的实证结果。我们引入了实验引导式排序任务，旨在根据先前已测试假设的结果来优先排序候选假设。然而，在自然科学领域中反复进行实际实验是不切实际的，这使得开发此类策略具有挑战性。为解决此问题，我们提出了一种基于三个领域知识假设的模拟器，它将假设性能建模为与已知真实假设相似度的函数，并受到噪声的扰动。我们整理了一个包含 124 个具有实验报告结果的化学假设数据集，用于验证该模拟器。基于该模拟器，我们开发了一种伪实验引导式排序方法，该方法通过共享功能特征对假设进行聚类，并根据模拟实验反馈中获得的见解来优先排序候选假设。实验表明，我们的方法优于预实验基线和强消融模型。",
    "keywords": [
      "假设排序",
      "实验引导式排序",
      "模拟实验反馈",
      "自动化科学发现",
      "化学"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "大模型"
    ],
    "content": "假设排序是自动化科学发现的关键组成部分，尤其是在湿实验室实验成本高昂且吞吐量有限的自然科学领域。现有方法侧重于预实验排序，仅依赖于大型语言模型的内部推理，而未整合实验的实证结果。我们引入了实验引导式排序任务，旨在根据先前已测试假设的结果来优先排序候选假设。然而，在自然科学领域中反复进行实际实验是不切实际的，这使得开发此类策略具有挑战性。为解决此问题，我们提出了一种基于三个领域知识假设的模拟器，它将假设性能建模为与已知真实假设相似度的函数，并受到噪声的扰动。我们整理了一个包含 124 个具有实验报告结果的化学假设数据集，用于验证该模拟器。基于该模拟器，我们开发了一种伪实验引导式排序方法，该方法通过共享功能特征对假设进行聚类，并根据模拟实验反馈中获得的见解来优先排序候选假设。实验表明，我们的方法优于预实验基线和强消融模型。",
    "published_time": "2025-05-23T13:24:50.000Z",
    "download_time": "2025-05-26 03:53:39",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17873.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17873",
      "arxiv_url": "https://arxiv.org/abs/2505.17873"
    }
  },
  {
    "id": "2505.16211",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16211",
    "title": "AudioTrust：用于评测音频大模型多方面可信度得基准",
    "summary": "音频大模型（ALLMs）的快速发展和应用扩展，迫切需要对其可信度进行严谨的理解。然而，对这些模型进行系统性评估（特别是针对音频模态特有风险）的研究仍基本未被探索。现有的评估框架主要聚焦于文本模态，或仅涉及有限的安全维度，未能充分考虑音频模态固有的独特性和应用场景。我们引入 AudioTrust——首个专为 ALLMs 设计的多方面可信度评估框架和基准。AudioTrust 支持在六个关键维度进行评估：公平性、幻觉、安全性、隐私性、鲁棒性和可认证性。为了全面评估这些维度，AudioTrust 构建了 18 种不同的实验设置。其核心是一个精心构建的数据集，包含超过 4,420 个来自真实场景（如日常对话、紧急呼叫、语音助手交互）的音频/文本样本，专门设计用于探查 ALLMs 的多方面可信度。为进行评估，该基准精心设计了 9 个针对音频模态的评估指标，并采用大规模自动化流程对模型输出进行客观且可扩展的评分。实验结果揭示了当前最先进的开源和闭源 ALLMs 在面对各种高风险音频场景时的可信度边界和局限性，为未来音频模型的安全和可信部署提供了宝贵见解。我们的平台和基准可在 https://github.com/JusperLee/AudioTrust 获取。",
    "keywords": [
      "Audio Large Language Models",
      "可信度",
      "基准",
      "评估框架",
      "音频模态"
    ],
    "area": [
      "人工智能",
      "大模型",
      "多模态"
    ],
    "content": "音频大模型（ALLMs）的快速发展和应用扩展，迫切需要对其可信度进行严谨的理解。然而，对这些模型进行系统性评估（特别是针对音频模态特有风险）的研究仍基本未被探索。现有的评估框架主要聚焦于文本模态，或仅涉及有限的安全维度，未能充分考虑音频模态固有的独特性和应用场景。我们引入 AudioTrust——首个专为 ALLMs 设计的多方面可信度评估框架和基准。AudioTrust 支持在六个关键维度进行评估：公平性、幻觉、安全性、隐私性、鲁棒性和可认证性。为了全面评估这些维度，AudioTrust 构建了 18 种不同的实验设置。其核心是一个精心构建的数据集，包含超过 4,420 个来自真实场景（如日常对话、紧急呼叫、语音助手交互）的音频/文本样本，专门设计用于探查 ALLMs 的多方面可信度。为进行评估，该基准精心设计了 9 个针对音频模态的评估指标，并采用大规模自动化流程对模型输出进行客观且可扩展的评分。实验结果揭示了当前最先进的开源和闭源 ALLMs 在面对各种高风险音频场景时的可信度边界和局限性，为未来音频模型的安全和可信部署提供了宝贵见解。我们的平台和基准可在 https://github.com/JusperLee/AudioTrust 获取。",
    "published_time": "2025-05-22T04:27:46.000Z",
    "download_time": "2025-05-26 03:53:50",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16211.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16211",
      "arxiv_url": "https://arxiv.org/abs/2505.16211"
    }
  },
  {
    "id": "2505.17618",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17618",
    "title": "通过测试时演化搜索扩展图像和视频生成",
    "summary": "随着模型预训练期间扩展计算（数据和参数）的边际成本持续显著增加，测试时扩展（TTS）作为一种通过在推理时分配额外计算来提升生成模型性能的有前景的方向应运而生。尽管TTS已在多项语言任务中取得了显著成功，但在理解图像和视频生成模型（基于扩散模型或基于流模型）的测试时扩展行为方面仍存在显著空白。尽管近期工作已开始探索视觉任务的推理时策略，但这些方法面临关键局限性：受限于特定任务领域，可扩展性差，或陷入牺牲样本多样性的奖励过度优化。在本文中，我们提出了演化搜索（EvoSearch），这是一种新颖、通用且高效的TTS方法，能够在无需额外训练或模型扩展的情况下，有效增强跨扩散模型和流模型的图像和视频生成的扩展性。EvoSearch将扩散模型和流模型的测试时扩展重新构想为一个演化搜索问题，利用生物演化原理高效探索和优化去噪轨迹。通过结合为随机微分方程去噪过程精心设计的选择和变异机制，EvoSearch在保持种群多样性的同时，迭代生成更高质量的后代。通过在图像和视频生成任务中对扩散模型和流模型架构进行广泛评估，我们证明了我们的方法持续优于现有方法，实现了更高的多样性，并对未见的评估指标展示了强大的泛化能力。我们的项目可在网站 https://tinnerhrhe.github.io/evosearch 上获取。",
    "keywords": [
      "EvoSearch",
      "Test-Time Scaling",
      "Image Generation",
      "Video Generation",
      "Diffusion Models"
    ],
    "area": [
      "计算机视觉",
      "生成式AI",
      "深度学习"
    ],
    "content": "随着模型预训练期间扩展计算（数据和参数）的边际成本持续显著增加，测试时扩展（TTS）作为一种通过在推理时分配额外计算来提升生成模型性能的有前景的方向应运而生。尽管TTS已在多项语言任务中取得了显著成功，但在理解图像和视频生成模型（基于扩散模型或基于流模型）的测试时扩展行为方面仍存在显著空白。尽管近期工作已开始探索视觉任务的推理时策略，但这些方法面临关键局限性：受限于特定任务领域，可扩展性差，或陷入牺牲样本多样性的奖励过度优化。在本文中，我们提出了演化搜索（EvoSearch），这是一种新颖、通用且高效的TTS方法，能够在无需额外训练或模型扩展的情况下，有效增强跨扩散模型和流模型的图像和视频生成的扩展性。EvoSearch将扩散模型和流模型的测试时扩展重新构想为一个演化搜索问题，利用生物演化原理高效探索和优化去噪轨迹。通过结合为随机微分方程去噪过程精心设计的选择和变异机制，EvoSearch在保持种群多样性的同时，迭代生成更高质量的后代。通过在图像和视频生成任务中对扩散模型和流模型架构进行广泛评估，我们证明了我们的方法持续优于现有方法，实现了更高的多样性，并对未见的评估指标展示了强大的泛化能力。我们的项目可在网站 https://tinnerhrhe.github.io/evosearch 上获取。",
    "published_time": "2025-05-23T08:25:46.000Z",
    "download_time": "2025-05-26 03:54:07",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17618.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17618",
      "arxiv_url": "https://arxiv.org/abs/2505.17618"
    }
  },
  {
    "id": "2505.17399",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17399",
    "title": "FullFront：跨越完整前端工程流程的多模态大语言模型基准测试",
    "summary": "前端工程涉及一个复杂的工作流程，工程师在此过程中构思设计、将其转化为代码并迭代优化实现。虽然最近的基准测试主要侧重于将视觉设计转化为代码，但我们提出了 FullFront，这是一个旨在评估多模态大语言模型（MLLMs）在完整前端开发流程中的表现的基准测试。FullFront 评估三个与前端工程流程直接相关的基本任务：网页设计（概念化阶段）、网页感知问答（理解视觉组织和元素）以及网页代码生成（实现阶段）。与现有的使用包含冗余代码的抓取网页或过度简化的LLM生成HTML的基准测试不同，FullFront采用了一种新颖的两阶段流程，将真实世界的网页转化为干净、标准化的HTML，同时保留多样化的视觉设计并避免版权问题。对最先进的 MLLMs 进行广泛测试揭示了其在页面感知、代码生成（尤其是在图像处理和布局方面）以及交互实现方面的显著局限性。我们的结果定量地展示了模型和任务之间的性能差异，并强调了当前 MLLM 能力与人类专家在前端工程中的表现之间存在的巨大差距。FullFront 基准测试和代码可在 https://github.com/Mikivishy/FullFront 获取。",
    "keywords": [
      "MLLMs",
      "前端工程",
      "基准测试",
      "代码生成",
      "网页感知"
    ],
    "area": [
      "人工智能",
      "多模态",
      "大模型"
    ],
    "content": "前端工程涉及一个复杂的工作流程，工程师在此过程中构思设计、将其转化为代码并迭代优化实现。虽然最近的基准测试主要侧重于将视觉设计转化为代码，但我们提出了 FullFront，这是一个旨在评估多模态大语言模型（MLLMs）在完整前端开发流程中的表现的基准测试。FullFront 评估三个与前端工程流程直接相关的基本任务：网页设计（概念化阶段）、网页感知问答（理解视觉组织和元素）以及网页代码生成（实现阶段）。与现有的使用包含冗余代码的抓取网页或过度简化的LLM生成HTML的基准测试不同，FullFront采用了一种新颖的两阶段流程，将真实世界的网页转化为干净、标准化的HTML，同时保留多样化的视觉设计并避免版权问题。对最先进的 MLLMs 进行广泛测试揭示了其在页面感知、代码生成（尤其是在图像处理和布局方面）以及交互实现方面的显著局限性。我们的结果定量地展示了模型和任务之间的性能差异，并强调了当前 MLLM 能力与人类专家在前端工程中的表现之间存在的巨大差距。FullFront 基准测试和代码可在 https://github.com/Mikivishy/FullFront 获取。",
    "published_time": "2025-05-23T02:16:11.000Z",
    "download_time": "2025-05-26 03:54:23",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17399.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17399",
      "arxiv_url": "https://arxiv.org/abs/2505.17399"
    }
  },
  {
    "id": "2505.15692",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.15692",
    "title": "思想增强策略优化：弥合外部指导与内部能力之间的鸿沟",
    "summary": "强化学习（RL）已成为一种训练推理模型的有效方法。然而，现有的强化学习方法通常会将模型的输出分布偏向于奖励最大化的路径，而未能引入外部知识。这限制了它们的探索能力，并导致相比基础模型，推理能力边界更为狭窄。为解决这一局限性，我们提出了 TAPO (Thought-Augmented Policy Optimization)，这是一种新颖的框架，通过整合外部高层指导（“思想模式”）来增强强化学习。通过在训练过程中自适应地整合结构化思想，TAPO 有效地平衡了模型内部探索与外部指导利用。广泛的实验表明，我们的方法在 AIME 上比 GRPO 显著提高了 99%，在 AMC 上提高了 41%，在 Minerva Math 上提高了 17%。值得注意的是，这些从仅 500 个先验样本中抽象出的高层思想模式，在各种任务和模型上都能有效泛化。这突显了 TAPO 在跨多个任务和领域的更广泛应用的潜力。我们的进一步分析表明，引入外部指导能够产生强大的推理模型，这些模型具有 superior explainability of inference behavior（推断行为的卓越可解释性）和 enhanced output readability（增强的输出可读性）。",
    "keywords": [
      "强化学习",
      "推理模型",
      "外部指导",
      "思想模式",
      "策略优化"
    ],
    "area": [
      "人工智能",
      "机器学习",
      "智能体"
    ],
    "content": "强化学习（RL）已成为一种训练推理模型的有效方法。然而，现有的强化学习方法通常会将模型的输出分布偏向于奖励最大化的路径，而未能引入外部知识。这限制了它们的探索能力，并导致相比基础模型，推理能力边界更为狭窄。为解决这一局限性，我们提出了 TAPO (Thought-Augmented Policy Optimization)，这是一种新颖的框架，通过整合外部高层指导（“思想模式”）来增强强化学习。通过在训练过程中自适应地整合结构化思想，TAPO 有效地平衡了模型内部探索与外部指导利用。广泛的实验表明，我们的方法在 AIME 上比 GRPO 显著提高了 99%，在 AMC 上提高了 41%，在 Minerva Math 上提高了 17%。值得注意的是，这些从仅 500 个先验样本中抽象出的高层思想模式，在各种任务和模型上都能有效泛化。这突显了 TAPO 在跨多个任务和领域的更广泛应用的潜力。我们的进一步分析表明，引入外部指导能够产生强大的推理模型，这些模型具有 superior explainability of inference behavior（推断行为的卓越可解释性）和 enhanced output readability（增强的输出可读性）。",
    "published_time": "2025-05-21T16:06:10.000Z",
    "download_time": "2025-05-26 03:54:29",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15692.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.15692",
      "arxiv_url": "https://arxiv.org/abs/2505.15692"
    }
  },
  {
    "id": "2505.17558",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17558",
    "title": "利用谎言教学：基于合成负例和课程化DPO的幻觉检测",
    "summary": "由于幻觉文本的复杂性，将大型语言模型（LLMs）对齐以准确检测幻觉仍然是一个重大挑战。认识到幻觉样本通常比传统负例表现出更高的欺骗性质量，我们将这些精心设计的幻觉作为负例用于DPO对齐过程。我们的方法融合了课程学习策略，逐步将训练从较容易的样本（基于独立事实核查模型概率得分下降幅度最大来识别）过渡到越来越难的样本。这种结构化的难度分级确保了稳定和渐进的学习。实验评估表明，我们使用课程化DPO方法和高质量负例训练的HaluCheck模型在各种指标上显著提高了模型性能，在MedHallu和HaluEval等困难基准上取得了高达24%的改进。此外，HaluCheck模型在零样本设置中表现出鲁棒性，在各种基准上显著优于更大的最先进模型。",
    "keywords": [
      "幻觉检测",
      "大型语言模型",
      "DPO",
      "课程学习",
      "合成负例"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "生成式AI"
    ],
    "content": "由于幻觉文本的复杂性，将大型语言模型（LLMs）对齐以准确检测幻觉仍然是一个重大挑战。认识到幻觉样本通常比传统负例表现出更高的欺骗性质量，我们将这些精心设计的幻觉作为负例用于DPO对齐过程。我们的方法融合了课程学习策略，逐步将训练从较容易的样本（基于独立事实核查模型概率得分下降幅度最大来识别）过渡到越来越难的样本。这种结构化的难度分级确保了稳定和渐进的学习。实验评估表明，我们使用课程化DPO方法和高质量负例训练的HaluCheck模型在各种指标上显著提高了模型性能，在MedHallu和HaluEval等困难基准上取得了高达24%的改进。此外，HaluCheck模型在零样本设置中表现出鲁棒性，在各种基准上显著优于更大的最先进模型。",
    "published_time": "2025-05-23T07:05:09.000Z",
    "download_time": "2025-05-26 03:54:46",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17558.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17558",
      "arxiv_url": "https://arxiv.org/abs/2505.17558"
    }
  },
  {
    "id": "2505.16479",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16479",
    "title": "明亮夜空在望：迈向多天气夜间图像恢复",
    "summary": "恢复受多种恶劣天气条件影响的夜间图像是一个实际但尚未充分探索的研究问题，因为多种天气条件在夜间常常与各种光照效应共存。本文首次探索了具有挑战性的多天气夜间图像恢复任务，其中各种类型的天气退化与眩光效应交织在一起。为了支持这项研究，我们贡献了 AllWeatherNight 数据集，该数据集包含大规模高质量夜间图像，具有多样化的组合退化，这些退化是利用我们提出的一种光照感知退化生成方法合成的。此外，我们提出了 ClearNight，一个统一的夜间图像恢复框架，它能有效地一步去除复杂的退化。具体而言，ClearNight 提取基于 Retinex 的双重先验，并明确引导网络分别关注不均匀的光照区域和内在纹理内容，从而增强夜间场景的恢复效果。为了更好地表示多种天气退化的共性和独特特性，我们引入了一种天气感知动态特异性-共性协作方法，该方法能识别天气退化并自适应地选择与特定天气类型相关的最优候选单元。我们的 ClearNight 在合成图像和真实世界图像上均取得了最先进的性能。全面的消融实验验证了 AllWeatherNight 数据集的必要性以及 ClearNight 的有效性。项目页面：https://henlyta.github.io/ClearNight/mainpage.html",
    "keywords": [
      "夜间图像恢复",
      "多天气退化",
      "AllWeatherNight 数据集",
      "Retinex",
      "天气感知方法"
    ],
    "area": [
      "计算机视觉",
      "深度学习",
      "人工智能"
    ],
    "content": "恢复受多种恶劣天气条件影响的夜间图像是一个实际但尚未充分探索的研究问题，因为多种天气条件在夜间常常与各种光照效应共存。本文首次探索了具有挑战性的多天气夜间图像恢复任务，其中各种类型的天气退化与眩光效应交织在一起。为了支持这项研究，我们贡献了 AllWeatherNight 数据集，该数据集包含大规模高质量夜间图像，具有多样化的组合退化，这些退化是利用我们提出的一种光照感知退化生成方法合成的。此外，我们提出了 ClearNight，一个统一的夜间图像恢复框架，它能有效地一步去除复杂的退化。具体而言，ClearNight 提取基于 Retinex 的双重先验，并明确引导网络分别关注不均匀的光照区域和内在纹理内容，从而增强夜间场景的恢复效果。为了更好地表示多种天气退化的共性和独特特性，我们引入了一种天气感知动态特异性-共性协作方法，该方法能识别天气退化并自适应地选择与特定天气类型相关的最优候选单元。我们的 ClearNight 在合成图像和真实世界图像上均取得了最先进的性能。全面的消融实验验证了 AllWeatherNight 数据集的必要性以及 ClearNight 的有效性。项目页面：https://henlyta.github.io/ClearNight/mainpage.html",
    "published_time": "2025-05-22T10:06:35.000Z",
    "download_time": "2025-05-26 03:55:02",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16479.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16479",
      "arxiv_url": "https://arxiv.org/abs/2505.16479"
    }
  },
  {
    "id": "2505.16483",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16483",
    "title": "通过合成任务和强化学习教授大语言模型保持语境忠实性",
    "summary": "教授大语言模型（LLMs）在给定上下文中保持忠实性对于构建可靠的信息检索系统至关重要。因此，我们提出了一个系统性框架 CANOE，旨在无需人工标注即可提高 LLMs 在短文本生成和长文本生成任务中的忠实性。具体而言，我们首先通过四种不同的任务合成问答（QA）短文本数据，以构建无需人工标注的高质量且易于验证的训练数据。此外，我们提出了 Dual-GRPO，这是一种基于规则的强化学习方法，该方法包含三个从合成的短文本 QA 数据中导出的定制规则奖励，同时优化短文本和长文本的响应生成。值得注意的是，Dual-GRPO 消除了手动标注偏好数据来训练奖励模型的需要，并避免了仅依赖合成的短文本 QA 数据时对短文本生成过度优化的问题。实验结果表明，CANOE 在 11 个不同的下游任务中显著提高了 LLMs 的忠实性，甚至超越了最先进的 LLMs，例如 GPT-4o 和 OpenAI o1。",
    "keywords": [
      "大语言模型 (LLMs)",
      "语境忠实性",
      "合成任务",
      "强化学习",
      "无人工标注"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "机器学习"
    ],
    "content": "教授大语言模型（LLMs）在给定上下文中保持忠实性对于构建可靠的信息检索系统至关重要。因此，我们提出了一个系统性框架 CANOE，旨在无需人工标注即可提高 LLMs 在短文本生成和长文本生成任务中的忠实性。具体而言，我们首先通过四种不同的任务合成问答（QA）短文本数据，以构建无需人工标注的高质量且易于验证的训练数据。此外，我们提出了 Dual-GRPO，这是一种基于规则的强化学习方法，该方法包含三个从合成的短文本 QA 数据中导出的定制规则奖励，同时优化短文本和长文本的响应生成。值得注意的是，Dual-GRPO 消除了手动标注偏好数据来训练奖励模型的需要，并避免了仅依赖合成的短文本 QA 数据时对短文本生成过度优化的问题。实验结果表明，CANOE 在 11 个不同的下游任务中显著提高了 LLMs 的忠实性，甚至超越了最先进的 LLMs，例如 GPT-4o 和 OpenAI o1。",
    "published_time": "2025-05-22T10:10:07.000Z",
    "download_time": "2025-05-26 03:55:14",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16483.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16483",
      "arxiv_url": "https://arxiv.org/abs/2505.16483"
    }
  },
  {
    "id": "2505.13508",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.13508",
    "title": "Time-R1：迈向大语言模型的全面时间推理",
    "summary": "大型语言模型 (LLMs) 展现了令人印象深刻的能力，但在稳健的时间智能方面存在不足，难以整合关于过去的推理与未来的预测和合理生成。同时，现有方法通常只针对孤立的时间技能，例如回答关于过去事件的问题或基本预测，并且泛化能力较差，特别是在处理超出其知识截断期的事件或需要创造性预见时。为了解决这些限制，我们引入了 Time-R1，这是第一个赋予中等规模 (3B 参数) LLM 全面时间能力——理解、预测和创造性生成——的框架。我们的方法包括一个新颖的三阶段开发路径；前两个阶段构成了一个由精心设计的动态基于规则的奖励系统驱动的强化学习 (RL) 课程。该框架逐步构建 (1) 来自历史数据的基础时间理解和逻辑事件-时间映射，(2) 超出其知识截断期的未来事件预测技能，最后 (3) 无需任何微调即可显著泛化到创造性未来场景生成。令人瞩目的是，实验表明 Time-R1 在极具挑战性的未来事件预测和创造性场景生成基准测试中，性能优于大 200 多倍的模型，包括最先进的 671B DeepSeek-R1。这项工作提供了强有力的证据，表明精心设计的渐进式强化学习微调能够使更小、更高效的模型实现卓越的时间性能，为迈向真正具有时间意识的 AI 提供了一条实用且可扩展的路径。为了促进进一步的研究，我们还发布了 Time-Bench，一个来源于 10 年新闻数据的大规模多任务时间推理数据集，以及我们的 Time-R1 系列检查点。",
    "keywords": [
      "大语言模型",
      "时间推理",
      "强化学习",
      "事件预测",
      "生成式AI"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "生成式AI"
    ],
    "content": "大型语言模型 (LLMs) 展现了令人印象深刻的能力，但在稳健的时间智能方面存在不足，难以整合关于过去的推理与未来的预测和合理生成。同时，现有方法通常只针对孤立的时间技能，例如回答关于过去事件的问题或基本预测，并且泛化能力较差，特别是在处理超出其知识截断期的事件或需要创造性预见时。为了解决这些限制，我们引入了 Time-R1，这是第一个赋予中等规模 (3B 参数) LLM 全面时间能力——理解、预测和创造性生成——的框架。我们的方法包括一个新颖的三阶段开发路径；前两个阶段构成了一个由精心设计的动态基于规则的奖励系统驱动的强化学习 (RL) 课程。该框架逐步构建 (1) 来自历史数据的基础时间理解和逻辑事件-时间映射，(2) 超出其知识截断期的未来事件预测技能，最后 (3) 无需任何微调即可显著泛化到创造性未来场景生成。令人瞩目的是，实验表明 Time-R1 在极具挑战性的未来事件预测和创造性场景生成基准测试中，性能优于大 200 多倍的模型，包括最先进的 671B DeepSeek-R1。这项工作提供了强有力的证据，表明精心设计的渐进式强化学习微调能够使更小、更高效的模型实现卓越的时间性能，为迈向真正具有时间意识的 AI 提供了一条实用且可扩展的路径。为了促进进一步的研究，我们还发布了 Time-Bench，一个来源于 10 年新闻数据的大规模多任务时间推理数据集，以及我们的 Time-R1 系列检查点。",
    "published_time": "2025-05-16T13:46:28.000Z",
    "download_time": "2025-05-26 03:55:30",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13508.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.13508",
      "arxiv_url": "https://arxiv.org/abs/2505.13508"
    }
  },
  {
    "id": "2505.17417",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17417",
    "title": "无声：面向低资源语言的无语音语音指令训练",
    "summary": "由大型语言模型（LLM）驱动的语音助手的快速发展突显了训练这些系统对语音指令数据的需求。尽管语音识别数据丰富，但语音指令数据明显稀缺，而这对于微调模型以理解和执行口头命令至关重要。生成高质量的合成语音需要一个优秀的文本到语音（TTS）模型，而这对低资源语言来说可能不可得。本文提出的新方法通过在语义表示层面停止合成，绕过了对TTS的需求，从而解决了这一挑战。我们通过将合成的语义表示与预训练的Whisper编码器对齐来实现这一点，使得LLM可以在文本指令上进行微调，同时在推理时保持理解语音指令的能力。这种简化的训练过程是为低资源语言构建语音助手的有前途的方法。",
    "keywords": [
      "语音指令训练",
      "低资源语言",
      "大语言模型 LLM",
      "语义表示",
      "Whisper 编码器"
    ],
    "area": [
      "机器学习",
      "自然语言处理",
      "大模型"
    ],
    "content": "由大型语言模型（LLM）驱动的语音助手的快速发展突显了训练这些系统对语音指令数据的需求。尽管语音识别数据丰富，但语音指令数据明显稀缺，而这对于微调模型以理解和执行口头命令至关重要。生成高质量的合成语音需要一个优秀的文本到语音（TTS）模型，而这对低资源语言来说可能不可得。本文提出的新方法通过在语义表示层面停止合成，绕过了对TTS的需求，从而解决了这一挑战。我们通过将合成的语义表示与预训练的Whisper编码器对齐来实现这一点，使得LLM可以在文本指令上进行微调，同时在推理时保持理解语音指令的能力。这种简化的训练过程是为低资源语言构建语音助手的有前途的方法。",
    "published_time": "2025-05-23T03:05:47.000Z",
    "download_time": "2025-05-26 03:55:44",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17417.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17417",
      "arxiv_url": "https://arxiv.org/abs/2505.17417"
    }
  },
  {
    "id": "2505.16770",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16770",
    "title": "RBench-V：针对具有多模态输出能力的视觉推理模型的初步评估",
    "summary": "以GPT-4o、Gemini和o3为代表的原生多模态模型和全能模型的快速发展，得益于它们处理和生成文本和图像等跨模态内容的能力，标志着智能进化的一个重要里程碑。系统评估其在视觉思维过程中的多模态输出能力（也称为多模态思维链，M-CoT）变得至关重要。然而，现有用于评估多模态模型的基准主要侧重于评估多模态输入和仅文本的推理，而忽略了通过多模态输出进行推理的重要性。在本文中，我们提出了一个名为RBench-V的基准，旨在评估模型的视觉不可或缺的推理能力。为了构建RBench-V，我们精心挑选了803个问题，涵盖数学、物理、计数和游戏。与通常指定特定输入模态的现有基准不同，RBench-V提出的问题侧重于多模态输出，这需要图像操作，例如生成新图像和构建辅助线来支持推理过程。我们在RBench-V上评估了许多开源和闭源模型，包括o3、Gemini 2.5 Pro、Qwen2.5-VL等。即使是表现最好的模型o3，在RBench-V上仅取得了25.8%的准确率，远低于人类的82.3%分数，这突显了当前模型在利用多模态推理方面仍然存在困难。数据和代码可在 https://evalmodels.github.io/rbenchv 获取。",
    "keywords": [
      "RBench-V",
      "多模态",
      "视觉推理",
      "多模态输出",
      "Benchmark"
    ],
    "area": [
      "人工智能",
      "计算机视觉",
      "多模态"
    ],
    "content": "以GPT-4o、Gemini和o3为代表的原生多模态模型和全能模型的快速发展，得益于它们处理和生成文本和图像等跨模态内容的能力，标志着智能进化的一个重要里程碑。系统评估其在视觉思维过程中的多模态输出能力（也称为多模态思维链，M-CoT）变得至关重要。然而，现有用于评估多模态模型的基准主要侧重于评估多模态输入和仅文本的推理，而忽略了通过多模态输出进行推理的重要性。在本文中，我们提出了一个名为RBench-V的基准，旨在评估模型的视觉不可或缺的推理能力。为了构建RBench-V，我们精心挑选了803个问题，涵盖数学、物理、计数和游戏。与通常指定特定输入模态的现有基准不同，RBench-V提出的问题侧重于多模态输出，这需要图像操作，例如生成新图像和构建辅助线来支持推理过程。我们在RBench-V上评估了许多开源和闭源模型，包括o3、Gemini 2.5 Pro、Qwen2.5-VL等。即使是表现最好的模型o3，在RBench-V上仅取得了25.8%的准确率，远低于人类的82.3%分数，这突显了当前模型在利用多模态推理方面仍然存在困难。数据和代码可在 https://evalmodels.github.io/rbenchv 获取。",
    "published_time": "2025-05-22T15:11:57.000Z",
    "download_time": "2025-05-26 03:56:00",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16770.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16770",
      "arxiv_url": "https://arxiv.org/abs/2505.16770"
    }
  },
  {
    "id": "2505.17826",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17826",
    "title": "Trinity-RFT：一个用于大语言模型强化微调的通用统一框架",
    "summary": "Trinity-RFT 是一个专为大语言模型强化微调（RFT）设计的通用、灵活且可扩展的框架。它采用解耦设计构建，包括：(1) 一个统一并推广同步/异步、同策略/离策略以及在线/离线RFT模式的RFT核心；(2) 与智能体-环境交互进行高效、鲁棒无缝集成；以及 (3) 针对RFT优化的系统化数据管线。Trinity-RFT 可以轻松适应各种应用场景，并作为一个统一平台来探索先进的强化学习范式。本技术报告概述了 Trinity-RFT 的愿景、特性、设计和实现，并附有大量示例，展示了所提出框架的实用性和用户友好性。",
    "keywords": [
      "Large Language Models",
      "Reinforcement Fine-Tuning",
      "Trinity-RFT",
      "Reinforcement Learning",
      "Framework"
    ],
    "area": [
      "大模型",
      "机器学习",
      "自然语言处理"
    ],
    "content": "Trinity-RFT 是一个专为大语言模型强化微调（RFT）设计的通用、灵活且可扩展的框架。它采用解耦设计构建，包括：(1) 一个统一并推广同步/异步、同策略/离策略以及在线/离线RFT模式的RFT核心；(2) 与智能体-环境交互进行高效、鲁棒无缝集成；以及 (3) 针对RFT优化的系统化数据管线。Trinity-RFT 可以轻松适应各种应用场景，并作为一个统一平台来探索先进的强化学习范式。本技术报告概述了 Trinity-RFT 的愿景、特性、设计和实现，并附有大量示例，展示了所提出框架的实用性和用户友好性。",
    "published_time": "2025-05-23T12:41:09.000Z",
    "download_time": "2025-05-26 03:56:14",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17826.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17826",
      "arxiv_url": "https://arxiv.org/abs/2505.17826"
    }
  },
  {
    "id": "2505.17412",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17412",
    "title": "Direct3D-S2：基于空间稀疏注意力的十亿规模三维生成简易框架",
    "summary": "使用体素表示（如符号距离函数）生成高分辨率三维形状面临巨大的计算和内存挑战。我们引入了 Direct3D S2，这是一个基于稀疏体素的可扩展三维生成框架，它以显著降低的训练成本实现了卓越的输出质量。我们的关键创新是空间稀疏注意力（Spatial Sparse Attention）机制，该机制极大地提高了 Diffusion Transformer 在稀疏体素数据上计算的效率。SSA 使得模型能够有效处理稀疏体素内的大量 token，显著降低了计算开销，并在前向传播中实现了 3.9 倍的加速，在后向传播中实现了 9.6 倍的加速。我们的框架还包括一个变分自编码器，它在输入、潜在空间和输出阶段保持一致的稀疏体素格式。与先前在三维 VAE 中使用异构表示的方法相比，这种统一的设计显著提高了训练效率和稳定性。我们的模型在公开数据集上进行训练，实验表明 Direct3D S2 不仅在生成质量和效率上超越了现有最佳方法，还仅使用 8 个 GPU 即可在 1024 分辨率下进行训练，而以往使用体素表示的方法在 256 分辨率下通常需要至少 32 个 GPU，从而使得十亿规模三维生成既实用又易于实现。项目页面：https://nju3dv.github.io/projects/Direct3D-S2/。",
    "keywords": [
      "3D生成",
      "稀疏体素",
      "Spatial Sparse Attention",
      "Direct3D-S2",
      "高分辨率"
    ],
    "area": [
      "生成式AI",
      "深度学习",
      "计算机视觉"
    ],
    "content": "使用体素表示（如符号距离函数）生成高分辨率三维形状面临巨大的计算和内存挑战。我们引入了 Direct3D S2，这是一个基于稀疏体素的可扩展三维生成框架，它以显著降低的训练成本实现了卓越的输出质量。我们的关键创新是空间稀疏注意力（Spatial Sparse Attention）机制，该机制极大地提高了 Diffusion Transformer 在稀疏体素数据上计算的效率。SSA 使得模型能够有效处理稀疏体素内的大量 token，显著降低了计算开销，并在前向传播中实现了 3.9 倍的加速，在后向传播中实现了 9.6 倍的加速。我们的框架还包括一个变分自编码器，它在输入、潜在空间和输出阶段保持一致的稀疏体素格式。与先前在三维 VAE 中使用异构表示的方法相比，这种统一的设计显著提高了训练效率和稳定性。我们的模型在公开数据集上进行训练，实验表明 Direct3D S2 不仅在生成质量和效率上超越了现有最佳方法，还仅使用 8 个 GPU 即可在 1024 分辨率下进行训练，而以往使用体素表示的方法在 256 分辨率下通常需要至少 32 个 GPU，从而使得十亿规模三维生成既实用又易于实现。项目页面：https://nju3dv.github.io/projects/Direct3D-S2/。",
    "published_time": "2025-05-23T02:58:01.000Z",
    "download_time": "2025-05-26 03:56:36",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17412.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17412",
      "arxiv_url": "https://arxiv.org/abs/2505.17412"
    }
  },
  {
    "id": "2505.15389",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.15389",
    "title": "视觉-语言模型在实际应用中安全吗？一项基于Meme的基准研究",
    "summary": "视觉-语言模型（VLM）的快速部署放大了安全风险，但大多数评估依赖于人工合成图像。本研究提出问题：当面对普通用户分享的Meme图像时，当前的VLM有多安全？为了研究这个问题，我们引入了MemeSafetyBench，一个包含50,430个实例的基准测试集，将真实的Meme图像与有害和良性指令配对。利用全面的安全分类法和基于大型语言模型的指令生成，我们评估了在单轮和多轮交互中的多个VLM。我们研究了真实的Meme如何影响有害输出、对话上下文的缓解效果以及模型规模与安全指标之间的关系。我们的研究结果表明，与合成或排版图像相比，VLM对基于Meme的有害提示显示出更大的脆弱性。相较于纯文本输入，Meme显著增加了有害响应并降低了拒绝率。尽管多轮交互提供了部分缓解，但提高的脆弱性仍然存在。这些结果强调了进行生态学有效评估和加强安全机制的必要性。",
    "keywords": [
      "视觉-语言模型",
      "安全性",
      "Meme图像",
      "基准测试",
      "有害输出"
    ],
    "area": [
      "多模态",
      "计算机视觉",
      "自然语言处理"
    ],
    "content": "视觉-语言模型（VLM）的快速部署放大了安全风险，但大多数评估依赖于人工合成图像。本研究提出问题：当面对普通用户分享的Meme图像时，当前的VLM有多安全？为了研究这个问题，我们引入了MemeSafetyBench，一个包含50,430个实例的基准测试集，将真实的Meme图像与有害和良性指令配对。利用全面的安全分类法和基于大型语言模型的指令生成，我们评估了在单轮和多轮交互中的多个VLM。我们研究了真实的Meme如何影响有害输出、对话上下文的缓解效果以及模型规模与安全指标之间的关系。我们的研究结果表明，与合成或排版图像相比，VLM对基于Meme的有害提示显示出更大的脆弱性。相较于纯文本输入，Meme显著增加了有害响应并降低了拒绝率。尽管多轮交互提供了部分缓解，但提高的脆弱性仍然存在。这些结果强调了进行生态学有效评估和加强安全机制的必要性。",
    "published_time": "2025-05-21T11:26:40.000Z",
    "download_time": "2025-05-26 03:56:51",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15389.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.15389",
      "arxiv_url": "https://arxiv.org/abs/2505.15389"
    }
  },
  {
    "id": "2505.14146",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.14146",
    "title": "s3：无需大量数据即可通过强化学习训练搜索智能体",
    "summary": "检索增强生成（RAG）系统使大型语言模型（LLMs）能够在推理期间访问外部知识。最近的进展使得LLMs能够通过强化学习（RL）充当搜索智能体，通过与检索引擎的多轮交互来改善信息获取。然而，现有方法要么使用忽略下游效用的纯搜索指标（如NDCG）来优化检索，要么微调整个LLM以联合进行推理和检索——将检索与生成纠缠在一起，限制了实际的搜索效用以及与固定或专有模型的兼容性。在这项工作中，我们提出了s3，一个轻量级、模型无关的框架，它将搜索器与生成器解耦，并使用一种“超越RAG收益”的奖励来训练搜索器：即相对于朴素RAG在生成准确性上的提升。s3仅需要2.4k训练样本即可超越使用多达70倍以上数据训练的基线，在六个通用问答和五个医学问答基准测试上持续提供了更强的下游性能。",
    "keywords": [
      "RAG",
      "LLM",
      "强化学习",
      "搜索智能体",
      "数据效率"
    ],
    "area": [
      "自然语言处理",
      "大模型",
      "智能体"
    ],
    "content": "检索增强生成（RAG）系统使大型语言模型（LLMs）能够在推理期间访问外部知识。最近的进展使得LLMs能够通过强化学习（RL）充当搜索智能体，通过与检索引擎的多轮交互来改善信息获取。然而，现有方法要么使用忽略下游效用的纯搜索指标（如NDCG）来优化检索，要么微调整个LLM以联合进行推理和检索——将检索与生成纠缠在一起，限制了实际的搜索效用以及与固定或专有模型的兼容性。在这项工作中，我们提出了s3，一个轻量级、模型无关的框架，它将搜索器与生成器解耦，并使用一种“超越RAG收益”的奖励来训练搜索器：即相对于朴素RAG在生成准确性上的提升。s3仅需要2.4k训练样本即可超越使用多达70倍以上数据训练的基线，在六个通用问答和五个医学问答基准测试上持续提供了更强的下游性能。",
    "published_time": "2025-05-20T09:53:56.000Z",
    "download_time": "2025-05-26 03:57:11",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14146.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.14146",
      "arxiv_url": "https://arxiv.org/abs/2505.14146"
    }
  },
  {
    "id": "2505.16270",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16270",
    "title": "Transformer Copilot：从大语言模型微调的错误日志中学习",
    "summary": "大语言模型通常通过在特定领域数据上进行监督微调来适应下游任务。标准微调侧重于最小化生成损失以优化模型参数，我们则通过保留和利用模型自身的学习信号迈出更深入的一步，这类似于人类学习者如何通过反思过去的错误来改进未来的表现。我们首先引入错误日志（Mistake Log）的概念，用于系统地跟踪模型在整个微调过程中的学习行为和重复出现的错误。将原始基于Transformer的模型视为Pilot，我们相应地设计了一个Copilot模型，通过对数修正（logits rectification）来提升Pilot的推理性能。我们将整体的Pilot-Copilot框架命名为Transformer Copilot，该框架引入了(i)新颖的Copilot模型设计，(ii)一种协同训练范式，其中Copilot与Pilot一起从不断演变的错误日志中持续学习，以及(iii)一种融合推理范式，其中Copilot修正Pilot的对数以增强生成能力。我们对这一新的学习框架进行了理论和实证分析。在涵盖常识、算术和推荐任务的12个基准测试上的实验表明，Transformer Copilot持续提升性能高达34.5%，同时对Pilot模型引入的计算开销极小，并表现出强大的可扩展性和可迁移性。",
    "keywords": [
      "LLM微调",
      "错误日志",
      "Copilot模型",
      "对数修正",
      "Transformer Copilot"
    ],
    "area": [
      "大模型",
      "深度学习",
      "自然语言处理"
    ],
    "content": "大语言模型通常通过在特定领域数据上进行监督微调来适应下游任务。标准微调侧重于最小化生成损失以优化模型参数，我们则通过保留和利用模型自身的学习信号迈出更深入的一步，这类似于人类学习者如何通过反思过去的错误来改进未来的表现。我们首先引入错误日志（Mistake Log）的概念，用于系统地跟踪模型在整个微调过程中的学习行为和重复出现的错误。将原始基于Transformer的模型视为Pilot，我们相应地设计了一个Copilot模型，通过对数修正（logits rectification）来提升Pilot的推理性能。我们将整体的Pilot-Copilot框架命名为Transformer Copilot，该框架引入了(i)新颖的Copilot模型设计，(ii)一种协同训练范式，其中Copilot与Pilot一起从不断演变的错误日志中持续学习，以及(iii)一种融合推理范式，其中Copilot修正Pilot的对数以增强生成能力。我们对这一新的学习框架进行了理论和实证分析。在涵盖常识、算术和推荐任务的12个基准测试上的实验表明，Transformer Copilot持续提升性能高达34.5%，同时对Pilot模型引入的计算开销极小，并表现出强大的可扩展性和可迁移性。",
    "published_time": "2025-05-22T06:00:45.000Z",
    "download_time": "2025-05-26 03:57:24",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16270.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16270",
      "arxiv_url": "https://arxiv.org/abs/2505.16270"
    }
  },
  {
    "id": "2505.17091",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17091",
    "title": "大语言模型仅通过阅读即可隐式习得视听能力",
    "summary": "本文提出了一个引人入胜的发现：通过在文本tokens上训练一个自回归大语言模型，该文本模型内在地习得了理解图像和音频的能力，从而仅通过阅读就发展出视听能力。流行的视听大语言模型通常微调文本大语言模型，以在给定图像和音频嵌入的情况下输出文本。与此不同，我们的架构接收图像块、音频波形或tokens作为输入，并输出典型的分类任务中的嵌入或类别标签。我们展示了文本权重在辅助FSD-50K和GTZAN等数据集音频分类中的通用性。此外，我们还展示了其在CIFAR-10和Fashion-MNIST数据集以及图像块上的图像分类效果。这进一步支持了文本大语言模型学习到强大内部电路的观点，这些电路可以通过激活必要连接用于各种应用，而无需每次都从头训练模型。",
    "keywords": [
      "大语言模型",
      "多模态",
      "文本预训练",
      "视听习得",
      "知识迁移"
    ],
    "area": [
      "大模型",
      "多模态",
      "自然语言处理"
    ],
    "content": "本文提出了一个引人入胜的发现：通过在文本tokens上训练一个自回归大语言模型，该文本模型内在地习得了理解图像和音频的能力，从而仅通过阅读就发展出视听能力。流行的视听大语言模型通常微调文本大语言模型，以在给定图像和音频嵌入的情况下输出文本。与此不同，我们的架构接收图像块、音频波形或tokens作为输入，并输出典型的分类任务中的嵌入或类别标签。我们展示了文本权重在辅助FSD-50K和GTZAN等数据集音频分类中的通用性。此外，我们还展示了其在CIFAR-10和Fashion-MNIST数据集以及图像块上的图像分类效果。这进一步支持了文本大语言模型学习到强大内部电路的观点，这些电路可以通过激活必要连接用于各种应用，而无需每次都从头训练模型。",
    "published_time": "2025-05-20T22:20:16.000Z",
    "download_time": "2025-05-26 03:57:42",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17091.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17091",
      "arxiv_url": "https://arxiv.org/abs/2505.17091"
    }
  },
  {
    "id": "2505.17508",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17508",
    "title": "关于用于大语言模型的KL正则化策略梯度算法的设计",
    "summary": "策略梯度算法已成功应用于增强大语言模型（LLM）的推理能力。尽管在策略梯度算法中KL散度（KL）正则化被广泛用于稳定训练，但如何系统性地探索不同KL散度公式的估计方法并将其集成到在线强化学习（RL）的替代损失函数中，仍是一个细致且值得系统性探索的设计空间。在本文中，我们提出了正则化策略梯度（RPG），这是一个用于在线RL环境中推导和分析KL正则化策略梯度方法的系统性框架。我们针对由前向和后向KL散度正则化的目标，推导了策略梯度及其对应的替代损失函数，并考虑了归一化和非归一化的策略分布。此外，我们提供了全可微损失函数以及REINFORCE风格的梯度估计器的推导过程，以适应不同的算法需求。我们使用这些方法在用于LLM推理的RL任务上进行了大量实验，结果表明与GRPO、REINFORCE++和DAPO等强基线相比，这些方法在训练稳定性和性能方面取得了改进或具有竞争力。相关代码已在 https://github.com/complex-reasoning/RPG 公开。",
    "keywords": [
      "大语言模型 (LLM)",
      "策略梯度 (Policy Gradient)",
      "KL正则化",
      "强化学习 (RL)",
      "替代损失函数"
    ],
    "area": [
      "大模型",
      "机器学习",
      "自然语言处理"
    ],
    "content": "策略梯度算法已成功应用于增强大语言模型（LLM）的推理能力。尽管在策略梯度算法中KL散度（KL）正则化被广泛用于稳定训练，但如何系统性地探索不同KL散度公式的估计方法并将其集成到在线强化学习（RL）的替代损失函数中，仍是一个细致且值得系统性探索的设计空间。在本文中，我们提出了正则化策略梯度（RPG），这是一个用于在线RL环境中推导和分析KL正则化策略梯度方法的系统性框架。我们针对由前向和后向KL散度正则化的目标，推导了策略梯度及其对应的替代损失函数，并考虑了归一化和非归一化的策略分布。此外，我们提供了全可微损失函数以及REINFORCE风格的梯度估计器的推导过程，以适应不同的算法需求。我们使用这些方法在用于LLM推理的RL任务上进行了大量实验，结果表明与GRPO、REINFORCE++和DAPO等强基线相比，这些方法在训练稳定性和性能方面取得了改进或具有竞争力。相关代码已在 https://github.com/complex-reasoning/RPG 公开。",
    "published_time": "2025-05-23T06:01:21.000Z",
    "download_time": "2025-05-26 03:57:57",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17508.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17508",
      "arxiv_url": "https://arxiv.org/abs/2505.17508"
    }
  },
  {
    "id": "2505.17063",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17063",
    "title": "合成数据强化学习：任务定义足矣",
    "summary": "强化学习（RL）是使基础模型适应专门任务的强大方法，但其对大规模人工标注数据的依赖限制了其广泛应用。我们引入了合成数据强化学习（Synthetic Data RL），这是一个简单且通用的框架，它仅使用从任务定义生成的合成数据对模型进行强化微调。我们的方法首先从任务定义和检索到的文档生成问答对，然后根据模型的求解能力调整问题难度，并利用模型在样本上的平均通过率选择用于RL训练的问题。在Qwen-2.5-7B模型上，我们的方法在GSM8K上比基础模型绝对提高了29.2%（相对于指令微调模型提高了2.9 pp，相对于Self-Instruct提高了6.6 pp），在MATH上提高了8.7%，在GPQA上提高了13.1%（相对于SynthLLM提高了7.0 pp），在MedQA上提高了8.9%，在CQA（法律）上提高了17.7%，在CFA（金融）上提高了13.7%。在相同数据预算下，它超越了有监督微调，并且在各数据集上（例如GSM8K上提高17.2 pp）几乎达到了使用全部人工数据进行强化学习的性能。增加100条人工示例只将GSM8K的性能提高了0.4 pp，表明其附加价值有限。通过减少人工数据标注，合成数据强化学习实现了可扩展且高效的基于强化学习的模型适应。代码和演示可在 https://github.com/gydpku/Data_Synthesis_RL/ 获取。",
    "keywords": [
      "强化学习",
      "合成数据",
      "大模型",
      "任务定义",
      "模型适应"
    ],
    "area": [
      "机器学习",
      "自然语言处理",
      "大模型"
    ],
    "content": "强化学习（RL）是使基础模型适应专门任务的强大方法，但其对大规模人工标注数据的依赖限制了其广泛应用。我们引入了合成数据强化学习（Synthetic Data RL），这是一个简单且通用的框架，它仅使用从任务定义生成的合成数据对模型进行强化微调。我们的方法首先从任务定义和检索到的文档生成问答对，然后根据模型的求解能力调整问题难度，并利用模型在样本上的平均通过率选择用于RL训练的问题。在Qwen-2.5-7B模型上，我们的方法在GSM8K上比基础模型绝对提高了29.2%（相对于指令微调模型提高了2.9 pp，相对于Self-Instruct提高了6.6 pp），在MATH上提高了8.7%，在GPQA上提高了13.1%（相对于SynthLLM提高了7.0 pp），在MedQA上提高了8.9%，在CQA（法律）上提高了17.7%，在CFA（金融）上提高了13.7%。在相同数据预算下，它超越了有监督微调，并且在各数据集上（例如GSM8K上提高17.2 pp）几乎达到了使用全部人工数据进行强化学习的性能。增加100条人工示例只将GSM8K的性能提高了0.4 pp，表明其附加价值有限。通过减少人工数据标注，合成数据强化学习实现了可扩展且高效的基于强化学习的模型适应。代码和演示可在 https://github.com/gydpku/Data_Synthesis_RL/ 获取。",
    "published_time": "2025-05-18T05:35:13.000Z",
    "download_time": "2025-05-26 03:58:18",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17063.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17063",
      "arxiv_url": "https://arxiv.org/abs/2505.17063"
    }
  },
  {
    "id": "2505.17540",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17540",
    "title": "RePrompt：基于强化学习的文生图推理增强式提示词重塑",
    "summary": "尽管文本到图像（T2I）生成取得了最新进展，但现有模型在从简短和不充分指定的提示词中忠实捕捉用户意图方面常常遇到困难。虽然先前的工作曾尝试使用大型语言模型（LLM）增强提示词，但由于缺乏视觉语义和真实世界构图的充分基础，这些方法经常生成风格化或不逼真的内容。受语言模型推理最新进展的启发，我们提出了 RePrompt，这是一个新颖的提示词重塑框架，通过强化学习将显式推理引入到提示词增强过程中。我们的方法不依赖于手工规则或风格化重写，而是训练一个语言模型，通过优化图像层面的结果来生成结构化、自反思的提示词。量身定制的奖励模型从人类偏好、语义对齐和视觉构图方面评估生成的图像，为提示词生成提供间接监督。我们的方法无需人工标注数据即可实现端到端训练。在 GenEval 和 T2I-Compbench 上的实验表明，RePrompt 显著提高了不同 T2I 主干模型的空间布局保真度和构图泛化能力，建立了新的最先进结果。",
    "keywords": [
      "RePrompt",
      "文生图",
      "强化学习",
      "推理增强",
      "提示词重塑"
    ],
    "area": [
      "生成式AI",
      "机器学习",
      "多模态"
    ],
    "content": "尽管文本到图像（T2I）生成取得了最新进展，但现有模型在从简短和不充分指定的提示词中忠实捕捉用户意图方面常常遇到困难。虽然先前的工作曾尝试使用大型语言模型（LLM）增强提示词，但由于缺乏视觉语义和真实世界构图的充分基础，这些方法经常生成风格化或不逼真的内容。受语言模型推理最新进展的启发，我们提出了 RePrompt，这是一个新颖的提示词重塑框架，通过强化学习将显式推理引入到提示词增强过程中。我们的方法不依赖于手工规则或风格化重写，而是训练一个语言模型，通过优化图像层面的结果来生成结构化、自反思的提示词。量身定制的奖励模型从人类偏好、语义对齐和视觉构图方面评估生成的图像，为提示词生成提供间接监督。我们的方法无需人工标注数据即可实现端到端训练。在 GenEval 和 T2I-Compbench 上的实验表明，RePrompt 显著提高了不同 T2I 主干模型的空间布局保真度和构图泛化能力，建立了新的最先进结果。",
    "published_time": "2025-05-23T06:44:26.000Z",
    "download_time": "2025-05-26 03:58:31",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17540.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17540",
      "arxiv_url": "https://arxiv.org/abs/2505.17540"
    }
  },
  {
    "id": "2505.17016",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.17016",
    "title": "视觉-语言-动作模型的交互式后训练",
    "summary": "我们引入了 RIPT-VLA，这是一种基于强化学习的简单且可扩展的交互式后训练范式，它仅使用稀疏的二元成功奖励来微调预训练的视觉-语言-动作 (VLA) 模型。现有的 VLA 训练流程严重依赖离线专家演示数据和监督模仿，这限制了它们在数据稀缺条件下适应新任务和新环境的能力。RIPT-VLA 通过基于动态 rollout 采样和留一法优势估计的稳定策略优化算法实现交互式后训练来解决这个问题。\nRIPT-VLA 具有以下特点。首先，它适用于各种 VLA 模型，使轻量级 QueST 模型提高了 21.2%，并使 7B OpenVLA-OFT 模型的成功率达到了前所未有的 97.5%。其次，它计算高效且数据高效：仅利用一次演示，RIPT-VLA 就能使一个不可用的 SFT 模型（成功率 4%）在 15 次迭代内达到 97% 的成功率。此外，我们证明了 RIPT-VLA 学习到的策略可以泛化到不同的任务和场景，并且对初始状态上下文具有鲁棒性。这些结果表明 RIPT-VLA 是一种通过最少监督对 VLA 模型进行后训练的实用且有效的范式。",
    "keywords": [
      "RIPT-VLA",
      "VLA Models",
      "Interactive Post-Training",
      "Reinforcement Learning",
      "Data Efficiency"
    ],
    "area": [
      "多模态",
      "强化学习",
      "智能体"
    ],
    "content": "我们引入了 RIPT-VLA，这是一种基于强化学习的简单且可扩展的交互式后训练范式，它仅使用稀疏的二元成功奖励来微调预训练的视觉-语言-动作 (VLA) 模型。现有的 VLA 训练流程严重依赖离线专家演示数据和监督模仿，这限制了它们在数据稀缺条件下适应新任务和新环境的能力。RIPT-VLA 通过基于动态 rollout 采样和留一法优势估计的稳定策略优化算法实现交互式后训练来解决这个问题。\nRIPT-VLA 具有以下特点。首先，它适用于各种 VLA 模型，使轻量级 QueST 模型提高了 21.2%，并使 7B OpenVLA-OFT 模型的成功率达到了前所未有的 97.5%。其次，它计算高效且数据高效：仅利用一次演示，RIPT-VLA 就能使一个不可用的 SFT 模型（成功率 4%）在 15 次迭代内达到 97% 的成功率。此外，我们证明了 RIPT-VLA 学习到的策略可以泛化到不同的任务和场景，并且对初始状态上下文具有鲁棒性。这些结果表明 RIPT-VLA 是一种通过最少监督对 VLA 模型进行后训练的实用且有效的范式。",
    "published_time": "2025-05-22T17:59:45.000Z",
    "download_time": "2025-05-26 03:58:45",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17016.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.17016",
      "arxiv_url": "https://arxiv.org/abs/2505.17016"
    }
  },
  {
    "id": "2505.16293",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16293",
    "title": "通过动态笔记生成增强大型语言模型在复杂问答中的推理能力",
    "summary": "迭代检索增强生成（RAG）在多跳问答中面临冗长上下文和无关信息累积的挑战，这阻碍了模型处理检索内容和对其进行推理的能力，并限制了性能。尽管近期方法侧重于压缩检索到的信息，但它们要么局限于单轮RAG，要么需要微调，要么在迭代RAG中缺乏可扩展性。为了应对这些挑战，我们提出了一种名为Notes Writing（笔记生成）的方法，该方法在每一步从检索到的文档中生成简洁相关的笔记，从而减少噪声并仅保留必要信息。这间接增加了大型语言模型（LLMs）的有效上下文长度，使其在处理大量输入文本时能够更有效地推理和规划。Notes Writing 方法与框架无关，可以与不同的迭代RAG方法集成。我们在三种迭代RAG方法、两种模型和四个评估数据集上验证了其有效性。Notes writing 总体平均提升了 15.6 个百分点，同时输出 token 增加量极小。",
    "keywords": [
      "LLM",
      "Iterative RAG",
      "Notes Writing",
      "复杂问答",
      "推理"
    ],
    "area": [
      "人工智能",
      "自然语言处理",
      "大模型"
    ],
    "content": "迭代检索增强生成（RAG）在多跳问答中面临冗长上下文和无关信息累积的挑战，这阻碍了模型处理检索内容和对其进行推理的能力，并限制了性能。尽管近期方法侧重于压缩检索到的信息，但它们要么局限于单轮RAG，要么需要微调，要么在迭代RAG中缺乏可扩展性。为了应对这些挑战，我们提出了一种名为Notes Writing（笔记生成）的方法，该方法在每一步从检索到的文档中生成简洁相关的笔记，从而减少噪声并仅保留必要信息。这间接增加了大型语言模型（LLMs）的有效上下文长度，使其在处理大量输入文本时能够更有效地推理和规划。Notes Writing 方法与框架无关，可以与不同的迭代RAG方法集成。我们在三种迭代RAG方法、两种模型和四个评估数据集上验证了其有效性。Notes writing 总体平均提升了 15.6 个百分点，同时输出 token 增加量极小。",
    "published_time": "2025-05-22T06:45:05.000Z",
    "download_time": "2025-05-26 03:58:56",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16293.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16293",
      "arxiv_url": "https://arxiv.org/abs/2505.16293"
    }
  },
  {
    "id": "2505.16022",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16022",
    "title": "NOVER：基于无验证器的语言模型激励训练",
    "summary": "DeepSeek R1-Zero 等最新进展突出显示了激励训练的有效性，这是一种强化学习范式，该范式仅基于语言模型输出的最终答案部分计算奖励，从而鼓励生成中间推理步骤。然而，这些方法从根本上依赖于外部验证器，这限制了它们在容易获得此类验证器的领域（如数学和编程）的应用。尽管奖励模型可以充当验证器，但它们需要高质量的标注数据并且训练成本高昂。在这项工作中，我们提出了 NOVER，即无验证强化学习（NO-VERifier Reinforcement Learning），这是一个通用的强化学习框架，它仅需要标准的监督微调数据，无需外部验证器。NOVER 能够在广泛的文本到文本任务中实现激励训练，并且性能比从 DeepSeek R1 671B 等大型推理模型蒸馏的同等规模模型高出 7.7%。此外，NOVER 的灵活性为优化大型语言模型带来了新的可能性，例如逆向激励训练。",
    "keywords": [
      "强化学习",
      "大型语言模型",
      "激励训练",
      "无验证器",
      "监督微调"
    ],
    "area": [
      "自然语言处理",
      "大模型",
      "强化学习"
    ],
    "content": "DeepSeek R1-Zero 等最新进展突出显示了激励训练的有效性，这是一种强化学习范式，该范式仅基于语言模型输出的最终答案部分计算奖励，从而鼓励生成中间推理步骤。然而，这些方法从根本上依赖于外部验证器，这限制了它们在容易获得此类验证器的领域（如数学和编程）的应用。尽管奖励模型可以充当验证器，但它们需要高质量的标注数据并且训练成本高昂。在这项工作中，我们提出了 NOVER，即无验证强化学习（NO-VERifier Reinforcement Learning），这是一个通用的强化学习框架，它仅需要标准的监督微调数据，无需外部验证器。NOVER 能够在广泛的文本到文本任务中实现激励训练，并且性能比从 DeepSeek R1 671B 等大型推理模型蒸馏的同等规模模型高出 7.7%。此外，NOVER 的灵活性为优化大型语言模型带来了新的可能性，例如逆向激励训练。",
    "published_time": "2025-05-21T21:12:35.000Z",
    "download_time": "2025-05-26 03:59:13",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16022.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16022",
      "arxiv_url": "https://arxiv.org/abs/2505.16022"
    }
  },
  {
    "id": "2505.15805",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.15805",
    "title": "保护安全性！大型语言模型在对抗间接攻击下上下文安全策略遵循能力的基准测试（面向问答）",
    "summary": "随着大型语言模型（LLM）越来越多地部署在企业和政府等敏感领域，确保它们在上下文中遵循用户定义的安全策略至关重要，尤其是在信息保密方面。虽然先前的LLM研究侧重于一般安全性和社会敏感数据，但针对攻击的上下文安全性保持能力的大规模基准测试仍然缺乏。为了解决这个问题，我们引入了一个新颖的大规模基准数据集CoPriva，用于评估LLM在问答中对上下文非泄露策略的遵循能力。我们的数据集源自现实场景，包含明确的策略和被设计为直接或具有挑战性间接攻击的查询，旨在获取被禁止的信息。我们在基准测试中评估了10个LLM模型，揭示了一个显著的漏洞：许多模型违反了用户定义的策略并泄露了敏感信息。这种失败在对抗间接攻击时尤为严重，突显了当前LLM安全对齐在敏感应用中存在的关键差距。我们的分析表明，虽然模型通常可以识别查询的正确答案，但在生成过程中难以整合策略约束。相比之下，当被明确提示时，它们表现出部分修改输出的能力。我们的发现强调迫切需要更强大的方法来保证上下文安全性。",
    "keywords": [
      "大型语言模型",
      "安全策略遵循",
      "间接攻击",
      "上下文安全",
      "基准测试"
    ],
    "area": [
      "人工智能",
      "大模型",
      "自然语言处理"
    ],
    "content": "随着大型语言模型（LLM）越来越多地部署在企业和政府等敏感领域，确保它们在上下文中遵循用户定义的安全策略至关重要，尤其是在信息保密方面。虽然先前的LLM研究侧重于一般安全性和社会敏感数据，但针对攻击的上下文安全性保持能力的大规模基准测试仍然缺乏。为了解决这个问题，我们引入了一个新颖的大规模基准数据集CoPriva，用于评估LLM在问答中对上下文非泄露策略的遵循能力。我们的数据集源自现实场景，包含明确的策略和被设计为直接或具有挑战性间接攻击的查询，旨在获取被禁止的信息。我们在基准测试中评估了10个LLM模型，揭示了一个显著的漏洞：许多模型违反了用户定义的策略并泄露了敏感信息。这种失败在对抗间接攻击时尤为严重，突显了当前LLM安全对齐在敏感应用中存在的关键差距。我们的分析表明，虽然模型通常可以识别查询的正确答案，但在生成过程中难以整合策略约束。相比之下，当被明确提示时，它们表现出部分修改输出的能力。我们的发现强调迫切需要更强大的方法来保证上下文安全性。",
    "published_time": "2025-05-21T17:58:11.000Z",
    "download_time": "2025-05-26 03:59:31",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15805.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.15805",
      "arxiv_url": "https://arxiv.org/abs/2505.15805"
    }
  },
  {
    "id": "2505.12891",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.12891",
    "title": "TIME：面向真实场景下大型语言模型时间推理的多层次基准",
    "summary": "时间推理对于大型语言模型 (LLMs) 理解真实世界至关重要。然而，现有工作忽视了时间推理在真实世界中面临的挑战：(1) 海量时间信息，(2) 快速变化的事件动态，以及 (3) 社交互动中复杂的时间依赖。为了弥合这一差距，我们提出了一个多层次的基准 TIME，专为真实场景下的时间推理而设计。TIME 包含 38,522 个问答对，涵盖 3 个层次和 11 个细粒度子任务。该基准包含 3 个子数据集，分别反映了不同的真实世界挑战：TIME-Wiki、TIME-News 和 TIME-Dial。我们对推理模型和非推理模型进行了大量实验。并对不同真实世界场景和任务下的时间推理性能进行了深入分析，总结了测试时缩放对时间推理能力的影响。此外，我们发布了 TIME-Lite，一个人类标注的子集，以促进未来在时间推理领域的研究和标准化评估。代码可在 https://github.com/sylvain-wei/TIME 获取，数据集可在 https://huggingface.co/datasets/SylvainWei/TIME 获取。",
    "keywords": [
      "LLMs",
      "时间推理",
      "Benchmark",
      "真实场景",
      "多层次"
    ],
    "area": [
      "人工智能",
      "自然语言处理",
      "大模型"
    ],
    "content": "时间推理对于大型语言模型 (LLMs) 理解真实世界至关重要。然而，现有工作忽视了时间推理在真实世界中面临的挑战：(1) 海量时间信息，(2) 快速变化的事件动态，以及 (3) 社交互动中复杂的时间依赖。为了弥合这一差距，我们提出了一个多层次的基准 TIME，专为真实场景下的时间推理而设计。TIME 包含 38,522 个问答对，涵盖 3 个层次和 11 个细粒度子任务。该基准包含 3 个子数据集，分别反映了不同的真实世界挑战：TIME-Wiki、TIME-News 和 TIME-Dial。我们对推理模型和非推理模型进行了大量实验。并对不同真实世界场景和任务下的时间推理性能进行了深入分析，总结了测试时缩放对时间推理能力的影响。此外，我们发布了 TIME-Lite，一个人类标注的子集，以促进未来在时间推理领域的研究和标准化评估。代码可在 https://github.com/sylvain-wei/TIME 获取，数据集可在 https://huggingface.co/datasets/SylvainWei/TIME 获取。",
    "published_time": "2025-05-19T09:22:02.000Z",
    "download_time": "2025-05-26 03:59:40",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12891.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.12891",
      "arxiv_url": "https://arxiv.org/abs/2505.12891"
    }
  },
  {
    "id": "2505.16056",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.16056",
    "title": "并非所有模型都适合专家卸载：关于混合专家模型的局部路由一致性研究",
    "summary": "混合专家模型（MoE）通过在推理过程中稀疏激活专家，实现了大型语言模型（LLMs）的有效扩展。为了在内存受限的设备上有效部署大型MoE模型，许多系统引入了*专家卸载*机制，将一部分专家缓存在高速内存中，而将其他专家留在慢速内存中以便在CPU上运行或按需加载。尽管一些研究已经利用了专家激活的局部性（即连续的token激活相似的专家），但这种**局部路由一致性**的程度在不同模型之间差异很大，并且尚未得到充分研究。本文提出了两个评估MoE模型局部路由一致性的指标：（1）**段路由最佳性能（SRP）**，用于评估一组固定专家在多大程度上能够满足一段token的需求；（2）**段缓存最佳命中率（SCH）**，用于衡量在给定缓存大小限制下的最优段级别缓存命中率。我们分析了20个不同大小和架构的MoE LLMs，发现每一层都应用MoE且不使用共享专家的模型表现出最高的局部路由一致性。我们进一步表明，领域专业化专家比词汇专业化专家对路由一致性的贡献更大，并且大多数模型可以在缓存大小约为活动专家数量的2倍时平衡缓存的有效性和效率。这些发现为在不牺牲推理速度的情况下设计和部署内存高效的MoE模型奠定了基础。我们在https://github.com/ljcleo/moe-lrc 上发布了用于复现实验的代码。",
    "keywords": [
      "Mixture-of-Experts",
      "Expert Offloading",
      "Local Routing Consistency",
      "LLMs",
      "Cache"
    ],
    "area": [
      "大模型",
      "自然语言处理",
      "深度学习"
    ],
    "content": "混合专家模型（MoE）通过在推理过程中稀疏激活专家，实现了大型语言模型（LLMs）的有效扩展。为了在内存受限的设备上有效部署大型MoE模型，许多系统引入了*专家卸载*机制，将一部分专家缓存在高速内存中，而将其他专家留在慢速内存中以便在CPU上运行或按需加载。尽管一些研究已经利用了专家激活的局部性（即连续的token激活相似的专家），但这种**局部路由一致性**的程度在不同模型之间差异很大，并且尚未得到充分研究。本文提出了两个评估MoE模型局部路由一致性的指标：（1）**段路由最佳性能（SRP）**，用于评估一组固定专家在多大程度上能够满足一段token的需求；（2）**段缓存最佳命中率（SCH）**，用于衡量在给定缓存大小限制下的最优段级别缓存命中率。我们分析了20个不同大小和架构的MoE LLMs，发现每一层都应用MoE且不使用共享专家的模型表现出最高的局部路由一致性。我们进一步表明，领域专业化专家比词汇专业化专家对路由一致性的贡献更大，并且大多数模型可以在缓存大小约为活动专家数量的2倍时平衡缓存的有效性和效率。这些发现为在不牺牲推理速度的情况下设计和部署内存高效的MoE模型奠定了基础。我们在https://github.com/ljcleo/moe-lrc 上发布了用于复现实验的代码。",
    "published_time": "2025-05-21T22:13:09.000Z",
    "download_time": "2025-05-26 03:59:55",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16056.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.16056",
      "arxiv_url": "https://arxiv.org/abs/2505.16056"
    }
  },
  {
    "id": "2505.11881",
    "source": "huggingface",
    "url": "https://huggingface.co/papers/2505.11881",
    "title": "重新审视残差连接：用于稳定高效深度网络的正交更新",
    "summary": "残差连接是深度神经网络的关键组成部分，通过缓解梯度消失问题使其能够构建更深的网络。然而，在标准的残差更新中，模块的输出直接添加到输入流中。这可能导致更新主要加强或调节现有流的方向，从而可能未能充分利用模块学习全新特征的能力。在这项工作中，我们引入了正交残差更新：我们将模块的输出相对于输入流进行分解，并仅添加与该流正交的分量。这种设计旨在引导模块主要贡献新的表示方向，从而促进更丰富的特征学习，同时提升训练效率。我们证明了我们的正交更新策略在多种架构（ResNetV2、Vision Transformers）和数据集（CIFARs、TinyImageNet、ImageNet-1k）上提高了泛化精度和训练稳定性，例如在 ImageNet-1k 数据集上使 ViT-B 模型取得了 +4.3%p 的 Top-1 精度提升。",
    "keywords": [
      "残差连接",
      "正交更新",
      "深度学习",
      "特征学习",
      "训练稳定性"
    ],
    "area": [
      "机器学习",
      "深度学习",
      "计算机视觉"
    ],
    "content": "残差连接是深度神经网络的关键组成部分，通过缓解梯度消失问题使其能够构建更深的网络。然而，在标准的残差更新中，模块的输出直接添加到输入流中。这可能导致更新主要加强或调节现有流的方向，从而可能未能充分利用模块学习全新特征的能力。在这项工作中，我们引入了正交残差更新：我们将模块的输出相对于输入流进行分解，并仅添加与该流正交的分量。这种设计旨在引导模块主要贡献新的表示方向，从而促进更丰富的特征学习，同时提升训练效率。我们证明了我们的正交更新策略在多种架构（ResNetV2、Vision Transformers）和数据集（CIFARs、TinyImageNet、ImageNet-1k）上提高了泛化精度和训练稳定性，例如在 ImageNet-1k 数据集上使 ViT-B 模型取得了 +4.3%p 的 Top-1 精度提升。",
    "published_time": "2025-05-17T07:16:11.000Z",
    "download_time": "2025-05-26 04:00:08",
    "visual_resource": [
      "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11881.png"
    ],
    "extra": {
      "url": "https://huggingface.co/papers/2505.11881",
      "arxiv_url": "https://arxiv.org/abs/2505.11881"
    }
  }
]